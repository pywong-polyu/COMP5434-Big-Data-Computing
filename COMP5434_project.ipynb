{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d429ccc6",
      "metadata": {
        "id": "d429ccc6"
      },
      "source": [
        "# About the Dataset\n",
        "\n",
        "The original CORD-19 is a resource of over 1,000,000 scholarly articles, including over 400,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.\n",
        "\n",
        "In our project, the dataset is sampled from the CORD-19 with size ~10,000 to reduce computation burden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UFYXtLsHeNX0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFYXtLsHeNX0",
        "outputId": "08825601-cf94-481a-e6b6-336a0132a717"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "\n",
        "# shared link: https://drive.google.com/drive/folders/1Td_ZTUVrsKeftDE5Zll7252YLJdWiNTk?usp=share_link \n",
        "# you can download the data via the shared link, and skip Step 0 and Step 1 if you want to run the code in your local machine \n",
        "\n",
        "\n",
        "# Step 0: add the shared folder to your google drive. e.g., /content/drive/MyDrive/CORD_19\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "!echo $PWD\n",
        "\n",
        "!ls /content/drive/MyDrive/CORD_19/\n",
        "\n",
        "# Step 2: unzip json files \n",
        "subset_dir = os.path.join(os.getcwd(),  \"CORD_19_subset\")\n",
        "\n",
        "\n",
        "zip_file_path=\"/content/drive/MyDrive/CORD_19/subset.zip\"\n",
        "\n",
        "# Check if the destination directory exists\n",
        "if not os.path.exists(subset_dir):\n",
        "    # Unzip the file\n",
        "    cmd = \"unzip {} -d {}\".format(zip_file_path, subset_dir)\n",
        "    proc = subprocess.Popen(cmd, shell=True)\n",
        "else:\n",
        "    print(f\"Directory {subset_dir} already exists. Skipping extraction.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eeabd1eb",
      "metadata": {
        "id": "eeabd1eb"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd6edbde",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1250a1",
      "metadata": {
        "id": "cc1250a1"
      },
      "source": [
        "# Data Load & Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d2222c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "83d2222c",
        "outputId": "d3082464-4669-4404-b139-0300e50112d4"
      },
      "outputs": [],
      "source": [
        "# Load Meta data from meta_10k.csv\n",
        "data_root = os.path.join(os.getcwd(),'input')\n",
        "\n",
        "metadata_path = os.path.join(data_root, 'meta_10k.csv')\n",
        "meta_df = pd.read_csv(metadata_path, index_col=0,converters={\n",
        "    'pubmed_id': str,\n",
        "    'Microsoft Academic Paper ID': str,\n",
        "    'doi': str\n",
        "})\n",
        "\n",
        "print(len(meta_df))\n",
        "meta_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1b611f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1b611f0",
        "outputId": "30b26c96-07b0-4490-e5c8-b09a73d75945"
      },
      "outputs": [],
      "source": [
        "# Display the info of the dataframe\n",
        "meta_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3139be0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3139be0",
        "outputId": "5cdaf76c-167c-4a56-b786-2838a18700f1"
      },
      "outputs": [],
      "source": [
        "def glob_files(path, f_type=\".json\"):\n",
        "    dst = []\n",
        "    for root, _, files in os.walk(path):\n",
        "        for f in files:\n",
        "            if f.endswith(f_type):\n",
        "                dst.append(os.path.join(root, f))\n",
        "    return dst\n",
        "\n",
        "# glob json files\n",
        "json_dir = os.path.join(data_root, \"subset\",\"subset\",\"document_parses\",\"pdf_json\")\n",
        "print(json_dir)\n",
        "json_files = glob_files(json_dir, \".json\")\n",
        "\n",
        "print(\"total json files:\", len(json_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20fb57d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20fb57d2",
        "outputId": "03c330eb-ba5a-4a05-81f6-420b007e3d43"
      },
      "outputs": [],
      "source": [
        "# Class to read JSON files\n",
        "class FileReader:\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path) as file:\n",
        "            content = json.load(file)\n",
        "            self.paper_id = content['paper_id']\n",
        "            self.abstract = []\n",
        "            self.body_text = []\n",
        "            # Abstract\n",
        "            for entry in content['abstract']:\n",
        "                self.abstract.append(entry['text'])\n",
        "            # Body text\n",
        "            for entry in content['body_text']:\n",
        "                self.body_text.append(entry['text'])\n",
        "            self.abstract = '\\n'.join(self.abstract)\n",
        "            self.body_text = '\\n'.join(self.body_text)\n",
        "            self.title = content['metadata']['title']\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.paper_id}: {self.title } : {self.abstract[:200]}... {self.body_text[:200]}...\"\n",
        "\n",
        "# Read the first row\n",
        "first_row = FileReader(json_files[0])\n",
        "print(first_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a78cce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "96a78cce",
        "outputId": "db3e15b8-5496-4752-901a-a0e8a39f3628"
      },
      "outputs": [],
      "source": [
        "# Function to add breaks to text\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_breaks(content, length):\n",
        "    data = \"\"\n",
        "    words = content.split(' ')\n",
        "    total_chars = 0\n",
        "\n",
        "    # Add break every length characters\n",
        "    for i in range(len(words)):\n",
        "        total_chars += len(words[i])\n",
        "        if total_chars > length:\n",
        "            data = data + \"<br>\" + words[i]\n",
        "            total_chars = 0\n",
        "        else:\n",
        "            data = data + \" \" + words[i]\n",
        "    return data\n",
        "\n",
        "# Dictionary to hold data\n",
        "dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [],\n",
        "         'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
        "\n",
        "# Iterate through json files and extract content\n",
        "for idx, entry in tqdm(enumerate(json_files), total=len(json_files)):\n",
        "    try:\n",
        "        content = FileReader(entry)\n",
        "    except Exception as e:\n",
        "        continue  # Invalid paper format, skip\n",
        "\n",
        "    # Get metadata information\n",
        "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
        "    # No metadata, skip this paper\n",
        "    if len(meta_data) == 0:\n",
        "        continue\n",
        "    if len(content.body_text) == 0:\n",
        "        continue\n",
        "    dict_['abstract'].append(content.abstract)\n",
        "    dict_['paper_id'].append(content.paper_id)\n",
        "    dict_['body_text'].append(content.body_text)\n",
        "    # Also create a column for the summary of abstract to be used in a plot\n",
        "    if len(content.abstract) == 0:\n",
        "        # No abstract provided\n",
        "        dict_['abstract_summary'].append(\"Not provided.\")\n",
        "    elif len(content.abstract.split(' ')) > 100:\n",
        "        # Abstract provided is too long for plot, take first 100 words append with ...\n",
        "        info = content.abstract.split(' ')[:100]\n",
        "        summary = get_breaks(' '.join(info), 40)\n",
        "        dict_['abstract_summary'].append(summary + \"...\")\n",
        "    else:\n",
        "        # Abstract is short enough\n",
        "        summary = get_breaks(content.abstract, 40)\n",
        "        dict_['abstract_summary'].append(summary)\n",
        "\n",
        "    # Get metadata information\n",
        "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
        "\n",
        "    try:\n",
        "        # If more than one author\n",
        "        authors = meta_data['authors'].values[0].split(';')\n",
        "        if len(authors) > 2:\n",
        "            # More than 2 authors, may be problem when plotting, so take first 2 append with ...\n",
        "            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n",
        "        else:\n",
        "            # Authors will fit in plot\n",
        "            dict_['authors'].append(\". \".join(authors))\n",
        "    except Exception as e:\n",
        "        # If only one author - or Null value\n",
        "        dict_['authors'].append(meta_data['authors'].values[0])\n",
        "\n",
        "    # Add the title information, add breaks when needed\n",
        "    try:\n",
        "        title = get_breaks(meta_data['title'].values[0], 40)\n",
        "        dict_['title'].append(title)\n",
        "    # If title was not provided\n",
        "    except Exception as e:\n",
        "        dict_['title'].append(meta_data['title'].values[0])\n",
        "\n",
        "    # Add the journal information\n",
        "    dict_['journal'].append(meta_data['journal'].values[0])\n",
        "\n",
        "    # Add doi\n",
        "    dict_['doi'].append(meta_data['doi'].values[0])\n",
        "\n",
        "# Create a DataFrame\n",
        "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text',\n",
        "                                        'authors', 'title', 'journal', 'abstract_summary'])\n",
        "print(df_covid.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2974a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c2974a3",
        "outputId": "9bc51e71-1d4d-4404-cb1d-b71b658b9349"
      },
      "outputs": [],
      "source": [
        "# Display the info of the dataframe\n",
        "df_covid.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f9af672",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f9af672",
        "outputId": "eceec098-206e-4fe8-d85b-f848a93e4720"
      },
      "outputs": [],
      "source": [
        "# Drop NaN values\n",
        "df = df_covid\n",
        "df.dropna(inplace=True)\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c54f8fda",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text cleaning function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply text cleaning to abstract and body_text\n",
        "df['abstract'] = df['abstract'].apply(clean_text)\n",
        "df['body_text'] = df['body_text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d3b3b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89d3b3b8",
        "outputId": "20fe501a-4c47-4f93-8e8b-b441089f2862"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2eacd72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2eacd72",
        "outputId": "0b8d3e3f-4841-4e5f-fd23-55297ac27fb6"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from langdetect import detect\n",
        "from langdetect import DetectorFactory\n",
        "\n",
        "# set seed\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# hold label - language\n",
        "languages = []\n",
        "\n",
        "# go through each text\n",
        "for ii in tqdm(range(0,len(df))):\n",
        "    # split by space into list, take the first x intex, join with space\n",
        "    text = df.iloc[ii]['body_text'].split(\" \")\n",
        "\n",
        "    lang = \"en\"\n",
        "    try:\n",
        "        if len(text) > 50:\n",
        "            lang = detect(\" \".join(text[:50]))\n",
        "        elif len(text) > 0:\n",
        "            lang = detect(\" \".join(text[:len(text)]))\n",
        "    # ught... beginning of the document was not in a good format\n",
        "    except Exception as e:\n",
        "        all_words = set(text)\n",
        "        try:\n",
        "            lang = detect(\" \".join(all_words))\n",
        "        # what!! :( let's see if we can find any text in abstract...\n",
        "        except Exception as e:\n",
        "\n",
        "            try:\n",
        "                # let's try to label it through the abstract then\n",
        "                lang = detect(df.iloc[ii]['abstract_summary'])\n",
        "            except Exception as e:\n",
        "                lang = \"unknown\"\n",
        "                pass\n",
        "\n",
        "    # get the language\n",
        "    languages.append(lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7396f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e7396f0",
        "outputId": "d54ed418-c5dd-4cb3-b9ab-2ed07790843a"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "languages_dict = {}\n",
        "for lang in set(languages):\n",
        "    languages_dict[lang] = languages.count(lang)\n",
        "\n",
        "print(\"Total: {}\\n\".format(len(languages)))\n",
        "pprint(languages_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e667fa35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e667fa35",
        "outputId": "38d0a17b-79c7-42de-be9b-e02922c23f11"
      },
      "outputs": [],
      "source": [
        "df['language'] = languages\n",
        "df = df[df['language'] == 'en']\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4bf462",
      "metadata": {},
      "source": [
        "# Histogram of year / journal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b707bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert publish_time to datetime\n",
        "meta_df['publish_time'] = pd.to_datetime(meta_df['publish_time'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaT values in publish_time\n",
        "meta_df = meta_df.dropna(subset=['publish_time'])\n",
        "\n",
        "# Plot histogram of publication years\n",
        "plt.figure(figsize=(12, 6))\n",
        "meta_df['publish_time'].dt.year.value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Histogram of Publication Years')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Papers')\n",
        "plt.show()\n",
        "\n",
        "# Plot histogram of journals\n",
        "plt.figure(figsize=(12, 6))\n",
        "df['journal'].value_counts().head(20).plot(kind='bar')\n",
        "plt.title('Top 20 Journals by Number of Papers')\n",
        "plt.xlabel('Journal')\n",
        "plt.ylabel('Number of Papers')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Filter years with too many or too few data points\n",
        "min_threshold = 10  # Minimum number of papers for a year to be considered\n",
        "max_threshold = 1000  # Maximum number of papers for a year to be considered\n",
        "year_counts = meta_df['publish_time'].dt.year.value_counts()\n",
        "filtered_years = year_counts[(year_counts >= min_threshold) & (year_counts <= max_threshold)].index\n",
        "\n",
        "# Filter the dataframe\n",
        "filtered_meta_df = meta_df[meta_df['publish_time'].dt.year.isin(filtered_years)]\n",
        "\n",
        "# Plot filtered histogram of publication years\n",
        "plt.figure(figsize=(12, 6))\n",
        "filtered_meta_df['publish_time'].dt.year.value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Filtered Histogram of Publication Years')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Papers')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cae116c4",
      "metadata": {},
      "source": [
        "# Map-Reduce "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d54ec16",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ac547210",
      "metadata": {},
      "source": [
        "# Association Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b12c03",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "48ec6f2e",
      "metadata": {},
      "source": [
        "# Similarity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4823d5b4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "adc2e25a",
      "metadata": {},
      "source": [
        "# Clustering Analysis"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

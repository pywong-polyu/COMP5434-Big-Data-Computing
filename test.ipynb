{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to test how to use the function in library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from association_mining import *\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join(os.getcwd(),'input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta data size: 9022\n",
      "c:\\Users\\pywong\\Desktop\\PolyU\\COMP5434 Big Data Computing\\Assignment\\COMP5434-Big-Data-Computing\\input\\subset\\subset\\document_parses\\pdf_json\n",
      "total json files: 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [00:53<00:00, 225.63it/s]\n",
      "100%|██████████| 8083/8083 [00:39<00:00, 207.11it/s]\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pywong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "df = utils.load_data_from_local(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 2**12\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=feature_size)\n",
    "X_count = count_vectorizer.fit_transform(df['processed_text'].values).toarray().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample script to use the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "\n",
    "def get_target_document_index(X,token_list,target_word_list,model_name='glove-wiki-gigaword-100',top_feature=20,top_document=100):\n",
    "    \n",
    "    '''\n",
    "    Compare each target word to all token used as feature and find the top related document index.\n",
    "    Using gensim model for semantic analysis by default using glove-wiki-gigaword-100.\n",
    "    Return a dictionary of most related documents index for each target word.\n",
    "    \n",
    "    Argument:\n",
    "    \n",
    "    - X:\n",
    "    A numpy array of characteristic matrix with row as number of feature and column as number of document. \n",
    "    Recommend to use the token count as feature.\n",
    "    \n",
    "    - token_list:\n",
    "    A list of string that use to build the feature of the characteristic matrix.\n",
    "    \n",
    "    - target_word_list:\n",
    "    A list of string that all token compare with.\n",
    "    Each target word can be any string that is not in the token_list but must be in the model.\n",
    "    \n",
    "    - model_name:\n",
    "    A string of model name that use to give a similarity of score between the target worad and the token used.\n",
    "    Must be a valid model name that can be called by using Gensim API.\n",
    "    \n",
    "    - top_feature:\n",
    "    An integer number of most related features compare with the target word.\n",
    "    \n",
    "    - top_document:\n",
    "    An integer number of most related documents compare with the target word.\n",
    "    '''\n",
    "    \n",
    "    print(f'Using Model: {model_name}\\n')\n",
    "    \n",
    "    model = api.load(model_name)\n",
    "    \n",
    "    # Check all target words are in the model\n",
    "    for target_word in target_word_list:\n",
    "        if target_word not in model:\n",
    "            ValueError(f'Argument: target_word is not in model {model_name}')\n",
    "            \n",
    "    # Check input top_feature and token_list size\n",
    "    if len(token_list) <= top_feature:\n",
    "        print(f'Warning: Argument top_feature ({top_feature}) is greater than or equal to token_list size ({len(token_list)})')\n",
    "        print('No feature filtering will be performed.\\n')\n",
    "        \n",
    "    # Check the proportion of token that is not in model.\n",
    "    token_not_exist = 0\n",
    "    for i in token_list:\n",
    "        if i not in model:\n",
    "            token_not_exist += 1\n",
    "            \n",
    "    print(f'Token not in model: {token_not_exist}/{len(token_list)} ({round(token_not_exist/(len(token_list))*100,1)}%)')\n",
    "    \n",
    "\n",
    "    # Create a result dictionary for all target words\n",
    "    \n",
    "    result_dict = {}\n",
    "    for i in range(0,len(target_word_list)):\n",
    "        \n",
    "        target_word = target_word_list[i]\n",
    "        print(f'Comparing target word {i}/{len(target_word_list)}: {target_word}')\n",
    "        \n",
    "        token_score_df = get_target_word_similarity(model,token_list,target_word)\n",
    "        doc_to_keep = get_most_similar_document(X,token_score_df,token_list,top_feature=top_feature,top_document=top_document)\n",
    "        result_dict[target_word] = doc_to_keep\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "        \n",
    "def get_target_word_similarity(model,token_list,target_word):\n",
    "    '''\n",
    "    Return the similarity score for all tokens used as feature compare with the target word.\n",
    "    Using cosine similarity by default.\n",
    "    '''\n",
    "    \n",
    "    target_word_feature = model[target_word]\n",
    "    target_word_norm = np.linalg.norm(target_word_feature)\n",
    "    \n",
    "    # List of tokne used to build feature\n",
    "    # token_list = list(vectorizer.get_feature_names_out())\n",
    "    \n",
    "    token_score_list = []\n",
    "    \n",
    "    # token_exist = 0\n",
    "    # token_not_exist = 0\n",
    "    \n",
    "    # Use cosine similarity between the target word and the token using feature created from gensim model.\n",
    "    for token in token_list:\n",
    "        if token in model:\n",
    "            token_norm = np.linalg.norm(model[token])\n",
    "            score = (model[token] @ target_word_feature.T)/(token_norm*target_word_norm)\n",
    "            # token_exist += 1\n",
    "        else:\n",
    "            score = None\n",
    "            # token_not_exist += 1\n",
    "            \n",
    "        token_score_list.append(score)\n",
    "\n",
    "    # print(f'Number of token exists in model:{token_exist}/{token_exist+token_not_exist} ({round(token_exist/(token_exist+token_not_exist)*100,1)}%)')\n",
    "\n",
    "    df = pd.DataFrame({'token':token_list,'score':token_score_list})\n",
    "    df = df.sort_values(by=['score'],ascending=False)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def get_most_similar_document(X,token_score_df,token_list,top_feature=20,top_document=100):\n",
    "    \n",
    "    '''\n",
    "    - X:\n",
    "    A 2D numpy array characteristic matrix with number of token as row, number of document as column.\n",
    "    Prefer using count of token occurrence as feature.\n",
    "    \n",
    "    - token_score_df:\n",
    "    A pandas dataframe with the similarity score for each token compare with the target word.\n",
    "    \n",
    "    - top_feature:\n",
    "    An integer number of most related features compare with the target word.\n",
    "    \n",
    "    - top_document:\n",
    "    An integer number of most related documents compare with the target word.\n",
    "    \n",
    "    Return a list of document number with zero based index.\n",
    "    The list is truncated by the size of top_document.\n",
    "    The document index is in the descending order of similarity score followed by the document index in the characteristic matrix.\n",
    "    '''\n",
    "    \n",
    "    keep_feature = list(token_score_df.head(top_feature)['token'])\n",
    "    \n",
    "    # Loop through all token in characteristic matrix to find the feature to keep\n",
    "    token_index_list = []\n",
    "    for i in range(0,len(token_list)):\n",
    "        if token_list[i] in keep_feature:\n",
    "            token_index_list.append(i)\n",
    "            \n",
    "    # Filter to only contain the most related features\n",
    "    X_modify = X[token_index_list,:]\n",
    "    \n",
    "    # Sum all remaining feature values to create a score for each document\n",
    "    doc_score = np.sum(X_modify,axis=0)\n",
    "    doc_rank = ss.rankdata(doc_score,method='min')\n",
    "    \n",
    "    distinct_rank = list(set(doc_rank))\n",
    "    distinct_rank.sort()\n",
    "    \n",
    "    doc_to_keep = []\n",
    "\n",
    "    # When multiple documents have the same rank, it may exceed the number of top document limit.\n",
    "    # Put the document with the lowest rank into the list first.\n",
    "    # Cap the list size with the number of top document limit.\n",
    "    for j in distinct_rank:\n",
    "\n",
    "        for i in range(0,len(doc_rank)):\n",
    "            rank = doc_rank[i]\n",
    "\n",
    "            if rank == j and len(doc_to_keep) < top_document:\n",
    "                doc_to_keep.append(i)\n",
    "                if len(doc_to_keep) >= top_document:\n",
    "                    break\n",
    "\n",
    "    return doc_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_list = ['vaccination','symptom','critical','infection','bitcoin'] # not in feature but in model\n",
    "token_list = list(count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: glove-wiki-gigaword-100\n",
      "\n",
      "Token not in model: 28/4096 (0.7%)\n",
      "Comparing target word 0/5: vaccination\n",
      "Comparing target word 1/5: symptom\n",
      "Comparing target word 2/5: critical\n",
      "Comparing target word 3/5: infection\n",
      "Comparing target word 4/5: bitcoin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vaccination': [2,\n",
       "  11,\n",
       "  16,\n",
       "  42,\n",
       "  72,\n",
       "  92,\n",
       "  97,\n",
       "  110,\n",
       "  127,\n",
       "  139,\n",
       "  146,\n",
       "  149,\n",
       "  152,\n",
       "  153,\n",
       "  175,\n",
       "  185,\n",
       "  194,\n",
       "  229,\n",
       "  236,\n",
       "  248,\n",
       "  262,\n",
       "  264,\n",
       "  272,\n",
       "  277,\n",
       "  279,\n",
       "  289,\n",
       "  306,\n",
       "  307,\n",
       "  322,\n",
       "  338,\n",
       "  354,\n",
       "  361,\n",
       "  376,\n",
       "  381,\n",
       "  390,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  434,\n",
       "  441,\n",
       "  456,\n",
       "  468,\n",
       "  473,\n",
       "  478,\n",
       "  481,\n",
       "  483,\n",
       "  492,\n",
       "  495,\n",
       "  499,\n",
       "  506,\n",
       "  511,\n",
       "  521,\n",
       "  525,\n",
       "  530,\n",
       "  544,\n",
       "  559,\n",
       "  560,\n",
       "  562,\n",
       "  566,\n",
       "  582,\n",
       "  583,\n",
       "  585,\n",
       "  605,\n",
       "  609,\n",
       "  613,\n",
       "  617,\n",
       "  636,\n",
       "  650,\n",
       "  651,\n",
       "  659,\n",
       "  665,\n",
       "  675,\n",
       "  691,\n",
       "  715,\n",
       "  733,\n",
       "  745,\n",
       "  760,\n",
       "  761,\n",
       "  765,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  778,\n",
       "  788,\n",
       "  799,\n",
       "  801,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  818,\n",
       "  831,\n",
       "  842,\n",
       "  843,\n",
       "  846,\n",
       "  847,\n",
       "  877,\n",
       "  894,\n",
       "  902,\n",
       "  903,\n",
       "  906],\n",
       " 'symptom': [8,\n",
       "  11,\n",
       "  47,\n",
       "  49,\n",
       "  72,\n",
       "  91,\n",
       "  110,\n",
       "  111,\n",
       "  117,\n",
       "  143,\n",
       "  151,\n",
       "  174,\n",
       "  185,\n",
       "  194,\n",
       "  203,\n",
       "  206,\n",
       "  212,\n",
       "  215,\n",
       "  229,\n",
       "  243,\n",
       "  246,\n",
       "  252,\n",
       "  272,\n",
       "  279,\n",
       "  289,\n",
       "  290,\n",
       "  297,\n",
       "  306,\n",
       "  316,\n",
       "  324,\n",
       "  334,\n",
       "  338,\n",
       "  344,\n",
       "  354,\n",
       "  360,\n",
       "  361,\n",
       "  376,\n",
       "  377,\n",
       "  381,\n",
       "  387,\n",
       "  394,\n",
       "  411,\n",
       "  432,\n",
       "  475,\n",
       "  477,\n",
       "  479,\n",
       "  483,\n",
       "  495,\n",
       "  500,\n",
       "  514,\n",
       "  519,\n",
       "  526,\n",
       "  560,\n",
       "  562,\n",
       "  566,\n",
       "  574,\n",
       "  582,\n",
       "  583,\n",
       "  602,\n",
       "  608,\n",
       "  617,\n",
       "  620,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  694,\n",
       "  705,\n",
       "  715,\n",
       "  716,\n",
       "  718,\n",
       "  748,\n",
       "  774,\n",
       "  778,\n",
       "  788,\n",
       "  801,\n",
       "  803,\n",
       "  809,\n",
       "  821,\n",
       "  831,\n",
       "  836,\n",
       "  846,\n",
       "  852,\n",
       "  861,\n",
       "  863,\n",
       "  871,\n",
       "  899,\n",
       "  902,\n",
       "  903,\n",
       "  906,\n",
       "  914,\n",
       "  928,\n",
       "  951,\n",
       "  973,\n",
       "  977,\n",
       "  980,\n",
       "  995,\n",
       "  1010,\n",
       "  1011,\n",
       "  1032,\n",
       "  1059],\n",
       " 'critical': [317,\n",
       "  583,\n",
       "  652,\n",
       "  1294,\n",
       "  1389,\n",
       "  1507,\n",
       "  1715,\n",
       "  2009,\n",
       "  2115,\n",
       "  2255,\n",
       "  2415,\n",
       "  2748,\n",
       "  2779,\n",
       "  2854,\n",
       "  3312,\n",
       "  3406,\n",
       "  3450,\n",
       "  4263,\n",
       "  4398,\n",
       "  4485,\n",
       "  4652,\n",
       "  4720,\n",
       "  4780,\n",
       "  5092,\n",
       "  5337,\n",
       "  5555,\n",
       "  5639,\n",
       "  5688,\n",
       "  5926,\n",
       "  6003,\n",
       "  6245,\n",
       "  6277,\n",
       "  6429,\n",
       "  6435,\n",
       "  6446,\n",
       "  6965,\n",
       "  7164,\n",
       "  7228,\n",
       "  7265,\n",
       "  7373,\n",
       "  7434,\n",
       "  7563,\n",
       "  7620,\n",
       "  40,\n",
       "  110,\n",
       "  137,\n",
       "  367,\n",
       "  489,\n",
       "  492,\n",
       "  547,\n",
       "  611,\n",
       "  612,\n",
       "  919,\n",
       "  1135,\n",
       "  1239,\n",
       "  1290,\n",
       "  1327,\n",
       "  1465,\n",
       "  1475,\n",
       "  1553,\n",
       "  1610,\n",
       "  2135,\n",
       "  2196,\n",
       "  2347,\n",
       "  2361,\n",
       "  2362,\n",
       "  2759,\n",
       "  2972,\n",
       "  3064,\n",
       "  3359,\n",
       "  3415,\n",
       "  3521,\n",
       "  3561,\n",
       "  3756,\n",
       "  3802,\n",
       "  3910,\n",
       "  4172,\n",
       "  4218,\n",
       "  4356,\n",
       "  4597,\n",
       "  4754,\n",
       "  4778,\n",
       "  4800,\n",
       "  4884,\n",
       "  4922,\n",
       "  4970,\n",
       "  5231,\n",
       "  5480,\n",
       "  5499,\n",
       "  5509,\n",
       "  5536,\n",
       "  5636,\n",
       "  5723,\n",
       "  5798,\n",
       "  5853,\n",
       "  6116,\n",
       "  6698,\n",
       "  6894,\n",
       "  6912,\n",
       "  6986],\n",
       " 'infection': [8,\n",
       "  11,\n",
       "  35,\n",
       "  47,\n",
       "  49,\n",
       "  72,\n",
       "  76,\n",
       "  91,\n",
       "  111,\n",
       "  117,\n",
       "  143,\n",
       "  151,\n",
       "  185,\n",
       "  198,\n",
       "  206,\n",
       "  212,\n",
       "  215,\n",
       "  229,\n",
       "  258,\n",
       "  289,\n",
       "  290,\n",
       "  294,\n",
       "  297,\n",
       "  306,\n",
       "  324,\n",
       "  338,\n",
       "  360,\n",
       "  361,\n",
       "  375,\n",
       "  376,\n",
       "  390,\n",
       "  442,\n",
       "  475,\n",
       "  477,\n",
       "  483,\n",
       "  519,\n",
       "  560,\n",
       "  562,\n",
       "  566,\n",
       "  574,\n",
       "  582,\n",
       "  583,\n",
       "  585,\n",
       "  602,\n",
       "  607,\n",
       "  608,\n",
       "  620,\n",
       "  650,\n",
       "  651,\n",
       "  660,\n",
       "  664,\n",
       "  705,\n",
       "  715,\n",
       "  716,\n",
       "  718,\n",
       "  748,\n",
       "  774,\n",
       "  778,\n",
       "  788,\n",
       "  801,\n",
       "  803,\n",
       "  809,\n",
       "  821,\n",
       "  831,\n",
       "  852,\n",
       "  871,\n",
       "  903,\n",
       "  906,\n",
       "  914,\n",
       "  928,\n",
       "  933,\n",
       "  944,\n",
       "  973,\n",
       "  977,\n",
       "  1010,\n",
       "  1011,\n",
       "  1021,\n",
       "  1032,\n",
       "  1056,\n",
       "  1059,\n",
       "  1069,\n",
       "  1082,\n",
       "  1095,\n",
       "  1152,\n",
       "  1155,\n",
       "  1160,\n",
       "  1189,\n",
       "  1202,\n",
       "  1221,\n",
       "  1227,\n",
       "  1233,\n",
       "  1241,\n",
       "  1268,\n",
       "  1279,\n",
       "  1284,\n",
       "  1285,\n",
       "  1298,\n",
       "  1319,\n",
       "  1350,\n",
       "  1351],\n",
       " 'bitcoin': [1,\n",
       "  4,\n",
       "  5,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  13,\n",
       "  15,\n",
       "  18,\n",
       "  19,\n",
       "  30,\n",
       "  31,\n",
       "  37,\n",
       "  38,\n",
       "  40,\n",
       "  44,\n",
       "  47,\n",
       "  49,\n",
       "  50,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  64,\n",
       "  68,\n",
       "  69,\n",
       "  72,\n",
       "  75,\n",
       "  76,\n",
       "  81,\n",
       "  83,\n",
       "  84,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  107,\n",
       "  108,\n",
       "  110,\n",
       "  115,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  126,\n",
       "  127,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  137,\n",
       "  140,\n",
       "  142,\n",
       "  143,\n",
       "  149,\n",
       "  151,\n",
       "  153,\n",
       "  155,\n",
       "  160,\n",
       "  161,\n",
       "  163,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  174,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  181,\n",
       "  185,\n",
       "  186,\n",
       "  188,\n",
       "  189,\n",
       "  192,\n",
       "  193,\n",
       "  195,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  204,\n",
       "  207,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  222,\n",
       "  224,\n",
       "  226,\n",
       "  229,\n",
       "  231,\n",
       "  236,\n",
       "  238]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dict = get_target_document_index(X_count,token_list,target_word_list)\n",
    "index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 13,\n",
       " 15,\n",
       " 18,\n",
       " 19,\n",
       " 30,\n",
       " 31,\n",
       " 37,\n",
       " 38,\n",
       " 40,\n",
       " 44,\n",
       " 47,\n",
       " 49,\n",
       " 50,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 64,\n",
       " 68,\n",
       " 69,\n",
       " 72,\n",
       " 75,\n",
       " 76,\n",
       " 81,\n",
       " 83,\n",
       " 84,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 107,\n",
       " 108,\n",
       " 110,\n",
       " 115,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 126,\n",
       " 127,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 137,\n",
       " 140,\n",
       " 142,\n",
       " 143,\n",
       " 149,\n",
       " 151,\n",
       " 153,\n",
       " 155,\n",
       " 160,\n",
       " 161,\n",
       " 163,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 174,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 181,\n",
       " 185,\n",
       " 186,\n",
       " 188,\n",
       " 189,\n",
       " 192,\n",
       " 193,\n",
       " 195,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 204,\n",
       " 207,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 222,\n",
       " 224,\n",
       " 226,\n",
       " 229,\n",
       " 231,\n",
       " 236,\n",
       " 238]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dict['bitcoin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

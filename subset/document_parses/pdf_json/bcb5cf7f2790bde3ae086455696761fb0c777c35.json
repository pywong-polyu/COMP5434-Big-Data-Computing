{
    "paper_id": "bcb5cf7f2790bde3ae086455696761fb0c777c35",
    "metadata": {
        "title": "FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection",
        "authors": [
            {
                "first": "Tuan",
                "middle": [],
                "last": "Truong",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Bayer AG",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Matthias",
                "middle": [],
                "last": "Lenga",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Bayer AG",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Antoine",
                "middle": [],
                "last": "Serrurier",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Uniklinik RWTH Aachen",
                    "location": {
                        "settlement": "Aachen",
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Sadegh",
                "middle": [],
                "last": "Mohammadi",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Bayer AG",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Audio-based classification techniques on body sounds have long been studied to support diagnostic decisions, particularly in pulmonary diseases. In response to the urgency of the COVID-19 pandemic, a growing number of models are developed to identify COVID-19 patients based on acoustic input. Most models focus on cough because the dry cough is the best-known symptom of COVID-19.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "However, other body sounds, such as breath and speech, have also been revealed to correlate with COVID-19 as well. In this work, rather than relying on a specific body sound, we propose Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov). It relies on constructing a joint feature vector obtained from a plurality of body sounds in waveform and spectrogram representation. The core component of FAIR4Cov is a self-attention fusion unit that is trained to establish the relation of multiple body sounds and audio representations and integrate it into a compact feature vector. We set up our experiments on different combinations of body sounds using only waveform, spectrogram, and a joint representation of waveform and spectrogram. Our findings show that the use of self-attention to combine extracted features from cough, breath, and speech sounds leads to the best performance with an Area Under the Receiver Operating Characteristic Curve (AUC) score of 0.8658, a sensitivity of 0.8057, and a specificity of 0.7958. This AUC is 0.0227 higher than the one of the models trained on spectrograms only and 0.0847 higher than the one of the models trained on waveforms only. The results demonstrate that the combination of spectrogram with waveform representation helps to enrich the extracted features and outperforms the models with single representation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Our body produces innumerable sounds every day, but most of the time we do not pay enough attention to them. Body sounds are known to reveal an individual state of health. A slight change in the physical state can modify the responsible organ and consequently produce irregular sound patterns. For example, snoring is a common sound produced when certain parts of the body, such as the tongue or pharynx, block the airflow as we breathe during sleep.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Snoring alone is in general not considered as symptomatic, but if it is coupled with breathing pauses, it can be a symptom of obstructive sleep apnea. More generally, body sounds can be used extensively to support diagnostic decisions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In particular, auscultation is a medical procedure in which a clinician listens to the internal sounds of the body using a stethoscope. Organs and systems such as the heart, lungs, and gastrointestinal system are usually listened to detect abnormal patterns. In respiratory diseases such as pneumonia, auscultation can be performed to look for crackles or percussion dullness, an indication of fluid in the lungs. Hence, body sound analysis is part of automated diagnostic applications such as in respiratory diseases [1, 2, 3, 4] , Parkinson's disease [5] , sleep apnea [6] . Although detecting irregular internal sounds might be insufficient for a conclusive diagnostic decision, it serves as a hallmark that can be combined with other clinical tests obtained from different diagnostic tools to conclude.",
            "cite_spans": [
                {
                    "start": 518,
                    "end": 521,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 522,
                    "end": 524,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 525,
                    "end": 527,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 528,
                    "end": 530,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 553,
                    "end": 556,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 571,
                    "end": 574,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this article we study an audio-based approach to detect Coronavirus Disease 2019 (COVID-19), a disease caused by Severe Acute Respiratory Syndrome CoronaVirus 2 (SARS-CoV-2). The SARS-CoV-2 infects most heavily the respiratory tract [7] . Therefore, infected individuals express flu-like symptoms, which can often be mistaken for a cold or flu. Complications are also typi-cally related to pulmonary disorders, such as pneumonia or acute respiratory distress syndrome. The best diagnostic approach is viral testing, often done using nucleic acid tests such as polymerase chain reaction (PCR) to detect viral RNA fragments. Although a gold standard, PCR tests return the result after approximately 4-6 hours, excluding the delivery time, and can take up to 24 or 48 hours. As the ultimate goal of management strategies is to break the infection chain by quickly identifying suspected cases for immediate isolation or quarantine, a test with such a long waiting time as PCR is not optimal. In addition, PCR testing requires qualified staff and well-equipped facility to operate, which is hardly accessible in remote and low-income countries. An alternative test, known as the antigen test, can retrieve results in less than 30 minutes by identifying viral proteins with specific antibodies. Although antigen tests are highly suitable for mass testing, they are less sensitive to detection. Meanwhile, since SARS-CoV-2 infects mainly the respiratory systems, it induces changes in body sounds by either modifying them, e.g., dysphonia, or creating them, e.g., cough or breath sounds. Several studies show that these changes are specific to COVID-19. For example, a study by [8] finds abnormal breathing sounds in all COVID-19 patients. The irregular sounds include cackles, asymmetrical vocal resonance, and indistinguishable murmurs. In a different study of vocal changes in COVID-19 individuals [9] , the authors validate the hypothesis that vocal fold oscillations are correlated with COVID-19, inducing not only changes in voice but also the inability to speak normally. Body sounds have therefore the potential to serve as standalone or in parallel with antigen tests to detect There are several advantages of using body sounds for screening COVID- 19 . First, because PCR testing capacities are limited, screening with body sound or in conjunction with antigen tests can help prioritize who is eligible for PCR tests. If anyone with flu-like symptoms can order a PCR test, it will soon exceed the testing capacity. Only suspects indicated by body sound screening could proceed with PCR tests. Body sound screening can rapidly identify suspect cases without asking them to quarantine while waiting for PCR results. Second, similar to antigen tests, body sound screening is fast, affordable, and conveniently conducted without medical professionals. The cost of running body sound screening can even be lower than that of antigen tests because it can be installed as software or a mobile application on any device and uses the device microphone. Users do not need to buy additional support kits and can use their device to record, analyze, and monitor their status an unlimited number of times. This is particularly useful in regions or countries where testing capacities are scarce, inaccessible, or expensive. Lastly, compared to antigen tests, it does not lead to (medical) waste because no physical products are manufactured, which alleviates the burden on the environment.",
            "cite_spans": [
                {
                    "start": 236,
                    "end": 239,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1674,
                    "end": 1677,
                    "text": "[8]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1897,
                    "end": 1900,
                    "text": "[9]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 2254,
                    "end": 2256,
                    "text": "19",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Given these advantages, the potential of body sounds for screening COVID-19 is enormous. However, a fully developed screening system using body sounds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "is not yet available. Current research on COVID-19 detection considering multiple body sounds often focuses on individual sounds and does not consider their interaction [10, 11] . We hypothesize on the contrary that the effects of COVID-19 may occur in different body sounds, or in a different combination of them, for different individuals. One or more body sounds may be affected while the others remain intact. It is thus sensible not to rely on a single but rather a combination of several body sounds. We propose combining the most meaningful body sounds that are indicative of COVID-19 expressed in terms of fusion rules within the detection algorithm. Our hypothesis is stated as follows:",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 173,
                    "text": "[10,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 174,
                    "end": 177,
                    "text": "11]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The cough, breath and speech sounds contain biomarkers that are indicative of COVID-19 and can be combined using an appropriate fusion rule to maximize the chances of correct detection. To this end, we propose self-attention as a fusion rule to combine features extracted from cough, breath, and speech sounds. Mainly, we use waveforms and spectrograms as input to our model. A waveform represents an audio signal in the time domain, whereas a spectrogram is a representation in the time-frequency domain. Our main contributions in this work are summarized as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We demonstrate that the cough, breath and speech sounds can be lever-aged to detect COVID-19 in a multi-instance audio classification approach based on self-attention fusion. Our experimental results indicate that combining multiple audio instances exceeds the performance of single instance baselines.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We experimentally show that an audio-based classification approach can benefit from combining waveform and spectrogram representations of the input signals. In other words, inputting the time-and frequency-domain dual representations to the network allows for a richer latent feature space which finally improves the overall classification performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We integrate the above contributions into the FAIR4Cov, a classification approach that combines multiple instances of body sound in waveform and spectrogram representations to classify negative and positive COVID-19",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "individuals. This approach can be extended to other respiratory diseases beyond COVID-19.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We briefly present the related work in body sound analysis for pulmonary diseases with a primary emphasis on the COVID-19 use case. Before COVID-19, there is a well-established line of research on body sound analysis for pulmonary disorders such as tuberculosis, pneumonia, or Chronic Obstructive Pulmonary Disease (COPD). Due to the urgency of the pandemic, this field of research has expanded and seen growing interest in newly developed techniques and collected datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "The majority of studies are centered on traditional machine learning techniques by building a classifier using extracted audio features of cough or respiratory sounds. Botha et al. [4] investigate on multichannel lung sounds of 50 subjects from a multi-media respiratory database [13] to classify COPD. The authors develop a Deep Belief Network using features extracted using the Hilbert-Huang transform [14] . The model achieves 93.67% accuracy, 91% sensitivity, and 96.33% specificity. Xu et al. [15] propose a multi-instance learning framework to process raw cough recordings and detect multiple pulmonary disorders including asthma and COPD. The presented framework achieves an F1-score of more than 0.8 in classifying healthy vs. unhealthy, obstructive vs. non-obstructive, and COPD vs. asthma. A recent and detailed review of disease classification from cough sounds can be found in [16] .",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 184,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 280,
                    "end": 284,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 404,
                    "end": 408,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 498,
                    "end": 502,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 889,
                    "end": 893,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Screening pulmonary diseases"
        },
        {
            "text": "Unlike datasets of other pulmonary diseases, there are large corpora of COVID-19 related audios collected from crowdsourcing. Voluntary participants submit recordings of their body sounds to a mobile app or website and provide metadata such as their COVID-19 status and comorbidity. Such large datasets enable researchers to develop COVID-19 detection algorithms as well as to benchmark their research work. To our knowledge, the largest crowdsourcing datasets are COUGHVID [17] , Coswara [18] , and Covid-19 Sounds [19] . COUGHVID comprises more than 20000 cough recordings, while Coswara and Covid-19 Sounds consist of cough, breath, and vocal sounds from more than 2000 and 30000 participants, respectively.",
            "cite_spans": [
                {
                    "start": 474,
                    "end": 478,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 489,
                    "end": 493,
                    "text": "[18]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 516,
                    "end": 520,
                    "text": "[19]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Screening COVID-19"
        },
        {
            "text": "In terms of technical development, a few studies follow the traditional machine learning approaches with extracted audio features [20, 21, 22, 10] . The most common audio features are still MFCC, log Mel spectrogram, ZCR, and kurtosis. Fakhry et al. [20] propose an ensemble network of ResNet50 and MLP on MFCC and Mel spectrograms of cough recordings to classify COVID-19 individuals. The proposed solution claims an AUC of 0.99 on COUGHVID dataset.",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 134,
                    "text": "[20,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 135,
                    "end": 138,
                    "text": "21,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 139,
                    "end": 142,
                    "text": "22,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 143,
                    "end": 146,
                    "text": "10]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 250,
                    "end": 254,
                    "text": "[20]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Screening COVID-19"
        },
        {
            "text": "In a similar approach, the study by [21] benchmarks 15 audio features in the time and frequency domains for the COVID-19 detection task using cough and breath sounds. Their findings indicate that spectral features slightly outperform cepstral features in the classification task, and the best model is achieved using a SVM and Random Forest classifier, with AUCs of 0.8768 and 0.8778, respectively.",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "text": "[21]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Screening COVID-19"
        },
        {
            "text": "Several studies adopt Deep Learning approaches by training CNN on spectrogram or waveform instead of extracted audio features [23, 24, 25, 11] . Rao et al.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 130,
                    "text": "[23,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 131,
                    "end": 134,
                    "text": "24,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 135,
                    "end": 138,
                    "text": "25,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 139,
                    "end": 142,
                    "text": "11]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Screening COVID-19"
        },
        {
            "text": "[23] present a VGG13 network [26] on spectrogram with combined cross-entropy and focal loss. The approach achieves an AUC of 0.78 on the COUGHVID dataset. Xia et al. [24] provide an analysis of combined cough, breath and speech sounds using a simple VGG-ish model. The study introduces the combination of the features of various body sounds to improve classification performance. The best performance has an AUC of 0.75 and sensitivity and specificity of 0.70.",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "text": "[26]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 166,
                    "end": 170,
                    "text": "[24]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Screening COVID-19"
        },
        {
            "text": "Other studies also attempt pretraining on an external dataset or the same dataset without labels. The pretrained model is later finetuned on the target dataset with labels [27, 28, 25] . Harvill et al. [27] pretrain all samples in COUGHVID dataset using autoregressive predictive coding with Long Short-Term Memory. The Mel spectrogram is split into several frames, and the model attempts to predict the next frame given the previous frames. The pretrained model is later finetuned on the DiCOVA dataset [29] and achieves an AUC of 0.95. Similarly, Pinkas et al. [25] pretrains a transformer-based architecture to predict the next frame of the spectrogram and transfers the pretrained features to a set of RNN expert classifiers. The final prediction is the average of the scores produced by all expert classifiers. The proposed training scheme reaches a sensitivity of 0.78 on a private dataset collected by the authors. Xue and Salim [28] propose use contrastive learning in a self-supervised pretraining phase. The contrastive pairs are created by randomly masking the inputs. The model is pretrained on the Coswara dataset without labels and finetuned with Covid-19 Sounds in the downstream task. The proposed technique achieves 0.9 AUC in the COVID-19 negative vs. positive classification task.",
            "cite_spans": [
                {
                    "start": 172,
                    "end": 176,
                    "text": "[27,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 177,
                    "end": 180,
                    "text": "28,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 181,
                    "end": 184,
                    "text": "25]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 202,
                    "end": 206,
                    "text": "[27]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 504,
                    "end": 508,
                    "text": "[29]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 563,
                    "end": 567,
                    "text": "[25]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 936,
                    "end": 940,
                    "text": "[28]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Screening COVID-19"
        },
        {
            "text": "In previous research, cough sounds have often been studied more than other body sounds. This is reasonable because dry cough is a known symptom of COVID-19. However, different body sounds, either being used together with cough or individually, are reported to have a performance comparable to or better than cough sounds. For example, Suppakitjanusant et al. [11] compare two separately trained models in cough and speech and show that speech outperforms cough in classifying COVID-19 patients. Xia et al. [24] also achieve the highest performance by concatenating features of cough, breath, and speech.",
            "cite_spans": [
                {
                    "start": 359,
                    "end": 363,
                    "text": "[11]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 506,
                    "end": 510,
                    "text": "[24]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Relation to our study"
        },
        {
            "text": "Unlike research works that usually study each body sound independently [11] or combined them by significant voting of prediction scores [10] , we explore fusion rules that combined them at the feature level. In other words, we train a network that learns a joint feature vector of all body sounds. Hence, the joint feature vector is optimized to implicitly reflect the relative importance of each body sound towards the final prediction. Although our work falls along the line of [24] , we investigate a more complex fusion rule than just concatenating features. We use self-attention [30] , which captures the dependencies among body sounds into a joint feature vector. Self-attention is used not only as a layer in the transformer architecture but can also be used to aggregate features [31] . In addition, instead of using handcrafted audio features, we train our model using waveform and spectrogram representations, therefore creating more robust features compared to previous methods. We experiment our approach on the Coswara dataset and achieve the state-of-the-art results. We report an average performance of the models obtained from cross-validation on a split test set (Section 4.2). However, we emphasize that there is no unique test set generated for the Coswara dataset and the data size was growing at the time we conducted our experiment.",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[11]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 136,
                    "end": 140,
                    "text": "[10]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 480,
                    "end": 484,
                    "text": "[24]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 585,
                    "end": 589,
                    "text": "[30]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 789,
                    "end": 793,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Relation to our study"
        },
        {
            "text": "We begin this section by first summarizing self-attention [30] , which is an attention mechanism used as the fusion rule and as a layer in the backbone network in our approach. We then describe the proposed FAIR4Cov approach with detailed components and how they interact with each other to extract the features of body sound.",
            "cite_spans": [
                {
                    "start": 58,
                    "end": 62,
                    "text": "[30]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Self-attention [30] is originally developed for language models. A sequence in language models consists of many tokens (e.g., words) that the model needs to memorize to synthesize the global information on top of that sequence. However, memorizing a long sequence is not always possible and the model is likely to forget the tokens that emerge early in the sequence. Self-attention therefore seeks to find a set of highly important tokens in the sequence and divert the focus of the model into these ones. The reason why these tokens are chosen is that they are highly similar in their content. Instead of memorizing the whole sequence, the model just needs to memorize these tokens because they carry the same (important) message repeatedly along the sequence. Let I be an input sequence of n tokens in d dimensions. The fundamental components of a selfattention layer are the query (Q), key (K), and value (V), which are the projection of the input sequence I with weights W Q , W K , W V . A n \u00d7 n selfattention matrix W a is then computed by taking the dot product of each query token with n keys. Each row i of W a denotes the similarity scores of query token i with n keys including itself. The dot product is scaled by \u221a d to stabilize the gradient. Hence it is known as the scaled dot product. Next, a softmax function is applied across each row of W a to normalize the scores between 0 and 1. The final output is a product between W a and V. The output has the same n tokens, but each new token is the sum of tokens in V weighted by each row in W a . In other words, every new token i in V is constructed based on the similarity of the query token i with other tokens.",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 19,
                    "text": "[30]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Self-attention"
        },
        {
            "text": "Each self-attention layer can comprise many heads in parallel and is called multiheaded self-attention [30] . The intuition behind multihead is that each head pays attention to a different property of the input token. Assume h is the number of heads, the self-attention layer outputs h different sequences where each token in the sequence has the length of d/h. The output tokens are then concatenated across the sequences, forming the final output of shape n \u00d7 d.",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 107,
                    "text": "[30]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Self-attention"
        },
        {
            "text": "We present in this section our proposed architecture. Let D = x ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov)"
        },
        {
            "text": "Our objective is to derive a representative feature vector for c body sounds per subject across waveform and spectrogram inputs. We denote by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov)"
        },
        {
            "text": "2c ] the aggregated input instance related to the i-th subject. The FAIR4Cov approach takes the input x (i) of the i-th subject and returns a joint feature vector z (i) that aggregates the information across multiple body sounds and representations as shown in the following equation. Here g w and g s denote neural networks that extract features from waveform and spectrogram inputs, and \u03c6 is the attention-based fusion unit. Figure 1 shows an overview of the FAIR4Cov approach and the main components along the pipeline. The feature extractors and the fusion unit are instrumental components in our proposed approach, and are further detailed in the next sections.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 427,
                    "end": 435,
                    "text": "Figure 1",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov)"
        },
        {
            "text": "Feature extractors are neural networks responsible for learning representative features for each body sound. As the input consists of waveform and spectrogram, two neural networks g w and g s are trained in parallel to handle both representations. In each network, the weights are shared across the input channels 1, .., c and c + 1, ..., 2c. We choose g w to be a pretrained wav2vec [33] and g s to be DeiT-S/16, a Vision Transformer (ViT) model [34] . DeiT-S/16 and wav2vec are transformer-based models and achieve state-of-the-art results in language and vision models.",
            "cite_spans": [
                {
                    "start": 384,
                    "end": 388,
                    "text": "[33]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 447,
                    "end": 451,
                    "text": "[34]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Feature extractors"
        },
        {
            "text": "wav2vec. The wav2vec network [33] was developed to process audio for the speech-to-text translation task. It comprises both convolutional and self-attention layers and is pretrained on a large audio corpus in an unsupervised fashion.",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "text": "[33]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Feature extractors"
        },
        {
            "text": "Therefore, we take advantage of the pre-trained wav2vec features and designed a finetuning unit to effectively leverage them in our target dataset. As shown in ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature extractors"
        },
        {
            "text": "We denote f ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusion unit"
        },
        {
            "text": "The fusion unit \u03c6 combines f (i) k with k = 1, .., c into a single vector z (i) by using a multiheaded self-attention layer (MSA) and a MLP h:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusion unit"
        },
        {
            "text": "The output of MSA for each subject i is a new set of feature vectors f There is no restriction on the duration of the recordings, so users can decide when they want to start and stop recording.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusion unit"
        },
        {
            "text": "The first step in the preprocessing pipeline is to remove the leading and trailing silence. We observe that long recordings (>20 seconds) mainly contain silence, and the duration at which people cough, breathe or speak lasts only 3-10 seconds. Hence, we trim automatically the silence and take only the clip with detectable amplitude. The next important step is to remove corrupted files.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data preprocessing"
        },
        {
            "text": "We define corrupted files as those that contain no sound, noise, or a different sound type than the one reported in the label. First, we remove recordings whose duration is less than 1 second because they do not contain any detected sound. Second, similar to the approach of [24] , we use a pretrained model called YAMNet 2 to systematically remove recordings where the detected sound is not the same as the provided label. YAMNet is a pretrained model on YouTube audio to classify 521 events, including cough, speech, and breath. If most of the predicted events in a recording are not cough, speech, or breath, we will remove all the recordings associated with this participant. In addition, we decide not to use shallow cough and breath in our experiments because the quality of such recordings is low and can be misdetected as noise. Altogether, 710 participants were discarded from the initially curated dataset through this process and the 1359 remaining participants were considered for our analyses. Out of them, 1136 people (83.6%) are COVID-19 negative and 223 people are COVID-19 positive.",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 279,
                    "text": "[24]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Data preprocessing"
        },
        {
            "text": "Each participant has exactly 7 recordings, which amounts to 9513 recordings used in our experiments. We provide in Table 1 ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 115,
                    "end": 122,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Data preprocessing"
        },
        {
            "text": "For audio processing and transformation, we use Torchaudio 3 , a library for audio and signal processing with Hyperparameters. Table 4 shows the complete hyperparameter settings in our experiments. Most hyperparameters are identical across architectures, representations, or fusion rules. For example, we train all models for 30 epochs without early stopping, and the best checkpoint is saved based on the best AUC obtained in the validation fold. The loss function that we use is binary crossentropy (BCE), and we optimize this loss with AdamW (Adam with weight decay) [36] . However, concerning the learning rate, we fix a base learning rate of 0.0001 for all experiments and adjust the learning rate scheduler and weight decay conditional on the architecture or fusion rules. The weight decay factor is set between 0.1 and 0.001.",
            "cite_spans": [
                {
                    "start": 570,
                    "end": 574,
                    "text": "[36]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [
                {
                    "start": 127,
                    "end": 134,
                    "text": "Table 4",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Data transformation and augmentation"
        },
        {
            "text": "Evaluation. Our primary metric for model selection is AUC. During training, we save the checkpoint with the highest performance based on AUC. During validation, we use AUC to compute the optimal threshold and take this thresh- old to compute other metrics such as sensitivity and specificity in the test set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data transformation and augmentation"
        },
        {
            "text": "We report the AUC scores in the main paper and provide the sensitivity and specificity in the Appendix. Table 5 shows the performance of the models trained on a single body sound instance. The input to the model is either a waveform (BA1) or a spectrogram (BA2). The results reveal that the models trained on spectrograms perform substantially better than those trained on waveforms. The average AUC scores for Average .6127 \u00b1 .0751 .7549 \u00b1 .0215 Table 5 : AUC scores of the baseline experiments for spectrogram (DeiT-S/16) and waveform (wav2vec) models. The bold scores denote the highest performance between spectrogram and waveform. Table 6 shows the results for the ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 104,
                    "end": 111,
                    "text": "Table 5",
                    "ref_id": "TABREF10"
                },
                {
                    "start": 447,
                    "end": 454,
                    "text": "Table 5",
                    "ref_id": "TABREF10"
                },
                {
                    "start": 636,
                    "end": 643,
                    "text": "Table 6",
                    "ref_id": "TABREF9"
                }
            ],
            "section": "Data transformation and augmentation"
        },
        {
            "text": "As can be seen in Table 6 , the AUC scores are not comparable among all combinations of body sound. Thus, it is valid to doubt whether there is a preferable combination of body sounds that leads to the best predictive outcome.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 18,
                    "end": 25,
                    "text": "Table 6",
                    "ref_id": "TABREF9"
                }
            ],
            "section": "Influence of body sound combinations"
        },
        {
            "text": "However, it is not conclusive based on our experimental results or the literature to decide the best combination choice or selection rules. Instead, we argue that no body sound is significantly better than the others and that the performance is correlated rather with the number of body sounds in that combination. To illustrate this point, we look at the performance of our model when (1) In addition to the number of body sounds in each combination, the varying duration of each instance can have an influence on the results. In this study, we truncate each recording to 4 seconds. However, a body sound such as cough could last less than 4 seconds and the rest of the audio be just breath. A finer analysis taking into consideration this aspect should be considered in a follow-up study.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Influence of body sound combinations"
        },
        {
            "text": "We analyze the effect of the dual representation of spectrogram and waveform in the absence of body sound fusion by conducting an ablation study similar to the FAIR4Cov framework but with the input of a single body sound. As there are no rules for body sound fusion, the features extracted from two representations are concatenated, flattened, and then projected onto a 128-dimensional vector by a MLP layer. Similar to the baseline experiment, we present the AUC scores of seven models trained on 7 body sound instances in Table 7 . Overall, the average AUC scores are on par with those of the DeiT-S/16 model (BA2)",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 524,
                    "end": 531,
                    "text": "Table 7",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Influence of dual representations"
        },
        {
            "text": "in Average .7519 \u00b1 .0137 Table 7 : Baseline performance (in AUC) of FAIR4Cov on a single body sound.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 25,
                    "end": 32,
                    "text": "Table 7",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Influence of dual representations"
        },
        {
            "text": "The fusion rule for body sound relies on self-attention. One of the interesting properties of self-attention is scaling, which is discussed in the work of Dosovitskiy et al. [34] . The authors note that the performance of the transformer-based model could be scaled up in response to an increase in resolution of patches or number of blocks. This contrasts with convolutional networks, in which the accuracy can reach saturation at a certain level of complexity. This scaling property explains why adding more body sounds leads to a steady increase in AUC scores. Adding more body sounds means adding more tokens and establishing stronger dependencies among them. When only two or three instances of body sound are adopted, the effect of body sound fusion is less significant. The combinations with less than or equal to three instances, i.e., cough-breath, fast and normal counting, /a-e-o/ vowel utterance, achieve AUC scores in the range of 0.75-0.79, which is on par or slightly better than the performance of models on a single instance (Table 7) . This happens because the number of instances is insufficient to establish long-range dependencies. As more body sounds are added, these dependencies are captured, and the performance of models with fusion units started to improve substantially. Likewise, the joint feature vector embeds more information when dual representation is adopted.",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 178,
                    "text": "[34]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [
                {
                    "start": 1042,
                    "end": 1051,
                    "text": "(Table 7)",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Influence of attention-based fusion"
        },
        {
            "text": "When the number of instances in the combination is small, i.e., less than three, the gain due to the dual representation is not noticeable. However, starting from five instances, the gap between FAIR4Cov and DeiT-S/16 becomes wider in favor of FAIR4Cov. We attribute this gain to the resonance of extra information given by the dual representation and the number of body sounds, efficiently captured the self-attention fusion rule.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Influence of attention-based fusion"
        },
        {
            "text": "In this article, we study Deep Learning approaches to detect COVID-19 using body sounds. To this end, we propose FAIR4Cov, a multi-instance audio classification approach with attention-based fusion on waveform and spectrogram representation. We prove the effectiveness of our approach by conducting extensive experiments on the Coswara dataset. The results demonstrate that the fusion of body sounds using self-attention helps extract richer features useful for the classification of COVID-19 negative and positive patients. In addition, we perform an in-depth analysis on the influence of fusion rule on the performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "We found that the scaling property of self-attention shows great efficiency when more instances of body sounds as well as representations are adopted. The best setting with a combination of cough, breath, and speech sounds in waveform and spectrogram representation results in an AUC score of 0.8658, a sensitivity of 0.8057, and a specificity of 0.7958 on our test set. The sensitivity of our model exceeds 0.75, the required threshold of COVID-19 screening test [37] . ",
            "cite_spans": [
                {
                    "start": 464,
                    "end": 468,
                    "text": "[37]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Diagnosis of pneumonia from sounds collected using low cost cell phones",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "2015 International Joint Conference on Neural Networks (IJCNN)",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {
                "DOI": [
                    "10.1109/IJCNN.2015.7280317"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "COVID-19 Artificial Intelligence Diagnosis Using Only Cough Recordings",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Laguarta",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hueto",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Subirana",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Open Journal of Engineering in Medicine and Biology",
            "volume": "1",
            "issn": "",
            "pages": "275--281",
            "other_ids": {
                "DOI": [
                    "10.1109/OJEMB.2020.3026928"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Detection of tuberculosis by automatic cough sound analysis",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "H R"
                    ],
                    "last": "Botha",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Theron",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "Warren",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Klopper",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Dheda",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "D"
                    ],
                    "last": "Van Helden",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "R"
                    ],
                    "last": "Niesler",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Physiological Measurement",
            "volume": "39",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1088/1361-6579/aab6d0"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Deep Learning on Computerized Analysis of Chronic Obstructive Pulmonary Disease",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Altan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kutlu",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Allahverdi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Journal of Biomedical and Health Informatics",
            "volume": "24",
            "issn": "",
            "pages": "1344--1350",
            "other_ids": {
                "DOI": [
                    "10.1109/JBHI.2019.2931395"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "PDVocal: Towards Privacy-preserving Parkinson's Disease Detection using Non-speech Body Sounds",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The 25th Annual International Conference on Mobile Computing and Networking",
            "volume": "",
            "issn": "",
            "pages": "1--16",
            "other_ids": {
                "DOI": [
                    "10.1145/3300061.3300125"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Apnea and heart rate detection from tracheal body sounds for the diagnosis of sleep-related breathing disorders",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kalkbrenner",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Eichenlaub",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "R\u00fcdiger",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kropf-Sanchen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Rottbauer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Brucher",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Medical & Biological Engineering & Computing",
            "volume": "56",
            "issn": "",
            "pages": "671--681",
            "other_ids": {
                "DOI": [
                    "10.1007/s11517-017-1706-y"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2): An overview of viral structure and host re",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Astuti",
                    "suffix": ""
                },
                {
                    "first": "Ysrafil",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "The respiratory sound features of COVID-19 patients fill gaps between clinical data and screening methods, preprint, Infectious Diseases (except HIV/AIDS)",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lei",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1101/2020.04.07.20051060"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Detection of Covid-19 Through the Analysis of Vocal Fold Oscillations",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ismail",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Deshmukh",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "volume": "",
            "issn": "",
            "pages": "1035--1039",
            "other_ids": {
                "DOI": [
                    "10.1109/ICASSP39728.2021.9414201"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Artificial intelligence enabled preliminary diagnosis for COVID-19 from voice cues and questionnaires",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shimon",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Shafat",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Dangoor",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ben-Shitrit",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "The Journal of the Acoustical Society of America",
            "volume": "149",
            "issn": "",
            "pages": "1120--1124",
            "other_ids": {
                "DOI": [
                    "10.1121/10.0003434"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Identifying individuals with recent COVID-19 through voice classification using deep learning",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Suppakitjanusant",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sungkanuparph",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Wongsinin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Virapongsiri",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kasemkosin",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Chailurkit",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ongphiphadhanakul",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Scientific Reports",
            "volume": "11",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1038/s41598-021-98742-x"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Automatic cough classification for tuberculosis screening in a real-world environment",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pahar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Klopper",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Reeve",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Warren",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Theron",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Niesler",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Physiological Measurement",
            "volume": "42",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1088/1361-6579/ac2fb8"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Multimedia Respiratory Database (RespiratoryDatabase@TR): Auscultation Sounds and Chest X-rays",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Altan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kutlu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Garbi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "O"
                    ],
                    "last": "Pekmezci",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "2",
            "issn": "",
            "pages": "59--72",
            "other_ids": {
                "DOI": [
                    "10.28978/nesciences.349282"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series analysis",
            "authors": [
                {
                    "first": "N",
                    "middle": [
                        "E"
                    ],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "C"
                    ],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "H"
                    ],
                    "last": "Shih",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "N.-C",
                    "middle": [],
                    "last": "Yen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Tung",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "H"
                    ],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences",
            "volume": "454",
            "issn": "",
            "pages": "903--995",
            "other_ids": {
                "DOI": [
                    "10.1098/rspa.1998.0193"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Listen2Cough: Leveraging End-to-End Deep Learning Cough Detection Model to Enhance Lung Health Assessment Using Passively Sensed Audio",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Nemati",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Vatanparvar",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nathan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Rahman",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Mccaffrey",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kuang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
            "volume": "5",
            "issn": "",
            "pages": "1--22",
            "other_ids": {
                "DOI": [
                    "10.1145/3448124"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Past and Trends in Cough Sound Acquisition, Automatic Detection and Automatic Classification: A Comparative Review",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Serrurier",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Neuschaefer-Rube",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "R\u00f6hrig",
                    "suffix": ""
                }
            ],
            "year": 2022,
            "venue": "Sensors",
            "volume": "22",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/s22082896"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "The COUGHVID crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Orlandic",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Teijeiro",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Atienza",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Scientific Data",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1038/s41597-021-00937-4"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Coswara -A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Krishnan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ramoji",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Chetupalli",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "R"
                    ],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "K"
                    ],
                    "last": "Ghosh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ganapathy",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Interspeech 2020, ISCA",
            "volume": "",
            "issn": "",
            "pages": "4811--4815",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory Sound Data",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chauhan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Grammenos",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hasthanasombat",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Spathis",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cicuta",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Mascolo",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
            "volume": "",
            "issn": "",
            "pages": "3474--3484",
            "other_ids": {
                "DOI": [
                    "10.1145/3394486.3412865"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Virufy: A Multi-Branch Deep Learning Network for Automated Detection of",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fakhry",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Chaudhari",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Khanzada",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--9",
            "other_ids": {
                "arXiv": [
                    "arXiv:2103.01806"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Audio feature ranking for sound-based COVID-19 patient detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Meister",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "A"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--22",
            "other_ids": {
                "arXiv": [
                    "arXiv:2104.07128"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "COVID-19 cough classification using machine learning and global smartphone recordings",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pahar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Klopper",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Warren",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Niesler",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Biology and Medicine",
            "volume": "135",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1016/j.compbiomed.2021.104572"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Deep Learning with hyper-parameter tuning for COVID-19 Cough Detection",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Rao",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Narayanaswamy",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Esposito",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thiagarajan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Spanias",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "2021 12th International Conference on Information, Intelligence, Systems & Applications (IISA)",
            "volume": "",
            "issn": "",
            "pages": "1--5",
            "other_ids": {
                "DOI": [
                    "10.1109/IISA52424.2021.9555564"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Proceedings of the 35th Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Spathis",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chauhan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Grammenos",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hasthanasombat",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Bondareva",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Floto",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cicuta",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Mascolo",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--13",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "SARS-CoV-2 Detection From Voice",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pinkas",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Karny",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Malachi",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Barkai",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Bachar",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Aharonson",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Open Journal of Engineering in Medicine and Biology",
            "volume": "1",
            "issn": "",
            "pages": "268--274",
            "other_ids": {
                "DOI": [
                    "10.1109/OJEMB.2020.3026468"
                ]
            }
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1409.1556"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Classification of COVID-19 from Cough Using Autoregressive Predictive Coding Pretraining and Spectral Data Augmentation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Harvill",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "R"
                    ],
                    "last": "Wani",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hasegawa-Johnson",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ahuja",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Beiser",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chestek",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Interspeech 2021, ISCA",
            "volume": "",
            "issn": "",
            "pages": "926--930",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Exploring Self-Supervised Representation Ensembles for COVID-19 Cough Classification",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xue",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "D"
                    ],
                    "last": "Salim",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2105.07566"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "DiCOVA Challenge: Dataset, task, and baseline system for COVID-19 diagnosis using acoustics",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Muguli",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Pinto",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "R"
                    ],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Krishnan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "K"
                    ],
                    "last": "Ghosh",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bhat",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Chetupalli",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ganapathy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ramoji",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nanda",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2103.09148"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Attention is All you Need, Advances in neural information processing systems",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "30",
            "issn": "",
            "pages": "1--11",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "How Transferable are Self-supervised Features in Medical Image Classification Tasks?",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Truong",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lenga",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of Machine Learning for Health",
            "volume": "",
            "issn": "",
            "pages": "2640--3498",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Signal estimation from modified short-time Fourier transform",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Griffin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lim",
                    "suffix": ""
                }
            ],
            "year": 1984,
            "venue": "conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing",
            "volume": "32",
            "issn": "",
            "pages": "236--243",
            "other_ids": {
                "DOI": [
                    "10.1109/TASSP.1984.1164317"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Baevski",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohamed",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Auli",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--19",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.11477"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dosovitskiy",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Beyer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kolesnikov",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Weissenborn",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhai",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Unterthiner",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dehghani",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Minderer",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Heigold",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gelly",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Houlsby",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2010.11929"
                ]
            }
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Training data-efficient image transformers & distillation through attention",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Touvron",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cord",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Douze",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Massa",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sablayrolles",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "J\u00e9gou",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of Machine Learning Research",
            "volume": "139",
            "issn": "",
            "pages": "10347--10357",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Decoupled Weight Decay Regularization",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Loshchilov",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--19",
            "other_ids": {
                "arXiv": [
                    "arXiv:1711.05101"
                ]
            }
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Comparative sensitivity evaluation for 122 CE-marked rapid diagnostic tests for SARS-CoV-2 antigen",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Scheiblauer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Filomena",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nitsche",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Puyskens",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "M"
                    ],
                    "last": "Corman",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Drosten",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zwirglmaier",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lange",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Emmerich",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Knauer",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "N\u00fcbling",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Eurosurveillance",
            "volume": "26",
            "issn": "",
            "pages": "1--13",
            "other_ids": {
                "DOI": [
                    "10.2807/1560-7917.ES.2021.26.44.2100441"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "study a combination of log spectral energies and Mel Frequency Cepstral Coefficients (MFCC) in screening tuberculosis using cough sounds of 38 subjects acquired in a specially designed facility. The authors achieve an accuracy of 0.98 and an Area Under the Receiver Operating Characteristic Curve (AUC) of 0.95 for the given task. Pahar et al. [12] investigate a similar task on cough sounds of 51 healthy and tuberculosis individuals in a primary healthcare clinic. The authors propose a linear regression model on extracted features, namely MFCC, log spectral energies, Zero-crossing Rate (ZCR), and kurtosis, which leads to a sensitivity and specificity of 0.93 and 0.95, respectively. Song [1] studies breath sounds to classify pneumonia among 376 children at three children's hospitals in Bangladesh. The author extracts a total of 18 acoustic features for the K-Nearest Neighbors (KNN) and Support Vector Machine (SVM) classifier. The proposed method achieves 0.9198 accuracy, 0.9206 sensitivity, and 0.9068 specificity. Altan et al.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "of n subjects, where i \u2208 {1, ..., n} denotes the subject index and j \u2208 {1, ..., 2c} denotes the index of sound instances in the set of c body sounds. the fixed-length waveform vectors related to the different c audio instances. The components x the associated spectrogram representation of c audio instances. The spectrogram is constructed by transforming the waveform representation with Discrete Short-Time Fourier Transform [32]. In our experiments, we use the Mel-Spectrogram, which is the logarithmic transformation of the frequency in Hertz to Mel scale given by the equation:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "An overview of FAIR4Cov approach. The feature extractors wav2vec and DeiT-S/16 are responsible for extracting waveform and spectrogram features across all body sounds. The fusion unit receives the extracted waveform and spectrogram features and fuses them into a single and compact feature vector using self-attention. The classifier uses this joint feature vector to make the final prediction.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "we first resample the audio to 8000 Hz to meet the input requirement of wav2vec. We freeze the whole wav2vec unit and use it to extract the features in every 25 ms of the input sequence. The wav2vec output has a shape of (t, d)where t is the time dimension and d is the feature dimension. For each feature along the time axis, we select the values at the 0.1 and 0.9 quantile, which can be considered to approximate the min and max pooling of feature vectors. The purpose of this step is to aggregate information over time by choosing only important information. After this step, we flatten the new feature matrix and feed it to a Multilayer Perceptron (MLP) layer to reduce the dimension of the feature embedding to 128. DeiT-S/16. The DeiT-S/16 architecture is a variant of ViT introduced by Touvron et al. [35] as part of the DeiT (Data-efficient image Transformers), which has the exact architecture of the original ViT [34] and differs only in the training strategy. The model is categorized into a small group where the projected embedding dimension through self-attention blocks is 384. It consists of 12 multi-headed self-attention layers, and each layer uses six heads. The resolution of each patch in the attention layer is 16 \u00d7 16 pixels. We change the last dense layer of DeiT-S/16 to be an identity unit so that we can extract the features from the previous layer. We use the pretrained DeiT-S/16 on ImageNet and finetune on our target dataset in all of our experiments. We projected the output unit with 128 feature vectors to have a fair comparison with wave2vec.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Wav2vec-based model for extracting waveform features. (Step 1) 8 kHz sampled audio is fed into the pretrained wav2vec model to extract features. (Step 2) wav2vec outputs a feature vector per every 25 ms of the audio, resulting in a t \u00d7 d matrix where t is the total time indices and d is the dimension of the feature vector. We select in each feature vector the element at the 0.1 and 0.9 quantile. (Step 3) The new feature matrix is flattened into a single vector. (Step 4) A MLP layer receives the feature vector and (Step 5) projects it into a fixed dimension of 128.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "with k \u2208 [1, c] the joint feature vector obtained by concatenating the feature vector of each individual body sound extracted from g w and g s :",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "the similarity score between feature k and all c features. Next, we concatenate all f (i) k across c vectors and feed the new concatenated vector into a MLP layer h to project it to the final 128-dimensional feature vector z (i) . The classifier takes z (i) and outputs the predicted probability of whether the subject is infected with COVID-19 or not. 18] is a crowdsourcing project to build an audio corpus from COVID-19 negative and positive individuals. The dataset is available publicly 1 to enable research on the development of diagnostic tools for respiratory diseases, in this case COVID-19. The audio recordings were collected between April 2020 and September 2021 through crowdsourcing. We accessed the database when it was still in the last collection stage. The data collection occurs through a web interface where users are prompted to provide their metadata and recordings using a device microphone. The metadata covers age, sex, location, and in particular COVID-19 status. The users are then instructed to submit nine audio recordings of (heavy and shallow) cough, (deep and shallow) breath, (fast and slow) counting from 1 to 20, and uttering the phonemes /a/, /e/ and /o/.The COVID-19 status must be selected from the categories negative, positive with or without symptoms, recovered, and not identified respiratory disease.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "The values of loaded audio are automatically normalized between -1 and 1. As users record with different device microphones, the sample rate is not consistent across all recordings. We resample all recordings with two sample rates; 44100 Hz and 8000 Hz. TheDeiT-S/16 uses a sample rate of 44100 Hz while the wav2vec model uses a sample rate of 8000 Hz. We decide to use only four seconds of each recording, which is an optimal value for getting best results after we finetune with different ones. In terms of spectrogram transformation, we take the Mel-spectrogram with 128 Mel filterbanks operating on 1025 frequency bins, i.e., FFT size of 2048, window size of 2048, and hop size of 1024. We perform on-the-fly data augmentation during training. For each training audio, we randomly select a continuous 4-second interval out of the first 5 seconds of the recording to ensure a slight variation. However, during evaluation, we select the first 4 seconds in the audio. We investigate many audio augmentation techniques such as pitch shift, time stretch, or masking, but not all prove helpful in our tasks. Ultimately, only amplitude scaling, time and frequency masking are retained. In the amplitude scaling, we randomly inject an amplitude gain between 0.9 and 1.3 on waveform.Amplitude scaling is always performed before spectrogram transformation in case the spectrogram representation is used. For spectrogram, we apply random time and frequency masking with a length of 10. This augmentation randomly sets consecutive blocks of size 10 in time or frequency bins to the value of 0 to help the network be robust to deformation in time and frequency direction.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "BA2) and wav2vec (BA1) are, respectively, 0.7549 and 0.6126. The performance of different body sounds across architectures and representations does not establish a consistent pattern. For example, using only cough sound leads to the highest AUC score in DeiT-S/16 but a lower score in wav2vec. There appears to be a countertrend between DeiT-S/16 and wav2vec. For example, the counting sound achieves better results than the fast counting sound in DeiT-S/16 but worse in wav2vec. Similarly, the utterance of /o/ outperforms other vowels in DeiT-S/16 but performs poorly in wav2vec.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "FAIR4Cov model (BE3) compared to the DeiT-S/16 (BE2) and wav2vec (BE1) models in different combinations of body sounds using self-attention fusion. In general, the FAIR4Cov approach significantly outperforms models trained on a single representation. The average AUC score of FAIR4Cov is 0.8316, which is 0.0227 more than DeiT-S/16 and 0.0847 more than wav2vec. FAIR4Cov achieves the highest AUC scores in all body sound combinations with the only exception in the cough-breath combination, which will be discussed in the next section. The cough-breath combination results in the lowest AUC score in all alternatives in terms of body sound combination. The largest combination, cough-breath-speech, gives the best results in FAIR4Cov and wav2vec but is behind the cough-speech combination in DeiT-S/16 by a margin of AUC 0.007. FAIR4Cov achieves the highest AUC score of 0.8658 with the combination of cough, breath, and speech. This score is 0.0343 and 0.0941 higher than the best scores produced by DeiT-S/16 and wav2vec. The results of the FAIR4Cov models find clear support for the use of dual audio representation along with body sound fusion. Model wav2vec (BE1) DeiT-S/16 (BE2) FAIR4Cov (BE3)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "only a single body sound instance and (2) a combination of body sounds is used.When we train models with only one body sound instance as input (Section 5.1), we find that no single body sound instance consistently outperforms the others. A body sound instance may perform better than others under specific architectures or audio representations while worse in another setting. For example, the DeiT-S/16 model (BA2) trained on cough sound is among the best body sounds with an AUC score of 0.7782. However, when the feature extractor is wav2vec (BA1), the AUC score drops to 0.4574. We also observe a similar standard error when replacing the DeiT-S/16 architecture with ResNet50 (Appendix B.2). This indicates that the subtle difference observed among body sounds may be due merely to stochasticity or the setting of the feature extractor, and no body sound is significantly better than the others as input to our model.Regarding the combinations of body sounds (Section 5.2), we observe that the combination of cough and breath leads invariably to the lowest AUC scores for all models. At the same time, this combination considers only two body sound instances, heavy cough, and deep breath, while all other combinations consider at least five sound instances. This observation suggests that the performance is likely to correlate with the number of body sound instances. To support this, we conduct additional experiments in a similar setting to benchmark experiments with the following combinations; counting (incl. fast and normal counting) and vowel (incl. utterance of /a/, /e/ and /o/). Figure 3 shows that counting and cough-breath combination, both with two instances, achieve a roughly similar performance. The combination of the /a-e-o/ utterances performs better than 2-instance combinations, i.e., cough-breath and (fast-normal) counting, by a margin of 0.03-0.04 AUC. This supports a correlation between performance and the number of body sounds.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "AUC scores of the DeiT-S/16 model and FAIR4Cov framework vs. number of instances in each combination. The x-axis shows the combination with the number of instances in the ascending order of quantity. Additional results can be found in Appendix B.1 and C.1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "shows the AUC scores of the FAIR4Cov and DeiT-S/16 models on the different combinations of body sounds sorted in ascending order of instances.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "the statistics of the audio length of all body sound instances after the preprocessing step. In our experiments, the participans are split into six folds for training and testing purposes. The details of the split is presented in the Section 4.2.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The statistics of audio length (in second) after the preprocessing step.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Baseline and benchmark experiments. The last experiment (BE3) is our proposed FAIR4Cov model that uses both waveform and spectrogram inputs and the body sound fusion unit.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Repartition of the subjects for the 5-fold cross-validation scheme",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Hyperparameter settings in baseline and benchmark experiments.",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF10": {
            "text": "Breath and counting sounds achieve the highest AUC score, while the utterance of vowel /e/ and /o/ leads to the lowest performance. The benefit of joint features from the dual representation is not observed because the change in the individual AUC scores of each body sound does not follow any pattern. In comparison with the DeiT-S/16 results inTable 5, except for cough, the difference in the performance is subtle. The result suggests that the waveform representation contributes little to the final classifier. The performance is indeed strongly influenced by the powerful DeiT-S/16 in the spectrogram representation, which eclipses the features obtained from the waveform. Hence, we conclude that using the dual representation in the absence of body sound fusion did not improve any performance. Nevertheless, when the dual representation is used for body sound fusion, the extra information from multiple body sounds is picked up by the fusion unit and enriches the joint extracted feature. We will discuss the role of dual representation when used with body sound fusion in the next section.",
            "latex": null,
            "type": "table"
        },
        "TABREF11": {
            "text": "The FAIR4Cov approach can be scaled to an unlimited number of body sounds and applied to other respiratory diseases.Speech.7562 \u00b1 .0152 .3557 \u00b1 .0409 .7592 \u00b1 .0586 Cough + Breath .6739 \u00b1 .0435 .2694 \u00b1 .0363 .7200 \u00b1 .0524 Cough + Speech .7644 \u00b1 .0088 .3922 \u00b1 .0771 .7906 \u00b1 .0937 Breath + Speech .7682 \u00b1 .0149 .3747 \u00b1 .0675 .6743 \u00b1 .0966 Cough + Breath + Speech .7717 \u00b1 .0128 .3358 \u00b1 .0347 .7236 \u00b1 .0669",
            "latex": null,
            "type": "table"
        },
        "TABREF12": {
            "text": "1: AUC, sensitivity, and specificity of wav2vec models on different combination of body sounds using self-attention fusion Appendix B. Self-attention fusion with only spectrogram inputs Speech .8081 \u00b1 .0239 .7486 \u00b1 .0775 .7717 \u00b1 .0711 Cough + Breath .7685 \u00b1 .0183 .6400 \u00b1 .0642 .8293 \u00b1 .0718 Cough + Speech .8315 \u00b1 .0306 .7371 \u00b1 .0836 .7927 \u00b1 .0892 Breath + Speech .8122 \u00b1 .0125 .6571 \u00b1 .0313 .8796 \u00b1 .0298 Cough + Breath + Speech .8241 \u00b1 .0266 .6914 \u00b1 .0796 .8408 \u00b1 .0838 Counting (fast + normal) (*) .7467 \u00b1 .0124 .6629 \u00b1 .0946 .7790 \u00b1 .0774 Phoneme (/a/-/e/-/o/) (*) .7806 \u00b1 .0208 .7886 \u00b1 .0100 .6827 \u00b1 .0753",
            "latex": null,
            "type": "table"
        },
        "TABREF13": {
            "text": "1: AUC, sensitivity, and specificity of DeiT-S/16 models on different combination of body sounds using self-attention fusion. (*) Ablation experiments on additional body sound combination for discussion in Section 5.3.3Speech .7531 \u00b1 .0362 .7314 \u00b1 .0983 .6817 \u00b1 .0818 Cough + Breath .7585 \u00b1 .0259 .6400 \u00b1 .0859 .8188 \u00b1 .0832 Cough + Speech .7817 \u00b1 .0282 .8000 \u00b1 .1352 .6628 \u00b1 .0992 Breath + Speech .7862 \u00b1 .0238 .7314 \u00b1 .0878 .7466 \u00b1 .1058 Cough + Breath + Speech .8026 \u00b1 .0229 .6914 \u00b1 .1120 .7959 \u00b1 .1175 Table B.2: AUC, sensitivity, and specificity of ResNet50 models on different combination of body sounds using self-attention fusion Appendix C. FAIR4Cov Speech .8434 \u00b1 .0290 .7429 \u00b1 .0767 .8356 \u00b1 .0266 Cough + Breath .7585 \u00b1 .0174 .6629 \u00b1 .0874 .8168 \u00b1 .0754 Cough + Speech .8584 \u00b1 .0308 .8171 \u00b1 .1063 .7738 \u00b1 .0977 Breath + Speech .8319 \u00b1 .0187 .7771 \u00b1 .0554 .7895 \u00b1 .0644 Cough + Breath + Speech .8658 \u00b1 .0115 .8057 \u00b1 .0554 .7958 \u00b1 .0678 Counting (fast + normal) (*) .7702 \u00b1 .0313 .7086 \u00b1 .0836 .7717 \u00b1 .0470 Phoneme (/a/-/e/-/o/) (*) .7906 \u00b1 .0095 .7886 \u00b1 .0530 .6848 \u00b1 .0499",
            "latex": null,
            "type": "table"
        },
        "TABREF14": {
            "text": "1: AUC, sensitivity, and specificity of FAIR4Cov models (DeiT-S/16 & wav2vec) on different combination of body sounds. (*) Ablation experiments on additional body sound combination for discussion in Section 5.3.3",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Appendix A. Self-attention fusion with only waveform inputs",
            "cite_spans": [],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}
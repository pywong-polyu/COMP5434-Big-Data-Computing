{"paper_id": "ce666fb2eb59f4bf52de669ae2ff84f519fa09a0", "metadata": {"title": "Sparse Matrix-Based HPC Tomography", "authors": [{"first": "Stefano", "middle": [], "last": "Marchesini", "suffix": "", "affiliation": {"laboratory": "", "institution": "Sigray, Inc", "location": {"addrLine": "5750 Imhoff Drive, Ste I", "postCode": "94520", "settlement": "Concord", "region": "CA", "country": "USA"}}, "email": "smarchesini@sigray.com"}, {"first": "Anuradha", "middle": [], "last": "Trivedi", "suffix": "", "affiliation": {"laboratory": "", "institution": "Virginia Polytechnic Institute and State University", "location": {"postCode": "24061", "settlement": "Blacksburg", "region": "VA", "country": "USA"}}, "email": ""}, {"first": "Pablo", "middle": [], "last": "Enfedaque", "suffix": "", "affiliation": {"laboratory": "Lawrence Berkeley National Laboratory", "institution": "", "location": {"addrLine": "1 Cyclotron Rd", "postCode": "94720", "settlement": "Berkeley", "region": "CA", "country": "USA"}}, "email": ""}, {"first": "Talita", "middle": [], "last": "Perciano", "suffix": "", "affiliation": {"laboratory": "Lawrence Berkeley National Laboratory", "institution": "", "location": {"addrLine": "1 Cyclotron Rd", "postCode": "94720", "settlement": "Berkeley", "region": "CA", "country": "USA"}}, "email": ""}, {"first": "Dilworth", "middle": [], "last": "Parkinson", "suffix": "", "affiliation": {"laboratory": "Advanced Light Source, Lawrence Berkeley National Laboratory", "institution": "", "location": {"addrLine": "1 Cyclotron Rd", "postCode": "94720", "settlement": "Berkeley", "region": "CA", "country": "USA"}}, "email": ""}]}, "abstract": [{"text": "Tomographic imaging has benefited from advances in X-ray sources, detectors and optics to enable novel observations in science, engineering and medicine. These advances have come with a dramatic increase of input data in the form of faster frame rates, larger fields of view or higher resolution, so high performance solutions are currently widely used for analysis. Tomographic instruments can vary significantly from one to another, including the hardware employed for reconstruction: from single CPU workstations to large scale hybrid CPU/GPU supercomputers. Flexibility on the software interfaces and reconstruction engines are also highly valued to allow for easy development and prototyping. This paper presents a novel software framework for tomographic analysis that tackles all aforementioned requirements. The proposed solution capitalizes on the increased performance of sparse matrixvector multiplication and exploits multi-CPU and GPU reconstruction over MPI. The solution is implemented in Python and relies on CuPy for fast GPU operators and CUDA kernel integration, and on SciPy for CPU sparse matrix computation. As opposed to previous tomography solutions that are tailor-made for specific use cases or hardware, the proposed software is designed to provide flexible, portable and highperformance operators that can be used for continuous integration at different production environments, but also for prototyping new experimental settings or for algorithmic development. The experimental results demonstrate how our implementation can even outperform state-of-theart software packages used at advanced X-ray sources worldwide.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Ever since Wilhelm R\u00f6ntgen shocked the world with a ghostly photograph of his wife's hand in 1896, the imaging power of X-rays has been exploited to help see the unseen. Their penetrating power allows us to view the internal structure of many objects. Because of this, X-ray sources are widely used in multiple imaging and microscopy experiments, e.g. in Computed Tomography (CT), or simply tomography. A tomography experiment measures a transmission absorption image (called radiograph) of a sample at multiple rotation angles. From 2D absorption images we can reconstruct a stack of slices (tomos in Greek) perpendicular to the radiographs measured, containing the 3D volumetric structure of the sample. Tomography is used in a variety of fields such as medical imaging, semiconductor technology, biology and materials science. Modern tomography instruments using synchrotron-based light sources can achieve measurement speeds of over 200 volumes per second using 40 kHz frame rate detectors [7] . Tomography can also be combined with microscopy techniques to achieve resolutions down to a single atom using electrons [17] . Its experimental versatility has also been exploited by combining it with spectroscopic techniques, to provide chemical, magnetic or even atomic orbital information about the sample.", "cite_spans": [{"start": 994, "end": 997, "text": "[7]", "ref_id": "BIBREF6"}, {"start": 1120, "end": 1124, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Introduction"}, {"text": "Nowadays, tomographic analysis software faces three main challenges. 1) The volume of the data is constantly increasing as X-ray sources become brighter and newer generation detectors increase their resolution and acquisition frame rate. 2) Instruments from different facilities (or even from the same one) present a variety of experimental settings that can be exclusive to said instrument, such as the geometry of the measurements, the data layout and format, noise levels, etc.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "3) New experimental use cases and algorithms are frequently explored and tested to accommodate new science requisites. These three requirements strongly force tomography analysis software to be HPC and flexible, both in terms of modularity and interfaces, as well as in hardware portability. Currently, TomoPy [9] and ASTRA [22] are the most popular solutions for tomographic reconstruction at multiple synchrotron and tabletop instruments. TomoPy is a Python-based open source framework optimized for performance using a C backend that can process a variety of data formats and algorithms. ASTRA is a tomography toolbox accelerated using both GPU and CPU computing and it is also available through TomoPy [16] . Although both solutions are highly optimized at different levels, they do not provide the level of flexibility required to be easily extendable by third parties regarding solver modifications or accessing specific operators.", "cite_spans": [{"start": 310, "end": 313, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 324, "end": 328, "text": "[22]", "ref_id": "BIBREF21"}, {"start": 706, "end": 710, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Introduction"}, {"text": "In this work we present a novel framework that focuses on providing multi-CPU and GPU acceleration with flexible operators and interfaces for both 1-step and iterative tomography reconstruction. The solution is based on Python 3 and relies on CuPy, mpi4py, SciPy and NumPy to provide transparent CPU/GPU computing and innocuous multiprocessing through MPI. The idea is to provide easy HPC support without compromising the solution lightweight so that development, integration and deployment is streamlined. The current operators are based on sparse matrix-vector multiplication (SpMV) computation which benefit from preexisting fast implementations on both CuPy and SciPy and provide faster reconstruction time than direct dense computation [10] . By minimizing code complexity, we can efficiently implement advanced iterative techniques [13, 19] that are not normally implemented for production, also due to their computational complexity; prior implementations could take up to a full day of a supercomputer to reconstruct a single tomogram [20] . The high level technologies and modular design employed in this project permits the proposed solution to be particularly flexible, both for exploratory uses (algorithm development or new experimental settings), and also in terms of hardware: we can scale the reconstruction from a single CPU, to a workstation using multiple CPU and GPU processors, to large distributed memory systems. The experimental results demonstrate how the proposed solution can reconstruct datasets of 68 GB in less than 5 s, even surpassing the performance of TomoPy's fastest reconstruction engine by 2.2X. This project is open source and available at [12] .", "cite_spans": [{"start": 741, "end": 745, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 838, "end": 842, "text": "[13,", "ref_id": "BIBREF12"}, {"start": 843, "end": 846, "text": "19]", "ref_id": "BIBREF18"}, {"start": 1043, "end": 1047, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 1679, "end": 1683, "text": "[12]", "ref_id": null}], "ref_spans": [], "section": "Introduction"}, {"text": "The paper is structured as follows: Sect. 2 overviews the main concepts regarding tomography reconstruction. Section 3 presents the proposed implementation with a detailed description of the challenges behind its design and the techniques employed, and Sect. 4 assesses its performance through experimental results. The last section summarizes this work.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Tomography is an imaging technique based on measuring a series of 2D radiographs of an object rotated at different angles relative to the direction of an X-ray beam (Fig. 1) . A radiograph of an object at a given angle is made up of line integrals (or projections). The collection of projections from different angles at the same slice of the object is called sinogram (2D); and the final reconstructed volume is called tomogram (3D), which is generally assembled from the independent reconstruction of each measured sinogram. Overview of a tomography experiment and reconstruction. A 3D sample is rotated at angles \u03b8 = 0, . . . , 180 \u2022 as X-rays produce 2D radiographs onto the detector. The collection of radiographs is combined to provide a sinogram for each detector row. Each sinogram is then processed to generate a 2D reconstructed slice, the entire collection of which can be assembled into a 3D tomogram.", "cite_spans": [], "ref_spans": [{"start": 165, "end": 173, "text": "(Fig. 1)", "ref_id": "FIGREF1"}], "section": "Tomography"}, {"text": "Physically, the collected data measures attenuation, which is the loss of flux through a medium. When the X-ray beams are parallel along the optical axis, the beam intensity impinging on the detector is given by: I \u03b8 (p, z) = I 0 e \u2212P \u03b8 (p,z) , P \u03b8 (p, z) = u (p cos \u03b8 \u2212 s sin \u03b8, p sin \u03b8 + s cos \u03b8, z) ds, where u(x, y, z) is the attenuation coefficient as a function of position x = (x, y, z) in the sample, I 0 is the input intensity collected without a sample, P \u03b8 is the projection after rotating \u03b8 around the z axis, and (p, z) are the coordinates on the detector that sample the data onto (n p \u00d7 n z ) detector pixels. The negative log of the normalized data provides the projection, also known as X-ray transform:", "cite_spans": [], "ref_spans": [], "section": "Tomography"}, {"text": "with element-wise log and division. The Radon transform (by H.A. Lorentz [2] ) at a fixed z is then given by the set of n \u03b8 projections for a series of angles \u03b8:", "cite_spans": [{"start": 73, "end": 76, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Tomography"}, {"text": "The tomography inverse problem can be expressed as follows:", "cite_spans": [], "ref_spans": [], "section": "Iterative Reconstruction Techniques"}, {"text": "To find u s.t. Radon(u) = \u2212 log(I/I 0 ).", "cite_spans": [], "ref_spans": [], "section": "Iterative Reconstruction Techniques"}, {"text": "The pseudo-inverse iRadon(\u2212 log(I/I 0 )) described below provides the fastest solution to this problem, and it is typically known as Filtered Back Projection in the literature. When implemented in Fourier space, the algorithm is referred to as non-uniform inverse FFT or gridrec.", "cite_spans": [], "ref_spans": [], "section": "Iterative Reconstruction Techniques"}, {"text": "The inverse problem can be under-determined and ill-conditioned when the number of angles is small. The equivalent least squares problem is:", "cite_spans": [], "ref_spans": [], "section": "Iterative Reconstruction Techniques"}, {"text": "where P = F \u2020 D 1/2 F is a preconditioning matrix, with D a diagonal matrix, and F denotes a 1D Fourier transform. Note that F does not need to be computed when using the Fubini-Radon operator (see below). The model-based problem is:", "cite_spans": [], "ref_spans": [], "section": "Iterative Reconstruction Techniques"}, {"text": "arg min", "cite_spans": [], "ref_spans": [], "section": "Iterative Reconstruction Techniques"}, {"text": "where \u00b7 w is a weighted norm to account for the noise model, P may incorporate streak noise removal [11] as well as preconditioning, Reg is a regularization term such as the Total Variation norm to account for prior knowledge about the sample, and \u03bc is a scalar parameter to balance the noise and prior models. Many algorithms have been proposed over the years including Filtered Back Projection (FBP), Simultaneous Iterative Reconstruction Technique (SIRT), Conjugate Gradient Least Squares (CGLS), and Total Variation (TV) [3] . FBP can be viewed as the first step of the preconditioned steepest descent when starting from 0. To solve any of these problems, one needs to compute a Radon transform and its (preconditioned) adjoint operation multiple times. (1), based on the Fourier slice theorem. The projection P \u03b8 (p, z) at a given angle \u03b8, height z, is related to an orthogonal 1D slice (different from the tomographic slice) by a Fourier transform:", "cite_spans": [{"start": 100, "end": 104, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 525, "end": 528, "text": "[3]", "ref_id": "BIBREF2"}], "ref_spans": [], "section": "Iterative Reconstruction Techniques"}, {"text": "). The slicing interpolation between the Cartesian and polar grid is the key step in this procedure and can be implemented with a sparse matrix operation.", "cite_spans": [], "ref_spans": [], "section": "Iterative Reconstruction Techniques"}, {"text": "One of the most efficient ways to perform Radon(u) is to use the Fourier central slice theorem by Fubini [5] . It consists on performing first a 2D Fourier transform, denoted by F of u, and then interpolating the transform onto a polar grid, to finally 1D inverse Fourier transforming the points along the radial lines:", "cite_spans": [{"start": 105, "end": 108, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "The Fubini-Radon Transform"}, {"text": "We will refer to this approach as the Fubini-Radon transform (Fig. 2 ). Numerically, the slicing interpolation between the Cartesian and polar grid in the Fubini-Radon transform is the key step in the procedure. It can be carried out using a gridding algorithm that maintains the desired accuracy with low computational complexity. The gridding algorithm essentially allows us to perform a non-uniform FFT. The projection operations require O(n 2 ) \u00b7 n \u03b8 arithmetic operations when computed directly using n = n p = n x = n y discretization of the line integrals, while the Fubini-Radon version requires O(n) \u00b7 n \u03b8 + O(n 2 log(n)) operations, where the first term is due to the slicing operation and the second term is due to the two dimensional FFT. For sufficiently large n \u03b8 , the Fubini-Radon transform requires fewer arithmetic operations than the standard Radon transform using projections. Early implementations on GPUs used ad hoc kernels to deal with atomic operations and load-balancing of the highly non-uniform distribution of the polar sampling points [11] , but became obsolete with new compute architectures. In this work we implement the slicing interpolation using a sparse matrix-vector multiplication. The SpMV and SpMM operations are level 2 and level 3 BLAS functions which have been heavily optimized (see e.g. [21] ) on numerous architectures for both CPUs and GPUs. ", "cite_spans": [{"start": 1065, "end": 1069, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 1333, "end": 1337, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [{"start": 61, "end": 68, "text": "(Fig. 2", "ref_id": "FIGREF2"}], "section": "The Fubini-Radon Transform"}, {"text": "The gridding operation requires the convolution between regular samples and a kernel to be calculated at irregular sample positions, and vice versa for the inverse gridding operation. To maintain high numerical accuracy and minimize the number of arithmetic operations, we want to limit the width of the convolution kernel. Small kernel width can be achieved by exploiting the finite sample dimensions (u(x) > 0 in the field of view) using a pair of functions k (x), k(x) so that k (x)k(x) = {1 if u(x) > 0, 0 otherwise}. By the convolution theorem:", "cite_spans": [], "ref_spans": [], "section": "Radon Transform by Sparse Matrix Multiplication"}, {"text": "where is the convolution operator, \u2022 denotes the Hadamard or elementwise product, K = Fk is called the convolution kernel and k the deapodization factor. We choose K with finite width k w , and the deapodization factor can be pre-computed as k = {(F * K(x)) \u22121 if u(x) > 0, 0 otherwise}. Several kernel functions have been proposed and employed in the literature, including truncated Gaussian, Kaiser-Bessel, or an interpolation kernel to minimize the worst-case approximation error over all signals of unit norm [6] .", "cite_spans": [{"start": 513, "end": 516, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Radon Transform by Sparse Matrix Multiplication"}, {"text": "The Fubini-Radon transform operator and its pseudo-inverse iRadon can be expressed using a sparse matrix (Fig. 3) to perform the interpolation [14] :", "cite_spans": [{"start": 143, "end": 147, "text": "[14]", "ref_id": "BIBREF13"}], "ref_spans": [{"start": 105, "end": 113, "text": "(Fig. 3)", "ref_id": "FIGREF3"}], "section": "Radon Transform by Sparse Matrix Multiplication"}, {"text": "where bold indicates 2D vectors such as p = (p x , p y ), x = (x, y), and D is a diagonal matrix to account for the density of sampling points in the polar grid. Fourier transforms and multiplication of S \u2208 C M \u00d7N with a sinogram in vector form (1 \u00d7 N , N = n \u03b8 \u00b7 n p ), (sparse matrix-vector multiplication or SpMV) produces a tomogram of dimension (1 \u00d7 M \u2192 n y \u00d7 n x ); multiplication with a stack of sinograms (sparse matrix-matrix multiplication or SpMM) produces the 3D tomogram(n z , n y , n x )). The diagonal matrix D can incorporate standard filters such as the Ram-Lak ramp, Shepp-Logan, Hamming, or a minimum residual filter based on the data itself [15] . We can also employ the density filter solution that minimizes the difference with the impulse response (a constant 1 in Fourier space) as arg min Dv SD v \u2212 1 nx\u00b7ny , with D v as the vector of the diagonal elements of D. Note that in this case, the matrix D = Diag(D v ) can be incorporated directly into S for better performance.", "cite_spans": [{"start": 661, "end": 665, "text": "[15]", "ref_id": "BIBREF14"}], "ref_spans": [], "section": "Radon Transform by Sparse Matrix Multiplication"}, {"text": "The row indices and values of the sparse matrix are related to the coordinates where the kernel windows are added up on the output 2D image as ", "cite_spans": [], "ref_spans": [], "section": "Radon Transform by Sparse Matrix Multiplication"}, {"text": "where \u2295s is the broadcasting sum with the window stencil reshaped to dimensions (1, 1, k w , k w ), map i\u2190p (p) = rint(p x ) * n y + rint(p y ) is the lexicographical mapping from 2D to 1D index, K is the kernel function and frac[p] = p\u2212round[p] is the decimal part, and \u22971 represents the Kronecker product with the unit vector 1 (kw) 2 = [1, 1, . . . , 1], for a window of width k w (see Fig. 3 ). S has at most nnz = n \u03b8 \u00b7 n p \u00b7 k 2 w non-zero elements, and the sparsity ratio is at most", "cite_spans": [], "ref_spans": [{"start": 389, "end": 395, "text": "Fig. 3", "ref_id": "FIGREF3"}], "section": "Radon Transform by Sparse Matrix Multiplication"}, {"text": "nx\u00b7ny , or (kw\u22121) 2 nx\u00b7ny when the kernel is set to 0 at the borders. We account for a possible shift c of the rotation axis and avoid FFTshifts in the tomogram and radon spaces by applying a phase ramp as \u03a6 s (p, c), with c = np 2 when the projected rotation axis matches the central column of the detector. For better performance, FFTshifts in Fourier space are incorporated in the sparse matrix by applying an FFTshift of the p coordinate, and by using a +1 \u22121... \u22121 +1... checkerboard pattern in the deapodization factor k .", "cite_spans": [], "ref_spans": [], "section": "Radon Transform by Sparse Matrix Multiplication"}, {"text": "The Fubini-Radon transform operates independently on each tomo and sinogram, so we can aggregate sinograms into chunks and distribute them over multiple processes operating in parallel. Denoising methods that operate across multiple slices can be handled using halos with negligible reduction in the final Signal-to-Noise-Ratio (SNR), while reducing or avoiding MPI neighborhood communication.", "cite_spans": [], "ref_spans": [], "section": "Parallel Workflow"}, {"text": "Pairs of sinograms are combined into a complex sinogram which is processed simultaneously, by means of complex arithmetic operations, and is split back at the end of the reconstruction. We can limit the amount of chunks assigned to each process in order to avoid memory constraints. Then, when the data has more slices than what can be handled by all processes, it is divided up ensuring that each process operates on similar size chunks of data and all processes loop through the data. When the number of slices cannot be distributed equally to all processes, only the last loop chunk is split unequally with the last MPI ranks receiving one less slice than the first ones.", "cite_spans": [], "ref_spans": [], "section": "Parallel Workflow"}, {"text": "The setup stage uses the experimental parameters of the data (number of pixels, slices and angles) and the choice of filters and kernels to compute the sparse matrix entries, deapodization factors and slice distribution across MPI ranks. During the setup stage, the output tomogram is initialized as either a memory mapped file, a shared memory window (whenever possible) or an array in rank-0 to gather all the results.", "cite_spans": [], "ref_spans": [], "section": "Parallel Workflow"}, {"text": "Several matrix formats and conversion routines exist to perform the SpMV operation efficiently. In our implementation, the sparse matrix entries are first computed in Coordinate list (COO) format which contains a list of (row, column, value) tuples. Zero-valued and out-of-bound entries are removed, and then the sparse matrix is converted to compressed sparse row (CSR) format, where the entries are sorted by column and row, and the row index is replaced by a compressed pointer. The sparse matrix and its transpose are stored separately and incorporate preconditioning filters and phase ramps to avoid all FFTshifts. The CSR matrix entries are saved in a cache file for reuse, with a hash function derived from the experimental parameters and filters to identify the corresponding sparse matrix from file. The FFT plans are computed at the first application and stored in memory until the reconstruction is restarted. When the data is loaded from file and/or the results are saved to disk, parallel processes pre-load the input to memory or flush the output from a double buffer as the next section of the data is processed.", "cite_spans": [], "ref_spans": [], "section": "Parallel Workflow"}, {"text": "Our implementation uses cuSPARSE and MKL libraries for the SpMV and FFT operations, MPI for distributed parallelism through shared memory when available, or scatterv/gatherv and non-blocking double buffers for I/O and MPI operations. All these libraries are accessed through Python, NumPy, CuPy, SciPy and mpi4py; we also rely on h5py or tifffile modules to interface with data files. This framework also provides the capability to call TomoPy and ASTRA solvers on distributed architectures using MPI.", "cite_spans": [], "ref_spans": [], "section": "Parallel Workflow"}, {"text": "We used this framework to implement the most popular algorithms described in Sect. 2.1, namely FBP, SIRT, CGLS, and TV. To achieve high throughput, our implementation of SIRT uses the Hamming preconditioning and the BB-step acceleration [1] , which provides 10-fold convergence rate speedup and makes it comparable to the conjugate gradient method but with fewer reductions and lower memory footprint. The CGLS implementation is based on the conjugate gradient squared method [18] , and the TV denoising employs the split-Bregman [8] technique.", "cite_spans": [{"start": 237, "end": 240, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 476, "end": 480, "text": "[18]", "ref_id": "BIBREF17"}, {"start": 530, "end": 533, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Parallel Workflow"}, {"text": "The experimental evaluation presented herein is two-fold. We assess the performance of our implementation on both shared and distributed memory systems and on CPU and GPU architectures, and we also study how it compares to TomoPy, the state-of-the-art solution on X-ray sources, in terms of run time and quality of reconstruction.", "cite_spans": [], "ref_spans": [], "section": "Experiments and Results"}, {"text": "We employ two different datasets for this analysis. The first one is a simulated Shepp-Logan phantom generated using TomoPy, with varying sizes to analyze the performance and scalability of the solution. The second one is an experimental dataset generated at Lawrence Berkeley National Laboratory's Advanced Light Source during an outreach program with local schools out of a bread-crumb inserted at the micro-tomography beamline 8.3.2. The specifics of the experiments were: 25 keV X-rays, pixel size 0.65 \u00b5, 200 ms per image and 1313 angles over 180 \u2022 . The detector consisted of 20 \u00b5 LuAG:Ce scintillator and Optique Peter lens system with Olympus 10x lens, and PCO.edge sCMOS detector. The total experiment time, including camera readout/overhead, was around 6 min, generating a sinogram stack of dimension (n z , n \u03b8 , n p ) = (2160, 1313, 3620).", "cite_spans": [], "ref_spans": [], "section": "Experiments and Results"}, {"text": "We use two different systems for this evaluation. The first is the Cori supercomputer (Cori.nersc.gov), a Cray XC40 system comprised of 2,388 nodes containing two 2.3 GHz 16-core Intel Haswell processors and 128 GB DDR4 2133 MHz memory, and 9,688 nodes containing a single 68-core 1.4 GHz Intel Xeon Phi 7250 (Knights Landing) processor and 96 GB DDR4 2400 GHz memory. Cori also provides 18 GPU nodes, where each node contains two sockets of 20-core Intel Xeon Gold 6148 2.40 GHz, 384 GB DDR4 memory, 8 NVIDIA V100 GPUs (each with 16 GB HBM2 memory). For our experiments, we use the Haswell processor and the GPU nodes. 1 The second system employed is CAM, a single node dual socket Intel Xeon CPU E5-2683 v4 @ 2.10 GHz with 16 cores 32 threads each, 128 GB DDR4 and 4 NVIDIA K80 (dual GPU with 12 GB of GDDR5 memory each).", "cite_spans": [{"start": 620, "end": 621, "text": "1", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Experiments and Results"}, {"text": "The first experiment reports the performance results and scaling studies of our iRadon implementation and of TomoPy-Gridrec, when executed on both Cori and CAM, over the simulated dataset. The primary objective is to compare their scalability using both CPUs and GPUs. We executed both algorithms at varying levels of concurrency using a simulation size of (2048, 2048, 2048). On Cori, we used up to 8 Haswell nodes in a distributed fashion, only using physical cores in each node. On CAM, we ran all the experiments on a single node, dual socket. The speedup plots are shown in Fig. 4 . The reported speedup is defined as S(n, p) = T * (n) T (n,p) where T (n, p) is the time it takes to run the parallel algorithm on p processes with an input size of n, and T * (n) is the time for the best serial algorithm on the same input.", "cite_spans": [], "ref_spans": [{"start": 579, "end": 585, "text": "Fig. 4", "ref_id": "FIGREF5"}], "section": "Experiments and Results"}, {"text": "First, we notice that the iRadon algorithm running on GPU has a super-linear speedup on both platforms. This is unusual in general, however possible in some cases. One known reason is the cache effect, i.e. the number of GPU changes, and so does the size of accumulated caches from different GPUs. Specifically, in a multi-GPU implementation, super-linear speedup can happen due to configurable cache memory. In the CPU case, we see a close to linear speedup. On CAM, the performance decreases because of MPI oversubscribe, i.e. when the number of processes is higher than the actual number of processors available.", "cite_spans": [], "ref_spans": [], "section": "Experiments and Results"}, {"text": "Finally, there is a clear difference in speedup results compared to the TomoPy-Gridrec implementation. We believe that the main difference here is due to the fact that TomoPy only uses a multithreaded implementation with OpenMP, while our implementation relies on MPI. For the purpose of comparison with our implementation, we use MPI to run TomoPy across nodes.", "cite_spans": [], "ref_spans": [], "section": "Experiments and Results"}, {"text": "We also evaluate our implementation by running multiple simulations with a fixed number of angles and rays (2048) and varying number of slices (128-2048) on 64 CPUs and 8 GPUs. Performance results in slices per second are shown in Fig. 5 . One can notice that the GPU implementation of iRadon presents an increase in performance when the number of GPU increases. This is a known behavior of GPU performance when the problem is too small compared to the capabilities of the GPU, and the device is not completely saturated with data, not taking full advantage of the parallelized computations. For both platforms, our CPU implementation of iRadon performs significantly better than TomoPy.", "cite_spans": [], "ref_spans": [{"start": 231, "end": 237, "text": "Fig. 5", "ref_id": "FIGREF6"}], "section": "Experiments and Results"}, {"text": "In terms of raw execution time, TomoPy-Gridrec outperforms our iRadon implementation by a factor of 2.3\u00d7 when running on a single CPU on Cori. On the other hand, the iRadon execution time using 256 CPU cores on Cori is 4.11 s, outperforming TomoPy by a factor of 2.2\u00d7. Our iRadon version also ourperforms TomoPy by a factor of 1.9\u00d7 using 32 cores. Our GPU implementation of iRadon runs in 1.55 s using 16 V100 GPUs, which improves the CPU implementation (1 core) by a factor of 600\u00d7, and runs 2.6\u00d7 faster compared with 256 CPU cores. Finally, our GPU version of iRadon runs 7.5\u00d7 faster (using 2 GPUs) than TomoPy (using 32 CPUs), which could be considered the level of hardware resources accessible to average users. The last experiment focuses on the analysis of the different algorithms implemented in this work, in terms of execution time and reconstruction quality. Figure 6 shows the reconstruction of 128 slices of the bread-crumb experimental dataset on CAM (32 CPUs and 8 GPUs), for 3 different implemented algorithms: iRadon, SIRT, and TV, and also for TomoPy-Gridrec and TomoPy-SIRT. All iterative implementations (SIRT and TV) run for 10 iterations. Our iRadon implementation presents the best execution time for CPU (9 s), while on GPU, it runs in 4 s. Our SIRT implementation outperforms TomoPy's by a factor of 175\u00d7. We report the SNR values (and corresponding execution times) of our implemented algorithms in Table 1 , using a simulation dataset of size (256, 1024, 1024). We can observe how both SIRT and TV present the best results in terms of reconstruction quality. Figure 7 shows a reconstructed slice of the bread-crumb data using the iRadon and the TV algorithms, along with a zoomed-in region of the same slice. The difference in reconstruction quality is minor in this case due to the dataset presenting high contrast and a large number of angles. Still, in the zoomed-out image we can appreciate higher contrast fine features on the TV reconstruction. Sparser datasets would be analyzed in the future to assess the performance of TV and iterative solutions on more challenging scenarios.", "cite_spans": [], "ref_spans": [{"start": 870, "end": 878, "text": "Figure 6", "ref_id": "FIGREF7"}, {"start": 1425, "end": 1432, "text": "Table 1", "ref_id": "TABREF0"}, {"start": 1586, "end": 1594, "text": "Figure 7", "ref_id": "FIGREF8"}], "section": "Experiments and Results"}, {"text": "It is important to remark that all the execution times presented in this section refer to the solver portion of the calculations. When running the TV algorithm on the complete bread-crumb data using 8 GPUs on CAM, for example, the solver time takes approximately 78% of the total execution time (44.82 min). Most of the remaining time is taken by I/O (18%) and gather (2%).", "cite_spans": [], "ref_spans": [], "section": "Experiments and Results"}, {"text": "This paper presents a novel solution for tomography analysis based on fast SpMV operators. The proposed software is implemented in Python relying on CuPy, SciPy and MPI for high performance and flexible CPU and GPU reconstruction. As opposed to existing solutions, the software presented tackles the main requirements existing in tomography analysis: it can run over most hardware setups and can be easily adapted and extended into new solvers and techniques, while greatly simplifying deployment at new beamlines. The experimental results of this work demonstrate the remarkable performance of the solution, being able to iteratively reconstruct datasets of 68 GB in less than 5 s using 256 cores and in less than 2 s using 16 GPUs. For the simulated datasets analyzed, the proposed software outperforms the reference tomography solution by a factor of up to 2.7\u00d7, while running on CPU. When reconstructing the experimental data, our implementation of the SIRT algorithm outperforms TomoPy by a factor of 175\u00d7 running on CPU. The code of this project is also open source and available at [12] .", "cite_spans": [{"start": 1089, "end": 1093, "text": "[12]", "ref_id": null}], "ref_spans": [], "section": "Conclusions"}, {"text": "As future work, we will employ CPU and GPU co-processing, Block Compressed Row (BSR) format and sparse matrix-dense matrix multiplication (SpMM) to enhance the throughout of the solution. We will also explore the Toeplitz approach [14] , which permits combining the Radon transform with its adjoint into a single operation, while also avoiding the forward and backward 1D FFTs. Half-precision arithmetic is also probably sufficient to deal with experimental data with photon counting noise obtained with 16 bits detectors and can further improve performance by up to an order of magnitude using tensor cores. Generalization to cone-beam, fan beam or helical scan geometries using generalized Fourier slice methods [23] will also be subject of future work. We will also explore the implementation of advanced denoising schemes based on wavelets or BM3D [4] , combining the operators presented in this work.", "cite_spans": [{"start": 231, "end": 235, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 714, "end": 718, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 852, "end": 855, "text": "[4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Conclusions"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Two-point step size gradient methods", "authors": [{"first": "J", "middle": [], "last": "Barzilai", "suffix": ""}, {"first": "J", "middle": ["M"], "last": "Borwein", "suffix": ""}], "year": 1988, "venue": "IMA J. Numer. Anal", "volume": "8", "issn": "1", "pages": "141--148", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "On the propagation of light in a biaxial crystal about a midpoint of oscillation", "authors": [{"first": "H", "middle": [], "last": "Bockwinkel", "suffix": ""}], "year": 1906, "venue": "Verh. Konink Acad. V. Wet. Wissen. Natur", "volume": "14", "issn": "636", "pages": "", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "A generalized Gaussian image model for edge-preserving map estimation", "authors": [{"first": "C", "middle": [], "last": "Bouman", "suffix": ""}, {"first": "K", "middle": [], "last": "Sauer", "suffix": ""}], "year": 1993, "venue": "IEEE Trans. Image Process", "volume": "2", "issn": "3", "pages": "296--310", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Image denoising by sparse 3-D transform-domain collaborative filtering", "authors": [{"first": "K", "middle": [], "last": "Dabov", "suffix": ""}, {"first": "A", "middle": [], "last": "Foi", "suffix": ""}, {"first": "V", "middle": [], "last": "Katkovnik", "suffix": ""}, {"first": "K", "middle": [], "last": "Egiazarian", "suffix": ""}], "year": 2007, "venue": "IEEE Trans. Image Process", "volume": "16", "issn": "8", "pages": "2080--2095", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Tomographic reconstruction with arbitrary directions", "authors": [{"first": "M", "middle": [], "last": "Davison", "suffix": ""}, {"first": "F", "middle": [], "last": "Grunbaum", "suffix": ""}], "year": 1981, "venue": "Commun. Pure Appl. Math", "volume": "34", "issn": "1", "pages": "77--119", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Nonuniform fast Fourier transforms using min-max interpolation", "authors": [{"first": "J", "middle": ["A"], "last": "Fessler", "suffix": ""}, {"first": "B", "middle": ["P"], "last": "Sutton", "suffix": ""}], "year": 2003, "venue": "IEEE Trans. Signal Process", "volume": "51", "issn": "2", "pages": "560--574", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Using x-ray tomoscopy to explore the dynamics of foaming metal", "authors": [{"first": "F", "middle": [], "last": "Garc\u00eda-Moreno", "suffix": ""}], "year": 2019, "venue": "Nature Commun", "volume": "10", "issn": "1", "pages": "1--9", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "The split Bregman method for L1-regularized problems", "authors": [{"first": "T", "middle": [], "last": "Goldstein", "suffix": ""}, {"first": "S", "middle": [], "last": "Osher", "suffix": ""}], "year": 2009, "venue": "SIAM J. Imaging Sci", "volume": "2", "issn": "2", "pages": "323--343", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "TomoPy: a framework for the analysis of synchrotron tomographic data", "authors": [{"first": "D", "middle": [], "last": "G\u00fcrsoy", "suffix": ""}, {"first": "F", "middle": [], "last": "De Carlo", "suffix": ""}, {"first": "X", "middle": [], "last": "Xiao", "suffix": ""}, {"first": "C", "middle": [], "last": "Jacobsen", "suffix": ""}], "year": 2014, "venue": "J. Synchrotron Radiat", "volume": "21", "issn": "5", "pages": "1188--1193", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Selection of a convolution function for Fourier inversion using gridding (computerised tomography application)", "authors": [{"first": "J", "middle": ["I"], "last": "Jackson", "suffix": ""}, {"first": "C", "middle": ["H"], "last": "Meyer", "suffix": ""}, {"first": "D", "middle": ["G"], "last": "Nishimura", "suffix": ""}, {"first": "A", "middle": [], "last": "Macovski", "suffix": ""}], "year": 1991, "venue": "IEEE Trans. Med. Imaging", "volume": "10", "issn": "3", "pages": "473--478", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Compressive phase contrast tomography", "authors": [{"first": "F", "middle": [], "last": "Maia", "suffix": ""}], "year": 2010, "venue": "p. 78000F. SPIE, International Society for Optics and Photonics", "volume": "7800", "issn": "", "pages": "", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Model-based iterative reconstruction for synchrotron x-ray tomography", "authors": [{"first": "K", "middle": ["A"], "last": "Mohan", "suffix": ""}, {"first": "S", "middle": [], "last": "Venkatakrishnan", "suffix": ""}, {"first": "L", "middle": ["F"], "last": "Drummy", "suffix": ""}, {"first": "J", "middle": [], "last": "Simmons", "suffix": ""}, {"first": "D", "middle": ["Y"], "last": "Parkinson", "suffix": ""}, {"first": "C", "middle": ["A"], "last": "Bouman", "suffix": ""}], "year": 2014, "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "volume": "", "issn": "", "pages": "6909--6913", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "gNUFFTW: auto-tuning for high-performance GPU-accelerated nonuniform fast Fourier transforms", "authors": [{"first": "T", "middle": [], "last": "Ou", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Improving filtered backprojection reconstruction by data-dependent filtering", "authors": [{"first": "D", "middle": ["M"], "last": "Pelt", "suffix": ""}, {"first": "K", "middle": ["J"], "last": "Batenburg", "suffix": ""}], "year": 2014, "venue": "IEEE Trans. Image Process", "volume": "23", "issn": "11", "pages": "4750--4762", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Integration of tomopy and the astra toolbox for advanced processing and reconstruction of tomographic synchrotron data", "authors": [{"first": "D", "middle": ["M"], "last": "Pelt", "suffix": ""}, {"first": "D", "middle": [], "last": "G\u00fcrsoy", "suffix": ""}, {"first": "W", "middle": ["J"], "last": "Palenstijn", "suffix": ""}, {"first": "J", "middle": [], "last": "Sijbers", "suffix": ""}, {"first": "F", "middle": [], "last": "De Carlo", "suffix": ""}, {"first": "K", "middle": ["J"], "last": "Batenburg", "suffix": ""}], "year": 2016, "venue": "J. Synchrotron Radiat", "volume": "23", "issn": "3", "pages": "842--849", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Electron tomography at 2.4-\u00e5ngstr\u00f6m resolution", "authors": [{"first": "M", "middle": [], "last": "Scott", "suffix": ""}], "year": 2012, "venue": "Nature", "volume": "483", "issn": "7390", "pages": "", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "CGS, a fast Lanczos-type solver for nonsymmetric linear systems", "authors": [{"first": "P", "middle": [], "last": "Sonneveld", "suffix": ""}], "year": 1989, "venue": "SIAM J. Sci. Stat. Comput", "volume": "10", "issn": "1", "pages": "36--52", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Plug-and-play priors for model based reconstruction", "authors": [{"first": "S", "middle": ["V"], "last": "Venkatakrishnan", "suffix": ""}, {"first": "C", "middle": ["A"], "last": "Bouman", "suffix": ""}, {"first": "B", "middle": [], "last": "Wohlberg", "suffix": ""}], "year": 2013, "venue": "IEEE Global Conference on Signal and Information Processing", "volume": "", "issn": "", "pages": "945--948", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Making advanced scientific algorithms and big scientific data management more accessible", "authors": [{"first": "S", "middle": [], "last": "Venkatakrishnan", "suffix": ""}], "year": 2016, "venue": "Electron. Imaging", "volume": "", "issn": "19", "pages": "1--7", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Optimization of sparse matrix-vector multiplication on emerging multicore platforms", "authors": [{"first": "S", "middle": [], "last": "Williams", "suffix": ""}, {"first": "L", "middle": [], "last": "Oliker", "suffix": ""}, {"first": "R", "middle": [], "last": "Vuduc", "suffix": ""}, {"first": "J", "middle": [], "last": "Shalf", "suffix": ""}, {"first": "K", "middle": [], "last": "Yelick", "suffix": ""}, {"first": "J", "middle": [], "last": "Demmel", "suffix": ""}], "year": 2007, "venue": "SC 2007: Proceedings of the 2007 ACM/IEEE Conference on Supercomputing", "volume": "", "issn": "", "pages": "1--12", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "Fast and flexible X-ray tomography using the ASTRA toolbox", "authors": [{"first": "W", "middle": [], "last": "Van Aarle", "suffix": ""}], "year": 2016, "venue": "Opt. Express", "volume": "24", "issn": "22", "pages": "25129--25147", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "Generalized Fourier slice theorem for cone-beam image reconstruction", "authors": [{"first": "S", "middle": ["R"], "last": "Zhao", "suffix": ""}, {"first": "D", "middle": [], "last": "Jiang", "suffix": ""}, {"first": "K", "middle": [], "last": "Yang", "suffix": ""}, {"first": "K", "middle": [], "last": "Yang", "suffix": ""}], "year": 2015, "venue": "J. X-ray Sci. Technol", "volume": "23", "issn": "2", "pages": "157--188", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Springer Nature Switzerland AG 2020 V. V. Krzhizhanovskaya et al. (Eds.): ICCS 2020, LNCS 12137, pp. 248-261, 2020. https://doi.org/10.1007/978-3-030-50371-0_18", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Fig. 1. Overview of a tomography experiment and reconstruction. A 3D sample is rotated at angles \u03b8 = 0, . . . , 180 \u2022 as X-rays produce 2D radiographs onto the detector. The collection of radiographs is combined to provide a sinogram for each detector row. Each sinogram is then processed to generate a 2D reconstructed slice, the entire collection of which can be assembled into a 3D tomogram.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Depiction of the Fubini-Radon transform", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Sparse matrix S \u2208 C M \u00d7N (right) representation of a set of convolutional kernel windows of width kw = 3 with stencils s = (sx, sy), sx = (\u22121, 0, 1), sy = s T x (top), centered around a set of coordinates pi = pi(cos \u03b8i, sin \u03b8i) + 1 2 (nx, ny) of each input point of the sinogram on the output image (left).", "latex": null, "type": "figure"}, "FIGREF4": {"text": "n y ), with kernel window stencil s = (s x , s y ), and s x = (\u22121, 0, 1), s y = s T x (for k w = 3). The column index is simply given by the consecutive sequence of natural numbers N N 1 = [0, 1, . . . , N \u2212 1], repeated k 2 w times, and the row index and value are given by:", "latex": null, "type": "figure"}, "FIGREF5": {"text": "Speedup of iRadon and TomoPy-Gridrec algorithms on CPU Cori (left), CPU CAM (center) and GPU Cori and CAM (right) for a (nz, n \u03b8 , np) = (2048, 2048, 2048) simulation. The horizontal axis is the concurrency level and the vertical axis measures the speedup.", "latex": null, "type": "figure"}, "FIGREF6": {"text": "Performance on Cori (left) and CAM (right), for varying sizes of simulated datasets as (nz, n \u03b8 , np) = (N, 2048, 2048), running both the iRadon and TomoPy-Gridrec algorithms. The horizontal axis is the number of slices (sinograms) of the input data, and the vertical axis measures performance as slices reconstructed per second. CPU experiments employ 64 processes and GPU experiments use 8 on CAM and 16 on Cori.", "latex": null, "type": "figure"}, "FIGREF7": {"text": "Comparison of execution time (seconds in log10 scale) for different algorithms, reconstructing 128 slices of the bread-crumb dataset on CAM. SIRT and TV run for 10 iterations.", "latex": null, "type": "figure"}, "FIGREF8": {"text": "Example of reconstructed slice from the bread-crumb dataset using the iRadon and the TV algorithms. This visual result shows a better quality of reconstruction obtained using iRadon.", "latex": null, "type": "figure"}, "TABREF0": {"text": "Execution times for CPU and GPU (minutes) and SNR values for each reconstruction algorithm implemented. SNR is computed for a simulation of size (256, 1024, 1024).", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Alg. </td><td>CPU </td><td>GPU </td><td>SNR\n</td></tr><tr><td>iRadon </td><td>0.14 </td><td>0.07 </td><td>3.51\n</td></tr><tr><td>SIRT </td><td>3.13 </td><td>0.19 </td><td>17.11\n</td></tr><tr><td>TV </td><td>57.8 </td><td>2.07 </td><td>17.78\n</td></tr></table></body></html>"}}, "back_matter": []}
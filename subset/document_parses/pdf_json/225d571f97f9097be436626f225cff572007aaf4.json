{"paper_id": "225d571f97f9097be436626f225cff572007aaf4", "metadata": {"title": "Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph Link Prediction", "authors": [{"first": "Jinheon", "middle": [], "last": "Baek", "suffix": "", "affiliation": {}, "email": "jinheon.baek@kaist.ac.kr"}, {"first": "Dong", "middle": ["Bok"], "last": "Lee", "suffix": "", "affiliation": {}, "email": ""}, {"first": "Sung", "middle": ["Ju"], "last": "Hwang", "suffix": "", "affiliation": {}, "email": "sjhwang82@kaist.ac.kr"}, {"first": "", "middle": [], "last": "Kaist", "suffix": "", "affiliation": {}, "email": ""}, {"first": "", "middle": [], "last": "Aitrics", "suffix": "", "affiliation": {}, "email": ""}, {"first": "South", "middle": [], "last": "Korea", "suffix": "", "affiliation": {}, "email": ""}]}, "abstract": [{"text": "Many practical graph problems, such as knowledge graph construction and drugto-drug interaction, require to handle multi-relational graphs. However, handling real-world multi-label graphs with Graph Neural Networks (GNNs) is often challenging due to their evolving nature, where new entities (nodes) can emerge over time. Moreover, newly emerged entities often have few links, which makes the learning even more difficult. Motivated by this challenge, we introduce a realistic problem of few-shot out-of-graph link prediction, where we not only predict the links between the seen and unseen nodes as in a conventional out-of-knowledge link prediction but also between the unseen nodes, with only few edges per node. We tackle this problem with a novel transductive meta-learning framework which we refer to as Graph Extrapolation Networks (GEN). GEN meta-learns both the node embedding network for inductive inference (seen-to-unseen) and the link prediction network for transductive inference (unseen-to-unseen). For transductive link prediction, we further propose a stochastic embedding layer to model uncertainty in the link prediction between unseen entities. We validate our model on multiple benchmark datasets for knowledge graph link prediction and drug-to-drug interaction prediction. The results show that our model significantly outperforms relevant baselines for out-of-graph link prediction tasks.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}, {"text": "Preprint. Under review.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Graphs have strong expressive power to represent structured data, as it can model data into a set of nodes (objects) and edges (relations). To exploit the graph structured data which works on a non-Euclidean domain, several recent works propose graph-based neural architectures, referred to as Graph Neural Networks (GNNs) [9, 22] . While early work mostly deals with simple graphs with unlabeled edges, recently proposed relation-aware GNNs [38, 39] consider multi-relational graphs with labels and directions on the edges. These multi-relational graphs expand the application of GNNs to more real-world domains such as social networks modeling [18] , natural language understanding [25] , modeling protein structure [13] , and drug-to-drug interaction prediction [60] .", "cite_spans": [{"start": 323, "end": 326, "text": "[9,", "ref_id": "BIBREF8"}, {"start": 327, "end": 330, "text": "22]", "ref_id": "BIBREF21"}, {"start": 442, "end": 446, "text": "[38,", "ref_id": "BIBREF37"}, {"start": 447, "end": 450, "text": "39]", "ref_id": "BIBREF38"}, {"start": 646, "end": 650, "text": "[18]", "ref_id": "BIBREF17"}, {"start": 684, "end": 688, "text": "[25]", "ref_id": "BIBREF24"}, {"start": 718, "end": 722, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 765, "end": 769, "text": "[60]", "ref_id": "BIBREF60"}], "ref_spans": [], "section": "Introduction"}, {"text": "Among multi-relational graphs, Knowledge Graphs (KGs), which represent knowledge bases (KBs) such as Freebase [3] and WordNet [27] , get the most attention. They represent entities as nodes and relations among the entities as edges, in the form of a triplet: (head entity, relation, tail entity) (e.g. (Louvre museum, is located in, Paris)). Although knowledge graphs in general contain a huge amount of triplets, they are well known to be highly incomplete [28] . Therefore, automatically completing knowledge graphs, which is known as the link prediction task, is a practically important problem for KGs. Prior work tackles this problem, i.e. inferring missing triplets, by learning embedding of entities and relations from existing triplets, achieving impressive performances [5, 57, 10, 31, 30] .", "cite_spans": [{"start": 110, "end": 113, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 126, "end": 130, "text": "[27]", "ref_id": "BIBREF26"}, {"start": 458, "end": 462, "text": "[28]", "ref_id": "BIBREF27"}, {"start": 779, "end": 782, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 783, "end": 786, "text": "57,", "ref_id": "BIBREF57"}, {"start": 787, "end": 790, "text": "10,", "ref_id": "BIBREF9"}, {"start": 791, "end": 794, "text": "31,", "ref_id": "BIBREF30"}, {"start": 795, "end": 798, "text": "30]", "ref_id": "BIBREF29"}], "ref_spans": [], "section": "Introduction"}, {"text": "LAN [Wang et al. 19] Seen Unseen Ours Figure 1 : Concept (Left): An illustration of out-of-graph link prediction for emerging entities. Blue dotted arrows denote inferred relationships between seen and unseen entities, and red dotted arrows denote inferred relationships between unseen entities. (Center): An illustration of our meta-learning framework for Out-Of-Graph link prediction task. Orange arrows denote the support (training) set S and green dotted arrows denote the query (test) set Q. Visualizations of the learned embeddings (Right): Our transductive GEN embeds the unseen entities on the manifold of seen entities, while the baseline [49] embeds them off-manifold. Despite this success, the link prediction for KGs in real-world scenarios remains to be challenging for a couple of reasons. First, knowledge graphs dynamically evolve over time, rather than staying static. Shi and Weninger [40] report that around 200 new entities emerge every day. Predicting links on these emerging entities pose a new challenge, especially when predicting the links between the emerging entities themselves. Moreover, real-world KGs generally exhibit long-tail distribution, where a large portion of the entities have only a few triplets to train (See Figure 2 ). The embedding-based methods, however, usually assume that a sufficient amount of associative triplets exist for training, and cannot embed unseen entities. Thus they are highly suboptimal for learning and inference on evolving real-world graphs.", "cite_spans": [{"start": 4, "end": 20, "text": "[Wang et al. 19]", "ref_id": null}, {"start": 648, "end": 652, "text": "[49]", "ref_id": "BIBREF48"}, {"start": 903, "end": 907, "text": "[40]", "ref_id": "BIBREF39"}], "ref_spans": [{"start": 38, "end": 46, "text": "Figure 1", "ref_id": null}, {"start": 1251, "end": 1259, "text": "Figure 2", "ref_id": "FIGREF0"}], "section": "Introduction"}, {"text": "Motivated by the limitations of existing approaches, we introduce a realistic problem of few-Shot Out-Of-Graph (OOG) link prediction for emerging entities. In this task, we not only predict the links between seen and unseen entities but also between the unseen entities themselves (Figure 1 , left). To this end, we propose a novel meta-learning framework for OOG link prediction, which we refer to as Graph Extrapolation Networks (GENs). GENs are meta-learned to extrapolate the knowledge from seen to unseen entities and transfer knowledge from entities with many links to few links.", "cite_spans": [], "ref_spans": [{"start": 281, "end": 290, "text": "(Figure 1", "ref_id": null}], "section": "Introduction"}, {"text": "Specifically, given embeddings of the seen entities for a multi-relational graph, we meta-train two GNNs to predict the links between seen-to-unseen, and unseen-to-unseen entities. The first GNN, inductive GEN, learns to predict the embeddings of unseen entities that are not observed, and predicts the links between seen and unseen entities. The second GNN, transductive GEN, learns to predict the links between the unseen entities. This transductive inference is possible since our meta-learning framework can simulate the unseen entities during meta-training, while they are unobservable in conventional learning frameworks. Also, since link prediction for unseen entities is inherently unreliable, which gets worse when few triplets are available for each entity, we learn the distribution of unseen representations for stochastic embedding to account for uncertainty. Moreover, we apply transfer learning strategy to model the long-tail distribution. These lead GEN to learn embeddings for unseen entities to be well aligned with seen entities (See Figure 1 , right).", "cite_spans": [], "ref_spans": [{"start": 1054, "end": 1062, "text": "Figure 1", "ref_id": null}], "section": "Introduction"}, {"text": "We validate GENs for their OOG link prediction performance on two benchmark knowledge graph completion datasets, namely FB15K-237 [3] and NELL-995 [55] . We also validate GENs for OOG drug-to-drug interaction prediction task on DeepDDI [36] and BIOSNAP-sub [26] . The experimental results on four datasets show that our model obtains significantly superior performance over the relevant baselines. Further analysis of each component shows that both inductive and transductive layer of GEN help with the accurate link prediction for out-of-graph entities. In sum, our main contributions are as follows:", "cite_spans": [{"start": 130, "end": 133, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 147, "end": 151, "text": "[55]", "ref_id": "BIBREF55"}, {"start": 236, "end": 240, "text": "[36]", "ref_id": "BIBREF35"}, {"start": 257, "end": 261, "text": "[26]", "ref_id": "BIBREF25"}], "ref_spans": [], "section": "Introduction"}, {"text": "\u2022 We tackle a realistic problem setting of few-shot out-of-graph link prediction, aiming to perform link prediction between unseen (emerging) entities for multi-relational graphs that exhibit long-tail distributions, where each entity has only few associative triplets. \u2022 To tackle this problem, we propose a novel meta-learning framework, Graph Extrapolation Network (GEN), which meta-learns the node embeddings for unseen entities, to obtain low error on link prediction for both seen-to-unseen (inductive) and unseen-to-unseen (transductive) cases. \u2022 We validate GEN for few-shot OOG link prediction tasks on four benchmark datasets for knowledge graph completion and drug-to-drug interaction prediction, on which it significantly outperforms relevant baselines.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Graph Neural Network Existing GNNs encode the nodes by aggregating the features from the neighboring nodes, that use recurrent neural networks [16, 37] , mean pooling with layer-wise propagation rule [22] , learnable attention-weighted combination of the features [47] , to name a few. While most of the existing models work with simple undirected graphs, some recent work tackles multi-relational graphs for their practical importance. Directed-GCN [25] and Weighted-GCN [39] consider direction and relation types, respectively. Also, R-GCN [38] considers direction and relation types simultaneously. Recently, Vashishth et al. [46] propose to jointly embed nodes and relations in a multi-relational graph. Since our GEN is a general framework for OOG link prediction rather than a specific GNN, it is compatible with any GNN implementations for multi-relational graphs.", "cite_spans": [{"start": 143, "end": 147, "text": "[16,", "ref_id": "BIBREF15"}, {"start": 148, "end": 151, "text": "37]", "ref_id": "BIBREF36"}, {"start": 200, "end": 204, "text": "[22]", "ref_id": "BIBREF21"}, {"start": 264, "end": 268, "text": "[47]", "ref_id": "BIBREF46"}, {"start": 450, "end": 454, "text": "[25]", "ref_id": "BIBREF24"}, {"start": 472, "end": 476, "text": "[39]", "ref_id": "BIBREF38"}, {"start": 542, "end": 546, "text": "[38]", "ref_id": "BIBREF37"}, {"start": 629, "end": 633, "text": "[46]", "ref_id": "BIBREF45"}], "ref_spans": [], "section": "Related Work"}, {"text": "Meta Learning Meta-learning, whose objective is to generalize over a distribution of tasks, is an essential approach for our few-shot OOG link prediction framework, where we simulate the unseen test nodes with a subset of training nodes. To mention a few, metric-based approaches [48, 41] learn a shared metric space to minimize the distance between correct and instance embeddings. Also, gradient-based approaches [12, 32] learn shared parameters for an initialization, to generalize over diverse tasks in a bi-level optimization framework. Relatively few works consider meta-learning with GNNs. Meta-GNN [59] uses meta-learning for few-shot node classification, and Meta-Graph [6] proposes to construct graphs over seen nodes, with only a small sample of known edges.", "cite_spans": [{"start": 280, "end": 284, "text": "[48,", "ref_id": "BIBREF47"}, {"start": 285, "end": 288, "text": "41]", "ref_id": "BIBREF40"}, {"start": 415, "end": 419, "text": "[12,", "ref_id": "BIBREF11"}, {"start": 420, "end": 423, "text": "32]", "ref_id": "BIBREF31"}, {"start": 606, "end": 610, "text": "[59]", "ref_id": "BIBREF59"}, {"start": 679, "end": 682, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Related Work"}, {"text": "Multi-relational Graphs A popular application of multi-relation graphs is knowledge graph completion. Previous methods for this problem can be broadly classified as translational distance based [5, 51] , semantic matching based [33, 57, 45] and deep neural network based methods [31, 10, 38, 30] . While they require a large amount of training instances, many real-world graphs exhibit long-tail distribution. Few-shot relational learning methods tackle this issue by learning few relations of seen entities [56, 7] . Nonetheless, the problem becomes more difficult as knowledge graphs have an evolving nature with new emerging entities. Several models [54, 52] tackle this problem by utilizing extra information about the entities, such as their textual description. Some recent methods [17, 49, 1] propose to handle unseen entities in an inductive manner. However, since they can not simulate the unseen entities in the training phase, there are some fundamental limitations on the generalization for handling actual unseen entities. On the other hand, our method entirely tackles both of seen-to-unseen and unseen-to-unseen link prediction, under the transductive meta-learning framework. Drug-to-drug interaction (DDI) prediction is another important real-world application of multi-relational graphs, where the problem is to predict interactions between drugs.", "cite_spans": [{"start": 194, "end": 197, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 198, "end": 201, "text": "51]", "ref_id": "BIBREF50"}, {"start": 228, "end": 232, "text": "[33,", "ref_id": "BIBREF32"}, {"start": 233, "end": 236, "text": "57,", "ref_id": "BIBREF57"}, {"start": 237, "end": 240, "text": "45]", "ref_id": "BIBREF44"}, {"start": 279, "end": 283, "text": "[31,", "ref_id": "BIBREF30"}, {"start": 284, "end": 287, "text": "10,", "ref_id": "BIBREF9"}, {"start": 288, "end": 291, "text": "38,", "ref_id": "BIBREF37"}, {"start": 292, "end": 295, "text": "30]", "ref_id": "BIBREF29"}, {"start": 508, "end": 512, "text": "[56,", "ref_id": "BIBREF56"}, {"start": 513, "end": 515, "text": "7]", "ref_id": "BIBREF6"}, {"start": 653, "end": 657, "text": "[54,", "ref_id": "BIBREF54"}, {"start": 658, "end": 661, "text": "52]", "ref_id": "BIBREF51"}, {"start": 788, "end": 792, "text": "[17,", "ref_id": "BIBREF16"}, {"start": 793, "end": 796, "text": "49,", "ref_id": "BIBREF48"}, {"start": 797, "end": 799, "text": "1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Related Work"}, {"text": "Recently, Zitnik et al. [60] and Ma et al. [24] propose end-to-end GNNs to tackle this problem, which demonstrate comparatively better performance over non-GNN methods [2, 34, 58] .", "cite_spans": [{"start": 24, "end": 28, "text": "[60]", "ref_id": "BIBREF60"}, {"start": 43, "end": 47, "text": "[24]", "ref_id": "BIBREF23"}, {"start": 168, "end": 171, "text": "[2,", "ref_id": "BIBREF1"}, {"start": 172, "end": 175, "text": "34,", "ref_id": "BIBREF33"}, {"start": 176, "end": 179, "text": "58]", "ref_id": "BIBREF58"}], "ref_spans": [], "section": "Related Work"}, {"text": "Our goal is to perform link prediction for emerging entities of a multi-relational graph, in which a large portion of the entities have only few triplets associated with them. We begin with the formal definition of the multi-relational graph and the link prediction task.", "cite_spans": [], "ref_spans": [], "section": "Few-Shot Out-Of-Graph Link Prediction"}, {"text": "Definition 3.1. (Multi-relational Graph) Let E and R be two sets of entities and relations respectively. Then a relation is defined as a triplet (e h , r, e t ), where e h , e t \u2208 E are the head and the tail entity, and r \u2208 R is a specific type of relation between them. A multi-relational graph G is represented as a collection of triplets. That is,", "cite_spans": [], "ref_spans": [], "section": "Few-Shot Out-Of-Graph Link Prediction"}, {"text": "Definition 3.2. (Link Prediction) Link prediction refers to the task of predicting an unknown item of a triplet, when given two other items. We consider both of the entity prediction and relation prediction tasks. Entity prediction refers to the problem of predicting e \u2286 E, given the entity and the relation: (e h , r, ?) or (?, r, e t ). Relation prediction refers to the problem of predicting r \u2286 R, given the head and tail entities: (e h , ?, e t ). Link prediction for multi-relational graphs Link prediction is essentially the problem of assigning high scores to the true triplets, and therefore, many existing methods use score function s(e h , r, e t ) to measure the score of a given triplet, where the inputs depend on their respective embeddings (see Table 1 ). As a result, the objective of the link prediction is to find the representation of triplet elements and the function parameters in a parametric model case, which maximize the score of the true triplets. Which embedding methods to use depends on their specific application domain. However, existing work mostly tackles the link prediction between seen entities that already exist in the given multi-relational graph. In this work, we tackle a task of the few-shot Out-Of-Graph (OOG) link prediction defined as follows: where e \u2208 (E \u222a E ). We further assume that each unseen entity e is associated with K triplets: |{(e , r, e) or (e, r, e )}| \u2264 K and e \u2208 (E \u222a E ) , where K is a small number (e.g., 1 or 3).", "cite_spans": [], "ref_spans": [{"start": 762, "end": 769, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Few-Shot Out-Of-Graph Link Prediction"}, {"text": "While few existing works [17, 49] tackle the entity prediction between seen and unseen entities, in real-world settings, unseen entities do not emerge one by one but may emerge simultaneously as a set, with only few triplets available for each entity. Thus, they are highly suboptimal for handling such real-world scenarios.", "cite_spans": [{"start": 25, "end": 29, "text": "[17,", "ref_id": "BIBREF16"}, {"start": 30, "end": 33, "text": "49]", "ref_id": "BIBREF48"}], "ref_spans": [], "section": "Few-Shot Out-Of-Graph Link Prediction"}, {"text": "We now introduce Graph Extrapolation Networks (GENs) for the out-of-graph link prediction task.", "cite_spans": [], "ref_spans": [], "section": "Learning to Extrapolate Knowledge with Graph Extrapolation Networks"}, {"text": "Since most of the previous methods assume that every entity in the test set is seen during training, they cannot handle emerging entities, which are unobserved during training. While few existing works [17, 49] train for seen-to-seen link prediction with the hope that the models generalize on seen-to-unseen cases, they are suboptimal in handling unseen entities. Therefore, we use metalearning framework to handle the OOG link prediction problem, whose goal is to train a model over a distribution of tasks such that the model generalizes well on unseen tasks. Figure 1 illustrates our learning framework. Basically, we meta-train GEN which performs both inductive and transductive inference on various simulated test sets of OOG entities, such that it extrapolates the knowledge of existing graphs to any unseen entities. We describe the framework in details in next few paragraphs.", "cite_spans": [{"start": 202, "end": 206, "text": "[17,", "ref_id": "BIBREF16"}, {"start": 207, "end": 210, "text": "49]", "ref_id": "BIBREF48"}], "ref_spans": [{"start": 563, "end": 571, "text": "Figure 1", "ref_id": null}], "section": "Learning to Extrapolate Knowledge with Graph Extrapolation Networks"}, {"text": "Learning Objective Suppose that we are given a multi-relational graph G \u2286 E \u00d7 R \u00d7 E, which consists of seen entities e \u2208 E and relations r \u2208 R. Then, we aim to represent the unseen entities e \u2208 E over a distribution p(E ), by extrapolating the knowledge on a given graph G, to predict the link between seen e and unseen e entities: (e, r, e ) or (e , r, e), or even between unseen entities themselves: (e , r, e ). Toward this goal, we have to maximize the score of a triplet s(e h , r, e t ) that contains any unseen entities e , with embedding and score function parameters \u03b8: max", "cite_spans": [], "ref_spans": [], "section": "Learning to Extrapolate Knowledge with Graph Extrapolation Networks"}, {"text": "While this is a seemingly impossible goal as it involves generalization to unseen entities, we can tackle it with meta-learning, which we describe next.", "cite_spans": [], "ref_spans": [], "section": "Learning to Extrapolate Knowledge with Graph Extrapolation Networks"}, {"text": "Meta-Learning Framework While conventional learning frameworks can not handle unseen entities in the training phase, with meta-learning, we can formulate a set of tasks such that the model learns to generalize over unseen entities, which are simulated using seen entities. To formulate the OOG link prediction problem into a meta-learning problem, we first randomly split the entities in a given graph into the meta-training and meta-test set. Then, we generate a task by sampling the set of (simulated) unseen entities for meta-training (See Figure 1 ). Formally, each task T corresponds to a set of unseen entities E T \u2282 E , with a predefined number of instances |E T | = N . Then we divide the triplets associative with each entity e i \u2208 E T into the support set S i and the query set", "cite_spans": [], "ref_spans": [{"start": 543, "end": 551, "text": "Figure 1", "ref_id": null}], "section": "Learning to Extrapolate Knowledge with Graph Extrapolation Networks"}, {"text": "K is the few-shot size, and M i is the number of triplets associated with each unseen entity e i . Our meta-objective is then learning to represent the unseen entities as \u03c6 using a support set S, to maximize the triplet score on a query set Q:", "cite_spans": [], "ref_spans": [], "section": "Learning to Extrapolate Knowledge with Graph Extrapolation Networks"}, {"text": "We refer to this specific setting as K-shot OOG link prediction throughout this paper. Once the model is trained with the meta-training tasks T train , we can apply it to unseen meta-test tasks T test , whose set of entities is disjoint from T train , as shown in Figure 1 .", "cite_spans": [], "ref_spans": [{"start": 264, "end": 272, "text": "Figure 1", "ref_id": null}], "section": "Learning to Extrapolate Knowledge with Graph Extrapolation Networks"}, {"text": "Require: Distribution over train tasks p (Ttrain), Require: Learning rate for meta-update \u03b1 1: Initialize parameters \u0398 = {\u03b8, \u03b8\u00b5, \u03b8\u03c3} 2: while not done do 3: Sample a task T \u223c p (Ttrain) 4: for all e i \u2208 T do 5: Sample support and query set {Si, Qi} correspond to e i 6:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "Inductively generate using (3): \u03c6i = f \u03b8 (Si) 7: end for 8: for all e i \u2208 T do 9:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "Transductively generate using (4): \u00b5i = g \u03b8\u00b5 (Si, \u03c6) and \u03c3i = g \u03b8\u03c3 (Si, \u03c6) 10: (6) 13: end while Figure 3 : The overall framework of our model for each task. We extrapolate knowledge by using a support set S with inductive and transductive learning, and then predict links with output of the embedding \u03c6 .", "cite_spans": [], "ref_spans": [{"start": 97, "end": 105, "text": "Figure 3", "ref_id": null}], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "Graph Extrapolation Networks In order to extrapolate knowledge of a given graph G to an unseen entity e i through a support set S i , we propose a GNN-based meta-learner that outputs the representation of unseen entities. We formulate our meta-learner f \u03b8 (\u00b7) as follows ( Figure 3 -", "cite_spans": [], "ref_spans": [{"start": 273, "end": 281, "text": "Figure 3", "ref_id": null}], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": ", where we denote the set of neighboring entity and relation as n (S i ) = {(r, e) | (e i , r, e) or (e, r, e i ) \u2208 S i }. Further, K is the size of n(S i ), W r \u2208 R d\u00d72d is a relation-specific transformation matrix that is meta-learned, and C r,e \u2208 R 2d is a concatenation of feature representation of the relation-entity pair and \u03c3 is the ReLU function. Since GEN is essentially a framework for OOG link prediction, it is compatible with any GNNs.", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "Transductive Meta-Learning of GENs The previously described inductive GEN constructs the representation of each unseen entity e i through a support set S i , as described in (3), and perform link prediction on a query set Q i , independently. A major drawback of this inductive scheme is that it does not consider the relationships between unseen entities. However, to tackle unseen entities simultaneously as a set, one should consider not only the relationships between seen and unseen entities as with inductive GEN, but also among unseen entities themselves. To tackle this issue, we extend the inductive GEN to further perform a transductive inference, which will allow knowledge to propagate between unseen entities.", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "More specifically, we add one more GEN layer g \u03b8 (\u00b7), which is similar to the inductive meta-learner f \u03b8 (\u00b7), to consider inter-relationships between unseen entities (Figure 3 ", "cite_spans": [], "ref_spans": [{"start": 166, "end": 175, "text": "(Figure 3", "ref_id": null}], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "is a weight matrix for the self-connection to consider embedding \u03c6 i , which is updated by the previous inductive layer f \u03b8 (S i ). To leverage the knowledge of neighboring unseen entities, our transductive layer g \u03b8 (\u00b7) aggregates the representations across all the associative neighbors with transductive weight matrix W r \u2208 R d\u00d72d , where the neighbors can include the representations of unseen entities \u03c6, rather than treating them as noises.", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "Stochastic Inference A naive transductive GEN generalizes to unseen entities by simulating them with seen entities during the meta-training. However, due to the intrinsic unreliability of few-shot OOG link prediction with each entity having only few triplets, there could be high uncertainties on the representation of unseen entities. To model such uncertainties, we stochastically embed the unseen entities by learning the distribution over an unseen entity embedding \u03c6 i . To this end, we first assume that the true posterior distribution has a following form: p(\u03c6 i | S i , \u03c6). Since computation of the true posterior distribution is intractable, we approximate the posterior using", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": ", and then compute the mean and variance via two individual transductive GEN layers \u00b5 i = g \u03b8\u00b5 (S i , \u03c6) and \u03c3 i = g \u03b8\u03c3 (S i , \u03c6), which modifies GraphVAE [21] to our setting. The form to maximize the score function is then defined as follows:", "cite_spans": [{"start": 155, "end": 159, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "where we set the MC sample size to L = 1 during meta-training for computational efficiency. Also, we perform MC approximation with sufficiently large sample size (e.g. L = 10) at meta-testing. We let the approximate posterior same as the prior to make the consistent pipeline at training and testing (see Sohn et al. [42] ). We also model the source of uncertainty on output embedding of an unseen entity from the transductive GEN layer via Monte Carlo dropout [14] . Our final GEN is trained for both the inductive and transductive steps, as described in Algorithm 1.", "cite_spans": [{"start": 317, "end": 321, "text": "[42]", "ref_id": "BIBREF41"}, {"start": 461, "end": 465, "text": "[14]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "Loss Function Each task T consists of a support set and a query set: T = {S, Q}. During training, we represent the embedding of unseen entities e i \u2208 E T using the support set S i with GENs. After that, at the test time, we use the true labeled query set Q i to optimize our GENs. Since every query set contains only positive triplets, we perform negative sampling [5, 57] to update meta-learner by allowing it to distinguish positive from negative triplets. Specifically, we replace the entity of each triplet in the query set:", "cite_spans": [{"start": 365, "end": 368, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 369, "end": 372, "text": "57]", "ref_id": "BIBREF57"}], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "where e \u2212 is the corrupted entity and Q \u2212 i holds negative samples for an unseen entity e i . We use hinge loss to optimize our model:", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "where \u03b3 > 0 is a margin hyper-parameter and s is the result of each score function in the Table 1 . s + and s \u2212 denote the scores of positive and negative triplets, respectively. For Drug-to-Drug interaction predict task, we follow Ryu et al. [36] to optimize our model, where binary cross-entropy loss is calculated for each label with a sigmoid output of the score function in the Table 1 .", "cite_spans": [{"start": 243, "end": 247, "text": "[36]", "ref_id": "BIBREF35"}], "ref_spans": [{"start": 90, "end": 97, "text": "Table 1", "ref_id": "TABREF0"}, {"start": 383, "end": 390, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "Meta-Learning for Long-Tail Tasks Since many real-world graphs follow the long-tail distribution (See Figure 2) , it would be beneficial to transfer the knowledge from entities with large links to entities with only few links. To this end, we follow a scheme similar to Wang et al. [50] and start to learn the model with many shot cases, then gradually decrease the number of shots to few shot cases in a logarithmic scale. Further implementation details are given in the Section B of the Appendix.", "cite_spans": [{"start": 282, "end": 286, "text": "[50]", "ref_id": "BIBREF49"}], "ref_spans": [{"start": 102, "end": 111, "text": "Figure 2)", "ref_id": "FIGREF0"}], "section": "Algorithm 1 Meta-Learning of GEN"}, {"text": "We validate GEN on few-shot OOG link prediction tasks for two different applications of multirelational graphs: knowledge graph (KG) completion and drug-to-drug interaction (DDI) prediction.", "cite_spans": [], "ref_spans": [], "section": "Experiment"}, {"text": "Datasets For knowledge graph completion datasets, we consider out-of-graph entity prediction, whose goal is to predict the other entity given an unseen entity and a relation. 1) FB15k-237. This dataset [44] consists of 310, 116 triplets from 14, 541 entities and 237 relations, which is collected via crowdsourcing. 2) NELL-995. This dataset [55] consists of 154, 213 triplets from 75, 492 entities and 200 relations, which is collected by a lifelong learning system [29] . Since existing benchmark datasets do not target OOG link prediction, they assume that all entities given at the test time are seen during training. Therefore, we modify the dataset such that the triplets used for link prediction at test time contain at least one unseen entity (see Appendix A.1 for the detailed setup).", "cite_spans": [{"start": 202, "end": 206, "text": "[44]", "ref_id": "BIBREF43"}, {"start": 342, "end": 346, "text": "[55]", "ref_id": "BIBREF55"}, {"start": 467, "end": 471, "text": "[29]", "ref_id": "BIBREF28"}], "ref_spans": [], "section": "Knowledge Graph Completion"}, {"text": "Baselines and our models 1) TransE. 2) RotatE. Translation distance based embedding methods for multi-relational graph [5, 43] . 3) DistMult. 4) ComplEx. Semantic matching based embedding methods [57, 45] . 5) R-GCN. This is a GNN-based method for modeling relational data [38] . 6) MEAN. 7) LAN. These are GNN models for a out-of-knowledge base task, which tackle unseen entities without meta-learning [17, 49] . 8) GMatching. This model tackles the link prediction on unseen relations of seen entities, and we extend it to our meta-learning framework [56] . 9) I-GEN. An inductive version of our GEN which is meta-learned to embed an unseen entity. 10) T-GEN. A transductive version of GEN, with additional stochastic transductive GNN layers to predict the link between unseen entities. We report detailed description in the Appendix A.2.", "cite_spans": [{"start": 119, "end": 122, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 123, "end": 126, "text": "43]", "ref_id": "BIBREF42"}, {"start": 196, "end": 200, "text": "[57,", "ref_id": "BIBREF57"}, {"start": 201, "end": 204, "text": "45]", "ref_id": "BIBREF44"}, {"start": 273, "end": 277, "text": "[38]", "ref_id": "BIBREF37"}, {"start": 403, "end": 407, "text": "[17,", "ref_id": "BIBREF16"}, {"start": 408, "end": 411, "text": "49]", "ref_id": "BIBREF48"}, {"start": 553, "end": 557, "text": "[56]", "ref_id": "BIBREF56"}], "ref_spans": [], "section": "Knowledge Graph Completion"}, {"text": "Implementation Details. For both I-GEN and T-GEN, we use DistMult for the initial embedding of entities and relations, and the score function. However, the models that use TransE for embedding and score function perform similarly, which we report in the Appendix C. Following Xiong et al. [56] , we train seen-to-seen link prediction baselines including support triplets of meta-valid and meta-test sets, since baselines assume that only seen entities will appear at the test time and thus unable to solve the problem if the entity in triplets is completely unseen. However, for our methods, we train them only with the meta-training set, where we generate OOG entities using episodic training. Detailed experimental setups used for both datasets are described in the Appendix A.3. ", "cite_spans": [{"start": 289, "end": 293, "text": "[56]", "ref_id": "BIBREF56"}], "ref_spans": [], "section": "Knowledge Graph Completion"}, {"text": "TransE [5] . Evaluation Metrics For evaluation, we use the ranking procedure by Bordes et al. [4] . For a triplet with an unseen head entity, we replace its corresponding tail entity with candidate entities from the dictionary to construct corrupted triplets. Then, we rank all the triplets, including the correct and corrupted ones by a scoring measure, to obtain the rank of the correct triplet. We provide the results using mean reciprocal rank (MRR) and Hits at n (H@n); MRR is the average of the multiplicative inverse of the rank of the correct triplets and H@n is the ratio of the correct triplets ranked smaller than or equal to n. Moreover, as done in previous works [5, 31, 38] , we measure the ranks in a filtered setting where we do not consider triplets that appeared in either training, validation, or test sets.", "cite_spans": [{"start": 7, "end": 10, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 94, "end": 97, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 676, "end": 679, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 680, "end": 683, "text": "31,", "ref_id": "BIBREF30"}, {"start": 684, "end": 687, "text": "38]", "ref_id": "BIBREF37"}], "ref_spans": [], "section": "1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S Seen to Seen"}, {"text": "Results Table 2 shows that our I-GEN and T-GEN outperform all baselines by impressive margins in all evaluation metrics with 1-shot and 3-shot settings. Baseline models work poorly on emerging entities that come with only a few triplets to train, even when they have seen the entities during training (with Support Set in Table 2 ). However, in our meta-learning framework, our GENs show superior performance over the baselines, with even one training triplet for each unseen entity. Moreover, while GMatching [56] , which handles unseen entities by searching for the closest entity pair, with our meta-learning framework achieves decent performance, it significantly underperforms ours. We also observe that T-GEN outperforms I-GEN on both datasets by all evaluation metrics.", "cite_spans": [{"start": 510, "end": 514, "text": "[56]", "ref_id": "BIBREF56"}], "ref_spans": [{"start": 8, "end": 15, "text": "Table 2", "ref_id": "TABREF1"}, {"start": 322, "end": 329, "text": "Table 2", "ref_id": "TABREF1"}], "section": "1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S Seen to Seen"}, {"text": "To see where the performance improvement comes from, we further examine the link prediction results for seen-to-unseen and unseen-to-unseen cases. Figure 4 shows that T-GEN obtains significant performance gain on the unseen-to-unseen link prediction problems, whereas I-GEN mostly cannot handle the case as it does not consider the relationships between unseen nodes. Also, while we mostly target a long-tail graph with the majority of the entities having few links, our method works well on many-shot cases as well ( Figure 5 ), on which I-and T-GENs still largely outperform the baselines, even though R-GCN sees the unseen entities during training. We further experiment our GEN with varying the number of triplets by considering 1-, 3-, 5-, and random-shot (between 1 and 5) during meta-training and meta-test. Table 3 shows that the difference in the number of shots used for training and test does not significantly affect the performance, which demonstrates the robustness of GENs on varying number of triplets at test time. Moreover, our model trained on 1-shot setting obtains even better performance on 3-shot setting. Furthermore, we conduct an ablation study of the T-GEN on seen-to-unseen (S/U) and unseen-to-unseen (U/U) cases. Table 4 shows that using stochastic modeling on the transductive inference layer helps significantly improve the unseen-to-unseen link prediction performance. Moreover, the meta-learning strategy of learning on entities with many links and then progressing to entities with few links performs well. Finally, we observe that using pre-trained embedding of a seen graph leads to better performance. We visualize the output representations of unseen entities with seen entities. Figure 1 (Right) shows that the embeddings of unseen entities are well aligned with the seen entities.", "cite_spans": [], "ref_spans": [{"start": 147, "end": 155, "text": "Figure 4", "ref_id": "FIGREF2"}, {"start": 518, "end": 526, "text": "Figure 5", "ref_id": "FIGREF3"}, {"start": 815, "end": 822, "text": "Table 3", "ref_id": "TABREF3"}, {"start": 1242, "end": 1249, "text": "Table 4", "ref_id": "TABREF4"}, {"start": 1718, "end": 1734, "text": "Figure 1 (Right)", "ref_id": null}], "section": "1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S Seen to Seen"}, {"text": "Regarding concrete examples of link prediction on NELL-995, see the Section D of the Appendix.", "cite_spans": [], "ref_spans": [], "section": "1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S Seen to Seen"}, {"text": "Datasets We further validate our GENs on the OOG relation prediction task using two public Drug-to-Drug Interaction (DDI) datasets. Baselines and our models 1) MLP. Feed-forward neural networks used in DDI task [36] . 2) MPNN. Graph Neural Networks that use edge-conditioned convolution [15] . 3) R-GCN. The same model used in the entity prediction on KG completion task [38] . 4) I-GEN. Inductive GEN, which only uses feature representation of an entity e k , instead of a relation-entity pair (r k , e k ). This is because the relation is the prediction target for the DDI tasks. 5) T-GEN. Transductive GEN with an additional transductive stochastic layers for unseen-to-unseen relation prediction.", "cite_spans": [{"start": 211, "end": 215, "text": "[36]", "ref_id": "BIBREF35"}, {"start": 287, "end": 291, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 371, "end": 375, "text": "[38]", "ref_id": "BIBREF37"}], "ref_spans": [], "section": "Drug-to-Drug Interaction"}, {"text": "for the initial embedding of entities with linear score function in Table 1 . To train baselines, we use the same training scheme as the KG completion task, where support triplets of meta-valid and meta-test sets are included in the training set. Detailed experimental settings are described in the Appendix A.3. For evaluation, we use the area under the receiver operating characteristic curve (ROC), the area under the precision-recall curve (PR), and the classification accuracy (Acc). Results Table 5 shows the DDI prediction performance of the baselines and GENs. Note that the performances on BIOSNAP-sub are comparatively lower in comparison to DeepDDI, due to the use of the preprocessed input features, as suggested by Ryu et al. [36] . Similarly with the KG completion tasks, both I-and T-GEN outperform all baselines by impressive margins in all evaluation metrics. These results demonstrate that our GENs can be extended to OOG link prediction for other real-world applications of multi-relational graphs. We also compare the link prediction performance for both seen-to-unseen and unseen-to-unseen cases on two DDI datasets. The rightmost two columns of Figure 4 show that T-GEN obtains superior performance over I-GEN on unseen-to-unseen link prediction, especially on stochastic modeling cases.", "cite_spans": [{"start": 739, "end": 743, "text": "[36]", "ref_id": "BIBREF35"}], "ref_spans": [{"start": 68, "end": 75, "text": "Table 1", "ref_id": "TABREF0"}, {"start": 497, "end": 504, "text": "Table 5", "ref_id": "TABREF6"}, {"start": 1167, "end": 1175, "text": "Figure 4", "ref_id": "FIGREF2"}], "section": "Implementation Details and Evaluation Metrics For both I-GEN and T-GEN, we use MPNN"}, {"text": "We proposed a realistic problem of the few-shot out-of-graph (OOG) link prediction, which considers link prediction between unseen (or emerging) entities for multi-relational graphs, where each entity comes with only few associative triplets. To this end, we proposed a novel meta-learning framework for OOG link prediction, which we refer to as Graph Extrapolation Network. Under the defined K-shot learning setting, GENs learn to extrapolate the knowledge of a given graph to unseen entities, with a stochastic transductive layer to further propagate the knowledge between the unseen entities and model uncertainty in the link prediction. We validated the OOG link prediction performance of GENs on four benchmark datasets, on which it largely outperformed the relevant baselines.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "Constructing knowledge bases which accurately reflect up-to-date knowledge about the entities and the links between them is crucial for its application in real-world scenarios. However, conventional link prediction methods for knowledge base systems mostly consider static knowledge graph that does not change over time. Yet, as new entities emerge every day [40] (e.g., , the ability to dynamically incorporating them into the existing knowledge graph is becoming a significantly important problem, which we mainly tackle in this paper.", "cite_spans": [{"start": 359, "end": 363, "text": "[40]", "ref_id": "BIBREF39"}], "ref_spans": [], "section": "Broader Impact"}, {"text": "As a specific example of our approach, the novel coronavirus, COVID-19, is threatening our lives around the globe. To eradicate the novel coronavirus, we may want to best utilize the accumulated knowledge about existing coronavirus variants [53, 8] by identifying the links between the seen (SARS and MERS) and unseen entities (COVID-19) , or the links between unseen entities that have newly emerged (COVID-19 and novel vaccine under study). The following are more use cases of our proposed out-of-graph link prediction system:", "cite_spans": [{"start": 241, "end": 245, "text": "[53,", "ref_id": null}, {"start": 246, "end": 248, "text": "8]", "ref_id": "BIBREF7"}], "ref_spans": [{"start": 327, "end": 337, "text": "(COVID-19)", "ref_id": null}], "section": "Broader Impact"}, {"text": "\u2022 The proposed meta-learning based few-shot out-of-graph link prediction method can infer and inform the relationship between the entities that describe past coronavirus outbreaks and the current COVID-19 situation. \u2022 Our transductive inference, with stochastic transductive GENs, can lead to finding the relationships among novel entities regarding COVID-19 which rapidly emerge over time, that may allow us to discover meaningful links among them. \u2022 Regarding drug-to-drug interaction prediction, our method can be further utilized to analyze the side-effects of simultaneously taking a novel antiviral drugs for COVID-19 and existing drugs, before the clinical trials.", "cite_spans": [], "ref_spans": [], "section": "Broader Impact"}, {"text": "While we describe the impact of our method on a specific, but significantly important topic, our method can be broadly applied to any real-world applications that require to predict the links which involve unseen entities. While our method obtains significantly better performance over existing methods on out-of-graph link prediction, its prediction performance is yet far from perfect. Thus, the model should be used more as a candidates selection tool (Hits@N) when inferring critical information (e.g. drug-to-drug interaction prediction for COVID- 19) , and more efforts should be made to develop a reliable system.", "cite_spans": [{"start": 553, "end": 556, "text": "19)", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Broader Impact"}, {"text": "Since existing benchmark datasets assume that all entities given at the test time are seen during training, we modify the datasets to formulate the Out-of-Graph (OOG) link prediction task, where completely unseen entities appear at the test time. Datasets modification processes are as follows:", "cite_spans": [], "ref_spans": [], "section": "A.1 Datasets"}, {"text": "\u2022 First, we randomly sample the unseen entities, which have a relatively small amount of triplets on each dataset. We then divide the sampled unseen entities into meta-training/validation/test sets. \u2022 Second, we select the triplets which are used for constructing an In-Graph, where the head and tail entities of every triplet in the In-Graph do not contain any unseen entity. \u2022 Finally, we match the unseen entities in the meta-sets with their triplets. Each triplet in meta-sets contains at least one unseen entity. Also, every triplet in meta-sets is not included in the In-Graph. 1) FB15k-237. This dataset [44] consists of 14,541 entities, which is used for the knowledge graph completion task. We randomly sample the 5,000 entities from 10,938 entities, which have associated triplets between 10 and 100. Also, we split the entities such that we have 2,500/1,000/1,500 unseen (Out-of-Graph) entities and 72,065/6,246/9,867 associated triplets containing unseen entities for meta-training/validation/test. The remaining triplets that do not hold an unseen entity are used for constructing In-Graph. As shown in the Figure 6 , this dataset follows a highly long-tailed distribution.", "cite_spans": [{"start": 611, "end": 615, "text": "[44]", "ref_id": "BIBREF43"}], "ref_spans": [{"start": 1120, "end": 1128, "text": "Figure 6", "ref_id": "FIGREF4"}], "section": "A.1 Datasets"}, {"text": "2) NELL-995. This dataset [55] consists of 75,492 entities, which is used for the knowledge graph completion task. We randomly sample the 3,000 entities from 5,694 entities, which have associated triplets between 7 and 100. Also, we split the entities such that we have 1,500/600/900 unseen (Out-of-Graph) entities and 22,345/3,676/5,852 associated triplets containing unseen entities for meta-training/validation/test. The remaining triplets that do not hold an unseen entity are used for constructing In-Graph. As shown in the Figure 6 , this dataset follows a highly long-tailed distribution.", "cite_spans": [{"start": 26, "end": 30, "text": "[55]", "ref_id": "BIBREF55"}], "ref_spans": [{"start": 529, "end": 537, "text": "Figure 6", "ref_id": "FIGREF4"}], "section": "A.1 Datasets"}, {"text": "3) DeepDDI. This dataset [36] consists of 1,861 entities, which is used for the drug-to-drug interaction prediction task. We randomly sample the 500 entities from 1,039 entities, which have associated triplets between 7 and 300. Also, we split the entities such that we have 250/100/150 unseen (Out-of-Graph) entities and 27,726/1,171/2,160 associated triplets containing unseen entities for meta-training/validation/test. The remaining triplets that do not hold an unseen entity are used for constructing In-Graph.", "cite_spans": [{"start": 25, "end": 29, "text": "[36]", "ref_id": "BIBREF35"}], "ref_spans": [], "section": "A.1 Datasets"}, {"text": "This dataset [26, 24] consists of 637 entities, which is used for the drug-todrug interaction prediction task. We randomly sample the 150 entities from 507 entities, which have associated triplets between 7 and 300. Also, we split the entities such that we have 75/30/45 unseen (Out-of-Graph) entities and 7,140/333/643 associated triplets containing unseen entities for meta-training/validation/test. The remaining triplets that do not hold an unseen entity are used for constructing In-Graph.", "cite_spans": [{"start": 13, "end": 17, "text": "[26,", "ref_id": "BIBREF25"}, {"start": 18, "end": 21, "text": "24]", "ref_id": "BIBREF23"}], "ref_spans": [], "section": "4) BIOSNAP-sub."}, {"text": "Knowledge Graph Completion We describe the baseline models and our graph extrapolation networks for few-shot out-of-graph entity prediction on the knowledge graph (KG) completion task.", "cite_spans": [], "ref_spans": [], "section": "A.2 Baselines and Our Models"}, {"text": "1) TransE. The translation embedding model for a multi-relational data by Bordes et al. [5] . It represents both entities and relations as vectors in the same space, where the relation in a triplet is used as a translation operation between the head and the tail entity.", "cite_spans": [{"start": 88, "end": 91, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "A.2 Baselines and Our Models"}, {"text": "2) RotatE. This model represents entities as complex vectors and relations as rotations in a complex vector space [43] , which extends TransE with a complex operation.", "cite_spans": [{"start": 114, "end": 118, "text": "[43]", "ref_id": "BIBREF42"}], "ref_spans": [], "section": "A.2 Baselines and Our Models"}, {"text": "3) DistMult. This model represents the relationship between the head and the tail entity in a bi-linear formulation, which captures pairwise interaction between entities [57] .", "cite_spans": [{"start": 170, "end": 174, "text": "[57]", "ref_id": "BIBREF57"}], "ref_spans": [], "section": "A.2 Baselines and Our Models"}, {"text": "This model extends the DistMult by introducing embeddings on a complex space to consider asymmetric relations, where scores are differently measured based on the order of the entities [45] .", "cite_spans": [{"start": 184, "end": 188, "text": "[45]", "ref_id": "BIBREF44"}], "ref_spans": [], "section": "4) ComplEx."}, {"text": "This is a GNN-based method for modeling relational data, which extends the graph convolutional network to consider multi-relational structure, by Schlichtkrull et al. [38] .", "cite_spans": [{"start": 167, "end": 171, "text": "[38]", "ref_id": "BIBREF37"}], "ref_spans": [], "section": "5) R-GCN."}, {"text": "6) MEAN. This model computes the embedding of entities by GNN based neighboring aggregation scheme, where they only train for seen-to-seen link prediction, with the hope that the model generalizes on seen-to-unseen cases [17] .", "cite_spans": [{"start": 221, "end": 225, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "5) R-GCN."}, {"text": "This model extends the MEAN to consider relation and neighbor-level information by utilizing attention mechanisms [49] .", "cite_spans": [{"start": 114, "end": 118, "text": "[49]", "ref_id": "BIBREF48"}], "ref_spans": [], "section": "7) LAN."}, {"text": "GMatching. This model tackles the link prediction on unseen relations of seen entities by searching for the closest entity pair, and we extend it in our meta-learning framework such that it can handle unseen entities [56] .", "cite_spans": [{"start": 217, "end": 221, "text": "[56]", "ref_id": "BIBREF56"}], "ref_spans": [], "section": "8)"}, {"text": ", that is meta-learned to embed an unseen entity into the embedding space to infer hidden links between seen and unseen entities.", "cite_spans": [], "ref_spans": [], "section": "9) I-GEN. An inductive version of our Graph Extrapolation Network (GEN)"}, {"text": "A transductive version of GEN, with additional stochastic transductive GNN layers on top of the I-GEN, that is meta-learned to predict the links not only between unseen entities but also between seen and unseen entities.", "cite_spans": [], "ref_spans": [], "section": "10) T-GEN."}, {"text": "Drug-to-Drug Interaction We describe the baseline models and our graph extrapolation networks for few-shot out-of-graph relation prediction on the drug-to-drug interaction (DDI) task.", "cite_spans": [], "ref_spans": [], "section": "10) T-GEN."}, {"text": "The feed-forward neural network used in DeepDDI [36] . It classifies the relation of two drugs using their pairwise features.", "cite_spans": [{"start": 48, "end": 52, "text": "[36]", "ref_id": "BIBREF35"}], "ref_spans": [], "section": "1) MLP."}, {"text": "2) MPNN. The GNN-based model which uses features about relation types with edge-conditioned convolution operations [15] .", "cite_spans": [{"start": 115, "end": 119, "text": "[15]", "ref_id": "BIBREF14"}], "ref_spans": [], "section": "1) MLP."}, {"text": "3) R-GCN. The same model used in the entity prediction on KG completion tasks, applied to DDI tasks.", "cite_spans": [], "ref_spans": [], "section": "1) MLP."}, {"text": "An inductive GEN, which only uses the feature representation of the entity e k , instead of using the concatenated representation of the relation-entity pair (r k , e k ), when aggregating neighboring information.", "cite_spans": [], "ref_spans": [], "section": "4) I-GEN."}, {"text": "A transductive GEN, with additional transductive stochastic layers for unseen-to-unseen relation prediction.", "cite_spans": [], "ref_spans": [], "section": "5) T-GEN."}, {"text": "For every dataset, we set the embedding dimension of entity and relation as 100. Also, we set the initial embedding of unseen entities as zero vector. Furthermore, since we consider a highly multi-relational graph, we use the basis decomposition on weight matrices W r and W r to prevent the excessive increase in the model size, proposed in Schlichtkrull et al. [38] :", "cite_spans": [{"start": 363, "end": 367, "text": "[38]", "ref_id": "BIBREF37"}], "ref_spans": [], "section": "A.3 Implementation Details"}, {"text": "where B is the number of basis, a r b is a coefficient of each relation r \u2208 R and V b \u2208 R d\u00d72d is a shared representation of various relations. For all the experiment, we use PyTorch [35] and PyTorch geometric [11] frameworks on a single Titan XP or a single GeForce RTX 2080 Ti GPU. We optimize the proposed GENs using Adam [20] . Knowledge Graph Completion For both I-GEN and T-GEN, we search for the learning rate \u03b1 in the range of 3 \u00d7 10 \u22124 , 1 \u00d7 10 \u22123 , 3 \u00d7 10 \u22123 , margin \u03b3 in the range of {0.25, 0.5, 1}, and dropout ratio at every GEN layer in the range of {0.1, 0.2, 0.3}. To select the best model, we use the mean reciprocal rank (MRR) as an evaluation metric. For FB15k-237 dataset, we set the \u03b1 = 1 \u00d7 10 \u22123 and \u03b3 = 1 with dropout rate 0.3. Also, we set the number of basis units B = 100 for the basis decomposition on each GEN layer, and sample 32 negative triplets for each positive triplet in both I-GEN and T-GEN. At every episodic training, we randomly sample 500 unseen entities in the meta-training set. For NELL-995 dataset, we use the same parameter settings with FB15k-237, except that we sample 64 negative triplets for each positive triplet. For both datasets, we consider the inverse relation as suggested by several recent works on multi-relational graphs [25, 38, 46] , where directed relation information flows along with both directions.", "cite_spans": [{"start": 183, "end": 187, "text": "[35]", "ref_id": "BIBREF34"}, {"start": 210, "end": 214, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 325, "end": 329, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 1281, "end": 1285, "text": "[25,", "ref_id": "BIBREF24"}, {"start": 1286, "end": 1289, "text": "38,", "ref_id": "BIBREF37"}, {"start": 1290, "end": 1293, "text": "46]", "ref_id": "BIBREF45"}], "ref_spans": [], "section": "A.3 Implementation Details"}, {"text": "Drug-to-Drug Interaction For both I-GEN and T-GEN, we search for the learning rate \u03b1 in the range of 5 \u00d7 10 \u22124 , 1 \u00d7 10 \u22123 , 5 \u00d7 10 \u22123 , and dropout ratio at every GEN layer in the range of {0.1, 0.2, 0.3}. As a score function, we use two linear layers with ReLU activation function at the end of the first layer. To select the best model, we use the area under the receiver operating characteristic curve (ROC) as an evaluation metric. For DeepDDI dataset, we set the \u03b1 = 1 \u00d7 10 \u22123 with dropout rate 0.3. Also, we set the number of basis units B = 200 for the basis decomposition. At every episodic training, we randomly sample 80 unseen entities in the meta-training set. For BIOSNAP-sub dataset, we set the \u03b1 = 1 \u00d7 10 \u22123 with dropout rate 0.1 for I-GEN and 0.2 for T-GEN, respectively. Also, we set the number of basis units B = 200 for the basis decomposition. At every episodic training, we randomly sample 50 unseen entities in the meta-training set. For both datasets, we consider the inverse relation as in the case of knowledge graph completion task, where directed relation information flows along with both directions.", "cite_spans": [], "ref_spans": [], "section": "1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S"}, {"text": "B Meta-learning for Long-tail Task Implementation Details Many real-world graphs follow the long-tail distribution, where few entities have many links while the majority have few (See Figure 6 ). For such an imbalanced graph, it would be beneficial to transfer the knowledge from entities with many links to entities with few links. To this end, we transfer the meta-knowledge on data-rich entities to data-poor entities by simulating the data-rich circumstance under the meta-learning framework, motivated by Wang et al. [50] . Specifically, we firstly meta-train our GENs with many shot cases (e.g., K = 10), and then gradually decrease the number of shots to few shots cases (e.g., K = 1 or 3) in logarithmic scale: K i = log 2 (max-iteration/i) + K, where K i is the training shot size at the current iteration number i, and K is the test shot size. In this way, GENs learn to represent the unseen entities using data-rich instances, and entities with few links regimes may experience like data-rich instances, with the model parameters trained on the entities with many links and fine-tuned on the entities with few links.", "cite_spans": [{"start": 522, "end": 526, "text": "[50]", "ref_id": "BIBREF49"}], "ref_spans": [{"start": 184, "end": 192, "text": "Figure 6", "ref_id": "FIGREF4"}], "section": "1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S"}, {"text": "More Ablation Studies Since knowledge graphs follow a highly long-tailed distribution (See Figure 6) , we provide the more experimental results about transfer strategies on knowledge graph completion tasks, to demonstrate the effectiveness of the proposed meta-learning scheme on a long-tail task. Table 6 shows that the transfer strategy outperforms naive I-GEN and T-GEN on all evaluation metrics, except for two H@1 cases of T-GEN on 3-shot OOG link prediction settings. We conjecture that the effectiveness of the meta-learning scheme is especially larger on 1-shot cases, where data is extremely poor, rather than the 3-shot cases. Table 7 : Total, seen-to-unseen and unseen-to-unseen results of 1-and 3-shot OOG link prediction on FB15k-237.", "cite_spans": [], "ref_spans": [{"start": 91, "end": 100, "text": "Figure 6)", "ref_id": "FIGREF4"}, {"start": 298, "end": 305, "text": "Table 6", "ref_id": "TABREF7"}, {"start": 637, "end": 644, "text": "Table 7", "ref_id": null}], "section": "1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S"}, {"text": "* means training a model within our meta-learning framework. Bold numbers denote the best results.", "cite_spans": [], "ref_spans": [], "section": "1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S"}, {"text": "Seen to Unseen Unseen to Unseen   Model  MRR  H@1  H@3  H@10  MRR  H@10 MRR H@10 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S Seen to Seen TransE [5] . Effect of Score Function We also evaluate proposed GENs on the few-shot OOG link prediction task with another popular score function, namely TransE [5] . We use the same settings with DistMult [57] score function, except that we use TransE for the initial embedding and the score measurement. Table 7 shows that our I-GEN and T-GEN with TransE score function also outperform all baselines by impressive margins, where they perform comparably to DistMult. These results suggest that our model works regardless of the score function.", "cite_spans": [{"start": 165, "end": 168, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 319, "end": 322, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 364, "end": 368, "text": "[57]", "ref_id": "BIBREF57"}], "ref_spans": [{"start": 25, "end": 71, "text": "Unseen   Model  MRR  H@1  H@3  H@10  MRR  H@10", "ref_id": "TABREF0"}, {"start": 464, "end": 471, "text": "Table 7", "ref_id": null}], "section": "Total"}, {"text": "We further demonstrate the meta-training effectiveness of our meta-learner, by randomly initializing In-Graph, in which GEN extrapolates knowledge for an unseen entity without using the pre-trained embedding of entity and relation. Table 7 shows that, while results with the random initialization are lower than pre-trained models, GENs are still powerful on the unseen entity, compared to the baseline. These results suggest that GENs trained under the meta-learning framework can be applied to more difficult situations, as pre-trained In-Graph might not be available for the few-shot OOG link prediction in real-world scenarios. Table 8 shows some concrete examples of the OOG link prediction result from NELL-995 dataset, where the 7 to 9 rows show that our T-GEN correctly performs link prediction for two unseen entities.", "cite_spans": [], "ref_spans": [{"start": 232, "end": 239, "text": "Table 7", "ref_id": null}, {"start": 632, "end": 639, "text": "Table 8", "ref_id": null}], "section": "Effect of Initialization"}, {"text": "In this section, we describe in detail about task-level transductive inference and meta-level inductive inference for the proposed transductive GEN (T-GEN) model. Since transductive GEN requires to predict links between two unseen test entities which is impossible to handle using conventional link prediction approaches, the problem is indeed transductive. Furthermore, the inference of unseento-unseen links could be also considered as inductive at meta-level, where we inductively learn the parameters of GEN across the batch of tasks. Thus, we are tackling transductive inference problems by considering them as meta-level inductive problems, but the intrinsic unseen-to-unseen link prediction is still transductive. To illustrate more concretely, different sets of unseen entities make mutually inconsistent predictions, which is caused by transduction. Other transductive meta-learning approaches such as TPN [23] and EGNN [19] tackle the problem with similar high-level ideas, where they classify unseen classes by leveraging both of the information on labeled and unlabeled nodes.", "cite_spans": [{"start": 915, "end": 919, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 929, "end": 933, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "E Discussion on Inductive and Transductive"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Out-of-sample representation learning for multi-relational graphs", "authors": [{"first": "Marjan", "middle": [], "last": "Albooyeh", "suffix": ""}, {"first": "Rishab", "middle": [], "last": "Goel", "suffix": ""}, {"first": "Seyed Mehran", "middle": [], "last": "Kazemi", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2004.13230"]}}, "BIBREF1": {"ref_id": "b1", "title": "Similarity network fusion for aggregating data types on a genomic scale", "authors": [{"first": "F", "middle": [], "last": "Demir M. Fiume", "suffix": ""}, {"first": "Z", "middle": [], "last": "Tu", "suffix": ""}, {"first": "M", "middle": [], "last": "Brudno", "suffix": ""}, {"first": "B", "middle": [], "last": "Wang", "suffix": ""}, {"first": "A", "middle": [], "last": "Mezlini", "suffix": ""}, {"first": "A", "middle": [], "last": "Goldenberg", "suffix": ""}], "year": 2014, "venue": "Nature Methods, page", "volume": "11", "issn": "", "pages": "333--337", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Freebase: a collaboratively created graph database for structuring human knowledge", "authors": [{"first": "Kurt", "middle": ["D"], "last": "Bollacker", "suffix": ""}, {"first": "Colin", "middle": [], "last": "Evans", "suffix": ""}, {"first": "Praveen", "middle": [], "last": "Paritosh", "suffix": ""}, {"first": "Tim", "middle": [], "last": "Sturge", "suffix": ""}, {"first": "Jamie", "middle": [], "last": "Taylor", "suffix": ""}], "year": 2008, "venue": "Proceedings of the ACM SIGMOD International Conference on Management of Data", "volume": "", "issn": "", "pages": "1247--1250", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Learning structured embeddings of knowledge bases", "authors": [{"first": "Antoine", "middle": [], "last": "Bordes", "suffix": ""}, {"first": "Jason", "middle": [], "last": "Weston", "suffix": ""}, {"first": "Ronan", "middle": [], "last": "Collobert", "suffix": ""}, {"first": "Yoshua", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2011, "venue": "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Translating embeddings for modeling multi-relational data", "authors": [{"first": "Antoine", "middle": [], "last": "Bordes", "suffix": ""}, {"first": "Nicolas", "middle": [], "last": "Usunier", "suffix": ""}, {"first": "Alberto", "middle": [], "last": "Garc\u00eda-Dur\u00e1n", "suffix": ""}, {"first": "Jason", "middle": [], "last": "Weston", "suffix": ""}, {"first": "Oksana", "middle": [], "last": "Yakhnenko", "suffix": ""}], "year": 2013, "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "2787--2795", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Meta-graph: Few shot link prediction via meta learning", "authors": [{"first": "Avishek Joey", "middle": [], "last": "Bose", "suffix": ""}, {"first": "Ankit", "middle": [], "last": "Jain", "suffix": ""}, {"first": "Piero", "middle": [], "last": "Molino", "suffix": ""}, {"first": "William", "middle": ["L"], "last": "Hamilton", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1912.09867"]}}, "BIBREF6": {"ref_id": "b6", "title": "Meta relational learning for few-shot link prediction in knowledge graphs", "authors": [{"first": "Mingyang", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Wen", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Wei", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Qiang", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Huajun", "middle": [], "last": "Chen", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing", "volume": "", "issn": "", "pages": "4216--4225", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Efficacy of hydroxychloroquine in patients with covid-19: results of a randomized clinical trial. medRxiv", "authors": [{"first": "Zhaowei", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Jijia", "middle": [], "last": "Hu", "suffix": ""}, {"first": "Zongwei", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Shan", "middle": [], "last": "Jiang", "suffix": ""}, {"first": "Shoumeng", "middle": [], "last": "Han", "suffix": ""}, {"first": "Dandan", "middle": [], "last": "Yan", "suffix": ""}, {"first": "Ruhong", "middle": [], "last": "Zhuang", "suffix": ""}, {"first": "Ben", "middle": [], "last": "Hu", "suffix": ""}, {"first": "Zhan", "middle": [], "last": "Zhang", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1101/2020.03.22.20040758"]}}, "BIBREF8": {"ref_id": "b8", "title": "Convolutional neural networks on graphs with fast localized spectral filtering", "authors": [{"first": "Micha\u00ebl", "middle": [], "last": "Defferrard", "suffix": ""}, {"first": "Xavier", "middle": [], "last": "Bresson", "suffix": ""}, {"first": "Pierre", "middle": [], "last": "Vandergheynst", "suffix": ""}], "year": 2016, "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "3837--3845", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Convolutional 2d knowledge graph embeddings", "authors": [{"first": "Tim", "middle": [], "last": "Dettmers", "suffix": ""}, {"first": "Pasquale", "middle": [], "last": "Minervini", "suffix": ""}, {"first": "Pontus", "middle": [], "last": "Stenetorp", "suffix": ""}, {"first": "Sebastian", "middle": [], "last": "Riedel", "suffix": ""}], "year": 2018, "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)", "volume": "", "issn": "", "pages": "1811--1818", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Fast graph representation learning with PyTorch Geometric", "authors": [{"first": "Matthias", "middle": [], "last": "Fey", "suffix": ""}, {"first": "Jan", "middle": ["E"], "last": "Lenssen", "suffix": ""}], "year": 2019, "venue": "ICLR Workshop on Representation Learning on Graphs and Manifolds", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Model-agnostic meta-learning for fast adaptation of deep networks", "authors": [{"first": "Chelsea", "middle": [], "last": "Finn", "suffix": ""}, {"first": "Pieter", "middle": [], "last": "Abbeel", "suffix": ""}, {"first": "Sergey", "middle": [], "last": "Levine", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 34th International Conference on Machine Learning", "volume": "", "issn": "", "pages": "1126--1135", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Protein interface prediction using graph convolutional networks", "authors": [{"first": "Alex", "middle": [], "last": "Fout", "suffix": ""}, {"first": "Jonathon", "middle": [], "last": "Byrd", "suffix": ""}, {"first": "Basir", "middle": [], "last": "Shariat", "suffix": ""}, {"first": "Asa", "middle": [], "last": "Ben-Hur", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "6530--6539", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "authors": [{"first": "Yarin", "middle": [], "last": "Gal", "suffix": ""}, {"first": "Zoubin", "middle": [], "last": "Ghahramani", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 33nd International Conference on Machine Learning", "volume": "48", "issn": "", "pages": "1050--1059", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Neural message passing for quantum chemistry", "authors": [{"first": "Justin", "middle": [], "last": "Gilmer", "suffix": ""}, {"first": "Samuel", "middle": ["S"], "last": "Schoenholz", "suffix": ""}, {"first": "Patrick", "middle": ["F"], "last": "Riley", "suffix": ""}, {"first": "Oriol", "middle": [], "last": "Vinyals", "suffix": ""}, {"first": "George", "middle": ["E"], "last": "Dahl", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 34th International Conference on Machine Learning", "volume": "", "issn": "", "pages": "1263--1272", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "A new model for learning in graph domains", "authors": [{"first": "M", "middle": [], "last": "Gori", "suffix": ""}, {"first": "G", "middle": [], "last": "Monfardini", "suffix": ""}, {"first": "F", "middle": [], "last": "Scarselli", "suffix": ""}], "year": 2005, "venue": "Proceedings. 2005 IEEE International Joint Conference on Neural Networks", "volume": "2", "issn": "", "pages": "729--734", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach", "authors": [{"first": "Takuo", "middle": [], "last": "Hamaguchi", "suffix": ""}, {"first": "Hidekazu", "middle": [], "last": "Oiwa", "suffix": ""}, {"first": "Masashi", "middle": [], "last": "Shimbo", "suffix": ""}, {"first": "Yuji", "middle": [], "last": "Matsumoto", "suffix": ""}], "year": 2017, "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "1802--1808", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "Inductive representation learning on large graphs", "authors": [{"first": "William", "middle": ["L"], "last": "Hamilton", "suffix": ""}, {"first": "Zhitao", "middle": [], "last": "Ying", "suffix": ""}, {"first": "Jure", "middle": [], "last": "Leskovec", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "1024--1034", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Edge-labeling graph neural network for few-shot learning", "authors": [{"first": "Jongmin", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Taesup", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Sungwoong", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Chang", "middle": ["D"], "last": "Yoo", "suffix": ""}], "year": 2019, "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019", "volume": "", "issn": "", "pages": "11--20", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Adam: A method for stochastic optimization", "authors": [{"first": "P", "middle": [], "last": "Diederik", "suffix": ""}, {"first": "Jimmy", "middle": [], "last": "Kingma", "suffix": ""}, {"first": "", "middle": [], "last": "Ba", "suffix": ""}], "year": 2015, "venue": "3rd International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Variational graph auto-encoders", "authors": [{"first": "N", "middle": [], "last": "Thomas", "suffix": ""}, {"first": "Max", "middle": [], "last": "Kipf", "suffix": ""}, {"first": "", "middle": [], "last": "Welling", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1611.07308"]}}, "BIBREF21": {"ref_id": "b21", "title": "Semi-supervised classification with graph convolutional networks", "authors": [{"first": "N", "middle": [], "last": "Thomas", "suffix": ""}, {"first": "Max", "middle": [], "last": "Kipf", "suffix": ""}, {"first": "", "middle": [], "last": "Welling", "suffix": ""}], "year": 2017, "venue": "5th International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "Learning to propagate labels: Transductive propagation network for few-shot learning", "authors": [{"first": "Yanbin", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Juho", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Minseop", "middle": [], "last": "Park", "suffix": ""}, {"first": "Saehoon", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Eunho", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Sung", "middle": ["Ju"], "last": "Hwang", "suffix": ""}, {"first": "Yi", "middle": [], "last": "Yang", "suffix": ""}], "year": 2019, "venue": "7th International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "GENN: predicting correlated drug-drug interactions with graph energy neural networks", "authors": [{"first": "Tengfei", "middle": [], "last": "Ma", "suffix": ""}, {"first": "Junyuan", "middle": [], "last": "Shang", "suffix": ""}, {"first": "Cao", "middle": [], "last": "Xiao", "suffix": ""}, {"first": "Jimeng", "middle": [], "last": "Sun", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1910.02107"]}}, "BIBREF24": {"ref_id": "b24", "title": "Encoding sentences with graph convolutional networks for semantic role labeling", "authors": [{"first": "Diego", "middle": [], "last": "Marcheggiani", "suffix": ""}, {"first": "Ivan", "middle": [], "last": "Titov", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "1506--1515", "other_ids": {}}, "BIBREF25": {"ref_id": "b25", "title": "BioSNAP Datasets: Stanford biomedical network dataset collection", "authors": [{"first": "Marinka", "middle": [], "last": "Sagar Maheshwari", "suffix": ""}, {"first": "Rok", "middle": [], "last": "Zitnik", "suffix": ""}, {"first": "Jure", "middle": [], "last": "Sosi\u010d", "suffix": ""}, {"first": "", "middle": [], "last": "Leskovec", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF26": {"ref_id": "b26", "title": "Wordnet: A lexical database for english", "authors": [{"first": "George", "middle": ["A"], "last": "Miller", "suffix": ""}], "year": 1995, "venue": "Commun. ACM", "volume": "38", "issn": "11", "pages": "39--41", "other_ids": {}}, "BIBREF27": {"ref_id": "b27", "title": "Distant supervision for relation extraction with an incomplete knowledge base", "authors": [{"first": "Bonan", "middle": [], "last": "Min", "suffix": ""}, {"first": "Ralph", "middle": [], "last": "Grishman", "suffix": ""}, {"first": "Li", "middle": [], "last": "Wan", "suffix": ""}, {"first": "Chang", "middle": [], "last": "Wang", "suffix": ""}, {"first": "David", "middle": [], "last": "Gondek", "suffix": ""}], "year": 2013, "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings", "volume": "", "issn": "", "pages": "777--782", "other_ids": {}}, "BIBREF28": {"ref_id": "b28", "title": "Neverending learning", "authors": [{"first": "Tom", "middle": ["M"], "last": "Mitchell", "suffix": ""}, {"first": "William", "middle": ["W"], "last": "Cohen", "suffix": ""}, {"first": "Estevam", "middle": ["R"], "last": "Hruschka", "suffix": ""}, {"first": "Justin", "middle": [], "last": "Partha Pratim Talukdar", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Betteridge", "suffix": ""}, {"first": "", "middle": [], "last": "Carlson", "suffix": ""}, {"first": "Matthew", "middle": [], "last": "Bhavana Dalvi Mishra", "suffix": ""}, {"first": "Bryan", "middle": [], "last": "Gardner", "suffix": ""}, {"first": "Jayant", "middle": [], "last": "Kisiel", "suffix": ""}, {"first": "Ni", "middle": [], "last": "Krishnamurthy", "suffix": ""}, {"first": "Kathryn", "middle": [], "last": "Lao", "suffix": ""}, {"first": "Thahir", "middle": [], "last": "Mazaitis", "suffix": ""}, {"first": "Ndapandula", "middle": [], "last": "Mohamed", "suffix": ""}, {"first": "", "middle": [], "last": "Nakashole", "suffix": ""}, {"first": "A", "middle": [], "last": "Emmanouil", "suffix": ""}, {"first": "Alan", "middle": [], "last": "Platanios", "suffix": ""}, {"first": "Mehdi", "middle": [], "last": "Ritter", "suffix": ""}, {"first": "Burr", "middle": [], "last": "Samadi", "suffix": ""}, {"first": "Richard", "middle": ["C"], "last": "Settles", "suffix": ""}, {"first": "Derry", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Abhinav", "middle": [], "last": "Wijaya", "suffix": ""}, {"first": "Xinlei", "middle": [], "last": "Gupta", "suffix": ""}, {"first": "Abulhair", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Malcolm", "middle": [], "last": "Saparov", "suffix": ""}, {"first": "Joel", "middle": [], "last": "Greaves", "suffix": ""}, {"first": "", "middle": [], "last": "Welling", "suffix": ""}], "year": 2015, "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "2302--2310", "other_ids": {}}, "BIBREF29": {"ref_id": "b29", "title": "Learning attention-based embeddings for relation prediction in knowledge graphs", "authors": [{"first": "Deepak", "middle": [], "last": "Nathani", "suffix": ""}, {"first": "Jatin", "middle": [], "last": "Chauhan", "suffix": ""}, {"first": "Charu", "middle": [], "last": "Sharma", "suffix": ""}, {"first": "Manohar", "middle": [], "last": "Kaul", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019", "volume": "1", "issn": "", "pages": "4710--4723", "other_ids": {}}, "BIBREF30": {"ref_id": "b30", "title": "A novel embedding model for knowledge base completion based on convolutional neural network", "authors": [{"first": "Tu", "middle": ["Dinh"], "last": "Dai Quoc Nguyen", "suffix": ""}, {"first": "Dat", "middle": [], "last": "Nguyen", "suffix": ""}, {"first": "Dinh", "middle": ["Q"], "last": "Quoc Nguyen", "suffix": ""}, {"first": "", "middle": [], "last": "Phung", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT", "volume": "2", "issn": "", "pages": "327--333", "other_ids": {}}, "BIBREF31": {"ref_id": "b31", "title": "On first-order meta-learning algorithms", "authors": [{"first": "Alex", "middle": [], "last": "Nichol", "suffix": ""}, {"first": "Joshua", "middle": [], "last": "Achiam", "suffix": ""}, {"first": "John", "middle": [], "last": "Schulman", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1803.02999"]}}, "BIBREF32": {"ref_id": "b32", "title": "A three-way model for collective learning on multi-relational data", "authors": [{"first": "Maximilian", "middle": [], "last": "Nickel", "suffix": ""}, {"first": "Hans-Peter", "middle": [], "last": "Volker Tresp", "suffix": ""}, {"first": "", "middle": [], "last": "Kriegel", "suffix": ""}], "year": 2011, "venue": "Proceedings of the 28th International Conference on Machine Learning", "volume": "", "issn": "", "pages": "809--816", "other_ids": {}}, "BIBREF33": {"ref_id": "b33", "title": "Label propagation prediction of drug-drug interaction", "authors": [{"first": "J", "middle": [], "last": "Hu", "suffix": ""}, {"first": "P", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "F", "middle": [], "last": "Wang", "suffix": ""}, {"first": "R", "middle": [], "last": "Sorrentino", "suffix": ""}], "year": 2015, "venue": "Scientific reports", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF34": {"ref_id": "b34", "title": "Pytorch: An imperative style, highperformance deep learning library", "authors": [{"first": "Adam", "middle": [], "last": "Paszke", "suffix": ""}, {"first": "Sam", "middle": [], "last": "Gross", "suffix": ""}, {"first": "Francisco", "middle": [], "last": "Massa", "suffix": ""}, {"first": "Adam", "middle": [], "last": "Lerer", "suffix": ""}, {"first": "James", "middle": [], "last": "Bradbury", "suffix": ""}, {"first": "Gregory", "middle": [], "last": "Chanan", "suffix": ""}, {"first": "Trevor", "middle": [], "last": "Killeen", "suffix": ""}, {"first": "Zeming", "middle": [], "last": "Lin", "suffix": ""}, {"first": "Natalia", "middle": [], "last": "Gimelshein", "suffix": ""}, {"first": "Luca", "middle": [], "last": "Antiga", "suffix": ""}, {"first": "Alban", "middle": [], "last": "Desmaison", "suffix": ""}, {"first": "Andreas", "middle": [], "last": "Kopf", "suffix": ""}, {"first": "Edward", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Zachary", "middle": [], "last": "Devito", "suffix": ""}, {"first": "Martin", "middle": [], "last": "Raison", "suffix": ""}, {"first": "Alykhan", "middle": [], "last": "Tejani", "suffix": ""}, {"first": "Sasank", "middle": [], "last": "Chilamkurthy", "suffix": ""}, {"first": "Benoit", "middle": [], "last": "Steiner", "suffix": ""}, {"first": "Lu", "middle": [], "last": "Fang", "suffix": ""}, {"first": "Junjie", "middle": [], "last": "Bai", "suffix": ""}, {"first": "Soumith", "middle": [], "last": "Chintala", "suffix": ""}], "year": 2019, "venue": "Advances in Neural Information Processing Systems", "volume": "32", "issn": "", "pages": "8024--8035", "other_ids": {}}, "BIBREF35": {"ref_id": "b35", "title": "Deep learning improves prediction of drug-drug and drug-food interactions", "authors": [{"first": "Jae", "middle": [], "last": "Yong Ryu", "suffix": ""}, {"first": "Hyun", "middle": ["Uk"], "last": "Kim", "suffix": ""}, {"first": "Sang", "middle": ["Yup"], "last": "Lee", "suffix": ""}], "year": 2018, "venue": "Proceedings of the National Academy of Sciences", "volume": "", "issn": "", "pages": "4304--4311", "other_ids": {}}, "BIBREF36": {"ref_id": "b36", "title": "The graph neural network model", "authors": [{"first": "Franco", "middle": [], "last": "Scarselli", "suffix": ""}, {"first": "Marco", "middle": [], "last": "Gori", "suffix": ""}, {"first": "Ah", "middle": [], "last": "Chung Tsoi", "suffix": ""}, {"first": "Markus", "middle": [], "last": "Hagenbuchner", "suffix": ""}, {"first": "Gabriele", "middle": [], "last": "Monfardini", "suffix": ""}], "year": 2009, "venue": "IEEE Trans. Neural Networks", "volume": "20", "issn": "1", "pages": "61--80", "other_ids": {}}, "BIBREF37": {"ref_id": "b37", "title": "Modeling relational data with graph convolutional networks", "authors": [{"first": "", "middle": [], "last": "Michael Sejr", "suffix": ""}, {"first": "Thomas", "middle": ["N"], "last": "Schlichtkrull", "suffix": ""}, {"first": "Peter", "middle": [], "last": "Kipf", "suffix": ""}, {"first": "Rianne", "middle": [], "last": "Bloem", "suffix": ""}, {"first": "", "middle": [], "last": "Van Den", "suffix": ""}, {"first": "Ivan", "middle": [], "last": "Berg", "suffix": ""}, {"first": "Max", "middle": [], "last": "Titov", "suffix": ""}, {"first": "", "middle": [], "last": "Welling", "suffix": ""}], "year": 2018, "venue": "The Semantic Web -15th International Conference", "volume": "", "issn": "", "pages": "593--607", "other_ids": {}}, "BIBREF38": {"ref_id": "b38", "title": "End-to-end structure-aware convolutional networks for knowledge base completion", "authors": [{"first": "Chao", "middle": [], "last": "Shang", "suffix": ""}, {"first": "Yun", "middle": [], "last": "Tang", "suffix": ""}, {"first": "Jing", "middle": [], "last": "Huang", "suffix": ""}, {"first": "Jinbo", "middle": [], "last": "Bi", "suffix": ""}, {"first": "Xiaodong", "middle": [], "last": "He", "suffix": ""}, {"first": "Bowen", "middle": [], "last": "Zhou", "suffix": ""}], "year": 2019, "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence", "volume": "2019", "issn": "", "pages": "3060--3067", "other_ids": {}}, "BIBREF39": {"ref_id": "b39", "title": "Open-world knowledge graph completion", "authors": [{"first": "Baoxu", "middle": [], "last": "Shi", "suffix": ""}, {"first": "Tim", "middle": [], "last": "Weninger", "suffix": ""}], "year": 2018, "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)", "volume": "", "issn": "", "pages": "1957--1964", "other_ids": {}}, "BIBREF40": {"ref_id": "b40", "title": "Prototypical networks for few-shot learning", "authors": [{"first": "Jake", "middle": [], "last": "Snell", "suffix": ""}, {"first": "Kevin", "middle": [], "last": "Swersky", "suffix": ""}, {"first": "Richard", "middle": ["S"], "last": "Zemel", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "4077--4087", "other_ids": {}}, "BIBREF41": {"ref_id": "b41", "title": "Learning structured output representation using deep conditional generative models", "authors": [{"first": "Kihyuk", "middle": [], "last": "Sohn", "suffix": ""}, {"first": "Honglak", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Xinchen", "middle": [], "last": "Yan", "suffix": ""}], "year": 2015, "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "3483--3491", "other_ids": {}}, "BIBREF42": {"ref_id": "b42", "title": "Rotate: Knowledge graph embedding by relational rotation in complex space", "authors": [{"first": "Zhiqing", "middle": [], "last": "Sun", "suffix": ""}, {"first": "Zhi-Hong", "middle": [], "last": "Deng", "suffix": ""}, {"first": "Jian-Yun", "middle": [], "last": "Nie", "suffix": ""}, {"first": "Jian", "middle": [], "last": "Tang", "suffix": ""}], "year": 2019, "venue": "7th International Conference on Learning Representations, ICLR 2019", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF43": {"ref_id": "b43", "title": "Representing text for joint embedding of text and knowledge bases", "authors": [{"first": "Kristina", "middle": [], "last": "Toutanova", "suffix": ""}, {"first": "Danqi", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Patrick", "middle": [], "last": "Pantel", "suffix": ""}, {"first": "Hoifung", "middle": [], "last": "Poon", "suffix": ""}, {"first": "Pallavi", "middle": [], "last": "Choudhury", "suffix": ""}, {"first": "Michael", "middle": [], "last": "Gamon", "suffix": ""}], "year": 2015, "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "1499--1509", "other_ids": {}}, "BIBREF44": {"ref_id": "b44", "title": "Complex embeddings for simple link prediction", "authors": [{"first": "Th\u00e9o", "middle": [], "last": "Trouillon", "suffix": ""}, {"first": "Johannes", "middle": [], "last": "Welbl", "suffix": ""}, {"first": "Sebastian", "middle": [], "last": "Riedel", "suffix": ""}, {"first": "\u00c9ric", "middle": [], "last": "Gaussier", "suffix": ""}, {"first": "Guillaume", "middle": [], "last": "Bouchard", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 33nd International Conference on Machine Learning", "volume": "", "issn": "", "pages": "2071--2080", "other_ids": {}}, "BIBREF45": {"ref_id": "b45", "title": "Composition-based multi-relational graph convolutional networks", "authors": [{"first": "Shikhar", "middle": [], "last": "Vashishth", "suffix": ""}, {"first": "Soumya", "middle": [], "last": "Sanyal", "suffix": ""}, {"first": "Vikram", "middle": [], "last": "Nitin", "suffix": ""}, {"first": "Partha", "middle": ["P"], "last": "Talukdar", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1911.03082"]}}, "BIBREF46": {"ref_id": "b46", "title": "Graph attention networks", "authors": [{"first": "Petar", "middle": [], "last": "Velickovic", "suffix": ""}, {"first": "Guillem", "middle": [], "last": "Cucurull", "suffix": ""}, {"first": "Arantxa", "middle": [], "last": "Casanova", "suffix": ""}, {"first": "Adriana", "middle": [], "last": "Romero", "suffix": ""}, {"first": "Pietro", "middle": [], "last": "Li\u00f2", "suffix": ""}, {"first": "Yoshua", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2018, "venue": "6th International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF47": {"ref_id": "b47", "title": "Matching networks for one shot learning", "authors": [{"first": "Oriol", "middle": [], "last": "Vinyals", "suffix": ""}, {"first": "Charles", "middle": [], "last": "Blundell", "suffix": ""}, {"first": "Tim", "middle": [], "last": "Lillicrap", "suffix": ""}, {"first": "Koray", "middle": [], "last": "Kavukcuoglu", "suffix": ""}, {"first": "Daan", "middle": [], "last": "Wierstra", "suffix": ""}], "year": 2016, "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "3630--3638", "other_ids": {}}, "BIBREF48": {"ref_id": "b48", "title": "Logic attention based neighborhood aggregation for inductive knowledge graph embedding", "authors": [{"first": "Peifeng", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Jialong", "middle": [], "last": "Han", "suffix": ""}, {"first": "Chenliang", "middle": [], "last": "Li", "suffix": ""}, {"first": "Rong", "middle": [], "last": "Pan", "suffix": ""}], "year": 2019, "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence", "volume": "2019", "issn": "", "pages": "7152--7159", "other_ids": {}}, "BIBREF49": {"ref_id": "b49", "title": "Learning to model the tail", "authors": [{"first": "Yu-Xiong", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Deva", "middle": [], "last": "Ramanan", "suffix": ""}, {"first": "Martial", "middle": [], "last": "Hebert", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems", "volume": "", "issn": "", "pages": "7029--7039", "other_ids": {}}, "BIBREF50": {"ref_id": "b50", "title": "Knowledge graph embedding by translating on hyperplanes", "authors": [{"first": "Zhen", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Jianwen", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Jianlin", "middle": [], "last": "Feng", "suffix": ""}, {"first": "Zheng", "middle": [], "last": "Chen", "suffix": ""}], "year": 2014, "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "1112--1119", "other_ids": {}}, "BIBREF51": {"ref_id": "b51", "title": "Tackling long-tailed relations and uncommon entities in knowledge graph completion", "authors": [{"first": "Zihao", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Ping", "middle": [], "last": "Kwun", "suffix": ""}, {"first": "Piji", "middle": [], "last": "Lai", "suffix": ""}, {"first": "Lidong", "middle": [], "last": "Li", "suffix": ""}, {"first": "Wai", "middle": [], "last": "Bing", "suffix": ""}, {"first": "", "middle": [], "last": "Lam", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing", "volume": "", "issn": "", "pages": "250--260", "other_ids": {}}, "BIBREF53": {"ref_id": "b53", "title": "Saliva is more sensitive for sars-cov-2 detection in covid-19 patients than nasopharyngeal swabs. medRxiv", "authors": [{"first": "Albert", "middle": ["I"], "last": "Grubaugh", "suffix": ""}, {"first": "", "middle": [], "last": "Ko", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1101/2020.04.16.20067835"]}}, "BIBREF54": {"ref_id": "b54", "title": "Representation learning of knowledge graphs with entity descriptions", "authors": [{"first": "Ruobing", "middle": [], "last": "Xie", "suffix": ""}, {"first": "Zhiyuan", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Jia", "middle": [], "last": "Jia", "suffix": ""}, {"first": "Huanbo", "middle": [], "last": "Luan", "suffix": ""}, {"first": "Maosong", "middle": [], "last": "Sun", "suffix": ""}], "year": 2016, "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "2659--2665", "other_ids": {}}, "BIBREF55": {"ref_id": "b55", "title": "Deeppath: A reinforcement learning method for knowledge graph reasoning", "authors": [{"first": "Wenhan", "middle": [], "last": "Xiong", "suffix": ""}, {"first": "Thien", "middle": [], "last": "Hoang", "suffix": ""}, {"first": "William", "middle": ["Yang"], "last": "Wang", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "564--573", "other_ids": {}}, "BIBREF56": {"ref_id": "b56", "title": "One-shot relational learning for knowledge graphs", "authors": [{"first": "Wenhan", "middle": [], "last": "Xiong", "suffix": ""}, {"first": "Mo", "middle": [], "last": "Yu", "suffix": ""}, {"first": "Shiyu", "middle": [], "last": "Chang", "suffix": ""}, {"first": "Xiaoxiao", "middle": [], "last": "Guo", "suffix": ""}, {"first": "William", "middle": ["Yang"], "last": "Wang", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "1980--1990", "other_ids": {}}, "BIBREF57": {"ref_id": "b57", "title": "Embedding entities and relations for learning and inference in knowledge bases", "authors": [{"first": "Bishan", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Wen-Tau", "middle": [], "last": "Yih", "suffix": ""}, {"first": "Xiaodong", "middle": [], "last": "He", "suffix": ""}, {"first": "Jianfeng", "middle": [], "last": "Gao", "suffix": ""}, {"first": "Li", "middle": [], "last": "Deng", "suffix": ""}], "year": 2015, "venue": "3rd International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF58": {"ref_id": "b58", "title": "Predicting potential drug-drug interactions by integrating chemical, biological, phenotypic and network data", "authors": [{"first": "Wen", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Yanlin", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Feng", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Fei", "middle": [], "last": "Luo", "suffix": ""}, {"first": "Gang", "middle": [], "last": "Tian", "suffix": ""}, {"first": "Xiaohong", "middle": [], "last": "Li", "suffix": ""}], "year": 2017, "venue": "BMC Bioinformatics", "volume": "18", "issn": "1", "pages": "", "other_ids": {}}, "BIBREF59": {"ref_id": "b59", "title": "Metagnn: On few-shot node classification in graph meta-learning", "authors": [{"first": "Fan", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "Chengtai", "middle": [], "last": "Cao", "suffix": ""}, {"first": "Kunpeng", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Goce", "middle": [], "last": "Trajcevski", "suffix": ""}, {"first": "Ting", "middle": [], "last": "Zhong", "suffix": ""}, {"first": "Ji", "middle": [], "last": "Geng", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019", "volume": "", "issn": "", "pages": "2357--2360", "other_ids": {}}, "BIBREF60": {"ref_id": "b60", "title": "Modeling polypharmacy side effects with graph convolutional networks", "authors": [{"first": "Marinka", "middle": [], "last": "Zitnik", "suffix": ""}, {"first": "Monica", "middle": [], "last": "Agrawal", "suffix": ""}, {"first": "Jure", "middle": [], "last": "Leskovec", "suffix": ""}], "year": 2018, "venue": "Bioinformatics", "volume": "", "issn": "", "pages": "457--466", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Distribution for entity frequency.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Definition 3.3. (Few-Shot Out-Of-Graph Link Prediction) Given a graph G \u2286 E \u00d7 R \u00d7 E, an unseen entity is an entity e \u2208 E , where E \u2229 E = \u2205. Then, an out-of-graph link prediction is the problem of performing link prediction on (e , r, ?), (?, r, e ), (e , ?, e), or (e, ?, e ),", "latex": null, "type": "figure"}, "FIGREF2": {"text": "The results of seen to unseen (S/U), unseen to unseen (U/U) and total link prediction of I-and T-GEN with deterministic (D) and stochastic (S) modeling on KG completion and DDI prediction tasks.", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Diverse shots link prediction results with baselines and GENs on KG completion tasks.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "Distribution for entity occurrences on four datasets.", "latex": null, "type": "figure"}, "TABREF0": {"text": "Score functions for multi-relational graph, where \u2295 denotes concatenation.", "latex": null, "type": "table"}, "TABREF1": {"text": "The results of 1-and 3-shot OOG link prediction on FB15k-237 and NELL-995. * means training a model within our meta-learning framework. Bold numbers denote the best results.", "latex": null, "type": "table"}, "TABREF3": {"text": "Cross-shot learning results of T-GEN on KG completion tasks.", "latex": null, "type": "table"}, "TABREF4": {"text": "Ablation study of T-GEN on FB15k-237.SI means whether to apply stochastic inference.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>\u00a0</td><td>S / U </td><td>U / U\n</td></tr><tr><td>Model </td><td>SI MRR H@3 MRR H@3\n</td></tr><tr><td>T-GEN </td><td>O </td><td>.379 </td><td>.424 </td><td>.185 </td><td>.187\n</td></tr><tr><td>\u00a0</td><td>O .374 </td><td>.414 </td><td>.183 </td><td>.175\n</td></tr><tr><td>w/o transfer strategy w/o pretrain w/o stochastic inference X w/o transductive scheme X </td><td>O </td><td>.361 </td><td>.400 </td><td>.168 </td><td>.164\n</td></tr><tr><td>\u00a0</td><td>.384 </td><td>.425 </td><td>.153 </td><td>.158\n</td></tr><tr><td>\u00a0</td><td>.366 </td><td>.403 </td><td>.000 </td><td>.000\n</td></tr></table></body></html>"}, "TABREF6": {"text": "The results of 3-shot relation prediction on DeepDDI and BIOSNAP-sub.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>DeepDDI </td><td>BIOSNAP-sub\n</td></tr><tr><td>Model </td><td>ROC PR Acc ROC PR Acc\n</td></tr><tr><td>MLP </td><td>.928 </td><td>.476 .528 </td><td>.597 .034 .049\n</td></tr><tr><td>MPNN [</td><td>15] .939 </td><td>.478 .681 </td><td>.597 .026 .067\n</td></tr><tr><td>R-GCN [</td><td>38] .928 </td><td>.397 .640 </td><td>.594 .041 .051\n</td></tr><tr><td>I-GEN </td><td>.946 </td><td>.681 .807 </td><td>.608 .062 .073\n</td></tr><tr><td>T-GEN </td><td>.954 </td><td>.708 .815 </td><td>.625 .067 .089\n</td></tr></table></body></html>"}, "TABREF7": {"text": "The naive and meta-learning strategy results of 1-and 3-shot OOG link prediction on FB15k-237 and NELL-995. Bold numbers denote the best results on I-GEN and T-GEN, respectively.", "latex": null, "type": "table"}, "TABREF8": {"text": "GEN .348 .367 .270 .281 .382 .407 .504 .537 .278 .285 .206 .214 .313 .322 .416 .426 w/o transfer strategy .344 .362 .264 .275 .379 .401 .503 .527 .272 .277 .198 .206 .309 .314 .413 .414 T-GEN .367 .382 .282 .289 .410 .430 .530 .565 .282 .291 .209 .217 .320 .333 .421 .433 w/o transfer strategy .362 .381 .278 .291 .400 .422 .527 .563 .273 .290 .198 .217 .310 .326 .412 .431", "latex": null, "type": "table"}, "TABREF9": {"text": "053 .048 .034 .026 .050 .050 .082 .077 .055 .050 .086 .081 .016 .014 .029 .025 DistMult [57] .017 .014 .010 .009 .019 .014 .029 .022 .018 .015 .029 .022 .011 .007 .025 .015 R-GCN [38] .008 .006 .004 .003 .007 .005 .011 .010 .003 .003 .005 .006 .076 .050 .101 .070 Seen to Unseen MEAN [17] .105 .114 .052 .058 .109 .119 .207 .217 .112 .121 .221 .231 .000 .000 .000 .000 LAN [49] .112 .112 .057 .055 .118 .119 .214 .218 .119 .119 .228 .232 .000 .000 .000 .000 GMatching* [56] .224 .238 .157 .168 .249 .263 .352 .372 .239 .254 .375 .400 .000 .000 .000 .000 Ours I-GEN (Random) .309 .319 .236 240 .337 .352 .455 .477 .329 .339 .485 .508 .000 .000 .000 .000 I-GEN (DistMult) .348 .367 .270 .281 .382 .407 .504 .537 .371 .391 .537 .571 .000 .000 .000 .000 I-GEN (TransE) .345 .371 .259 .275 .385 .416 .515 .559 .367 .395 .548 .594 .000 .000 .000 .000 T-GEN (Random) .349 .360 .268 .273 .385 .398 .508 .532 .361 .373 .529 .554 .168 .164 .185 .192 T-GEN (DistMult) .367 .382 .282 .289 .410 .430 .530 .565 .379 .396 .550 .588 .185 .175 .220 .201 T-GEN (TransE) .356 .374 .267 .282 .403 .425 .531 .552 .368 .387 .552 .572 .175 .175 .205 .235 C More Experimental Results", "latex": null, "type": "table"}, "TABREF10": {"text": "Table 1). As a result, the objective of\n3\nTable 1:", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>Score functions for multi-relational graph, where \u2295 denotes concatenation.\n</td></tr><tr><td>Model </td><td>Score Function </td><td>Domain\n</td></tr><tr><td>TransE [5] </td><td>\u2212\u2016eh + r \u2212 et\u20162 </td><td>Knowledge Graph\n</td></tr><tr><td>DistMult [57] </td><td>\u3008eh, r, et\u3009 r(eh \u2295 et) </td><td>Knowledge Graph\n</td></tr><tr><td>\u00a0</td><td>Linear [15] </td><td>Drug Interaction\n</td></tr></table></body></html>"}, "TABREF11": {"text": "Table 2: The results of 1- and 3-shot OOG link prediction on FB15k-237 and NELL-995. * means training a", "latex": null, "type": "table", "html": "<html><body><table><tr><td>model within our meta-learning framework. Bold numbers denote the best results.\n</td><td>FB15k-237 </td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>NELL-995\n</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>\u00a0</td><td>Model </td><td>MRR 1-S </td><td>3-S </td><td>H@1 1-S </td><td>3-S </td><td>H@3 1-S </td><td>3-S </td><td>H@10 1-S </td><td>3-S </td><td>MRR 1-S </td><td>3-S </td><td>H@1 1-S </td><td>3-S </td><td>H@3 1-S </td><td>3-S </td><td>H@10\n1-S </td><td>3-S\n</td></tr><tr><td>Seen to Seen\n</td><td>TransE [5] </td><td>.053 </td><td>.048 </td><td>.034 </td><td>.026 </td><td>.050 </td><td>.050 </td><td>.082 </td><td>.077 </td><td>.009 </td><td>.010 </td><td>.002 </td><td>.002 </td><td>.007 </td><td>.008 </td><td>.020 </td><td>.021\n</td></tr><tr><td>DistMult [57] </td><td>.017 </td><td>.014 </td><td>.010 </td><td>.009 </td><td>.019 </td><td>.014 </td><td>.029 </td><td>.022 </td><td>.017 </td><td>.016 </td><td>.009 </td><td>.008 </td><td>.017 </td><td>.017 </td><td>.029 </td><td>.028\n</td></tr><tr><td>R-GCN [38] </td><td>.008 </td><td>.006 </td><td>.004 </td><td>.003 </td><td>.007 </td><td>.005 </td><td>.011 </td><td>.010 </td><td>.004 </td><td>.004 </td><td>.001 </td><td>.001 </td><td>.003 </td><td>.003 </td><td>.007 </td><td>.006\n</td></tr><tr><td>\u00a0</td><td>TransE [5] </td><td>.071 </td><td>.120 </td><td>.023 </td><td>.057 </td><td>.086 </td><td>.137 </td><td>.159 </td><td>.238 </td><td>.071 </td><td>.118 </td><td>.037 </td><td>.061 </td><td>.079 </td><td>.132 </td><td>.129 </td><td>.223\n</td></tr><tr><td>Seen to Seen\n(with Support Set)\n</td><td>DistMult [57] </td><td>.059 </td><td>.094 </td><td>.034 </td><td>.053 </td><td>.064 </td><td>.101 </td><td>.103 </td><td>.172 </td><td>.075 </td><td>.134 </td><td>.045 </td><td>.083 </td><td>.083 </td><td>.143 </td><td>.131 </td><td>.233\n</td></tr><tr><td>ComplEx [45] </td><td>.062 </td><td>.104 </td><td>.037 </td><td>.058 </td><td>.067 </td><td>.114 </td><td>.110 </td><td>.188 </td><td>.069 </td><td>.124 </td><td>.045 </td><td>.077 </td><td>.071 </td><td>.134 </td><td>.117 </td><td>.213\n</td></tr><tr><td>RotatE [43] </td><td>.063 </td><td>.115 </td><td>.039 </td><td>.069 </td><td>.071 </td><td>.131 </td><td>.105 </td><td>.200 </td><td>.054 </td><td>.112 </td><td>.028 </td><td>.060 </td><td>.064 </td><td>.131 </td><td>.104 </td><td>.209\n</td></tr><tr><td>\u00a0</td><td>R-GCN [38] </td><td>.099 </td><td>.140 </td><td>.056 </td><td>.082 </td><td>.104 </td><td>.154 </td><td>.181 </td><td>.255 </td><td>.112 </td><td>.199 </td><td>.074 </td><td>.141 </td><td>.119 </td><td>.219 </td><td>.184 </td><td>.307\n</td></tr><tr><td>Seen to Unseen\n</td><td>MEAN [17] </td><td>.105 </td><td>.114 </td><td>.052 </td><td>.058 </td><td>.109 </td><td>.119 </td><td>.207 </td><td>.217 </td><td>.158 </td><td>.180 </td><td>.107 </td><td>.124 </td><td>.173 </td><td>.189 </td><td>.263 </td><td>.296\n</td></tr><tr><td>LAN [49] </td><td>.112 </td><td>.112 </td><td>.057 </td><td>.055 </td><td>.118 </td><td>.119 </td><td>.214 </td><td>.218 </td><td>.159 </td><td>.172 </td><td>.111 </td><td>.116 </td><td>.172 </td><td>.181 </td><td>.255 </td><td>.286\n</td></tr><tr><td>GMatching* [56] </td><td>.224 </td><td>.238 </td><td>.157 </td><td>.168 </td><td>.249 </td><td>.263 </td><td>.352 </td><td>.372 </td><td>.120 </td><td>.139 </td><td>.074 </td><td>.092 </td><td>.136 </td><td>.151 </td><td>.215 </td><td>.235\n</td></tr><tr><td>Ours </td><td>I-GEN .348 .367 .270 .281 .382 .407 .504 .537 .278 .285 .206 .214 .313 .322 .416 .426T-GEN .367 .382 .282 .289 .410 .430 .530 .565 .282 .291 .209 .217 .320 .333 .421 .433\n</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td></tr></table></body></html>"}, "TABREF12": {"text": "Table 3: Cross-shot learning results of", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>T-GEN on KG completion tasks.\n1-Shot </td><td>Test MRR H@1 H@10 MRR H@1 H@10\n</td><td>3-Shot\n</td></tr><tr><td>1-S </td><td>.367 </td><td>.282 </td><td>.530 </td><td>.346 </td><td>.262 </td><td>.507\n</td></tr><tr><td>3-S .377 </td><td>.288 </td><td>.556 </td><td>.382 </td><td>.289 </td><td>.565\n</td></tr><tr><td>5-S </td><td>.362 </td><td>.266 </td><td>.562 </td><td>.370 </td><td>.269 </td><td>.570\n</td></tr><tr><td>R-S </td><td>.375 </td><td>.287 </td><td>.548 </td><td>.373 </td><td>.282 </td><td>.547\n</td></tr></table></body></html>"}, "TABREF13": {"text": "Table 6: The naive and meta-learning strategy results of 1- and 3-shot OOG link prediction on FB15k-237 and", "latex": null, "type": "table", "html": "<html><body><table><tr><td>NELL-995. Bold numbers denote the best results on I-GEN and T-GEN, respectively.\n</td><td>FB15k-237 </td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>NELL-995\n</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>Model </td><td>MRR 1-S </td><td>3-S </td><td>H@1 1-S </td><td>3-S </td><td>H@3 1-S </td><td>3-S </td><td>H@10 1-S </td><td>3-S </td><td>MRR 1-S </td><td>3-S </td><td>H@1 1-S </td><td>3-S </td><td>H@3 1-S </td><td>3-S </td><td>H@10\n1-S </td><td>3-S\n</td></tr><tr><td>I-GEN </td><td>.348 </td><td>.367 </td><td>.270 </td><td>.281 </td><td>.382 </td><td>.407 </td><td>.504 </td><td>.537 </td><td>.278 </td><td>.285 </td><td>.206 </td><td>.214 </td><td>.313 </td><td>.322 </td><td>.416 </td><td>.426\n</td></tr><tr><td>w/o transfer strategy </td><td>.344 </td><td>.362 </td><td>.264 </td><td>.275 </td><td>.379 </td><td>.401 </td><td>.503 </td><td>.527 </td><td>.272 </td><td>.277 </td><td>.198 </td><td>.206 </td><td>.309 </td><td>.314 </td><td>.413 </td><td>.414\n</td></tr><tr><td>T-GEN </td><td>.367 </td><td>.382 </td><td>.282 </td><td>.289 </td><td>.410 </td><td>.430 </td><td>.530 </td><td>.565 </td><td>.282 </td><td>.291 </td><td>.209 </td><td>.217 </td><td>.320 </td><td>.333 </td><td>.421 </td><td>.433\n</td></tr><tr><td>w/o transfer strategy </td><td>.362 </td><td>.381 </td><td>.278 </td><td>.291 </td><td>.400 </td><td>.422 </td><td>.527 </td><td>.563 </td><td>.273 </td><td>.290 </td><td>.198 </td><td>.217 </td><td>.310 </td><td>.326 </td><td>.412 </td><td>.431\n</td></tr></table></body></html>"}, "TABREF14": {"text": "Table 7: Total, seen-to-unseen and unseen-to-unseen results of 1- and 3-shot OOG link prediction on FB15k-237.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>Total </td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>Seen to Unseen </td><td>* means training a model within our meta-learning framework. Bold numbers denote the best results.\n</td><td>Unseen to Unseen\n</td></tr><tr><td>\u00a0</td><td>Model </td><td>MRR </td><td>H@1 </td><td>H@3 </td><td>H@10 </td><td>MRR </td><td>H@10 </td><td>MRR </td><td>H@10\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>1-S </td><td>3-S </td><td>1-S </td><td>3-S </td><td>1-S </td><td>3-S </td><td>1-S </td><td>3-S </td><td>1-S </td><td>3-S </td><td>1-S </td><td>3-S </td><td>1-S </td><td>3-S </td><td>1-S </td><td>3-S\n</td></tr><tr><td>Seen to Seen\n</td><td>TransE [5] </td><td>.053 </td><td>.048 </td><td>.034 </td><td>.026 </td><td>.050 </td><td>.050 </td><td>.082 </td><td>.077 </td><td>.055 </td><td>.050 </td><td>.086 </td><td>.081 </td><td>.016 </td><td>.014 </td><td>.029 </td><td>.025\n</td></tr><tr><td>DistMult [57] </td><td>.017 </td><td>.014 </td><td>.010 </td><td>.009 </td><td>.019 </td><td>.014 </td><td>.029 </td><td>.022 </td><td>.018 </td><td>.015 </td><td>.029 </td><td>.022 </td><td>.011 </td><td>.007 </td><td>.025 </td><td>.015\n</td></tr><tr><td>R-GCN [38] </td><td>.008 </td><td>.006 </td><td>.004 </td><td>.003 </td><td>.007 </td><td>.005 </td><td>.011 </td><td>.010 </td><td>.003 </td><td>.003 </td><td>.005 </td><td>.006 </td><td>.076 </td><td>.050 </td><td>.101 </td><td>.070\n</td></tr><tr><td>Seen to Unseen\n</td><td>MEAN [17] </td><td>.105 </td><td>.114 </td><td>.052 </td><td>.058 </td><td>.109 </td><td>.119 </td><td>.207 </td><td>.217 </td><td>.112 </td><td>.121 </td><td>.221 </td><td>.231 </td><td>.000 </td><td>.000 </td><td>.000 </td><td>.000\n</td></tr><tr><td>LAN [49] </td><td>.112 </td><td>.112 </td><td>.057 </td><td>.055 </td><td>.118 </td><td>.119 </td><td>.214 </td><td>.218 </td><td>.119 </td><td>.119 </td><td>.228 </td><td>.232 </td><td>.000 </td><td>.000 </td><td>.000 </td><td>.000\n</td></tr><tr><td>GMatching* [56] </td><td>.224 </td><td>.238 </td><td>.157 </td><td>.168 </td><td>.249 </td><td>.263 </td><td>.352 </td><td>.372 </td><td>.239 </td><td>.254 </td><td>.375 </td><td>.400 </td><td>.000 </td><td>.000 </td><td>.000 </td><td>.000\n</td></tr><tr><td>\u00a0</td><td>I-GEN (Random) </td><td>.309 </td><td>.319 </td><td>.236 </td><td>240 </td><td>.337 </td><td>.352 </td><td>.455 </td><td>.477 </td><td>.329 </td><td>.339 </td><td>.485 </td><td>.508 </td><td>.000 </td><td>.000 </td><td>.000 </td><td>.000\n</td></tr><tr><td>Ours\n</td><td>I-GEN (DistMult) </td><td>.348 </td><td>.367 </td><td>.270 </td><td>.281 </td><td>.382 </td><td>.407 </td><td>.504 </td><td>.537 </td><td>.371 </td><td>.391 </td><td>.537 </td><td>.571 </td><td>.000 </td><td>.000 </td><td>.000 </td><td>.000\n</td></tr><tr><td>I-GEN (TransE) </td><td>.345 </td><td>.371 </td><td>.259 </td><td>.275 </td><td>.385 </td><td>.416 </td><td>.515 </td><td>.559 </td><td>.367 </td><td>.395 </td><td>.548 </td><td>.594 </td><td>.000 </td><td>.000 </td><td>.000 </td><td>.000\n</td></tr><tr><td>T-GEN (Random) </td><td>.349 </td><td>.360 </td><td>.268 </td><td>.273 </td><td>.385 </td><td>.398 </td><td>.508 </td><td>.532 </td><td>.361 </td><td>.373 </td><td>.529 </td><td>.554 </td><td>.168 </td><td>.164 </td><td>.185 </td><td>.192\n</td></tr><tr><td>\u00a0</td><td>T-GEN (DistMult) </td><td>.367 </td><td>.382 </td><td>.282 </td><td>.289 </td><td>.410 </td><td>.430 </td><td>.530 </td><td>.565 </td><td>.379 </td><td>.396 </td><td>.550 </td><td>.588 </td><td>.185 </td><td>.175 </td><td>.220 </td><td>.201\n</td></tr><tr><td>\u00a0</td><td>T-GEN (TransE) </td><td>.356 </td><td>.374 </td><td>.267 </td><td>.282 </td><td>.403 </td><td>.425 </td><td>.531 </td><td>.552 </td><td>.368 </td><td>.387 </td><td>.552 </td><td>.572 </td><td>.175 </td><td>.175 </td><td>.205 </td><td>.235\n</td></tr></table></body></html>"}, "TABREF15": {"text": "Table 8: Examples of the OOG link prediction on NELL-995. S: seen, U: unseen, O: correct prediction, X:\nincorrect prediction, (H): head entity, (R): relation, (T): tail entity, and : unseen entity.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Type </td><td>I-GEN </td><td>T-GEN Triplet\n(H) musician_vivaldi,\n(R) musician_plays_instrument, (T) music_instrument_string\n(H) city_hawthorne, (R) city_located_in_state,\n(T) state_or_province_california\n(H) journalist_maureen_dowd, (R) works_for,\n(T) company_york_times\n(H) person_monroe,\n(R) person_born_in_location, (T) county_york_city\n(H) ceo_stan_o_neal, (R) works_for,\n(T) retailstore_merrill\n(H) insect_insects,\n(R) invertebrate_feed_on_food , (T) agricultural_product_wood\n(H) person_katsuaki_watanabe, (R) person_leads_organization, (T) automobilemaker_toyota\n(H) mlauthor_web_search,\n(R) agent_competes_with_agent, (T) website_altavista_com\n(H) chemical_chromium,\n(R) chemical_is_type_of_chemical, (T) chemical_heavy_metals\n(H) food_meals,\n(R) food_decreases_the_risk_of_disease, (T) disease_heart_disease\n</td></tr><tr><td>S-U </td><td>O </td><td>O\n</td></tr><tr><td>S-U </td><td>O </td><td>O\n</td></tr><tr><td>S-U </td><td>O </td><td>O\n</td></tr><tr><td>S-U </td><td>O </td><td>O\n</td></tr><tr><td>S-U </td><td>O </td><td>O\n</td></tr><tr><td>S-U </td><td>O </td><td>O\n</td></tr><tr><td>U-U </td><td>X </td><td>O\n</td></tr><tr><td>U-U </td><td>X </td><td>O\n</td></tr><tr><td>U-U </td><td>X </td><td>O\n</td></tr><tr><td>U-U </td><td>X </td><td>X\n</td></tr></table></body></html>"}}, "back_matter": []}
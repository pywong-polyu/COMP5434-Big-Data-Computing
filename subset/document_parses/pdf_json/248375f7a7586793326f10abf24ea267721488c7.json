{
    "paper_id": "248375f7a7586793326f10abf24ea267721488c7",
    "metadata": {
        "title": "Monocular Pedestrian 3D Localization for Social Distance Monitoring",
        "authors": [
            {
                "first": "Yiru",
                "middle": [],
                "last": "Niu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "China University of Mining and Technology (Beijing)",
                    "location": {
                        "postCode": "100083",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "y.niu@student.cumtb.edu.cny.n."
            },
            {
                "first": "Zhihua",
                "middle": [],
                "last": "Xu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "China University of Mining and Technology (Beijing)",
                    "location": {
                        "postCode": "100083",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Ershuai",
                "middle": [],
                "last": "Xu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "China University of Mining and Technology (Beijing)",
                    "location": {
                        "postCode": "100083",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Gongwei",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "China University of Mining and Technology (Beijing)",
                    "location": {
                        "postCode": "100083",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Yuan",
                "middle": [],
                "last": "Huo",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "China University of Mining and Technology (Beijing)",
                    "location": {
                        "postCode": "100083",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "y.huo@student.cumtb.edu.cny.h."
            },
            {
                "first": "Wenbin",
                "middle": [],
                "last": "Sun",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "China University of Mining and Technology (Beijing)",
                    "location": {
                        "postCode": "100083",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Citation: Niu, Y.; Xu, Z.; Xu, E.; Li, G.; Huo, Y.; Sun, W. Monocular Pedestrian 3D Localization for Social Distance Monitoring. Sensors 2021, 21, 5908.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Abstract: Social distancing protocols have been highly recommended by the World Health Organization (WHO) to curb the spread of COVID-19. However, one major challenge to enforcing social distancing in public areas is how to perceive people in three dimensions. This paper proposes an innovative pedestrian 3D localization method using monocular images combined with terrestrial point clouds. In the proposed approach, camera calibration is achieved based on the correspondences between 2D image points and 3D world points. The vertical coordinates of the ground plane where pedestrians stand are extracted from the point clouds. Then, using the assumption that the pedestrian is always perpendicular to the ground, the 3D coordinates of the pedestrian's feet and head are calculated iteratively using collinear equations. This allows the three-dimensional localization and height determination of pedestrians using monocular cameras, which are widely distributed in many major cities. The performance of the proposed method was evaluated using two different datasets. Experimental results show that the pedestrian localization error of the proposed approach was less than one meter within tens of meters and performed better than other localization techniques. The proposed approach uses simple and efficient calculations, obtains accurate location, and can be used to implement social distancing rules. Moreover, since the proposed approach also generates accurate height values, exclusionary schemes to social distancing protocols, particularly the parent-child exemption, can be introduced in the framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "At the beginning of 2020, the COVID-19 pandemic swept the world, significantly impacting people's lives and global economic development. While researchers have been developing therapeutics and vaccines for coronaviruses, social distancing is still recommended, at least until 2022, to minimize the risks of another outbreak [1] . Recent studies have shown that when more than one-meter distancing is maintained, the risk of transmission declines by about 82% [2] . In many countries, various rules and strategies have been introduced to observe and monitor interpersonal distancing in public places to curb the spread of the virus [3] . South Korea and India, for example, use Global Positioning System (GPS) data to monitor the movements of pedestrians to identify potential contacts. In addition, India has been utilizing smartphones to locate and monitor COVID-19 patients in target areas with the help of GPS and Bluetooth [4] . However, inside buildings, GPS is susceptible to signal attenuation and multipath effect, resulting in incorrect positioning and limiting its use in implementing social distancing.",
            "cite_spans": [
                {
                    "start": 324,
                    "end": 327,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 459,
                    "end": 462,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 631,
                    "end": 634,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 927,
                    "end": 930,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The fundamental challenge in implementing social distancing and suppressing the spread of the virus is how to perceive people in 3D. Over the past decades, several emerging pedestrian 3D localization technologies have been introduced, from multi-sensor approaches to vision-based solutions. Guo et al. [5] put forward the Indoor Knowledge Graph Framework, integrating spatial information from various mobile phone sensors (including inertial sensor, Wi-Fi, and magnetic field) to achieve indoor positioning. Levchev et al. [6] used cameras, LiDAR, wireless sensors, inertial gyroscopes, and other sensors as data acquisition platforms and proposed a multi-sensor hybrid database configuration. Liang et al. [7] used different sensors to acquire image data and geographic location data simultaneously to construct an indoor 3D map with geographic coordinates. Zhang et al. [8] fused 3D point clouds with scene images to obtain the 3D positioning of outdoor scenes. However, as data acquisition platforms, most of these methods rely on a variety of sensors and have great dependence on the diversity of devices. In addition, some sensors (e.g., wireless) require multiple receiving antennas. Ultra-Wide Band (UWB) needs the target to wear tags or chips, which can be difficult to deploy in actual scenarios.",
            "cite_spans": [
                {
                    "start": 302,
                    "end": 305,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 523,
                    "end": 526,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 707,
                    "end": 710,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 872,
                    "end": 875,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Stereovision is a relatively mature 3D localization technology, but our goal is to propose a solution that primarily uses monocular closed-circuit cameras. At present, monocular vision research mainly focuses on the 3D position estimation of vehicles [9, 10] since pedestrians have varying heights and body shapes and lack enough attribute information. Most traditional methods for pedestrian 3D localization using monocular cameras rely on known information about the scene or pedestrian to obtain the unknown metrics. For example, Gunel et al. [11] and Das and Meher [12] used medical statistical data to locate pedestrians based on the relationship of human height and body parts (or stride length). Using physical constraints, Bieler et al. [13] converted pixel measurements into human height through a simple motion trajectory analysis (jumping or running). In most cases, this estimation approach indirectly obtains the pedestrian's 3D position using some other metrics, which may result in error propagation.",
            "cite_spans": [
                {
                    "start": 251,
                    "end": 254,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 255,
                    "end": 258,
                    "text": "10]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 546,
                    "end": 550,
                    "text": "[11]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 569,
                    "end": 573,
                    "text": "[12]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 745,
                    "end": 749,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "With developments in artificial intelligence in recent years, various studies have used machine learning technology for monocular pedestrian 3D localization. For example, Deng et al. [14] proposed an end-to-end trainable neural network, which only uses synthetic data to obtain stereo images and body pose estimation. Bertoni et al. [15] used a feedforward neural network to predict confidence intervals of 2D human posture and 3D position based on the loss function of Laplace distribution. Bertoni et al. [16] also used a neural network to predict confidence intervals and estimate the 3D position and orientation of pedestrian. However, in most of these studies, pedestrians are assumed to be of equal height, resulting in a task error of about one meter for pedestrians 20 m away.",
            "cite_spans": [
                {
                    "start": 183,
                    "end": 187,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 333,
                    "end": 337,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 507,
                    "end": 511,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Visual-based technology has allowed the remote determination of pedestrians' posture and acquisition of texture information. However, due to the limitations of having only one projection line corresponding to each object point and the lack of depth value, additional information is needed to convert 2D images into 3D. In this context, we propose an effective alternative to achieve three-dimensional positioning. We used terrestrial LiDAR to capture 3D maps to estimate the parameters of the monocular camera. Since the pedestrian is always perpendicular to the ground, we could determine the 3D localization of pedestrians. Our method focuses on solving the limitations of plane positioning and improving the applications of traditional photogrammetry in three-dimensional space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "A monocular pedestrian 3D localization for social distance monitoring framework is proposed in this study. Figure 1 presents the general flowchart of the proposed method. The proposed 3D localization approach includes three main parts: monocular camera calibration (preprocessing), pedestrian 3D localization (localization), and social distance calculation (analysis). ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 107,
                    "end": 115,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Proposed Method"
        },
        {
            "text": "In computer vision, camera calibration is used to determine the camera position and establish the imaging model. Then the relative relationship between the space coordinate system and the image coordinate system can be obtained. To obtain 3D positioning based on monocular images, camera calibration is needed to recover internal (i.e., the focal length and the pixel coordinates of the principal point ( , )) and external parameters (i.e., translation vector and rotation matrix ). For every scenario, at least six pairs of feature points are manually selected from the terrestrial point clouds and monocular image to obtain the 2D-3D matching relationship. The 2D and 3D point coordinates are respectively denoted by ( , ) and ( , , ) where represents different matching points. We use direct linear transformation (DLT) [17] to acquire the camera orientation. The parameters can be formulated as:",
            "cite_spans": [
                {
                    "start": 823,
                    "end": 827,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "Express it as an equation for the unknown quantity ( = 1,2, \u2026 12):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "The homogeneous coordinate does not change the values. We set = 1. Then substitute the ( \u2265 6) pairs of matching points into the Equation (2): ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "In computer vision, camera calibration is used to determine the camera position and establish the imaging model. Then the relative relationship between the space coordinate system and the image coordinate system can be obtained. To obtain 3D positioning based on monocular images, camera calibration is needed to recover internal (i.e., the focal length f and the pixel coordinates of the principal point (u 0 , v 0 )) and external parameters (i.e., translation vector T and rotation matrix R). For every scenario, at least six pairs of feature points are manually selected from the terrestrial point clouds and monocular image to obtain the 2D-3D matching relationship. The 2D and 3D point coordinates are respectively denoted by (u i , v i ) and (X i , Y i , Z i ) where i represents different matching points. We use direct linear transformation (DLT) [17] to acquire the camera orientation. The parameters can be formulated as:",
            "cite_spans": [
                {
                    "start": 855,
                    "end": 859,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "Express it as an equation for the unknown quantity l j (j = 1, 2, . . . 12):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "The homogeneous coordinate l 12 does not change the values. We set l 12 = 1. Then substitute the i (i \u2265 6) pairs of matching points into the Equation (2):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "Sensors 2021, 21, 5908",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "Then write Equation (3) as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "Solve for L using the following formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "We can then use mathematical operations to get the values of the internal and external parameters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Camera Calibration"
        },
        {
            "text": "According to Section 2.1.1, it is very important to obtain accurate 2D pixel coordinates of the pedestrian. The YOLO object detector [18] is used for pedestrian detection in monocular image coordinates. Only one CNN operation is needed to realize the end-to-end prediction. The algorithm uses the whole monocular image as input to the network and simultaneously predicts the target area and its category. In this paper, only the detection results of people are selected. This operation would provide the midpoint pixel coordinates of the detection box's upper and lower edges, which would then be used to represent the head and feet of the pedestrian.",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 137,
                    "text": "[18]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Detector"
        },
        {
            "text": "The gyroscope in the LiDAR helps maintain the z-axis of the point cloud coordinate system in a vertical orientation, and a pedestrian remains upright while walking. The 3D coordinates of pedestrian feet are calculated with this constraint condition. We crop the terrestrial point clouds based on the view range of the monocular image. Then a grid-based point cloud segmentation algorithm is used to obtain the ground plane. The algorithm first projects the 3D point clouds onto the grid map, with a single grid regarded as the minimum computing unit. Then, the statistical characteristics of the elevation direction in each grid unit, including extremum, mean, and variance are computed. The average height difference of a grid is compared with its adjacent grids. Grids with small height differences are marked as potential ground location. Adjacent potential ground locations constitute a connected domain. The connected domain with the largest area can be regarded as the ground. Then from the point clouds, we can acquire the vertical coordinates of the ground, which provides necessary data for the subsequent pedestrian 3D localization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extraction of the Vertical Coordinates of the Ground"
        },
        {
            "text": "The midpoint t of the pedestrian bounding box's upper edge (i.e., the position of the pedestrian's head) and the midpoint b of its lower edge (i.e., the position of pedestrian's feet) are used as marking points. For pedestrians who only appear in a single frame of the monocular camera, their 3D coordinates cannot be directly obtained from a single uncalibrated image and would have to be solved using additional information. Given the presence of a gyroscope in LiDAR, the z-axis in its coordinate system would always be perpendicular to the ground. Therefore, Z g is introduced to represent the z-axis coordinate of the ground plane in the point cloud coordinate system. Based on the camera calibration and posture recovery, the 3D coordinates (X b , Y b , Z b ) of the pedestrian feet point b can be obtained using the space intersection:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3D Localization"
        },
        {
            "text": "where (u b ,v b ) are the pixel coordinates of foot-point b, X S , Y S , and Z S are the line elements in the translation vector T, and a i , b i , and c i (i = 1, 2, 3) are the elements in rotation matrix R. The reprojection error (\u2206u t , \u2206v t ) of point t can then be calculated based on the inherent condition that the pedestrian is always perpendicular to the ground. The pixel coordinates are computed using the expressions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3D Localization"
        },
        {
            "text": "where (u t , v t ) are the pixel coordinates of the head point t by central projection, and (u 0 t , v 0 t ) are the pixel coordinates of point t that were directly detected. Based on Taylor's theorem,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3D Localization"
        },
        {
            "text": "Accordingly, the reprojection error in Equation (8) is sorted into an equation of Z t given by the formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3D Localization"
        },
        {
            "text": "In Equation (11), matrix operation is performed to calculate the correction \u2206Z t . After several iterations, the head 3D coordinates (X t , Y t , Z t ) satisfying the precision requirements can then be obtained. After the 3D coordinates of the pedestrian's head and feet are obtained, the height h can be calculated using the formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3D Localization"
        },
        {
            "text": "The midpoint b at the bottom edge of the pedestrian bounding box is used as the reference point (i.e., feet location). After calculating the 3D position, the interpersonal distance in a homogeneous space is determined using the person-to-person Euclidean distance [3] . If the interpersonal distance is more than the threshold value r (here, r is set to one meter based on the epidemic prevention and control requirements), the social distancing requirement is satisfied.",
            "cite_spans": [
                {
                    "start": 264,
                    "end": 267,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Social Distance Monitoring"
        },
        {
            "text": "Sensors 2021, 21, 5908 6 of 16",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Social Distance Monitoring"
        },
        {
            "text": "For quantitative evaluation, we used two indicators: average localization error (ALE) and average localization precision (ALP). ALE is the average Euclidean distance between the predicted positions of feature points and their real positions. Here, we used the pedestrians' heads and feet as feature points for reference. ALE reflects the stability and accuracy of the proposed method and is given by the expression:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance Evaluation"
        },
        {
            "text": "where n is the number of feature points, (X k , Y k , Z k ) are the predicted positions of feature points, and (X k ,\u0176 k ,\u1e90 k ) are their real positions. The ALP indicator, defined by Xiang et al. [19] , was originally used in evaluating car classification. In order to ensure the detection effect of YOLO, images with relatively large number of pedestrians and less occlusion between pedestrians were selected from the dataset. Then, the localization errors of all pedestrians detected in the selected images are calculated. The probabilities of errors less than the threshold r were used as the ALP r values in our proposed method. ALP r can be calculated using the equation:",
            "cite_spans": [
                {
                    "start": 197,
                    "end": 201,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Performance Evaluation"
        },
        {
            "text": "where P(k, r) is the localization precision. The localization prediction is considered correct if the error between the predicted distance and the ground truth is within the given threshold.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance Evaluation"
        },
        {
            "text": "For the experiments, we used two test data (i.e., CUMTB-Campus and KITTI) to evaluate the performance of our proposed method. First, the CUMTB-Campus dataset was used to show the positioning accuracy in different conditions. Second, the KITTI dataset was used to compare and contrast our proposed method against other localization methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments and Results"
        },
        {
            "text": "Along the main road of CUMTB, we used a hand-held LiDAR scanner GeoSLAM-ZEB-HORIZON to collect 3D point clouds of the campus. In standard conditions, the measuring distance of this scanner is 100 m, and the relative accuracy is 1.5-3 cm. Figure 2 gives an example of our self-collected point clouds, which colored based on height. Video images were obtained simultaneously. The data includes major buildings such as teaching buildings, libraries, ethnic buildings, complex buildings, roads, and other facilities, covering about 80,000 square meters of the campus. At the same time, the video images of surveillance cameras widely distributed in the campus were obtained. In addition, non-metric camera was also used to take photos of pedestrians in some indoor and outdoor scenes on campus (such as teaching buildings, outside the library). Based on the above data, we built our self-collected dataset CUMTB-Campus. Specific parameters in standard conditions are shown in Table 1 . ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 238,
                    "end": 246,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 972,
                    "end": 979,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Self-Collected Data in CUMTB"
        },
        {
            "text": "As the largest dataset for evaluating computer vision algorithms in autonomous driving scenarios, KITTI [20] contains real image data from urban, rural, and highway scenes, with up to 15 cars and 30 pedestrians per image. The dataset also has varying degrees of occlusion and truncation. In this paper, several unobstructed images of pedestrians and corresponding point clouds were selected for testing. Table 1 summarizes the device parameters of the datasets used in the study. ",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 108,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 404,
                    "end": 411,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "The KITTI Dataset"
        },
        {
            "text": "The CUMTB-Campus experiment was designed based on the process described in Section 2. To calibrate the camera, non-coplanar feature points were selected from the image and point clouds, respectively. Using these feature points, the camera elements on interior and exterior orientation were obtained. Here we select the data of two different scenarios to show our positioning results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation Details and Results for the CUMTB-Campus Dataset"
        },
        {
            "text": "1. Scene 1: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation Details and Results for the CUMTB-Campus Dataset"
        },
        {
            "text": "As the largest dataset for evaluating computer vision algorithms in autonomous driving scenarios, KITTI [20] contains real image data from urban, rural, and highway scenes, with up to 15 cars and 30 pedestrians per image. The dataset also has varying degrees of occlusion and truncation. In this paper, several unobstructed images of pedestrians and corresponding point clouds were selected for testing. Table 1 summarizes the device parameters of the datasets used in the study.",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 108,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 404,
                    "end": 411,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "The KITTI Dataset"
        },
        {
            "text": "The CUMTB-Campus experiment was designed based on the process described in Section 2. To calibrate the camera, non-coplanar feature points were selected from the image and point clouds, respectively. Using these feature points, the camera elements on interior and exterior orientation were obtained. Here we select the data of two different scenarios to show our positioning results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation Details and Results for the CUMTB-Campus Dataset"
        },
        {
            "text": "Scene 1:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "1."
        },
        {
            "text": "Scene 1 was an indoor scenario in the teaching building of CUMTB. The image with 15.1 megapixel resolution was taken by Canon EOS 50D on 24 October 2019, while the GeoSLAM-ZEB-HORIZON scanner acquired LiDAR point clouds along the corridor in the same region. We extracted the coordinates for the vertical direction of the ground plane where the pedestrian is situated from the point clouds. Then, as shown in Figure 3 , the YOLO algorithm was used to obtain the pixel coordinates for each pedestrian detection box. Scene 1 was an indoor scenario in the teaching building of CUMTB. The image with 15.1 megapixel resolution was taken by Canon EOS 50D on 24 October 2019, while the GeoSLAM-ZEB-HORIZON scanner acquired LiDAR point clouds along the corridor in the same region. We extracted the coordinates for the vertical direction of the ground plane where the pedestrian is situated from the point clouds. Then, as shown in Figure 3 , the YOLO algorithm was used to obtain the pixel coordinates for each pedestrian detection box. Then using our proposed approach, each pedestrian's 3D position and height can be obtained using a single RGB image. Table 2 shows pedestrians' localization and height errors with different distances from the camera in Scene 1 of the CUMTB-Campus. The social distance errors between adjacent pedestrians are also listed. Camera calibration is realized based on Section 2.1.1. As shown in Figure 3 , in order to facilitate the selection of feature points, we stick a number of reflectors in the scene. Here, the center points of eight uniformly distributed reflectors are manually selected from the point clouds and monocular image as matching points (represented by red five-pointed stars). The 2D pixel coordinates and 3D coordinates of matching points are read and substituted into Equation (3) Based on the obtained parameters, test points are selected to verify the calibration accuracy. Taking the yellow four-pointed star as an example, its reprojected pixel coordinates are (2064.925, 2392.275) and the reprojection errors are 3.079 pixels.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 409,
                    "end": 417,
                    "text": "Figure 3",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 924,
                    "end": 932,
                    "text": "Figure 3",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1147,
                    "end": 1154,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1418,
                    "end": 1426,
                    "text": "Figure 3",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "1."
        },
        {
            "text": "Then using our proposed approach, each pedestrian's 3D position and height can be obtained using a single RGB image. Table 2 shows pedestrians' localization and height errors with different distances from the camera in Scene 1 of the CUMTB-Campus. The social distance errors between adjacent pedestrians are also listed. In Figure 4 , the left and right vertical axes represent the localization and height errors of four pedestrians in Scene 1 at different distances to the camera. The experimental results show that the localization error was less than half a meter within the 36-m vision range. In Figure 4 , the left and right vertical axes represent the localization and height e of four pedestrians in Scene 1 at different distances to the camera. The experimental re show that the localization error was less than half a meter within the 36-m vision ran There are more people in this scene, and they are in different positions inclu people overlapping. There are also several pedestrians on the steps. As shown in F 5, the YOLO algorithm was used to obtain the pixel coordinates for each pedestria tection box. There are more people in this scene, and they are in different positions including people overlapping. There are also several pedestrians on the steps. As shown in Figure 5 , the YOLO algorithm was used to obtain the pixel coordinates for each pedestrian detection box. As can be seen from Figure 5 , there are two pedestrians walking side by side partially covered, and our detection algorithm only obtains one detection box (the ye one). In order to evaluate the positioning accuracy rather than the 2D detection accu the detection box is treated as a pedestrian for 3D localization. Then, using the a As can be seen from Figure 5 , there are two pedestrians walking side by side and partially covered, and our detection algorithm only obtains one detection box (the yellow one). In order to evaluate the positioning accuracy rather than the 2D detection accuracy, the detection box is treated as a pedestrian for 3D localization. Then, using the above method, we can obtain the 3D position and height of pedestrians. Table 3 shows the localization and height errors of eight pedestrians in Scene 2. Similarly, we also list the social distance errors between adjacent pedestrians. In Figure 6 , the left and right vertical axes represent the localization and height errors of eight pedestrians in Scene 2 at different positions. In this scenario, pedestrian 5 has the largest localization error because he is more than 50 m away from the camera. The second one is the yellow box due to the occlusion. The maximum relative error of social distance is 0.207 m. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 117,
                    "end": 124,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 324,
                    "end": 332,
                    "text": "Figure 4",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 600,
                    "end": 608,
                    "text": "Figure 4",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 1281,
                    "end": 1289,
                    "text": "Figure 5",
                    "ref_id": "FIGREF12"
                },
                {
                    "start": 1407,
                    "end": 1415,
                    "text": "Figure 5",
                    "ref_id": "FIGREF12"
                },
                {
                    "start": 1741,
                    "end": 1749,
                    "text": "Figure 5",
                    "ref_id": "FIGREF12"
                },
                {
                    "start": 2137,
                    "end": 2144,
                    "text": "Table 3",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 2303,
                    "end": 2311,
                    "text": "Figure 6",
                    "ref_id": "FIGREF14"
                }
            ],
            "section": "1."
        },
        {
            "text": "From the KITTI dataset, we randomly selected multiple images of fully visible pedestrians. Using the camera calibration parameters provided by the dataset, the three-dimensional coordinates and height of pedestrians were obtained. Figure 7 shows Scene 3 selected from the KITTI dataset and nine pedestrian localization results. Table 4 summarizes the localization errors of the pedestrians' feet for Scene 3 and their height and social distancing errors. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 231,
                    "end": 239,
                    "text": "Figure 7",
                    "ref_id": "FIGREF17"
                },
                {
                    "start": 328,
                    "end": 335,
                    "text": "Table 4",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Implementation Details and Results for the KITTI Dataset"
        },
        {
            "text": "From the KITTI dataset, we randomly selected multiple images of fully visible pedestrians. Using the camera calibration parameters provided by the dataset, the threedimensional coordinates and height of pedestrians were obtained. Figure 7 shows Scene 3 selected from the KITTI dataset and nine pedestrian localization results. Table 4 summarizes the localization errors of the pedestrians' feet for Scene 3 and their height and social distancing errors.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 230,
                    "end": 238,
                    "text": "Figure 7",
                    "ref_id": "FIGREF17"
                },
                {
                    "start": 327,
                    "end": 334,
                    "text": "Table 4",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Implementation Details and Results for the KITTI Dataset"
        },
        {
            "text": "From the KITTI dataset, we randomly selected multiple images of fully visible p destrians. Using the camera calibration parameters provided by the dataset, the three-d mensional coordinates and height of pedestrians were obtained. Figure 7 shows Scene selected from the KITTI dataset and nine pedestrian localization results. Table 4 summ rizes the localization errors of the pedestrians' feet for Scene 3 and their height and soc distancing errors. In Figure 8 , the left and right vertical axes represent the localization and height errors of nine pedestrians in Scene 3. Figure 8 shows that the biggest localization and height errors of Scene 3 were 0.775 m and 0.159 m. Among them, pedestrian 2 and pedestrian 4 are affected by other pedestrians, leading to a large error in two-dimensional detection results, affecting the positioning accuracy. The maximum relative error of social distance is 0.246 m. In Figure 8 , the left and right vertical axes represent the localization and height e of nine pedestrians in Scene 3. Figure 8 shows that the biggest localization and h errors of Scene 3 were 0.775 m and 0.159 m. Among them, pedestrian 2 and pedestr are affected by other pedestrians, leading to a large error in two-dimensional dete results, affecting the positioning accuracy. The maximum relative error of social dist is 0.246 m. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 231,
                    "end": 239,
                    "text": "Figure 7",
                    "ref_id": "FIGREF17"
                },
                {
                    "start": 326,
                    "end": 333,
                    "text": "Table 4",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 453,
                    "end": 461,
                    "text": "Figure 8",
                    "ref_id": "FIGREF19"
                },
                {
                    "start": 574,
                    "end": 582,
                    "text": "Figure 8",
                    "ref_id": "FIGREF19"
                },
                {
                    "start": 911,
                    "end": 919,
                    "text": "Figure 8",
                    "ref_id": "FIGREF19"
                },
                {
                    "start": 1027,
                    "end": 1035,
                    "text": "Figure 8",
                    "ref_id": "FIGREF19"
                }
            ],
            "section": "Implementation Details and Results for the KITTI Dataset"
        },
        {
            "text": "We then compared our positioning results with three monocular 3D localiz methods based on the KITTI dataset, i.e., Mono3D [21] , MonoDepth [22] , and Mono ",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 126,
                    "text": "[21]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 139,
                    "end": 143,
                    "text": "[22]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Methods Comparison"
        },
        {
            "text": "We then compared our positioning results with three monocular 3D localization methods based on the KITTI dataset, i.e., Mono3D [21] , MonoDepth [22] , and Monoloco [15] . We also evaluated our results with a binocular approach, i.e., 3DOP [23] . Its input is a stereo image pair, which uses 3D information and a context model specific to the autonomous driving field to obtain a 3D bounding box. As shown in Table 5 , we evaluate our method with other localization results in [15] . Due to limitations of our implementation method, five images (including Scene 3) were randomly selected from the KITTI dataset containing 25 fully visible pedestrians at different distances. The 3D coordinates of 50 pedestrian points (each pedestrian contains two points, i.e., head and feet) were then calculated, and the accuracy was evaluated using the ALP r (r = 0.5, 1, 2) and ALE mentioned above. As shown in Table 5 , the average localization error of our proposed method in the test data was within one meter, i.e., 0.20 m. Among them, 78% of the localization errors were within 0.5 m. With the aid of high-precision point clouds, our proposed approach outperformed other monocular localization methods, such as the Monoloco [15] . This method uses detected 2D joints of a network as input and generates the 3D location with confidence interval. However, this method assumes that all pedestrains have the same height, resulting in error-prone estimates. Then, our proposed method achieved comparable performance with the 3DOP, which uses stereo images for training and testing. However, our method does not require stereo images to obtain better accuracy and does not require datasets for training. In addition, our proposed method was able to calculate pedestrians' height. In the selected images, the average height error of twenty-five pedestrians was 0.05 m.",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 131,
                    "text": "[21]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 144,
                    "end": 148,
                    "text": "[22]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 164,
                    "end": 168,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 239,
                    "end": 243,
                    "text": "[23]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 476,
                    "end": 480,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1216,
                    "end": 1220,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 408,
                    "end": 415,
                    "text": "Table 5",
                    "ref_id": "TABREF9"
                },
                {
                    "start": 898,
                    "end": 905,
                    "text": "Table 5",
                    "ref_id": "TABREF9"
                }
            ],
            "section": "Methods Comparison"
        },
        {
            "text": "Based on COVID-19 prevention and control protocols, one-meter social distancing should be observed between pedestrians. However, this rule on social distancing is not considered absolute. For instance, if adults and children walk side by side, the rule on social distancing is bypassed even if the distance between them is less than one meter. To account for this special circumstance, an exclusion scheme for parent and child pedestrians, determined based on height, is adopted. We adopt the standard of free admission for children in most public places (such as tourist attractions, amusement parks, cinemas, etc.) in China and choose 1.2 m as the reference height for children. At the same time, according to the relevant statistical data [25] , the average height of 1.715 m was selected as the adult height reference. So in the proposed scheme, pedestrians walking side by side with a height difference of more than 51.5 cm would be considered parent and child.",
            "cite_spans": [
                {
                    "start": 742,
                    "end": 746,
                    "text": "[25]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Social Distance Monitoring"
        },
        {
            "text": "Since our proposed localization method includes 3D location determination and height estimation, the parent-and-child exemption could be executed. In Figures 9 and 10 , the 3D position and height of pedestrians are marked in different colors. Red is used to mark violations to social distancing rules and green pertains to adherence to social distancing protocols. and child.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 150,
                    "end": 166,
                    "text": "Figures 9 and 10",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Social Distance Monitoring"
        },
        {
            "text": "Since our proposed localization method includes 3D location determination height estimation, the parent-and-child exemption could be executed. In Figures 9 an the 3D position and height of pedestrians are marked in different colors. Red is us mark violations to social distancing rules and green pertains to adherence to s distancing protocols. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 146,
                    "end": 155,
                    "text": "Figures 9",
                    "ref_id": "FIGREF21"
                }
            ],
            "section": "Social Distance Monitoring"
        },
        {
            "text": "This study investigated the monocular pedestrian 3D localization for social dis monitoring. According to the experimental results in Section 3, the pedestrian localiz and child.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Since our proposed localization method includes 3D location determination height estimation, the parent-and-child exemption could be executed. In Figures 9 an the 3D position and height of pedestrians are marked in different colors. Red is us mark violations to social distancing rules and green pertains to adherence to s distancing protocols. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 146,
                    "end": 155,
                    "text": "Figures 9",
                    "ref_id": "FIGREF21"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "This study investigated the monocular pedestrian 3D localization for social dis monitoring. According to the experimental results in Section 3, the pedestrian localiz ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "This study investigated the monocular pedestrian 3D localization for social distance monitoring. According to the experimental results in Section 3, the pedestrian localization and height errors obtained by the proposed method are mainly affected by the following factors:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "(1). The quality of point clouds:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Mobile mapping systems (MMS) devices are used to acquire the point clouds in both tests (CUMTB-Campus and KITTI). When there are pedestrians or other moving objects in the scenes, the integrity of point clouds may be affected. In addition, the observation accuracy of LiDAR scanner is also related to the scanning distance and the fine degree of scanning. In the process of camera calibration, 3D coordinates of feature points are obtained from point clouds. In addition, point cloud segmentation is needed to obtain the vertical coordinate of the ground plane. Therefore, point cloud accuracy will have a certain impact on camera calibration and localization accuracy. However, in the proposed method, this factor has little influence on the final result.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "(2). Select the corresponding point pairs:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "As described in Section 2.1.1, at least six pairs of matching points should be selected from the point clouds and monocular image for DLT to obtain the camera parameters. As shown in Figure 3 , eight evenly distributed feature points are selected around pedestrians. Then, the number and location distribution of the selected points will have a certain impact on the calibration results.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 183,
                    "end": 191,
                    "text": "Figure 3",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "(3). Distance from pedestrian to the projection center of monocular image:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "In the indoor scene shown in Figure 3 , the distance differences between the pedestrians and the camera are not too large. The final result is mainly affected by other factors such as calibration accuracy and research environment. Thus, the localization and height errors are not linearly connected to the distances amplitude. As shown in Figure 5 , some pedestrians are far from the camera. The YOLO algorithm may fail to obtain the pedestrian detection box or the coordinates of the detection box are inaccurate. In addition, the actual distance corresponding to a pixel is larger. In this case, the accuracy is mainly affected by distance, and the localization and height errors are approximately linearly related to distance.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 29,
                    "end": 37,
                    "text": "Figure 3",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 339,
                    "end": 347,
                    "text": "Figure 5",
                    "ref_id": "FIGREF12"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "(4). Environmental situation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Our method may be influenced by the environmental situation, such as whether there are steps or slopes in the scene and whether pedestrians are blocked or truncated. As shown in Figure 5 , it can be seen that for pedestrians in different height ground planes, the proposed method can still achieve pedestrian 3D localization. However, since our approach requires the assistance of terrestrial point clouds, it is difficult to extract the ground plane of pedestrians if there are slopes or steps with large inclination in the scene. In addition, by analyzing the experimental results, it can be found that when pedestrians are blocked or truncated, the YOLO algorithm may fail to obtain accurate detection box coordinates. At this time, the localization error mainly comes from 2D detection. The use of additional auxiliary information may help solve this problem. For example, Gunel et al. [11] confirmed that human height could be inferred from images by obtaining joint posture, bone length ratio, and facial features. Kundegorski and Breckon [26] used infrared images for pedestrian tracking and obtained 3D locations based on the medical statistics assuming adult height.",
            "cite_spans": [
                {
                    "start": 890,
                    "end": 894,
                    "text": "[11]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1045,
                    "end": 1049,
                    "text": "[26]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [
                {
                    "start": 178,
                    "end": 186,
                    "text": "Figure 5",
                    "ref_id": "FIGREF12"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "To sum up, the accuracy of our proposed method relies on some external factors. During data collection, the quality of point clouds should be ensured as far as possible, and a certain number of uniformly distributed matching points can improve the localization accuracy. In general, our method is a simple and accurate monocular pedestrian 3D localization and social distance monitoring solution. The proposed approach can be extended and applied to other fields, such as automatic piloting, crowd analysis, fire safety, and merchant passenger flow statistics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "An innovative and accurate approach using monocular RGB images to obtain pedestrian localization and assess social distancing is proposed in this paper. The proposed method takes advantage of the inherent condition that the pedestrian is always perpendicular to the ground plane. Our method improves on the current localization in various aspects. First, our method only needs to obtain the pedestrian detection box without extracting the skeleton. Second, it does not rely on the base station, network or tag constraints, and can obtain the pedestrian's 3D position based on monocular image and terrestrial point clouds, and the precision meets the social distancing protocol of one meter. Finally, since the proposed localization approach also generates accurate height values, exclusionary schemes to social distancing protocols, particularly the parent-child exemption, can be introduced in the framework. Using height difference between pedestrians, the proposed approach can identify violations of social distancing rules and determine cases most likely covered by the parent-child exemption. During the epidemic prevention and control period, for some places requiring key prevention and control, such as schools, hospitals, shopping malls, LiDAR scanners can be used to obtain their 3D point clouds. During data collection, only the point clouds on the ground containing the pedestrian accessible area need to be scanned. Then, using widely distributed surveillance cameras, 3D positioning of pedestrians in the range of view can be obtained. In future works, we will continue to study the pedestrian 3D localization when they are blocked or truncated and try to solve this problem by obtaining the pedestrian body proportions through skeleton detection. While social distancing is important to minimize community transmission of the virus, a moderate amount of social interaction is also necessary. Using statistical analysis of the spatial-temporal data of pedestrians, high-risk areas for virus transmission and community spread can be identified. This would help authorities reevaluate pedestrian traffic design and implement more proactive measures to mitigate risks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "Author Contributions: Z.X. and Y.N. contributed to the study's conception and design. Y.N. designed the experiments, analyzed the data, and wrote the first draft. Z.X. proposed key ideas and gave suggestions for the manuscript. E.X., G.L. and Y.H. participated in the data curation. W.S. gave suggestions for modifications to the manuscript. All authors have read and agreed to the published version of the manuscript. Data Availability Statement: Open-source dataset KITTI is available from Karlsruhe Institute of Technology at http://www.cvlibs.net/datasets/kitti/raw_data.php accessed on 25 May 2021. Selfcollected dataset CUMTB-Campus that support the findings of this study are available from the corresponding author upon reasonable request.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Projecting the transmission dynamics of SARS-CoV-2 through the postpandemic period",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "M"
                    ],
                    "last": "Kissler",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Tedijanto",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Goldstein",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "H"
                    ],
                    "last": "Grad",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lipsitch",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Science",
            "volume": "368",
            "issn": "",
            "pages": "860--868",
            "other_ids": {
                "DOI": [
                    "10.1126/science.abb5793"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Physical distancing, face masks, and eye protection to prevent person-to-person transmission of SARS-CoV-2 and COVID-19: A systematic review and meta-analysis",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "K"
                    ],
                    "last": "Chu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "A"
                    ],
                    "last": "Akl",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Duda",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Solo",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yaacoub",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "J"
                    ],
                    "last": "Sch\u00fcnemann",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "K"
                    ],
                    "last": "Chu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "A"
                    ],
                    "last": "Akl",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "El-Harakeh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bognanni",
                    "suffix": ""
                }
            ],
            "year": 1973,
            "venue": "Lancet",
            "volume": "395",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1016/S0140-6736(20)31142-9"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment in COVID-19 Pandemic",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rezaei",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Azarmi",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Appl. Sci",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/app10217514"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Role of Telecom Network to Manage COVID-19 in India",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jhunjhunwala",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Aarogya Setu. Trans. Indian Natl. Acad. Eng",
            "volume": "5",
            "issn": "",
            "pages": "157--161",
            "other_ids": {
                "DOI": [
                    "10.1007/s41403-020-00109-7"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "An Indoor Knowledge Graph Framework for Efficient Pedestrian Localization",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "O"
                    ],
                    "last": "Pun",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Sens. J",
            "volume": "21",
            "issn": "",
            "pages": "5151--5163",
            "other_ids": {
                "DOI": [
                    "10.1109/JSEN.2020.3029098"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Simultaneous Fingerprinting and Mapping for Multimodal Image and WiFi Indoor Positioning",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Levchev",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "N"
                    ],
                    "last": "Krishnan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Menke",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zakhor",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 International Conference on Indoor Positioning and Indoor Navigation",
            "volume": "",
            "issn": "",
            "pages": "442--450",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Image Based Localization in Indoor Environments",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "Z"
                    ],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Corso",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Turner",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zakhor",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 2013 Fourth International Conference on Computing for Geospatial Research and Application",
            "volume": "",
            "issn": "",
            "pages": "70--75",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Sensor Fusion for Semantic Segmentation of Urban Scenes",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Candra",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Vetter",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zakhor",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 IEEE International Conference on Robotics and Automation",
            "volume": "",
            "issn": "",
            "pages": "1850--1857",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "3D Bounding Box Estimation Using Deep Learning and Geometry",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mousavian",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Flynn",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kosecka",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "21--26",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Vehicle global 6-DoF pose estimation under traffic surveillance camera",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ISPRS J. Photogramm. Remote Sens",
            "volume": "159",
            "issn": "",
            "pages": "114--128",
            "other_ids": {
                "DOI": [
                    "10.1016/j.isprsjprs.2019.11.005"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "What Face and Body Shapes Can Tell Us About Height",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gunel",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Rhodin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fua",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 IEEE/Cvf International Conference on Computer Vision Workshops",
            "volume": "",
            "issn": "",
            "pages": "1819--1827",
            "other_ids": {
                "DOI": [
                    "10.1109/iccvw.2019.00226"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Automatic Extraction of Height and Stride parameters for Human Recognition",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Meher",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 2013 Students Conference on Engineering and Systems",
            "volume": "",
            "issn": "",
            "pages": "12--14",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Gravity as a Reference for Estimating a Person's Height From Video",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Bieler",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "G"
                    ],
                    "last": "G\u00fcnel",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fua",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Rhodin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
            "volume": "21",
            "issn": "",
            "pages": "5908--5924",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Joint Human Pose Estimation and Stereo 3D Localization",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bertoni",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kreiss",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Alahi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 IEEE International Conference on Robotics and Automation (ICRA)",
            "volume": "",
            "issn": "",
            "pages": "2324--2330",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Monocular 3d pedestrian localization and uncertainty estimation",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bertoni",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kreiss",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Alahi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Monoloco",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "6861--6871",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Perceiving Humans: From Monocular 3D Localization to Social Distancing",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bertoni",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kreiss",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Alahi",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Trans. Intell. Transp. Syst",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1109/TITS.2021.3069376"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Direct linear transformation from comparator coordinates into object space coordinates in close-range photogrammetry",
            "authors": [
                {
                    "first": "Y",
                    "middle": [
                        "I"
                    ],
                    "last": "Abdel-Aziz",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Karara",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hauck",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Photogramm. Eng. Remote Sens",
            "volume": "81",
            "issn": "",
            "pages": "103--107",
            "other_ids": {
                "DOI": [
                    "10.14358/PERS.81.2.103"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "You Only Look Once: Unified, Real-Time Object Detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Divvala",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "779--788",
            "other_ids": {
                "DOI": [
                    "10.1109/cvpr.2016.91"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Data-Driven 3D Voxel Patterns for Object Category Recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "G"
                    ],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "Q"
                    ],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Savarese",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1903--1911",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Geiger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lenz",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Urtasun",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3354--3361",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Monocular 3D Object Detection for Autonomous Driving",
            "authors": [
                {
                    "first": "X",
                    "middle": [
                        "Z"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kundu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "Y"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "M"
                    ],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Fidler",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Urtasun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "27--30",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Godard",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Mac Aodha",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "J"
                    ],
                    "last": "Brostow",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "6602--6611",
            "other_ids": {
                "DOI": [
                    "10.1109/cvpr.2017.699"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Urtasun, R. 3D Object Proposals for Accurate Object Class Detection",
            "authors": [
                {
                    "first": "X",
                    "middle": [
                        "Z"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kundu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "K"
                    ],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Berneshawi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "M"
                    ],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Fidler",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Neural Information Processing Systems (Nips)",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cortes",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "D"
                    ],
                    "last": "Lawrence",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "D"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sugiyama",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Garnett",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "28",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Composite Fields for Human Pose Estimation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kreiss",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bertoni",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Alahi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pifpaf",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 IEEE/Cvf Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "11969--11978",
            "other_ids": {
                "DOI": [
                    "10.1109/cvpr.2019.01225"
                ]
            }
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Sizing up human height variation",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "M"
                    ],
                    "last": "Visscher",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Nat. Genet",
            "volume": "40",
            "issn": "",
            "pages": "489--490",
            "other_ids": {
                "DOI": [
                    "10.1038/ng0508-489"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "A Photogrammetric Approach for Real-time 3D Localization and Tracking of Pedestrians in Monocular Infrared Imagery. In Optics and Photonics for Counterterrorism, Crime Fighting, and Defence X; and Optical Materials and Biomaterials in Security and Defence Systems Technology Xi",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "E"
                    ],
                    "last": "Kundegorski",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "P"
                    ],
                    "last": "Breckon",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "International Society for Optics and Photonicsing",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Burgess",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Owen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Rana",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zamboni",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Kajzar",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "9253",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The flowchart of the proposed method.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The flowchart of the proposed method.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "LiDAR point clouds colored based on height in CUMTB-Campus.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "LiDAR point clouds colored based on height in CUMTB-Campus.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Scene 1 in CUMTB-Campus.Camera calibration is realized based on Section 2.1.1. As shown inFigure 3, in order to facilitate the selection of feature points, we stick a number of reflectors in the scene. Here, the center points of eight uniformly distributed reflectors are manually selected from the point clouds and monocular image as matching points (represented by red fivepointed stars). The 2D pixel coordinates and 3D coordinates of matching points are read and substituted into Equation(3)to calculate the internal and external parameters of the monocular image. The results are as follows: = 3938.104 pixels, the pixel coordinates of the principal point ( , ) are (2386.167, 1596.880), the translation vector = (3.563,5.210,2.058) meters, the rotation matrix = obtained parameters, test points are selected to verify the calibration accuracy. Taking the yellow four-pointed star as an example, its reprojected pixel coordinates are (2064.925, 2392.275) and the reprojection errors are 3.079 pixels.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Scene 1 in CUMTB-Campus.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "to calculate the internal and external parameters of the monocular image. The results are as follows: f = 3938.104 pixels, the pixel coordinates of the principal point (u 0 , v 0 ) are (2386.167, 1596.880), the translation vector T = (3.563, 5.210, 2.058) meters, the rotation matrix R =",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Localization and height error of Scene 1 in CUMTB-Campus. 2. Scene 2: Scene 2 was an outdoor scene on the main road besides the library of the CUM The image with 8.3 megapixel resolution was taken by GeoSLAM-ZEB-HORIZON ner on 22 October 2019. The scanner also acquired LiDAR point clouds along the road.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Localization and height error of Scene 1 in CUMTBan outdoor scene on the main road besides the library of the CUMTB. The image with 8.3 megapixel resolution was taken by GeoSLAM-ZEB-HORIZON scanner on 22 October 2019. The scanner also acquired LiDAR point clouds along the same road.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "21, x FOR PEER REVIEW Scene 2 in CUMTB-Campus.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Scene 2 in CUMTB-Campus.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "21, x FOR PEER REVIEW 11 of 17",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Localization and height error of Scene 2 in CUMTB-Campus.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Localization and height error of Scene 2 in CUMTB-Campus.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "Scene 3 and pedestrian localization results.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Scene 3 and pedestrian localization results.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Localization and height error of Scene 3 in KITTI.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "Localization and height error of Scene 3 in KITTI.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "An example scene showing acceptable social distancing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "An example scene with various social distancing types.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "An example scene showing acceptable social distancing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "An example scene showing acceptable social distancing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF25": {
            "text": "An example scene with various social distancing types.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF26": {
            "text": "An example scene with various social distancing types.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF27": {
            "text": "This research was funded in part by the National Natural Science Foundation of China, grant number 41701534, in part by the Open Fund of State Key Laboratory of Coal Resources and Safe Mining under Grant SKLCRSM19KFA01, in part by the State Key Laboratory of Geohazard Prevention and Geoenvironment Protection under Grant SKLGP2019K015, and in part by the China Fundamental Research Funds for Central University under Grant 2020YQDC02 and 2020YJSDC27. Institutional Review Board Statement: Not applicable. Informed Consent Statement: Not applicable.",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "Equipment parameters of two datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Equipment parameters of two datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Pedestrian localization and height errors of Scene 1 in CUMTB-Campus.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Pedestrian localization and height errors of Scene 2 in CUMTB-Campus.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Pedestrian localization and height errors of Scene 3 in KITTI.",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Pedestrian localization and height errors of Scene 3 in KITTI.",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "Comparing our proposed method against baseline results on the KITTI dataset.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "We would like to thank Xuesheng Zhao, and Jun Li of College of Geoscience and Surveying Engineering, China University of Mining and Technology (Beijing) for giving some comments and writing suggestions. We would also like to thank the anonymous reviewers and the editors for their comments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments:"
        },
        {
            "text": "The authors declare no conflict of interest.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conflicts of Interest:"
        }
    ]
}
{"paper_id": "383cc2837e547791e3774280f8d9f4bd63ede5e3", "metadata": {"title": "A New EDA with Dimension Reduction Technique for Large Scale Many-Objective Optimization", "authors": [{"first": "Mingli", "middle": [], "last": "Shi", "suffix": "", "affiliation": {"laboratory": "", "institution": "Northeastern University", "location": {"settlement": "Shenyang", "country": "China"}}, "email": ""}, {"first": "Lianbo", "middle": [], "last": "Ma", "suffix": "", "affiliation": {"laboratory": "", "institution": "Northeastern University", "location": {"settlement": "Shenyang", "country": "China"}}, "email": ""}, {"first": "Guangming", "middle": [], "last": "Yang", "suffix": "", "affiliation": {"laboratory": "", "institution": "Northeastern University", "location": {"settlement": "Shenyang", "country": "China"}}, "email": "yanggm@swc.neu.edu.cn"}]}, "abstract": [{"text": "The performance of many-objective evolutionary algorithms deteriorates appreciably in solving large-scale many-objective optimization problems (MaOPs) which encompass more than hundreds variables. One of the known rationales is the curse of dimensionality. Estimation of distribution algorithms sample new solutions with a probabilistic model built from the statistics extracting over the existing solutions so as to mitigate the adverse impact of genetic operators. In this paper, an Gaussian Bayesian network-based estimation of distribution algorithm (GBNEDA-DR) is proposed to effectively tackle continued large-scale MaOPs. In the proposed algorithm, dimension reduction technique (i.e. LPP) is employed in the decision space to speed up the estimation search of the proposed algorithm. The experimental results show that the proposed algorithm performs significantly better on many of the problems and for different decision space dimensions, and achieves comparable results on some compared with many existing algorithms.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Many-objective optimization problems (MaOPs) refer to the problems that involve a large number of conflicting objectives to be optimized simultaneously. Due to the complexity and difficulty of MaOPs, it is meaningful to investigate the ways of dealing with a given difficult MaOP. The main difficulty associated with MaOPs is often referred to as the curse of dimensionality. Currently, scalability with respect to the number of objectives has attracted considerable research interests. This is due to the fact that in many-objective optimization, most candidate solutions become nondominated with each other, thus causing failure of dominance-based selection strategies in traditional MOEAs. To tackle the MaOPs, a number of new multiobjective evolutionary algorithms (MOEAs) have been proposed, such as NSGA-III [6] , MOEA/D [22] , Tk-MaOEA [12] , IBEA [26] , and KnEA [24] . However, in spite of the various approaches that are focused on the scalability of MOEAs to the number of objectives, scalability in terms of the number of decision variables remains inadequately explored.", "cite_spans": [{"start": 769, "end": 776, "text": "(MOEAs)", "ref_id": null}, {"start": 814, "end": 817, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 827, "end": 831, "text": "[22]", "ref_id": "BIBREF21"}, {"start": 843, "end": 847, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 855, "end": 859, "text": "[26]", "ref_id": "BIBREF25"}, {"start": 871, "end": 875, "text": "[24]", "ref_id": "BIBREF23"}], "ref_spans": [], "section": "Introduction"}, {"text": "Recently, large-scale optimization has already attracted certain interests in the single-objective optimization problem (SOP). Akin to the large-scale singleobjective optimization [14, 19] , some authors have attempted to adapt existing techniques for large-scale single-objective optimization to the MaOPs context, such as MOEA/DVA [13] , LMEA [23] , WOF [25] , MOEA/D-RDG [18] and CCGDE3 [2] . The main idea of these methods is divide-and-conquer strategy. Since the target of many-objective optimization is different from that of singleobjective optimization [11] . Thus, it is not trivial to generalize such divide-andconquer strategy proposed in single objective optimization problem (SOP) to solve MaOPs because the objective functions of an MaOP are conflicting with one another.", "cite_spans": [{"start": 180, "end": 184, "text": "[14,", "ref_id": "BIBREF13"}, {"start": 185, "end": 188, "text": "19]", "ref_id": "BIBREF18"}, {"start": 333, "end": 337, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 345, "end": 349, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 356, "end": 360, "text": "[25]", "ref_id": "BIBREF24"}, {"start": 374, "end": 378, "text": "[18]", "ref_id": "BIBREF17"}, {"start": 390, "end": 393, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 562, "end": 566, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Introduction"}, {"text": "Deb et al. [4] concluded that the performances of MOEAs are significantly influenced by the genetic operators (i.e. crossover and mutation) which cannot ensure to generate promising offspring. Estimation of distribution algorithms (EDAs) are a relatively new computational paradigm proposed to generate new offspring. EDA generates new solutions by applying probabilistic models, which inferred from a set of selected solutions. These models capture statistics about the values of problem variables and the dependencies among these variables [9] .", "cite_spans": [{"start": 11, "end": 14, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 542, "end": 545, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Introduction"}, {"text": "It has been observed that under mild smoothness conditions, the Pareto set of a continuous MOP is a piecewise continuous (m-1)-dimensional manifold, where m is the number of the objectives. In paper [21] , it has shown that reproduction of new trial solutions based on this regularity property can effectively cope with the variable linkages in continuous MOPs. Hence, this characteristics of MOPs can be integrated into EDA to effectively solve large-scale MOPs. This paper proposes a new EDA applied to large-scale MOPs, called GBNEDA-DR. The idea is to combine EDA with dimension reduction methods (i.e. LPP [8] ), which are responsible for embedding the solutions used on the probabilistic models in low dimension space.", "cite_spans": [{"start": 199, "end": 203, "text": "[21]", "ref_id": "BIBREF20"}, {"start": 611, "end": 614, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Introduction"}, {"text": "The rest of this paper is organized as follows. In Sect. 2, we briefly recall some related work on MOEAs for solving large-scale MOPs. Section 3 describes the proposed algorithm GBNEDA/DR. Section 4 illustrates and analyzes the experimental results. Section 5 concludes this paper.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Bayesian networks [15] are multivariate probabilistic graphical models, consisting of two components.", "cite_spans": [{"start": 18, "end": 22, "text": "[15]", "ref_id": "BIBREF14"}], "ref_spans": [], "section": "Gaussian Bayesian Network"}, {"text": "1) The structure, represented by a directed acyclic graph (DAG), where the nodes are the problem variables and the arcs are conditional (in)dependencies between twins of variables; 2) The parameters, expressing for each variable X i the conditional probability of each of its values, given different value-settings of its parent variables (P a(X i )) according to the structure, i.e.", "cite_spans": [], "ref_spans": [], "section": "Gaussian Bayesian Network"}, {"text": "where P a(X i ) is a value-setting for the parent variables in P a(X i ).", "cite_spans": [], "ref_spans": [], "section": "Gaussian Bayesian Network"}, {"text": "In domains with continuous-valued variables, it is usually assumed that the variables follow a Gaussian distribution. The Bayesian network learned for a set of variables, having a multivariate Gaussian distribution p(x) = N (\u03bc, \u03a3) as their joint probability function, is called a Gaussian Bayesian network (GBN). Here, \u03bc is the mean vector and \u03a3 is the covariance matrix of the distribution.", "cite_spans": [], "ref_spans": [], "section": "Gaussian Bayesian Network"}, {"text": "The structure of a GBN is similar to any other Bayesian network. However, for each node the conditional probability represented by the parameters is a univariate Gaussian distribution, which is determined by the values of the parent variables [7] p(x|pa(", "cite_spans": [{"start": 243, "end": 246, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Gaussian Bayesian Network"}, {"text": "where \u03bc i is the mean of variable X i , v i is the conditional standard deviation of the distribution, and regression coefficients \u03c9 ij specify the importance of each of the parents. x j is the corresponding value of X j in P a(X i ). These are the parameters stored in each node of a GBN.", "cite_spans": [], "ref_spans": [], "section": "Gaussian Bayesian Network"}, {"text": "Traditional genetic operators used for generating new solutions in evolutionary algorithms act almost blindly and are very likely to disrupt the good subsolutions found so far which will affect the optimization convergence. This disruption is more likely to occur as the correlation between problem variables increases, rendering the algorithm inefficient for such problems. EDAs make use of probabilistic models to replace the genetic operators in order to overcome this shortcoming. A general framework of EDAs is illustrated in Algorithm 1. Typically, the EDAs-based MOEAs are broadly classified into two categories based on their estimation models. The first category covers the Bayesian network-based EDAs. For example, multiobjective Bayesian optimization algorithm (BOA) [10] . The other category is often known as the mixture probability model-based EDAs. Such as, in [16] , the multiobjective hierarchical BOA was designed by the mixture Bayesian network-based probabilistic model for discrete MOPs. It is believed that EDAs are capable of solving MaOPs without suffering the disadvantages of MOEAs with traditional genetic operators. ", "cite_spans": [{"start": 778, "end": 782, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 876, "end": 880, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Estimation of Distribution Algorithm"}, {"text": "Locality Preserving Projection (LPP) [8] is a general method for manifold learning. Though it is still a linear technique, it seems to recover important aspects of the intrinsic nonlinear manifold structure by preserving local structure. In many real-world applications, the local structure is more important. In this section, we give a brief description of LPP. The complete derivation and theoretical justifications of LPP can be traced back to [8] . LPP seeks to preserve the intrinsic geometry of the data and local structure. Given a set x 1 , x 2 , ..., x m in R n , find a transformation matrix A that maps these m points to a set of points y 1 , y 2 , ..., y m in R l . The objective function of LPP is as follows:", "cite_spans": [{"start": 37, "end": 40, "text": "[8]", "ref_id": "BIBREF7"}, {"start": 447, "end": 450, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Locality Preserving Projections"}, {"text": "where the matrix S is a similarity matrix. The following restrictions are imposed on the equation (3):", "cite_spans": [], "ref_spans": [], "section": "Locality Preserving Projections"}, {"text": "D is a diagonal matrix, its entries are column sum of S,D ii = \u03a3 j S ij . A possible way of defining S is as follows:", "cite_spans": [], "ref_spans": [], "section": "Locality Preserving Projections"}, {"text": "Here, \u03b5 defines the radius of the local neighborhood and t is a custom parameter.", "cite_spans": [], "ref_spans": [], "section": "Locality Preserving Projections"}, {"text": "In this section, the framework of the proposed algorithm, i.e., GBNEDA-DR, is given first. Then elaborate on the four important components in it, i.e., reducing the dimension of decision space, building the probability model, repairing and environmental selection.", "cite_spans": [], "ref_spans": [], "section": "Proposed Algorithm"}, {"text": "The framework of the proposed algorithm is listed in Algorithm 2. It consists of the following three main steps. First, a population of N candidate solutions is randomly initialized. Second, by constructing the Gauss Bayesian network model, N new individuals are sampled and generated. This step has many substeps. Algorithm 2 shows an overview of this step. Next, the main sub-steps are described in detail. Finally, an environmental selection strategy is used to select excellent individuals, thus, a set of solutions with a better quality in convergence and diversity are obtained. The three steps are performed one by one in a limit number of fitness evaluations. In addition, maximum fitness evaluation, population size and respective threshold for dimension reduction and model building need to be made available prior to the proposed algorithm running. ", "cite_spans": [], "ref_spans": [], "section": "Framework of the Proposed Algorithm"}, {"text": "In this paper, LPP [8] is used to reduce the volume of exploration space to speed up the search of sampling new solutions. A set of Pareto solutions (PS) is selected to be the training data and then exploitation is performed in the subspace. In this paper, the LPP is employed because: 1) LPP is based on the inner geometric structure of manifolds, it shows the stability of embedding; 2) LPP algorithm is a linear dimension reduction method.", "cite_spans": [{"start": 19, "end": 22, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Reducing the Dimension of Decision Space"}, {"text": "At the beginning of dimensionality reduction, the Pareto solutions, which are denoted by Sp, are selected from the population. For convenience of the development, a matrix X is used to represent Sp. Specifically, each row in X denotes one solution while the columns refer to the different dimension of decision variables.", "cite_spans": [], "ref_spans": [], "section": "Reducing the Dimension of Decision Space"}, {"text": "In most PCA-based methods, none of solutions sampled from the reduced space needs to be operated in the original space. However, in the proposed design, the solutions must be transformed back to the original space for fitness evaluation. After the new offspring is projected back to its original space, it can participate in environmental selection.", "cite_spans": [], "ref_spans": [], "section": "Reducing the Dimension of Decision Space"}, {"text": "The contribution should contain no more than four levels of headings. Table 1 gives a summary of all heading levels.", "cite_spans": [], "ref_spans": [{"start": 70, "end": 77, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Reducing the Dimension of Decision Space"}, {"text": "The probabilistic model used in this paper for model learning is the Gaussian Bayesian network (GBN). A search+score strategy is used in GBNEDA-DR to learn the GBN from the data. In this strategy, a search algorithm is employed to explore the space of possible GBN structures to find a structure that closely matches the data. The quality of different GBN structures obtained in this search process is measured using a scoring metric, usually computed from data. A greedy local search algorithm is used to learn the structure of GBN. The algorithm finally returns the highest scoring network in all these subsearches. The Bayesian information criterion (BIC) [17] is used to score possible GBN structures.", "cite_spans": [{"start": 659, "end": 663, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Building the Probability Model"}, {"text": "The parameters of this type of GBN are computed from the mean vector and covariance matrix of the Gaussian distribution (GD) estimated for the joint vector of variables and objectives:N (\u03bc 1,n+m ,\u03a3 n+m,n+m ). Usually the maximum likelihood (ML) estimation is used to estimate the parameters of GD (the mean vector and covariance matrix) from the data.", "cite_spans": [], "ref_spans": [], "section": "Building the Probability Model"}, {"text": "After the new offspring is projected back to its original space, values in some dimensions are illegal. Repairing methods are commonly used to guarantee the feasibility of solutions. They modify (repair) a given individual to guarantee that the constraints are satisfied. In GBNEDA-DR the repairing procedure is invoked at every generation. In this paper, there are two methods to fix illegal values. The first method changes the values of each out of range variable to the minimum (respectively maximum) bounds if variables are under (respectively over) the variables ranges. The second method truncates the values of each out of range variable to a random value within the feasible range. Each method is triggered in a random way.", "cite_spans": [], "ref_spans": [], "section": "Repairing Cross-Border Values"}, {"text": "The purpose of environmental selection is for maintaining a size of population with the same number of initialized population. Figure 1 shows an example of the structure of an GBN. The set of arcs in the structure is partitioned into three subsets. The red arc represents the dependency of decision variables, the blue arc represents the dependency of objective variables, and the black arc represents the dependency between the decision variables and the objective variables. An analysis of the structures learnt by GBNEDA-DR along the evolution path show that the proposed algorithm is able to distinguish between relevant and irrelevant variables. It can also capture stronger dependencies between similar objectives. The dependencies learnt between objectives in the MOP structure can be used to analyze relationships like conflict or redundancy between sets of objectives.", "cite_spans": [], "ref_spans": [{"start": 127, "end": 135, "text": "Figure 1", "ref_id": null}], "section": "Environmental Selection"}, {"text": "With an increase of objective, almost the entire population acquires the samerank of non-domination. This makes the Pareto-dominance based primary selection ineffective. Objective reduction approaches can solve this problem very well. In this paper, with the help of GBN, GBNEDA-DR can remove redundant objectives and make effective use of non-dominated sorting. In other words, if a class node in the GBN has no parent, it is not redundant, whereas if a class node (i.e. a objective) has a parent, it does not participate in the comparison in the non-dominant sort. For example, there are no arrows pointing at f1, f3 and f4 in Fig. 1 . But there are arrows pointing at f2, therefore, f2 is redundant and can be ignored in non-dominant sort. The framework of the environmental selection in GBNEDA-DR is similar to that of VaEA [20] . This method performs a careful elitist preservation strategy and maintains diversity among solutions by putting more emphasis on solutions that have the maximum vector angle to individuals which have already survived (more details can be found in [20] ).", "cite_spans": [{"start": 828, "end": 832, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 1082, "end": 1086, "text": "[20]", "ref_id": "BIBREF19"}], "ref_spans": [{"start": 629, "end": 635, "text": "Fig. 1", "ref_id": null}], "section": "Fig. 1. An example of a Gaussian Bayesian network structure"}, {"text": "Since the proposed GBNEDA-DR is an EDA-based algorithm for solving largescale MaOPs, we verify the performance of GBNEDA-DR by empirically comparing it with four state-of-the-art MOEAs covering two categories: 1) traditional MOEAs (MOEA/DVA [13] and NSGA-III [6] ) and 2) EDA-based evolutionary algorithm (RM-MEDA [21] and MBN-EDA [9] ). In the following sections, the selected benchmark test problems are introduced first. Then, the performance indicators and parameter settings are introduced and declared respectively. Finally, experiments on compared algorithms are performed and their results measured by the selected performance indicators are analyzed.", "cite_spans": [{"start": 241, "end": 245, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 259, "end": 262, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 314, "end": 318, "text": "[21]", "ref_id": "BIBREF20"}, {"start": 331, "end": 334, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Experiments"}, {"text": "The experiments are conducted on 14 test problems taken from two widely used test suites, DTLZ and LSMOP. The DTLZ test suite is a class of widely used benchmark problems for testing the performance of MOEAs. The first four test problems are DTZL1-DTLZ4 taken from the DTLZ test suite. DTLZ test problems are considered less challengeable. LSMOP is a recently developed test suite for large-scale multiobjective and many-objective optimization. It can better reflect the challenges in large-scale MaOPs than existing test suits. Hence, the LSMOP1-LSMOP5 problems from the LSMOP test suit are included into the considered benchmark test problems.", "cite_spans": [], "ref_spans": [], "section": "Test Problems and Performance Metrics"}, {"text": "One widely used performance metric, inverted generational distance (IGD) [3] , which can simultaneously quantify the performance in convergence and diversity of the algorithms, is adopted in these experiments. The definition of the IGD from P * (a set of uniformly distributed points in the objective space along the PF) to P (an approximation to the PF) is illustrated as follow,", "cite_spans": [{"start": 73, "end": 76, "text": "[3]", "ref_id": "BIBREF2"}], "ref_spans": [], "section": "Test Problems and Performance Metrics"}, {"text": "where d(v, P ) represents the minimum Euclidean distance between v and the points in P .", "cite_spans": [], "ref_spans": [], "section": "Test Problems and Performance Metrics"}, {"text": "In order to show the best effect of all algorithms, all compared algorithms adopt the recommended parameter values to achieve the best performance. To be specific, the parameter settings for all conducted experiments are as follows.", "cite_spans": [], "ref_spans": [], "section": "Parameter Settings"}, {"text": "1) Crossover and Mutation: SBX [1] and polynomial mutations [5] are employed by traditional MaOEAs as the crossover operator and mutation operator, respectively. The crossover probability and mutation probability are set to P c = 1.0 and P m = 1/D, respectively, where D denotes the number of decision variables. In addition, the distribution index of NSGA-III is set to be 30 according to the suggestions in [6] while others are set to be 20. 2) Population Sizing: The population size of NSGA-III cannot be arbitrarily specified, which is equal to the number of reference vectors, other peer algorithms adopt the same population size for a fair comparison. Furthermore, the two-layer reference vector generation strategy is adopted here. The settings for reference vector and population size are listed in Table 1 . ", "cite_spans": [{"start": 31, "end": 34, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 60, "end": 63, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 409, "end": 412, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [{"start": 807, "end": 814, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Parameter Settings"}, {"text": "Tables 2 present the results of the IGD metric values of the three compared algorithms on the five DTLZ test problems with 100 decision variables. As can be seen from the tables, the IGD values obtained by GBNEDA-DR on each test problem are consistently good as the number of objectives increases from 3 to 5, which confirms a promising scalability of GBNEDA-DR. The results on the eight LSMOP test problems with 1000 decision variables are given in Table 3 , with both the mean and standard deviation of the IGD values averaged over 10 independent runs being listed for the five compared MOEAs, where the best mean among the five compared algorithms is highlighted. It is clearly shown in Table 3 that when the number of decision variables is increased, the proposed algorithm obtains all the best mean IGD results against its competitors over LSMOP1 and LSMOP4 with 5-, and 10-objective. In summary, the dimension reduction technique can significantly improve the performance of the proposed algorithm especially in solving large scale MaOPs. From these results, we can see that GBNEDA-DR outperforms MOEA/DVA, RM-MEDA, MBN-EDA, and NSGA-III on DTLZ and LSMOP test problems in terms of IGD, especially for problems with more than 100 decision variables. Therefore, we can conclude that the proposed GBNEDA-DR is effective to handle large-scale MaOPs.", "cite_spans": [], "ref_spans": [{"start": 450, "end": 457, "text": "Table 3", "ref_id": "TABREF1"}, {"start": 690, "end": 697, "text": "Table 3", "ref_id": "TABREF1"}], "section": "Results"}, {"text": "In this paper, we have proposed a Gaussian Bayesian network-based EDA, termed GBNEDA-DR, for solving large-scale MaOPs. GBNEDA-DR, the proposed algorithm, models a promising area in the search space by a probability model, which is used to generate new solutions. This model can capture the relationships between variables like other EDAs. It must be pointed out that the variables here are those in the low-dimensional space after dimensionality reduction. Because dimension reduction technique (i.e. LPP) is utilized to reduce the cost of exploitation and exploration. Through experimental evaluation we showed significant improvements of the performance on various benchmark problems compared to classical optimization methods as well as existing large scale approaches. This paper demonstrates that the idea of using EDA to generates new solutions for large-scale MaOPs is very promising. In our future research, we would also like to further improve the computational efficiency of the dimension reduction procedure, which is the main computational cost of the proposed GBNEDA-DR.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Simulated binary crossover for continuous search space", "authors": [{"first": "R", "middle": [], "last": "Agrawal", "suffix": ""}, {"first": "K", "middle": [], "last": "Deb", "suffix": ""}, {"first": "R", "middle": [], "last": "Agrawal", "suffix": ""}], "year": 2000, "venue": "Complex Syst", "volume": "9", "issn": "2", "pages": "115--148", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Use of cooperative coevolution for solving large scale multiobjective optimization problems", "authors": [{"first": "L", "middle": ["M"], "last": "Antonio", "suffix": ""}, {"first": "C", "middle": ["A C"], "last": "Coello", "suffix": ""}], "year": 2013, "venue": "IEEE Congress on Evolutionary Computation", "volume": "", "issn": "", "pages": "2758--2765", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "The balance between proximity and diversity in multiobjective evolutionary algorithms", "authors": [{"first": "P", "middle": ["A N"], "last": "Bosman", "suffix": ""}, {"first": "D", "middle": [], "last": "Thierens", "suffix": ""}], "year": 2003, "venue": "IEEE Trans. Evol. Comput", "volume": "7", "issn": "2", "pages": "174--188", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Multi-objective test problems, linkages, and evolutionary methodologies", "authors": [{"first": "K", "middle": [], "last": "Deb", "suffix": ""}, {"first": "A", "middle": [], "last": "Sinha", "suffix": ""}, {"first": "S", "middle": [], "last": "Kukkonen", "suffix": ""}], "year": 2006, "venue": "Proceeding of Genetic and Evolutionary Computation Conference", "volume": "2", "issn": "", "pages": "1141--1148", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "A combined genetic adaptive search (GeneAS) for engineering design", "authors": [{"first": "K", "middle": [], "last": "Deb", "suffix": ""}, {"first": "M", "middle": [], "last": "Goyal", "suffix": ""}], "year": 1999, "venue": "Comput. Sci. Inf", "volume": "26", "issn": "", "pages": "", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I: solving problems with box constraints", "authors": [{"first": "K", "middle": [], "last": "Deb", "suffix": ""}, {"first": "H", "middle": [], "last": "Jain", "suffix": ""}], "year": 2014, "venue": "IEEE Trans. Evol. Comput", "volume": "18", "issn": "4", "pages": "577--601", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Learning Gaussian networks", "authors": [{"first": "D", "middle": [], "last": "Geiger", "suffix": ""}, {"first": "D", "middle": [], "last": "Heckerman", "suffix": ""}], "year": 2013, "venue": "Proceedings of the Tenth International Conference on Uncertainty in Artificial Intelligence (UAI-1994)", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Locality preserving projections (LPP)", "authors": [{"first": "X", "middle": [], "last": "He", "suffix": ""}, {"first": "P", "middle": [], "last": "Niyogi", "suffix": ""}], "year": 2002, "venue": "IEEE Trans. Reliab. TR", "volume": "16", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Multiobjective estimation of distribution algorithm based on joint modeling of objectives and variables", "authors": [{"first": "H", "middle": [], "last": "Karshenas", "suffix": ""}, {"first": "R", "middle": [], "last": "Santana", "suffix": ""}, {"first": "C", "middle": [], "last": "Bielza", "suffix": ""}, {"first": "P", "middle": [], "last": "Larra\u00f1aga", "suffix": ""}], "year": 2014, "venue": "IEEE Trans. Evol. Comput", "volume": "18", "issn": "4", "pages": "519--542", "other_ids": {"DOI": ["10.1109/TEVC.2013.2281524"]}}, "BIBREF9": {"ref_id": "b9", "title": "Multiple-objective Bayesian optimization algorithm", "authors": [{"first": "N", "middle": [], "last": "Khan", "suffix": ""}, {"first": "D", "middle": [], "last": "Goldberg", "suffix": ""}, {"first": "M", "middle": [], "last": "Pelikan", "suffix": ""}], "year": 2002, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Enhancing learning efficiency of brain storm optimization via orthogonal learning design", "authors": [{"first": "M", "middle": [], "last": "Lianbo", "suffix": ""}, {"first": "S", "middle": [], "last": "Cheng", "suffix": ""}, {"first": "Y", "middle": [], "last": "Shi", "suffix": ""}], "year": 2020, "venue": "IEEE Trans. Syst. Man Cybern. Syst", "volume": "1", "issn": "20", "pages": "", "other_ids": {"DOI": ["10.1109/TSMC.2020.2963943"]}}, "BIBREF11": {"ref_id": "b11", "title": "A novel many-objective evolutionary algorithm based on transfer matrix with Kriging model", "authors": [{"first": "L", "middle": [], "last": "Ma", "suffix": ""}], "year": 2020, "venue": "Inf. Sci", "volume": "509", "issn": "", "pages": "437--456", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "A multiobjective evolutionary algorithm based on decision variable analyses for multiobjective optimization problems with large-scale variables", "authors": [{"first": "X", "middle": [], "last": "Ma", "suffix": ""}], "year": 2016, "venue": "IEEE Trans. Evol. Comput", "volume": "20", "issn": "2", "pages": "275--298", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Metaheuristics in large-scale global continues optimization: a survey", "authors": [{"first": "S", "middle": [], "last": "Mahdavi", "suffix": ""}, {"first": "M", "middle": ["E"], "last": "Shiri", "suffix": ""}, {"first": "S", "middle": [], "last": "Rahnamayan", "suffix": ""}], "year": 2015, "venue": "Inf. Sci", "volume": "295", "issn": "", "pages": "407--428", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Bayesian networks: a model of self-activated memory for evidential reasoning", "authors": [{"first": "J", "middle": [], "last": "Pearl", "suffix": ""}], "year": 1985, "venue": "Proceedings of the 7th Conference of the Cognitive Science Society", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Multiobjective hBOA, clustering, and scalability", "authors": [{"first": "M", "middle": [], "last": "Pelikan", "suffix": ""}, {"first": "K", "middle": [], "last": "Sastry", "suffix": ""}, {"first": "D", "middle": [], "last": "Goldberg", "suffix": ""}], "year": 2005, "venue": "GECCO 2005 -Genetic and Evolutionary Computation Conference", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Estimating the dimension of a model", "authors": [{"first": "G", "middle": [], "last": "Schwarz", "suffix": ""}], "year": 1978, "venue": "Ann. Stat", "volume": "6", "issn": "", "pages": "461--464", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "A random-based dynamic grouping strategy for large scale multi-objective optimization", "authors": [{"first": "A", "middle": [], "last": "Song", "suffix": ""}, {"first": "Q", "middle": [], "last": "Yang", "suffix": ""}, {"first": "W", "middle": [], "last": "Chen", "suffix": ""}, {"first": "J", "middle": [], "last": "Zhang", "suffix": ""}], "year": 2016, "venue": "2016 IEEE Congress on Evolutionary Computation (CEC)", "volume": "", "issn": "", "pages": "468--475", "other_ids": {"DOI": ["10.1109/CEC.2016.7743831"]}}, "BIBREF18": {"ref_id": "b18", "title": "Benchmark functions for the CEC 2008 special session and competition on large scale global optimization", "authors": [{"first": "K", "middle": [], "last": "Tang", "suffix": ""}, {"first": "X", "middle": [], "last": "Li", "suffix": ""}, {"first": "P", "middle": [], "last": "Suganthan", "suffix": ""}, {"first": "Z", "middle": [], "last": "Yang", "suffix": ""}, {"first": "T", "middle": [], "last": "Weise", "suffix": ""}], "year": 2009, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "A vector angle-based evolutionary algorithm for unconstrained many-objective optimization", "authors": [{"first": "Y", "middle": [], "last": "Xiang", "suffix": ""}, {"first": "Y", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "M", "middle": [], "last": "Li", "suffix": ""}, {"first": "Z", "middle": [], "last": "Chen", "suffix": ""}], "year": 2017, "venue": "IEEE Trans. Evol. Comput", "volume": "21", "issn": "1", "pages": "131--152", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "RM-MEDA: a regularity model-based multiobjective estimation of distribution algorithm", "authors": [{"first": "Q", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "A", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "Y", "middle": [], "last": "Jin", "suffix": ""}], "year": 2008, "venue": "IEEE Trans. Evol. Comput", "volume": "12", "issn": "1", "pages": "41--63", "other_ids": {"DOI": ["10.1109/TEVC.2007.894202"]}}, "BIBREF21": {"ref_id": "b21", "title": "MOEA/D: a multiobjective evolutionary algorithm based on decomposition", "authors": [{"first": "Q", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "L", "middle": [], "last": "Hui", "suffix": ""}], "year": 2008, "venue": "IEEE Trans. Evol. Comput", "volume": "11", "issn": "6", "pages": "712--731", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "A decision variable clustering-based evolutionary algorithm for large-scale many-objective optimization", "authors": [{"first": "X", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Y", "middle": [], "last": "Tian", "suffix": ""}, {"first": "R", "middle": [], "last": "Cheng", "suffix": ""}, {"first": "Y", "middle": [], "last": "Jin", "suffix": ""}], "year": 2016, "venue": "IEEE Trans. Evol. Comput", "volume": "22", "issn": "", "pages": "", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "A knee point driven evolutionary algorithm for manyobjective optimization", "authors": [{"first": "X", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Y", "middle": [], "last": "Tian", "suffix": ""}, {"first": "Y", "middle": [], "last": "Jin", "suffix": ""}], "year": 2014, "venue": "IEEE Trans. Evol. Comput", "volume": "19", "issn": "6", "pages": "761--776", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "A framework for large-scale multiobjective optimization based on problem transformation", "authors": [{"first": "H", "middle": [], "last": "Zille", "suffix": ""}, {"first": "H", "middle": [], "last": "Ishibuchi", "suffix": ""}, {"first": "S", "middle": [], "last": "Mostaghim", "suffix": ""}, {"first": "Y", "middle": [], "last": "Nojima", "suffix": ""}], "year": 2017, "venue": "IEEE Trans. Evol. Comput", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1109/TEVC.2017.2704782"]}}, "BIBREF25": {"ref_id": "b25", "title": "Indicator-based selection in multiobjective search", "authors": [{"first": "E", "middle": [], "last": "Zitzler", "suffix": ""}, {"first": "S", "middle": [], "last": "K\u00fcnzli", "suffix": ""}], "year": 2004, "venue": "PPSN 2004", "volume": "3242", "issn": "", "pages": "832--842", "other_ids": {"DOI": ["10.1007/978-3-540-30217-9_84"]}}}, "ref_entries": {"FIGREF0": {"text": "Pt=Randomly initialize the population 3: while termination is not satisfied do 4: M = Built probabilistic models from Pt 5: t = t + 1 6: Ut = Generate offspring from M 7: Pt = Select promising solutions from Ut \u222a Pt\u22121 8: end while 9: return Pt", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Main framework of GBNEDA-DR Input: N (population size) nSel(the number of Sel Pop) Output: P (final population) 1: P =Initialize(N ) 2: while termination criterion not fulfilled do 3: Sel P op = Selection(P, nSel) 4: Sel P op = LPP(Sel P op) 5: \u03a6 = Establishing probability model in low dimensional space by Sel P op 6: of f spring = sampling(\u03a6, N ) 7: of f spring = Inverse transformation of (of f spring) 8: of f spring = repairing(of f spring ) 9: P = Environmental Selection(P, of f spring ) 10: end while 11: return P", "latex": null, "type": "figure"}, "FIGREF2": {"text": "H1 and H2 are the simplex-lattice design factors for generating uniformly distributed reference vectors on the outer boundaries and the inside layers, respectively. 3) Other Settings: The number of evaluations is used as the termination criterion for all considered algorithms. The maximum number of function evaluations is set to 100 000 in all experiments. On each test instance, 10 independent runs are performed for each algorithm to obtain statistical results. For MOEA/DVA, the number of interaction analysis and the number of control property analysis are set to the recommended values, namely, NIA = 6 and NCA = 50. For RM-MEDA, the number of clusters in local PCA varies in [31 50 50].", "latex": null, "type": "figure"}, "TABREF0": {"text": "Settings for reference vectors and population size.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>M </td><td># of division </td><td># of reference vectors </td><td>Population size\n</td></tr><tr><td>H1 </td><td>H2\n</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>3 </td><td>14 </td><td>0 </td><td>120 </td><td>120\n</td></tr><tr><td>5 </td><td>5 </td><td>0 </td><td>126 </td><td>128\n</td></tr><tr><td>8 </td><td>3 </td><td>2 </td><td>156 </td><td>156\n</td></tr><tr><td>10 </td><td>4 </td><td>2 </td><td>265 </td><td>265\n</td></tr></table></body></html>"}, "TABREF1": {"text": "Mean and standard deviation results of IGD obtained on LSMOP1-8 (Dimension of decision space is 1000). Obj. NSGA-III MOEA/DVA RM-MEDA MBN-EDA GBNEDA-DR", "latex": null, "type": "table"}, "TABREF2": {"text": "Table 2. Mean and standard deviation results of IGD obtained on DTLZ1-5 (Dimen-\nsion of decision space is 100).", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Problem </td><td>DTLZ1 </td><td>DTLZ2 </td><td>DTLZ3 </td><td>DTLZ4\n</td></tr><tr><td>Obj. </td><td>3 </td><td>5 </td><td>3 </td><td>5 </td><td>3 </td><td>5 </td><td>3 </td><td>5\n</td></tr><tr><td>RM-MEDA\n</td><td>2.69e+1\n</td><td>2.21e+1\n</td><td>2.43e-1\n</td><td>4.62e+0\n</td><td>3.20e+3\n</td><td>3.12e+3\n</td><td>4.64e-1\n</td><td>3.38e+0\n</td></tr><tr><td>(2.18e-1)\n</td><td>(5.97e-1)\n</td><td>(1.80e-2)\n</td><td>(5.76e-1)\n</td><td>(7.26e+1)\n</td><td>(2.48e+2)\n</td><td>(5.97e-2)\n</td><td>(2.45e-1)\n</td></tr><tr><td>\u00a0</td><td>5.10e+1\n</td><td>1.58e+1\n</td><td>2.47e-1\n</td><td>3.02e+0\n</td><td>2.39e+2\n</td><td>6.52e+3\n</td><td>2.62e-1\n</td><td>1.42e+0\n</td></tr><tr><td>MBN-EDA\n</td></tr><tr><td>(1.88e-2)\n</td><td>(4.20e-2)\n</td><td>(2.05e-3)\n</td><td>(1.63e+0)\n</td><td>(5.03e+1)\n</td><td>(7.36e+2)\n</td><td>(4.48e-4)\n</td><td>(1.37e-1)\n</td></tr><tr><td>\u00a0</td><td>2.17e+1\n</td><td>3.82e+1\n</td><td>5.67e-1\n</td><td>7.58e-1\n</td><td>6.62e+3\n</td><td>1.24e+3\n</td><td>9.70e-1\n</td><td>1.14e+0\n</td></tr><tr><td>GBNEDA-DR\n</td></tr><tr><td>(1.23+0)\n</td><td>(1.02e-2)\n</td><td>(1.17e-2)\n</td><td>(9.80e-3)\n</td><td>(9.13e+2)\n</td><td>(1.18e+3)\n</td><td>(7.14e-3)\n</td><td>(3.54e-3)\n</td></tr></table></body></html>"}, "TABREF3": {"text": "Table 3. Mean and standard deviation results of IGD obtained on LSMOP1-8 (Dimen-\nsion of decision space is 1000).", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Problem Obj. </td><td>NSGA-III </td><td>MOEA/DVA RM-MEDA MBN-EDA GBNEDA-DR\n</td></tr><tr><td>\u00a0</td><td>5\n</td><td>7.5893e+0\n</td><td>9.9607e+0\n</td><td>1.0004e+1\n</td><td>9.9980e+0\n</td><td>4.1242e+0\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(2.64e-1)\n</td><td>(3.66e-1)\n</td><td>(3.07e-1)\n</td><td>(7.19e-1)\n</td><td>(1.04e+0)\n</td></tr><tr><td>LSMOP1\n</td><td>10\n</td><td>8.9901e+0\n</td><td>9.6706e+0\n</td><td>9.4195e+0\n</td><td>9.0837e+0\n</td><td>7.3057e+0\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(2.33e-1)\n</td><td>(6.75e-1)\n</td><td>(1.74e-1)\n</td><td>(1.59e+0)\n</td><td>(1.60e-1)\n</td></tr><tr><td>\u00a0</td><td>5\n</td><td>8.9592e-2\n</td><td>1.2071e-1\n</td><td>1.1071e-1\n</td><td>1.0065e-1\n</td><td>1.0832e-1\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(1.97e-5)\n</td><td>(7.29e-3)\n</td><td>(1.98e-4)\n</td><td>(1.08e-3)\n</td><td>(1.35e-3)\n</td></tr><tr><td>LSMOP2\n</td><td>10\n</td><td>3.6174e-1\n</td><td>3.8925e-1\n</td><td>2.8695e-1\n</td><td>2.5539e-1\n</td><td>2.0060e-1\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(1.37e-2)\n</td><td>(3.90e-4)\n</td><td>(2.29e-3)\n</td><td>(9.35e-3)\n</td><td>(4.26e-4)\n</td></tr><tr><td>\u00a0</td><td>5\n</td><td>1.4664e+1\n</td><td>7.2671e+1\n</td><td>1.8609e+1\n</td><td>2.0636e+1\n</td><td>2.1846e+1\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(1.05e-1)\n</td><td>(4.77e+1)\n</td><td>(1.35e-1)\n</td><td>(2.26e-1)\n</td><td>(6.18e-1)\n</td></tr><tr><td>LSMOP3\n</td><td>10\n</td><td>2.8425e+1\n</td><td>1.3015e+3\n</td><td>2.4350e+1\n</td><td>2.4913e+1\n</td><td>1.8471e+2\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(1.95e+0)\n</td><td>(9.78e+2)\n</td><td>(4.12e+0)\n</td><td>(5.76e+0)\n</td><td>(1.04e+1)\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>1.7878e-1\n</td><td>1.9876e-1\n</td><td>2.0692e-1\n</td><td>1.9606e-1\n</td><td>1.6982e-1\n</td></tr><tr><td>\u00a0</td><td>5\n</td><td>(1.31e-3)\n</td><td>(1.47e-4)\n</td><td>(2.99e-3)\n</td><td>(8.35e-5)\n</td><td>(3.67e-4)\n</td></tr><tr><td>LSMOP4\n</td><td>10\n</td><td>3.6897e-1\n</td><td>3.8537e-1\n</td><td>3.0729e-1\n</td><td>2.9669e-1\n</td><td>2.3592e-1\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(7.09e-3)\n</td><td>(9.83e-3)\n</td><td>(2.11e-3)\n</td><td>(9.93e-3)\n</td><td>(4.51e-4)\n</td></tr><tr><td>\u00a0</td><td>5\n</td><td>1.2234e+1\n</td><td>8.7623e+1\n</td><td>6.4356e+0\n</td><td>9.5447e+0\n</td><td>1.4250e+1\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(1.75e-1)\n</td><td>(5.02e+0)\n</td><td>(2.82e+0)\n</td><td>(5.39e-2)\n</td><td>(3.61e-1)\n</td></tr><tr><td>LSMOP5\n</td><td>10\n</td><td>1.5426e+1\n</td><td>4.8658e+1\n</td><td>2.1896e+1\n</td><td>1.8065e+1\n</td><td>7.3963e+0\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(9.83e-1)\n</td><td>(4.81e+1)\n</td><td>(2.10e-1)\n</td><td>(5.37e-1)\n</td><td>(3.31e-1)\n</td></tr></table></body></html>"}}, "back_matter": []}
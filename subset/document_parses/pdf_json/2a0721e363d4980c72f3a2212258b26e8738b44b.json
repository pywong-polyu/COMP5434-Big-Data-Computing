{
    "paper_id": "2a0721e363d4980c72f3a2212258b26e8738b44b",
    "metadata": {
        "title": "Solving Fredholm Integral Equations Using Deep Learning",
        "authors": [
            {
                "first": "Yu",
                "middle": [],
                "last": "Guan",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Tingting",
                "middle": [],
                "last": "Fang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "\u00b7",
                "middle": [
                    "Diankun"
                ],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "\u00b7",
                "middle": [],
                "last": "Congming",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Jin",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "The aim of this paper is to provide a deep learning based method that can solve highdimensional Fredholm integral equations. A deep residual neural network is constructed at a fixed number of collocation points selected randomly in the integration domain. The loss function of the deep residual neural network is defined as a linear least-square problem using the integral equation at the collocation points in the training set. The training iteration is done for the same set of parameters for different training sets. The numerical experiments show that the deep learning method is efficient with a moderate generalization error at all points. And the computational cost does not suffer from \"curse of dimensionality\" problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Integral equations have wide applications in electrical engineering [1] , optics [2] , mathematical biology [3] and other fields. The most popular integral equations are the Fredhom integral equations and the Volterra integral equations. The Fredholm integral equation can be considered as a reformulation of the elliptic partial differential equation and the Volterra integral equation is a reformulation of the fractional-order differential equation, which has wide applications in modeling the real problems, for instance, the chaotic system [4] , the dynamics of COVID-19 [5] , the motion of beam on nanowire [6] , the capacitor microphone dynamical system [7] , etc. Since these integral equations usually can not be solved explicitly, numerical methods are necessary to be considered.",
            "cite_spans": [
                {
                    "start": 68,
                    "end": 71,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 81,
                    "end": 84,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 108,
                    "end": 111,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 545,
                    "end": 548,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 576,
                    "end": 579,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 613,
                    "end": 616,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 661,
                    "end": 664,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We consider the linear Fredholm integral equation of the second kind",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "where x, y \u2208 \u2282 R m , the function g(x) and the kernel k(x, y) are given, and f (x) is the unknown that we want to find. So far, many numerical methods have been proposed to solve the Fredholm integral equations, for example, the Nystr\u00f6m method [3, 8, 9] , the Galerkin method [10] , the wavelet analysis method [11] , the neural network [12, 13] , the collocation method [14] , the maximum entropy method [15] , etc. However, most of these traditional methods can only solve low-dimensional Fredholm integral equations and suffer from \"curse of dimensionality\".",
            "cite_spans": [
                {
                    "start": 244,
                    "end": 247,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 248,
                    "end": 250,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 251,
                    "end": 253,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 276,
                    "end": 280,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 311,
                    "end": 315,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 337,
                    "end": 341,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 342,
                    "end": 345,
                    "text": "13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 371,
                    "end": 375,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 405,
                    "end": 409,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The neural network has been successful in solving partial differential equations in mathematical modelling and the applied science, such as medical smoking model [16] , nonlinear high order singular models [17] , food chain system [18] [19] [20] , Li\u00e9nard differential model [21] , etc. The neural network was also used to solve the Fredholm integral equations in [12, 13] , where the authors only evaluated the approximation at some fixed points without generalization. And the integral was evaluated using numerical integral method whose cost depends on the dimension exponentially.",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 166,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 206,
                    "end": 210,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 231,
                    "end": 235,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 236,
                    "end": 240,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 241,
                    "end": 245,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 275,
                    "end": 279,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 364,
                    "end": 368,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 369,
                    "end": 372,
                    "text": "13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In recent years, deep learning method has been successfully used in artificial intelligence solving high-dimensional problems, such as image recognition [22, 23] , speech recognition [24, 25] , natural language processing [26] , and also in mathematical problems [27] [28] [29] and physical problems [30] .",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 157,
                    "text": "[22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 158,
                    "end": 161,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 183,
                    "end": 187,
                    "text": "[24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 188,
                    "end": 191,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 222,
                    "end": 226,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 263,
                    "end": 267,
                    "text": "[27]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 268,
                    "end": 272,
                    "text": "[28]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 273,
                    "end": 277,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 300,
                    "end": 304,
                    "text": "[30]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "E and his collaborators have done a series of works on solving high-dimensional differential equations based on deep learning method. In [28] , a deep learning-based algorithm was proposed for solving high-dimensional semilinear parabolic partial differential equations and reverse stochastic differential equations from a relation between BSDE (backward stochastic differential equations) and reinforcement learning. In [29] , the deep Ritz method for elliptic differential equations was given by numerically solving variational problems. In [27] , a machine learning approximation algorithm was raised to solve high-dimensional fully nonlinear second-order partial differential equations. These works show that deep learning method provides a new idea to solve high-dimensional mathematical problems.",
            "cite_spans": [
                {
                    "start": 137,
                    "end": 141,
                    "text": "[28]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 421,
                    "end": 425,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 543,
                    "end": 547,
                    "text": "[27]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, a deep residual neural network method is proposed to approximate the solution of the high-dimensional linear Fredholm integral equations of the second kind. Few novel highlights of this deep learning method are briefly provided as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 A deep residual neural network is constructed to solve numerically the linear Fredholm integral equations of the second kind. \u2022 The proposed method can solve high-dimensional Fredholm integral equations and does not suffer from \"curse of dimensionality\" problem, that is the cost depends on the dimension linearly. \u2022 The reasonable absolute error values validate the reliability of the deep learning method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 The proposed method has a small generalization error in the domain. This paper is organized as follows. In Sect. 2 we construct a deep residual neutral network for solving the Fredholm integral equations. In Sect. 3, some numerical experiments are given to show the efficiency of the numerical method. The conclusion is given in Sect. 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The output F(x, \u03b8 ) of the neural network is a composite function of the input x, where \u03b8 denotes the parameters of the neural network including the weighs and bias. Let x be any point in the domain . Now we want to train a deep neural network whose output F(x, \u03b8 ) is ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Deep Learning Method for Solving Fredhom Integral Equations"
        },
        {
            "text": "To learn the parameters \u03b8 , and so the function F(x, \u03b8 ), take randomly n points {x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n } with a uniform distribution as the training set. Initializing the parameter vector \u03b8 , the prediction values F(x i , \u03b8 ) for i = 1, 2, \u00b7 \u00b7 \u00b7 , n, can be obtained by forward propagation neural network.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Deep Learning Method for Solving Fredhom Integral Equations"
        },
        {
            "text": "Define the loss function as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Deep Learning Method for Solving Fredhom Integral Equations"
        },
        {
            "text": "The training of the neural network is to minimize the loss function (2) by the backward propagation neural network, which is a least-square problem",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Deep Learning Method for Solving Fredhom Integral Equations"
        },
        {
            "text": "In Eq. (2) the integral term k(x i , y)F(y, \u03b8)dy can be evaluated using the Monte Carlo method, leading to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Deep Learning Method for Solving Fredhom Integral Equations"
        },
        {
            "text": "where \u03b2 = dx is the volume of .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Deep Learning Method for Solving Fredhom Integral Equations"
        },
        {
            "text": "The training can be done repeatedly for different training set until we get a stationary loss function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Deep Learning Method for Solving Fredhom Integral Equations"
        },
        {
            "text": "As the network deepens, minimizing the loss function has great difficulties, such as vanishing gradient problem, gradient explosion, and degradation problem. The residual neural network can avoid the vanishing gradient problem and may greatly improve the solution. It also can reduce the risk of over-adapting the parameters to a specific dataset [22] . A residual block is shown in Fig. 1 , where an identity shortcut connection is added to a shallow neural network, whose output is H (x) = \u03d5(x) + x, where \u03d5(x) is the output of the shallow neural network. Then the output of the residual block is taken as the input of the next residual block.",
            "cite_spans": [
                {
                    "start": 347,
                    "end": 351,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [
                {
                    "start": 383,
                    "end": 389,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Proposed Deep Learning Method for Solving Fredhom Integral Equations"
        },
        {
            "text": "Our algorithm of deep residual neural network for solving Fredholm integral equations is shown in algorithm 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Deep Learning Method for Solving Fredhom Integral Equations"
        },
        {
            "text": "Input: The number of training points n and the number of training iterations M; Output: The parameters \u03b8 of the residual neural network; 1: Initialize \u03b8 randomly; 2: for k = 1; k \u2264 M; k + + do 3:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Framework of deep residual neural network."
        },
        {
            "text": "Sample the region with a uniform distribution to generate the training set {x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n }; 4:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Framework of deep residual neural network."
        },
        {
            "text": "Minimize the loss function in equation (2) by the following iteration. 5:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Framework of deep residual neural network."
        },
        {
            "text": "while not converge do 6:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Framework of deep residual neural network."
        },
        {
            "text": "Forward propagate the neural network to get F(x i , \u03b8) , i = 1, 2, \u00b7 \u00b7 \u00b7 , n; 7:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 40,
                    "end": 54,
                    "text": "get F(x i , \u03b8)",
                    "ref_id": null
                }
            ],
            "section": "Algorithm 1 Framework of deep residual neural network."
        },
        {
            "text": "Back propagate the neural network to update \u03b8; 8:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Framework of deep residual neural network."
        },
        {
            "text": "Sample with a uniform distribution to generate the test set and evaluate the generalization error on the test set; 9: end while 10: end for 11: Output \u03b8 and obtain the approximate solution of the Fredholm integral equation;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Framework of deep residual neural network."
        },
        {
            "text": "In this section, several Fredholm integral equations are numerically solved using algorithm 1. In the numerical experiments, n = 1000 points in are randomly sampled uniformly as the training set to train the deep residual neural network, and the number of training iterations is M = 2000. The neural network consists of one input layer, two blocks of residual neural network shown in Fig. 1 , and one output layer. There are 30 neurons in the second layer and the forth layer and 10 neurons in the other layers. The ReLU function is used as the active function in the neural network. Minimization is realized by \"AdamOptimizer\" [31] built in TensorFlow (version 1.13.1 ) with a learning rate 0.001.",
            "cite_spans": [
                {
                    "start": 628,
                    "end": 632,
                    "text": "[31]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [
                {
                    "start": 384,
                    "end": 390,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Numerical Experiments"
        },
        {
            "text": "To measure the efficiency of the deep learning method for solving the Fredholm integral equations, we consider several examples whose exact solutions are known. Denote f * (x 1 ), f * (x 2 ), \u00b7 \u00b7 \u00b7 , f * (x n ) as the exact solutions at n points x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n in the test set. Define the generalization error between the exact solution f * (x) of the integral equation and the approximate solution F(x, \u03b8 ) obtained by using the deep residual neural network as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical Experiments"
        },
        {
            "text": "The generalization error is evaluated for each example in the following numerical experiments. For Example 1, the convergence of the loss function and the generalization error are shown in Fig. 2 . Some typical iteration data of the loss function (loss) and the generalization error (err or ) are given in Table 1 . The loss function converges to 10 \u22125 , and the generalization error converges to 10 \u22123 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 189,
                    "end": 195,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 306,
                    "end": 313,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Numerical Experiments"
        },
        {
            "text": "f (x, y, z) + 2 1 2 1 2 1 k(x, y, z, s, t, v) f (s, t, v)dsdtdv = g(x, y, z),(5)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 1 Consider the three-dimensional Fredholm integral equation"
        },
        {
            "text": "Eq. (5) , that is",
            "cite_spans": [
                {
                    "start": 4,
                    "end": 7,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Example 2 Consider a high-dimensional version of the three-dimensional Fredholm integral"
        },
        {
            "text": "The exact solution is f * (x) = 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 2 Consider a high-dimensional version of the three-dimensional Fredholm integral"
        },
        {
            "text": "For Example 2, when the dimension m = 100, the convergence of the loss function and the generalization error are shown in Fig. 3 . Some typical iteration data of the loss function (loss) and the generalization error (err or ) are given in Table 2 . The loss function converges to 10 \u22124 , and the generalization error converges to 10 \u22123 . The exact solution is f * (x, y, z, w) = x yzw.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 122,
                    "end": 128,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 239,
                    "end": 246,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Example 2 Consider a high-dimensional version of the three-dimensional Fredholm integral"
        },
        {
            "text": "For Example 3, the convergence of the loss function and the generalization error are shown in Fig. 4 , and some typical iteration data of the loss function (loss) and the generalization error (err or ) are given in Table 3 . The loss function and the generalized error function synchronously converge very fast to a stable state. The loss function converges to 10 \u22123 , and the generalization error converges to 10 \u22122 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 94,
                    "end": 100,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 215,
                    "end": 222,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Example 2 Consider a high-dimensional version of the three-dimensional Fredholm integral"
        },
        {
            "text": "Example 4 Consider a high-dimensional version of the four-dimensional Fredholm integral Eq. (6) , that is ",
            "cite_spans": [
                {
                    "start": 92,
                    "end": 95,
                    "text": "(6)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Example 2 Consider a high-dimensional version of the three-dimensional Fredholm integral"
        },
        {
            "text": "The exact solution of the equation is f * (x) = x 1 x 2 \u00b7 \u00b7 \u00b7 x m .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 2 Consider a high-dimensional version of the three-dimensional Fredholm integral"
        },
        {
            "text": "For Example 4, when the dimension m = 100, the convergence of the loss function and the generalization error are shown in Fig. 5 , and some typical iteration data of the loss function (loss) and the generalization error (err or ) are given in Table 4 . The loss function converges to 10 \u22127 , and the generalization error converges to 10 \u22124 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 122,
                    "end": 128,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 243,
                    "end": 250,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Example 2 Consider a high-dimensional version of the three-dimensional Fredholm integral"
        },
        {
            "text": "In this paper, we propose a deep learning method based on the residual neural network to solve numerically the linear Fredholm integral equations of the second kind. The output of the deep residual network is used as the numerical solution. The loss function is defined using the Fredholm integral equation. The loss function is optimized by Adam method built in TensorFlow. Then the numerical results, including high-dimensional problems, confirm the efficiency of the method. The main advantage of this method is that it can solve highdimensional Fredholm integral equations with a cost less sensitive to the dimensionality of the problem. The accuracy of the residual neural network is not as good as that of the traditional method, such as the Galerkin method. Some error analysis of the neural network has been discussed in [32] [33] [34] . But so far rigorous error analysis for neural network can not be given yet. The error of the neural network consists of three parts, that is the error between the space of the output of the neural network and the exact solution of the Fredholm integral equation, the optimization error in Eq. (3), and the approximation error in Eq. (4). The error in our numerical experiments has a good accuracy compared to the error of the Monte Carlo method 1/ \u221a n in Eq. (4). In the future we will explore more techniques or theory to improve the convergent accuracy. Additionally, we will try to construct a deep residual neural network to solve the Volterra integral equations.",
            "cite_spans": [
                {
                    "start": 829,
                    "end": 833,
                    "text": "[32]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 834,
                    "end": 838,
                    "text": "[33]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 839,
                    "end": 843,
                    "text": "[34]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Multilayered media Green's functions in integral equation formulations",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "A"
                    ],
                    "last": "Michalski",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Mosig",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "IEEE T. Antenn. Propag",
            "volume": "45",
            "issn": "3",
            "pages": "508--519",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Geometric-optics-integral-equation method for light scattering by nonspherical ice crystals",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "N"
                    ],
                    "last": "Liou",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Appl. Opt",
            "volume": "35",
            "issn": "",
            "pages": "6568--6584",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Nyst\u00f6rm methods for approximating the solutions of an integral equation arising from a problem in mathematical biology",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "C"
                    ],
                    "last": "De Bonis",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "P"
                    ],
                    "last": "Stani\u0107",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "V"
                    ],
                    "last": "Tomovi\u0107 Mladenovi\u0107",
                    "suffix": ""
                }
            ],
            "year": 2022,
            "venue": "Appl. Numer. Math",
            "volume": "171",
            "issn": "",
            "pages": "193--211",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "A nonstandard finite difference scheme for the modelling and nonidentical synchronization of a novel fractional chaotic system",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Baleanu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zibaei",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Namjoo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jajarmi",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Adv. Differ. Equ",
            "volume": "2021",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "A new comparative study on the general fractional model of COVID-19 with isolation and quarantine effects",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Baleanu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hassan Abadi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jajarmi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zarghami Vahid",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "J"
                    ],
                    "last": "Nieto",
                    "suffix": ""
                }
            ],
            "year": 2022,
            "venue": "Alex. Eng. J",
            "volume": "61",
            "issn": "6",
            "pages": "4779--4791",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Novel fractional-order Lagrangian to describe motion of beam on nanowire",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "S"
                    ],
                    "last": "Erturk",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Godwe",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Baleanu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Asad",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jajarmi",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Acta Phys. Pol. A",
            "volume": "140",
            "issn": "3",
            "pages": "265--272",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A new and general fractional Lagrangian approach: a capacitor microphone case study",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jajarmi",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Baleanu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zarghami Vahid",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Mohammadi Pirouz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Asad",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Results Phys",
            "volume": "31",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Iterative variants of the Nystr\u00f6m method for the numerical solution of integral equations",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Atkinson",
                    "suffix": ""
                }
            ],
            "year": 1974,
            "venue": "Numer. Math",
            "volume": "22",
            "issn": "1",
            "pages": "17--31",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Nystr\u00f6m method for solution of Fredholm integral equations of the second kind under interval data",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Khorrami",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Shamloo",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Parsa Moghaddam",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "J. Intell. Fuzzy Syst",
            "volume": "36",
            "issn": "3",
            "pages": "2807--2816",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Richardson extrapolation of iterated discrete Galerkin solution for two-dimensional Fredholm integral equations",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "J. Comput. Appl. Math",
            "volume": "139",
            "issn": "1",
            "pages": "49--63",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Numerical solution of nonlinear Fredholm integral equations of the second kind using Haar wavelets",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Babolian",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shahsavaran",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "J. Comput. Appl. Math",
            "volume": "225",
            "issn": "1",
            "pages": "87--95",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Utilizing artificial neural network approach for solving twodimensional integral equations",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Asady",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hakimzadegan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nazarlue",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Math. Sci",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A neural network approach for solving Fredholm integral equations of the second kind",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Effati",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Buzhabadi",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Neural Comput. Appl",
            "volume": "21",
            "issn": "5",
            "pages": "843--852",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Taylor collocation method and convergence analysis for the Volterra-Fredholm integral equations",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "J. Comput. Appl. Math",
            "volume": "260",
            "issn": "",
            "pages": "294--300",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Solving Fredholm integral equations via a piecewise linear maximum entropy method",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "J. Comput. Appl. Math",
            "volume": "304",
            "issn": "",
            "pages": "130--137",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "An advanced heuristic approach for a nonlinear mathematical based medical smoking model",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Saeed",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sabir",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "S"
                    ],
                    "last": "Alhodaly",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "H"
                    ],
                    "last": "Alsulami",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guerrero S\u00e1nchez",
                    "suffix": ""
                }
            ],
            "year": 2022,
            "venue": "Results Phys",
            "volume": "32",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "An efficient stochastic numerical computing framework for the nonlinear higher order singular models",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sabir",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "A"
                    ],
                    "last": "Wahab",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Javeed",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "M"
                    ],
                    "last": "Baskonus",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Fractal Fract",
            "volume": "5",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Stochastic numerical investigations for nonlinear three-species food chain system",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sabir",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Int. J. Biomath",
            "volume": "2250005",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Evolutionary heuristic with Gudermannian neural networks for the nonlinear singular models of third kind",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sabir",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "A"
                    ],
                    "last": "Wahab",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Phys. Scr",
            "volume": "96",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Gudermannian neural networks using the optimization procedures of genetic algorithm and active set approach for the three-species food chain nonlinear model",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sabir",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Ali",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Sadat",
                    "suffix": ""
                }
            ],
            "year": 2022,
            "venue": "J. Ambient Intell. Human. Comput",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1007/s12652-021-03638-3"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Gudermannian neural networks to investigate the Li\u00e9nard differential model",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "G\u00f3mez-Aguilar",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sabir",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A Z"
                    ],
                    "last": "Raja",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Jahanshahi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "O"
                    ],
                    "last": "Alassafi",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "E"
                    ],
                    "last": "Alsaadi",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Fractals",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1142/S0218348X22500505"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Deep learning face representation by joint identification-verification",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "NIPS",
            "volume": "27",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Speech recognition with deep recurrent neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Graves",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "R"
                    ],
                    "last": "Mohamed",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Dahl",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohamed",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Jaitly",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Senior",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vanhoucke",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "N"
                    ],
                    "last": "Sainath",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kingsbury",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Signal Proc. Mag",
            "volume": "29",
            "issn": "6",
            "pages": "82--97",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Recent trends in deep learning based natural language processing",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tom",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Devamanyu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Soujanya",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Erik",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Comput. Intell. Mag",
            "volume": "13",
            "issn": "3",
            "pages": "55--75",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "E W"
                    ],
                    "last": "Beck",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jentzen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "J. Nonlinear Sci",
            "volume": "29",
            "issn": "",
            "pages": "1563--1619",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jentzen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Commun. Math. Stat",
            "volume": "5",
            "issn": "",
            "pages": "349--380",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "The deep Ritz method: a deep learning-based numerical algorithm for solving variational problems",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Commun. Math. Stat",
            "volume": "6",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "E W"
                    ],
                    "last": "Car",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Phys. Rev. Lett",
            "volume": "120",
            "issn": "14",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Adam: A Method for stochastic Optimization",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "L"
                    ],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "The International Conference on Learning Representations (ICLR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Universal approximation bounds for superpositions of a sigmoidal function",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "R"
                    ],
                    "last": "Barron",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "IEEE T. Inform. Theory",
            "volume": "39",
            "issn": "3",
            "pages": "930--945",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Approximation and estimation for high-dimensional deep learning networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "R"
                    ],
                    "last": "Barron",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Klusowski",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1809.03090v2"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Towards a mathematical understanding of neural network-based machine learning: what we know and what we don",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wojtowytsch",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2009.10713v2"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Congming Jin jincm@zstu.edu.cn 1 Department of Mathematics, Zhejiang Sci-Tech University, Hangzhou 310018, China 0123456789().: V,-vol",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Residual neural network block the solution of the Fredholm integral Eq. (1), that is",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "x, y, z, s, t, v) = e \u2212xs\u2212yt\u2212zv ,and g(x, y, z) = 1 \u2212 1 x yz (e \u22122x \u2212 e \u2212x )(e \u22122y \u2212 e \u2212y )(e \u22122z \u2212 e \u2212z ).The exact solution of Eq. (5) is f * (x, y, z) = 1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Convergence of the loss function (left) and the generalization error (right) of Example 1",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Convergence of the loss function (left) and the generalization error (right) of Example 2",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Consider the four-dimensional Fredholm integral equation f (x, y, z, w) (s, t, v, r )dsdtdvdr x, y, z, w, s, t, v, r ) = x yzw, and g(x, y, z, w) = 15 16 x yzw.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Convergence of the loss function (left) and the generalization error (right) of Example 3",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Convergence of the loss function (left) and the generalization error (right) of Example 4",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Partial iterative results of the loss function and the generalization error for Example 1",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Partial iterative results of the loss function and the generalization error for Example 3",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Partial iterative results of the loss function and the generalization error for Example 4 when the dimension m = 100",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The research of Congming Jin was supported by the National Natural Science Foundation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        }
    ]
}
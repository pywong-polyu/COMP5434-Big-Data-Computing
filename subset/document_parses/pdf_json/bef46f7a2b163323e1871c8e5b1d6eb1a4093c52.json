{
    "paper_id": "bef46f7a2b163323e1871c8e5b1d6eb1a4093c52",
    "metadata": {
        "title": "Empowering differential networks using Bayesian analysis",
        "authors": [
            {
                "first": "Jarod",
                "middle": [
                    "Smith"
                ],
                "last": "Id",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Pretoria",
                    "location": {
                        "postCode": "0002",
                        "settlement": "Pretoria",
                        "country": "South Africa"
                    }
                },
                "email": ""
            },
            {
                "first": "Andri\u00ebtte",
                "middle": [],
                "last": "Bekker Id",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Pretoria",
                    "location": {
                        "postCode": "0002",
                        "settlement": "Pretoria",
                        "country": "South Africa"
                    }
                },
                "email": ""
            },
            {
                "first": "Mohammad",
                "middle": [],
                "last": "Arashi Id",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Pretoria",
                    "location": {
                        "postCode": "0002",
                        "settlement": "Pretoria",
                        "country": "South Africa"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Differential networks (DN) are important tools for modeling the changes in conditional dependencies between multiple samples. A Bayesian approach for estimating DNs, from the classical viewpoint, is introduced with a computationally efficient threshold selection for graphical model determination. The algorithm separately estimates the precision matrices of the DN using the Bayesian adaptive graphical lasso procedure. Synthetic experiments illustrate that the Bayesian DN performs exceptionally well in numerical accuracy and graphical structure determination in comparison to state of the art methods. The proposed method is applied to South African COVID-19 data to investigate the change in DN structure between various phases of the pandemic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Probabilistic networks are becoming ever-present in a multitude of scientific disciplines. These networks aim to illustrate the relationships, if any, between the components of complex systems [1] . If the data is assumed to be Gaussian distributed with mean \u00b5 and covariance matrix \u03a3; the precision matrix \u0398 directly determines the conditional dependence relations and structure of the undirected graphical model [2] .",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 196,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 414,
                    "end": 417,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Numerous statistical methods exist for Gaussian covariance and precision matrix estimation, as well as graphical model determination. In particular, from a frequentist approach, [3] introduce a computationally efficient neighborhood selection procedure. The lasso is used for covariance estimation which enjoys consistency for sparse high-dimensional graphs. The approach is quite effective, in that the sparse precision matrix is estimated by fitting the lasso to each variable using the remaining as predictors. Finally, the estimated precision matrix entry (\u03b8 ij ) is non-zero if the estimated coefficient of i on j or vice versa is non-zero. Importantly, their algorithm can consistently estimate the set of non-zero entries in \u0398, [4] . For a penalised likelihood methodology for sparse precision matrix estimation see [5, 6] . More so, [7] estimate the undirected graphical model using both a block coordinate descent algorithm, as well as Nesterov's first order method [8] . Additionally, [9] propose a 1 constraint estimation technique for both sparse and non-sparse high dimensional matrices with applicability on a wide range of sparsity patterns and class of matrices; precision estimation in Gaussian graphical models for example. For a joint graphical model estimation approach see [10, 11] .",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 181,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 735,
                    "end": 738,
                    "text": "[4]",
                    "ref_id": null
                },
                {
                    "start": 823,
                    "end": 826,
                    "text": "[5,",
                    "ref_id": null
                },
                {
                    "start": 827,
                    "end": 829,
                    "text": "6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 841,
                    "end": 844,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 975,
                    "end": 978,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 995,
                    "end": 998,
                    "text": "[9]",
                    "ref_id": null
                },
                {
                    "start": 1294,
                    "end": 1298,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1299,
                    "end": 1302,
                    "text": "11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Fully Bayesian treatments of Gaussian graphical models are, also, well rooted in literature. In particular, [12] introduce the Bayesian adaptive graphical lasso (BAGLASSO) which utilises a generalised Pareto distribution in the hierarchical formulation of the Bayesian graphical lasso. [13] provide a method for graphical model determination by invoking positive prior mass on the event that there is no conditional dependencies between variables. In terms of joint graphical model inference from a Bayesian perspective see [14] . Lastly, [15] propose using Kullback-Leibler divergence and cross-validation for graphical model structure estimation.",
            "cite_spans": [
                {
                    "start": 108,
                    "end": 112,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 286,
                    "end": 290,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 524,
                    "end": 528,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 539,
                    "end": 543,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Background DN analysis is a statistical methodology that involves functions of at least two graphical models. Numerous measures exist for comparing and evaluating the differences between these graphical structures [1] . Let G = (V, E) define a graphical model with nodes V = {1, 2, ..., p} and a set of edges E \u2286 V \u00d7 V. The graph visually depicts the conditional dependence structure between the nodes of the system. For this work, the focus will be on the difference of two Gaussian graphical models, G 1 and G 2 that share the same set of nodes V. In particular, the edge sets given here are equivalent to the adjacency matrices obtained from the Gaussian graphical model estimation. More specifically, assume that the observations, x 1 , x 2 , ..., x n1 and y 1 , y 2 , ..., y n2 are generated from a p variate Gaussian distribution, N p (\u00b5 1 , \u03a3 1 ) and N p (\u00b5 2 , \u03a3 2 ), respectively. The interest here is estimating the DN (\u2206 = \u03a3 \u22121 2 \u2212 \u03a3 \u22121 1 ), that is the difference between two precision matrices. DN analysis is becoming increasingly popular and important, for example in biological systems where protein interaction networks can be utilised as informative biosignatures for prevalent diseases [16, 17] . The fundamental idea here is that, if two molecules interact with one another then a statistical dependency between them should be observed. Additionally, another application of DNs is multivariate statistical quadratic discriminant analysis [18, 19] , under the Gaussian distribution assumption.",
            "cite_spans": [
                {
                    "start": 214,
                    "end": 217,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1205,
                    "end": 1209,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1210,
                    "end": 1213,
                    "text": "17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1458,
                    "end": 1462,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1463,
                    "end": 1466,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recently, a plethora of statistical techniques have emerged for estimating DNs. These techniques can largely be classified into two main categories. The first estimating the individual precision matrices, \u0398 1 and \u0398 2 separately; where the estimated DN is the difference between the estimated precision matrices. For example, the methods and references for Gaussian graphical model estimation outlined in the introduction can be used to directly to estimate \u2206. The second methodology estimates both the precision matrices simultaneously. The approach here, typically penalises a joint loss function for both precision matrices. [20] provide a methodology for inference and estimation of functions of Gaussian graphical models. In particular, the Intertwined Graphical LASSO (IGL) approach biases the estimation of the precision matrices towards a common value. More so, their Graphical Cooperative-LASSO (GCL) utilises a group-penalty for solutions that favour a common sparsity pattern. [10] and [21] estimate separate graphical models using a joint penalised loss function. [22] propose a method for estimating \u2206 directly which relaxes the need for both individual precision matrices to be sparse nor be estimated directly. Similarly, [23] and [19] utilise an alternating direction method of multipliers (ADMM) algorithm for estimating \u2206 from their joint 1 penalised convex loss function. More recently, [24] introduce a computationally efficient iterative shrinkage-thresholding algorithm for minimising the 1 loss function defined in [19] , namely",
            "cite_spans": [
                {
                    "start": 627,
                    "end": 631,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 987,
                    "end": 991,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1245,
                    "end": 1249,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1405,
                    "end": 1409,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "is convex and S 1 and S 2 are the sample covariance matrices. The DN estimate is obtained by minimising the penalised loss Eq (1). An analogous symmetric convex loss function and estimator is proposed by [23] . The shrinkage-thresholding algorithm proposed by [24] , based on the fast-iterative shrinkage-thresholding algorithm in [25] , aims to minimise Eq (1) without computing May 31, 2021 2/20 matrix inverses in the estimation process. The objective function is given by arg min",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 208,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 260,
                    "end": 264,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 331,
                    "end": 335,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The optimisation objective converges to the solution sequentially using a quadratic approximation and a gradient descent algorithm. The efficiency of the procedure is attested to this approach, resulting in superior computational complexity in contrast to the ADMM approaches by [23] and [19] .",
            "cite_spans": [
                {
                    "start": 279,
                    "end": 283,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 288,
                    "end": 292,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, the objective is to develop a framework for Bayesian DN estimation, which remains unexplored. In particular, the BAGLASSO is adapted for the Bayesian DN estimation; noting that frequent references thereto are included throughout the development of the novel DN architecture. The block Gibbs sampler is used for estimating each component of the DN. An adjusted edge inclusion threshold, based on a conjugate Wishart prior, for graphical structure learning is also exhibited. Comparisons in synthetic data studies illustrate that the Bayesian DN is proficient in both graphical structure learning and matrix estimation, when compared to current state of the art methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions"
        },
        {
            "text": "Finally, it is worth noting the main contributions of this paper. First, the novel Bayesian approach to estimating DNs, using the Bayesian adaptive graphical lasso (BAGLASSO), is introduced. A threshold selection strategy for graphical structure determination, based on a conjugate Wishart prior is explored. An application to South African COVID-19 data is investigated to examine the change in DN structure of key daily metrics between various phases of the pandemic. Lastly, an R package has been developed for the BAGLASSO block Gibbs sampler. The Markov Chain Monte Carlo (MCMC) sampler simulates precision matrices from the posterior distribution of the BAGLASSO. The R package is available on The Comprehensive R Archive Network (CRAN) abglasso.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions"
        },
        {
            "text": "A fully Bayesian treatment of DNs remains unexplored and the novel methodology here aims to develop a simple yet highly accurate Bayesian DN estimation procedure. The novel contribution utilises the BAGLASSO as a launching point to separately estimate the components of the DN. Moreover, the framework has been develop for low to moderate, p \u2208 {10 \u2212 100}, dimensions where n \u2265 p.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Bayesian DN"
        },
        {
            "text": "Recall that the graphical lasso objective is maximising the penalized log-likelihood arg max",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Bayesian graphical lasso prior"
        },
        {
            "text": "where M + is the space of positive definite matrices and S is the sample covariance matrix. More over, \u03c1 \u2265 0 is the shrinkage parameter and \u0398 = (\u03b8 ij ) is the precision matrix. The Bayesian connection to the graphical lasso problem is the maximum a posteriori (MAP) estimate, assuming a random sample from N p (\u00b5, \u0398 \u22121 ), of the following",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Bayesian graphical lasso prior"
        },
        {
            "text": "The prior distribution is given by the product of a double exponential (DE) with form p(y) = \u03bb/2 exp(\u2212\u03bb|y|) for the off diagonal elements and an exponential (EXP) with form p(y) = \u03bb exp(\u2212\u03bby)1 y>0 , otherwise. The value of \u0398 which maximizes the posterior density is the graphical lasso estimate when \u03c1 = \u03bb/n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Bayesian graphical lasso prior"
        },
        {
            "text": "Hierarchical representation [12] propose a hierarchical representation of the graphical lasso prior Eq (3), using the Bayesian lasso formulation in [26] . The Gibbs sampler in [26] utilises the structure of the double exponential distribution as a scale mixture of Gaussians (assuming independence of the conditional double exponential priors) ( [27] ; [28] ) to simulate from the desired posterior distribution. The positive definite constraint on the precision matrix in the graphical lasso Eq (3) implies that the Gaussian components for \u03b8 ij in the scale mixture formulation are no longer independent given the scale parameters. To address this issue, the hierarchical representation of the graphical lasso prior is given by",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 32,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 148,
                    "end": 152,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 176,
                    "end": 180,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 346,
                    "end": 350,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 353,
                    "end": 357,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "The Bayesian graphical lasso prior"
        },
        {
            "text": "where \u03b8 = {\u03b8 ij } i\u2264j is a vector of the upper triangular matrix entries of \u0398 and \u03c4 = {\u03c4 ij } i<j the scale parameters. The normalising constant has no closed-form solution. Obtaining the marginal distribution Eq (3), [12] propose an exponential mixing density with rate parameter \u03bb 2 /2. Simple substitution yields that the mixing density circumvents the intractable normalising constant. Finally, the hierarchical representation in Eq (4) is used in the development of the data-augmented block Gibbs sampler, available in the supplementary material, with a target distribution given by",
            "cite_spans": [
                {
                    "start": 218,
                    "end": 222,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "The Bayesian graphical lasso prior"
        },
        {
            "text": "It is well known that the double exponential prior in Eq (3) may over-shrink (under-shrink) large (small) coefficients in \u0398. The limitations within a regression context have been studied in ( [29] ; [30] ; [31] ) with alternative proposals. The BAGLASSO, Bayesian analog to the adaptive graphical lasso, exploit the framework and flexibility of the hierarchical representation in Eq (4) to address the aforementioned limitation",
            "cite_spans": [
                {
                    "start": 192,
                    "end": 196,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 199,
                    "end": 203,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 206,
                    "end": 210,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "BAGLASSO"
        },
        {
            "text": "where \u03be ij = 1/|\u03b8 ij | are the adaptive weights and the weight matrix (\u03b8 ij ) is the sample precision matrix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BAGLASSO"
        },
        {
            "text": "The Bayesian graphical lasso Eq (3) enables the selection of an appropriate hyperprior on the shrinkage parameter \u03bb, recall that \u03c1 = \u03bb/n in the Bayesian formulation of Eq (2) . Adhering to the positive definite constraint on \u0398, the normalising constant in Eq (3) for a single prior \u03bb for all elements in \u0398 can be obtained by applying the substitution\u0398 = \u03bb\u0398. Thereafter, a diffuse gamma prior \u03bb \u223c GA(r, s) and corresponding conditional posterior \u03bb \u223c GA(r + p(p + 1), s + \u0398 1 /2) can be obtained and sampled from. When allowing for individual \u03bb ij 's for different \u03b8 ij 's, the normalising constant C will inevitably depend on \u03bb ij and a hierarchical formulation can be used to construct a set of prior distributions for various \u03bb ij . In particular, assuming a random sample from N p (\u00b5, \u0398 \u22121 ),",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 174,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "BAGLASSO"
        },
        {
            "text": "The normalising constant C {\u03bbij } i\u2264j is intractable and the set {\u03bb ii } p i=1 are hyperparameters for the diagonal elements of \u0398. As it happens, the computation of \u03bb ij is simplified by circumventing the intractable normalising constant.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BAGLASSO"
        },
        {
            "text": "The BAGLASSO selects the amount of shrinkage \u03bb ij proportionally to the current value of \u03b8 ij . To see this, [12] demonstrate that the conditional posterior, \u03bb ij | \u0398 \u223c GA(r + 1, |\u03b8 ij | + s), has an expected value that is inversely related to magnitude of \u03b8 ij . The data augmented block Gibbs sampler for the hierarchical representation in Eq (7) is the fundamental building block upon which the novel Bayesian DN is devised.",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 113,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "BAGLASSO"
        },
        {
            "text": "A conjugate Wishart prior may be used to infer on events such as \u03b8 ij = 0 when a purely Bayesian posterior inference regarding network structure is desired. An alternative thresholding strategy is presented which is an adaption of the recommendation by [31] . In particular the conjugate Wishart W(3, I p ) prior is used. The corresponding posterior is W(3 + n, (S + I p ) \u22121 ), where = 0.001 and I p a p dimensional identity matrix. The posterior samples are used to compute the posterior distribution of the p \u00d7 p partial correlation matrix. The recommended strategy here suggests",
            "cite_spans": [
                {
                    "start": 253,
                    "end": 257,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Technicalities on conditional dependencies"
        },
        {
            "text": "The Bayesian posterior thresholding recommendation by [12] claim that {\u03b8 ij = 0} if and only if\u03c1",
            "cite_spans": [
                {
                    "start": 54,
                    "end": 58,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Technicalities on conditional dependencies"
        },
        {
            "text": "Noting that\u03c1 ij is the posterior sample mean estimate of the partial correlation under graphical lasso priors Eq (3); g is the standard conjugate Wishart W(3, I p ) and h the standard conjugate Wishart W(3, I p ). Moreover, \u03b7 \u2208 {0 \u2212 1} with the lower and upper bounds resulting in a completely dense or sparse estimate, respectively. The original recommendation for \u03b7 in Eq (9) is 0.5. The forthcoming synthetic data analysis section describes the simulation procedure, as well as, illustrates the performance of the Bayesian DN with regards to different graph structures, namely an AR(1), AR(2), sparse random, scale-free, band, cluster, star and circle. The goal here, however, is to suggest a suitable sparsity threshold region under the varying graph structures. The Bayesian DN is applied across all graph structures with thresholds, \u03b7, in the range of 0.2 and 0.6 in increments of 0.02. The absolute sparsity error is computed for each graph structure and threshold candidate for both Bayesian sparsity criterion in Eq (9) and Eq (8) . The results are based on the median of 40 replications and the Matthews Correlation Coefficient (MCC), see [32] , is used to determine the best performing threshold. Figure 1 (1a) -(1i) display the optimal threshold, based on the top performing MCC, for each graph structure and Bayesian sparsity criterion for p = 10. The optimal threshold plots for p = 30 and p = 100 are available in the supplementary material. The optimal threshold based on Eq (8) , \u03b7 * , for the Bayesian DN is, in most cases, in the neighborhood of the minimum absolute sparsity error and in the region of \u03b7 * \u2208 {0.2 \u2212 0.4}. Both Bayesian sparsity criterion candidates perform comparably well noting, however, that Eq (8) requires less computation. ",
            "cite_spans": [
                {
                    "start": 1036,
                    "end": 1039,
                    "text": "(8)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1149,
                    "end": 1153,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [
                {
                    "start": 1208,
                    "end": 1216,
                    "text": "Figure 1",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Technicalities on conditional dependencies"
        },
        {
            "text": "The synthetic experiment is designed to test the parameter estimation and graphical structure determination of the DN estimation for both the novel Bayesian approach (referred to as 'B-net') and the iterative shrinkage-thresholding estimator (referred to as 'D-net') from [24] . The iterative shrinkage-thresholding estimator uses the lasso penalty and Bayesian Information Criterion (BIC) for model estimation and selection, respectively. For all simulations, the assumption is that the observations, x 1 , x 2 , ..., x n1 and y 1 , y 2 , ..., y n2 are generated from a Gaussian N p (0, \u03a3 1 ) and N p (0, \u03a3 2 ) respectively. The true DN is",
            "cite_spans": [
                {
                    "start": 272,
                    "end": 276,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "The Bayesian DN applies the BAGLASSO Eq (7) to each sample, i.e. separately estimates the precision matrices. Furthermore, for excellent performance set r = 10 \u22122 and s = 10 \u22126 , see supplementary material for more details, for the hyperparameters of the prior distributions of \u03bb ij for i < j and \u03bb ii = 1 for i = 1, . . . , p. The iterative shrinkage-thresholding approach jointly estimates the precision matrices for Eq (1). The following 9 graphical structure variations are considered -where the structure of each is applied to each component in the DN's composition to achieve the desired structure in the DN itself -in the simulation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "\u2022 structure 1 : An AR(1) model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "-Component 1 : \u03b8 ij = 0.7 |i\u2212j| .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "-Component 2 : \u03b8 ij = 0.75 |i\u2212j| .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "\u2022 structure 2 : An AR(2) model. \u2022 structure 4 : A moderately sparse random model where both components have approximately up to 40% off-diagonal elements set to zero.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "\u2022 structure 5 : A scale-free model where the second component is a scalar multiple of the first.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "\u2022 structure 6 : A band or diagonal model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "-Component 2 : \u03b8 ii = 1, \u03b8 ij = 0.7 for 1 \u2264 i = j \u2264 p/2, \u03b8 ij = 0.9 for p/2 + 1 \u2264 i = j \u2264 p and \u03b8 ij = 0 otherwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "\u2022 structure 7 : A cluster model containing two disjoint groups.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "-Component 1 : \u03b8 ii = 1, \u03b8 ij = 0.5 for 1 \u2264 i = j \u2264 p/2, \u03b8 ij = 0.5 for p/2 + 1 \u2264 i = j \u2264 p and \u03b8 ij = 0 otherwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "-Component 2 : \u03b8 ii = 1, \u03b8 ij = 0.9 for 1 \u2264 i = j \u2264 p/2, \u03b8 ij = 0.9 for p/2 + 1 \u2264 i = j \u2264 p and \u03b8 ij = 0 otherwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "\u2022 structure 8 : A star model with every node connected to the first node.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "-Component 1 : \u03b8 ii = 1, \u03b8 1,i = \u03b8 i,1 = 0.1 and \u03b8 i,j = 0. otherwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "-Component 2 : \u03b8 ii = 1, \u03b8 1,i = \u03b8 i,1 = 2.1 and \u03b8 i,j = 0. otherwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data analysis"
        },
        {
            "text": "\u2022 structure 9 : A circular model. Table 1 , where p denotes the dimension and \u03b3 i the i th eigenvalue, respectively. Notice that some loss functions utilise the true DN matrix and its estimates, while others utilise the eigenvalues and their respective estimates. Table 2 reports the median of L1, L2, EL1, EL2, MAXEL1 and MINEL1 for p = 10, 30, 100 in structures 1 \u2212 9 based on 40 replications. For each scenario, the best performing measure is boldfaced. Table 1 . Loss functions used in the synthetic data analysis to assess the numerical accuracy of the B-net and D-net estimates.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 34,
                    "end": 41,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 264,
                    "end": 271,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 457,
                    "end": 464,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Synthetic data analysis"
        },
        {
            "text": "Matrix",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Measure Loss function Abbreviation"
        },
        {
            "text": "The eigenvalue based loss functions are designed to investigate the extremes of the eigenvalue spectrum. In particular, the MAXEL1 loss function highlights which estimator is favourable in a principal component setting, [33] . A couple of observations are worth noting from Table 2 and Table 3 . First, the D-net estimator performs better with the AR(1) structure. Second, the B-net estimator performs exceptionally well in remaining structures. Third, the standard errors for both DN estimation techniques remain relatively consistent throughout the dimension spectrum considered, noting that the D-net estimator yields, in general, better results. This may be due to the fact that the best performing tuning parameter in the solution path leads to highly sparse estimates. The B-net estimation procedure inherits the utilisation of multiple penalty parameters in the precision matrix estimation, leading to robust estimation of the precision matrices.",
            "cite_spans": [
                {
                    "start": 220,
                    "end": 224,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [
                {
                    "start": 274,
                    "end": 293,
                    "text": "Table 2 and Table 3",
                    "ref_id": null
                }
            ],
            "section": "Measure Loss function Abbreviation"
        },
        {
            "text": "To assess the performance on graphical structure determination, the specificity, sensitivity, false negative rate, f1 score and the MCCs are computed and defined in Table 5 . Noting that, TP, TN, FP and FN denote the number of true positives, true negatives, false positives and false negatives, respectively. Values of specificity, sensitivity, f1-score and MCC closer to one, imply better classification performance. The closer the values of false negative rate are to zero the better. The sparsity for the B-net estimator is determined by the thresholding rule in Eq (8) and Figures (1a) -(1i) . Similarly, the best performing tuning parameter in the solution path of the D-net algorithm determines the sparsity of the estimator. The median performance scores, based on 40 repetitions, for each graphical structure is presented in Table 4 . The main diagonals of the adjacency matrices were not included in the scoring. ",
            "cite_spans": [
                {
                    "start": 570,
                    "end": 573,
                    "text": "(8)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 165,
                    "end": 172,
                    "text": "Table 5",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 578,
                    "end": 596,
                    "text": "Figures (1a) -(1i)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 834,
                    "end": 841,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Measure Loss function Abbreviation"
        },
        {
            "text": "This section focuses on applying the novel Bayesian DN estimator, B-net, as well as the terative shrinkage-thresholding estimator, D-net, to the spambase dataset, available at https://archive.ics.uci.edu/ml/datasets/spambase to investigate changes in DN structure between spam and non-spam data. In addition, the B-net estimator is applied to South African COVID-19 data, obtained from https://www.nicd.ac.za/diseases-a-z-index/covid-19/surveillance-reports to investigate the change in DN structure between various phases of the pandemic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Real data analysis"
        },
        {
            "text": "The objective here is to compare the B-net and D-net graphical model estimates of the spam and non-spam emails. The dataset consists of 1813 spam emails and 2788 non-spam emails. The attributes include, amongst others, the average length of uninterrupted sequences of capital letters; total number of capital letters in the e-mail; an indicator denoting whether the e-mail was considered spam or not, in this study.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spam data"
        },
        {
            "text": "Following the approach of [24] , the data is standardised using a non-paranormal transformation in order to satisfy the Gaussian assumption. The B-net estimates are based on 10000 iterations of the Monte Carlo sampler after 5000 burn-in iterations. Figure 4 illustrates the difference between the B-net and D-net estimates. Both estimators indicate the presence of several common hub features namely, 'edu', 'original', 'direct', 'lab', 'telnet' and 'addresses'. It is clear from both panes that the state of the covariance matrix structure between the spam and non-spam emails may very well be different. Furthermore, given that Hewlett-Packard Labs donated the data, words such as 'telnet' and 'hp' appear more often in the non-spam emails and can be used to distinguish between spam and non-spam emails. ",
            "cite_spans": [
                {
                    "start": 26,
                    "end": 30,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [
                {
                    "start": 249,
                    "end": 257,
                    "text": "Figure 4",
                    "ref_id": "FIGREF11"
                }
            ],
            "section": "Spam data"
        },
        {
            "text": "The 2019 novel coronavirus (COVID-19) has affected more than 180 countries around the world, including South Africa. Understanding the interaction of key metrics and attributes between various phases, cycles or waves of the pandemic may prove to be invaluable in strategic planning and prevention. The goal here, is to use the Bayesian DN, B-net, to illustrate that the interactivity of key daily metrics between suspected homogeneous and heterogeneous phases within the pandemic life cycle is ever changing. In particular, the B-net is used to model the interactivity of daily metrics between the first two peaks or waves; the first wave and the following plateau and finally the difference between the first and second post wave plateaus. The data consists of 446 observations from the 7 th of February 2020 to the 27 th of April 2021. The daily metrics include, deaths; performed tests; positive test rate; active cases; tests per active case; recoveries; hospital admissions; hospital discharges; ICU admissions and the number of ventilated patients. Due to the irregularities in data capturing and publishing, a seven day moving average is applied across all daily metrics. The data is standardised using a non-paranormal transformation in order to satisfy the Gaussian assumption. The B-net is applied to the data using 10000 iterations of the Monte Carlo sampler after 5000 burn-in iterations. Figure 5 highlights the temporal nature of the pandemic between suspected homogeneous and heterogeneous phases. In other words, comparing the cyclical behaviour of individual daily metrics may seem clearly distinctive over time; a peak or wave is always followed by a plateau. Furthermore, extrapolation of the temporal behaviour of individual daily metrics may incorrectly allude to distinct multi modality of multiple daily metrics. Upon observing multiple metrics simultaneously, the crisp group-wise multi modality diminishes rather rapidly. The figures in Table 6 illustrate the higher proportions of hub features present in the DNs. Interestingly, the Bayesian DN provides insight to the change in interaction between daily metrics between perceived homogeneous pandemic phases, that is comparisons between the two peaks and two post-peak plateaus. This change in behaviour could be as a result of the change in population adherence to public sanitation awareness; weather conditions; virus mutations or complacency of over time. < 0.001 Table 6 . The Bayesian DN and corresponding BAGLASSO graphical models between the first two waves; the first wave and the following plateau and finally the difference between the first and second post wave plateaus. This figures are accompanied by the associated p \u2212 values from the Box's M-test for homogeneity of covariance matrices between the contributing precision matrices [34] .",
            "cite_spans": [
                {
                    "start": 2824,
                    "end": 2828,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [
                {
                    "start": 1401,
                    "end": 1409,
                    "text": "Figure 5",
                    "ref_id": "FIGREF16"
                },
                {
                    "start": 1962,
                    "end": 1969,
                    "text": "Table 6",
                    "ref_id": null
                },
                {
                    "start": 2445,
                    "end": 2452,
                    "text": "Table 6",
                    "ref_id": null
                }
            ],
            "section": "South African COVID-19 data"
        },
        {
            "text": "The Bayesian differential network estimator is the first of its kind which utilises the excellent graphical structure determination and matrix estimation of the Bayesian graphical lasso [12] . In comparison with the state of the art iterative shrinkage-thresholding approach, the Bayesian differential network offers MCMC outputs that allow the user to gain deeper insight and inference in the estimation procedure. The numerical accuracy and graphical structure determination of the Bayesian differential network are, in general, superior to that of the iterative shrinkage-thresholding estimator. The graphical structure learning is a crucial component of the Bayesian differential network estimator. The ad hoc approach provided in Eq (8) suggests a suitable sparsity threshold under the varying graph structures. The Bayesian differential network also provides key insights to changes in the interactive behaviour of real data metrics ranging from filtering spam emails to COVID-19 life cycles. For high-dimensional data, the block Gibbs sampler may be adjusted to incorporate the singular normal distribution presented in [35] in the hierarchical representation Eq (7). Furthermore, research on simultaneous Bayesian estimation and optimisation of both \u03a3 1 \u22121 and \u03a3 2 \u22121 in the construction of the differential network is underway.",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 190,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1127,
                    "end": 1131,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Supporting information S1 Appendix. Supplementary file containing a block Gibbs sampler, as well as, additional optimal threshold; adjacency heatmaps and graphical network figures for dimensions p = 30 and p = 100. (PDF) Moreover, \u03c4 can be updated by observing that the conditional posterior distribution of the 1/\u03c4 ij 's in Eq (5) are independently inverse Gaussian (INV \u2212 GAU) with \u03bb ij = \u03bb 2 ij and \u00b5 = (\u03bb 2 ij /\u03b8 2 ij )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "The block Gibbs sampler algorithm is summarised by Algorithm 1 Block Gibbs sampler",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Differential network analysis: A statistical perspective",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shojaie",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Wiley Interdisciplinary Reviews: Computational Statistics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Probabilistic Graphical Models: Principles and Techniques",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Koller",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "High-dimensional graphs and variable selection with the lasso",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Meinshausen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "B\u00fchlmann",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "The Annals of Statistics",
            "volume": "34",
            "issn": "3",
            "pages": "1436--1462",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Graphical models",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lauritzen",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Model selection and estimation in the Gaussian graphical model",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Biometrika",
            "volume": "94",
            "issn": "1",
            "pages": "19--35",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Sparse inverse covariance estimation with the graphical lasso",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Biostatistics",
            "volume": "9",
            "issn": "3",
            "pages": "432--441",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "E"
                    ],
                    "last": "Ghaoui",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Aspremont",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "The Journal of Machine Learning Research",
            "volume": "9",
            "issn": "",
            "pages": "485--516",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Smooth minimization of non-smooth functions",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Nesterov",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Mathematical Programming",
            "volume": "103",
            "issn": "1",
            "pages": "127--152",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A constrained 1 minimization approach to sparse precision matrix estimation",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Journal of the American Statistical Association",
            "volume": "106",
            "issn": "494",
            "pages": "594--607",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Joint estimation of multiple graphical models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Levina",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Michailidis",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Biometrika",
            "volume": "98",
            "issn": "1",
            "pages": "1--15",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "The joint graphical lasso for inverse covariance estimation across multiple classes",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Danaher",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Witten",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
            "volume": "76",
            "issn": "2",
            "pages": "373--397",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Bayesian graphical lasso models and efficient posterior computation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Bayesian Analysis",
            "volume": "7",
            "issn": "4",
            "pages": "867--886",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Bayesian structure learning in graphical models",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ghosal",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Journal of Multivariate Analysis",
            "volume": "136",
            "issn": "",
            "pages": "147--162",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Bayesian inference of multiple Gaussian graphical models",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Peterson",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Stingo",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Vannucci",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Journal of the American Statistical Association",
            "volume": "110",
            "issn": "509",
            "pages": "159--174",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Bayesian estimation of Gaussian graphical models with predictive covariance selection",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Williams",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Piironen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vehtari",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Rast",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Network-based classification of breast cancer metastasis. Molecular systems biology",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chuang",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ideker",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Dynamic modularity in protein interaction networks predicts breast cancer outcome",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Taylor",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Linding",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Warder-Farley",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Pesquita",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Faria",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Nature Biotechnology",
            "volume": "27",
            "issn": "2",
            "pages": "199--204",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Sparse quadratic discriminant analysis for high dimensional data",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Statistica Sinica",
            "volume": "25",
            "issn": "",
            "pages": "457--473",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "A direct approach for sparse quadratic discriminant analysis",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Leng",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "The Journal of Machine Learning Research",
            "volume": "19",
            "issn": "1",
            "pages": "1098--1134",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Inferring multiple graphical structures",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chiquet",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Grandvalet",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ambroise",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Statistics and Computing",
            "volume": "21",
            "issn": "4",
            "pages": "537--553",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Multiple matrix gaussian graphs estimation",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
            "volume": "80",
            "issn": "5",
            "pages": "927--950",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Direct estimation of differential networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Biometrika",
            "volume": "101",
            "issn": "2",
            "pages": "253--268",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Differential network analysis via lasso penalized D-trace loss",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Xi",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Biometrika",
            "volume": "104",
            "issn": "4",
            "pages": "755--770",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "A fast iterative algorithm for high-dimensional differential network",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Computational Statistics",
            "volume": "35",
            "issn": "1",
            "pages": "95--109",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Beck",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Teboulle",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "SIAM Journal on Imaging Sciences",
            "volume": "2",
            "issn": "1",
            "pages": "183--202",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "The bayesian lasso",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Casella",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Journal of the American Statistical Association",
            "volume": "103",
            "issn": "482",
            "pages": "681--686",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Scale mixtures of normal distributions",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "F"
                    ],
                    "last": "Andrews",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Mallows",
                    "suffix": ""
                }
            ],
            "year": 1974,
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
            "volume": "36",
            "issn": "1",
            "pages": "99--102",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "On scale mixtures of normal distributions",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "West",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "Biometrika",
            "volume": "74",
            "issn": "3",
            "pages": "646--648",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "The Bayesian elastic net",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Bayesian Analysis",
            "volume": "5",
            "issn": "1",
            "pages": "151--170",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Inference with normal-gamma prior distributions in regression problems",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Griffin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Philip",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Bayesian Analysis",
            "volume": "5",
            "issn": "1",
            "pages": "171--188",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "The horseshoe estimator for sparse signals",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "Carvalho",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "G"
                    ],
                    "last": "Polson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "G"
                    ],
                    "last": "Scott",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Biometrika",
            "volume": "97",
            "issn": "2",
            "pages": "465--480",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Comparison of the predicted and observed secondary structure of T4 phage lysozyme",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "W"
                    ],
                    "last": "Matthews",
                    "suffix": ""
                }
            ],
            "year": 1975,
            "venue": "Biochimica et Biophysica Acta (BBA)-Protein Structure",
            "volume": "405",
            "issn": "2",
            "pages": "442--451",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "A regularized profile likelihood approach to covariance matrix estimation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Monni",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wells",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Journal of Statistical Planning and Inference",
            "volume": "179",
            "issn": "",
            "pages": "36--59",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "A general distribution theory for a class of likelihood criteria",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Box",
                    "suffix": ""
                }
            ],
            "year": 1949,
            "venue": "Biometrika",
            "volume": "36",
            "issn": "3/4",
            "pages": "317--346",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "A note on singular normal distributions",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "P"
                    ],
                    "last": "Bland",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "B"
                    ],
                    "last": "Owen",
                    "suffix": ""
                }
            ],
            "year": 1966,
            "venue": "Annals of the Institute of Statistical Mathematics",
            "volume": "18",
            "issn": "1",
            "pages": "113--116",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Set the hyperparameters (s, r) for the gamma distribution in Eq (7) and initialise {\u03bb ii } p i=1 and \u03c4",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "2",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Sample \u03b3 \u223c GA(n/2 + 1, (s 22 + \u03bb)/2) and \u03b2 \u223c N(\u2212Cs 21 , C)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Update \u03b8 12 = \u03b2, \u03b8 21 = \u03b2 , \u03b8 22 = \u03b3 + \u03b2 \u0398 \u22121 ij \u223c GA(1 + r, |\u03b8 ij | + s)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Sample \u03b4 ij \u223c INV \u2212 GAU(\u00b5 , \u03bb ) and update \u03c4 ij = 1/\u03b4 ij 10: end for For the choice of the hyperparameters (r, s), [?] suggest r = 10 \u22122 and s = 10 \u22126 for excellent performance in the adaptability of \u03bb ij to each \u03b8 ij and \u03bb ii = 1",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Model 2: AR(2). (c) Model 3: at most 80% sparse.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Model 4: at most 40% sparse. (e) Model 5: scale-free. (f) Model 6: band. (g) Model 7: cluster. (h) Model 8: star. (i) Model 9: circle.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The median of the absolute sparsity error and best performing MCC for various graph structures under varying thresholds for each Bayesian sparsity criterion in Eq (9) (dotted) and Eq (8) (dot-dash) for dimension p = 10. The best performing threshold is indicated by a vertical line with the accompanying MCC value displayed in the legend.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Component 1 : \u03b8 ii = 0.1, \u03b8 i,i\u22121 = \u03b8 i\u22121,i = 0.05 and \u03b8 i,i\u22122 = \u03b8 i\u22122,i = 0.025. -Component 2 : \u03b8 ii = 1, \u03b8 i,i\u22121 = \u03b8 i\u22121,i = 0.5 and \u03b8 i,i\u22122 = \u03b8 i\u22122,i = 0.25. \u2022 structure 3 : A sparse random model where both components have approximately up to 80% off-diagonal elements set to zero.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Component 1 : \u03b8 ii = 2, \u03b8 i,i\u22121 = \u03b8 i\u22121,i = 1 and \u03b8 1,p = \u03b8 p,1 = 0.45. Component 2 : \u03b8 ii = 4, \u03b8 i,i\u22121 = \u03b8 i\u22121,i = 2 and \u03b8 1,p = \u03b8 p,1 = 0.95. The sample sizes and dimensions for each model are n 1 = n 2 \u2208 {50, 100, 200} and p 1 = p 2 \u2208 {10, 30, 100}, respectively. The Bayesian estimates are based on 10000 Monte Carlo iterations after 5000 burn-in iterations. To assess the performance of DN matrix estimation, six loss functions are considered and defined in",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Summary of L1, L2, EL1, EL2, MAXEL1 and MINEL1 for an AR(1), AR(2), sparse random, scale-free, band, cluster, star and circle graphical model. The median loss values reported here are based on 40 replications for both the B-net and D-net estimators. The best performing values are boldfaced.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Summary of L1, L2, EL1, EL2, MAXEL1 and MINEL1 for an AR(1), AR(2), sparse random, scale-free, band, cluster, star and circle graphical model. The median standard errors reported here are based on 40 replications for both the B-net and D-net estimators. The best performing values are boldfaced. net D-net B-net D-net B-net D-net B-net D-net B-net D-net B-net D-net B-net D-net B-net D-net B-net DSummary of SE, SP, F1, MC and for an AR(1), AR(2), sparse random, scale-free, band, cluster, star and circle graphical model. The median performance scores reported here are based on 40 replications for both the B-net and D-net estimators. The best performing values are boldfaced and scores that were not attainable due to single class classification are encoded as NA. net D-net B-net D-net B-net D-net B-net D-net B-net D-net B-net D-net B-net D-net B-net D-net B-net D",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "TP+FP)(TP+FN)(TN+FP)(TN+FN) MC The B-net estimator generally outperforms the D-net estimator across all models and all dimensions according to the MCC, f1-score, sensitivity and false negative rate, with the exception of the star case for p = 100. Figure 3 (1a) -(1i) display the true and inferred undirected DN graphs for both the B-net and D-net estimators for p = 10; higher dimensions are available in the supplementary material. Lastly, Figure 2 (2a) -(2i) display the true and inferred adjacency matrices for p = 10. Both Figures 3 and 2 visually demonstrate the superiority of the B-net estimator.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Model 1: AR(1). (b) Model 2: AR(2). (c) Model 3: at most 80% sparse. (d) Model 4: at most 40% sparse. (e) Model 5: scale-free. (f) Model 6: band .(g) Model 7: cluster. (h) Model 8: star. (i) Model 9: circle.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Comparison of the true DN, B-net and D-net adjacency matrices for an AR(1), AR(2), sparse random, scale-free, band, cluster, star and circle graphical model and p = 10.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "The Bayesian DN for the spam emails dataset.(b) The iterative-shrinkage DN for the spam emails dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "A comparison of the D-net and B-net DN estimates for the spam emails dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Fig 5. 7-day moving average filled area line plots with standardised counts for daily new cases; deaths; tests; positive test rate; active cases; tests per active case; recoveries; hospital admissions; hospital discharges; ICU admissions and ventilated patients.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Bayesian DN for first wave and first post wave plateau.(b) BAGLASSO graphical model of the first wave.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "Bayesian DN for first and second wave.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "BAGLASSO graphical model of the first wave.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "BAGLASSO graphical model of the second wave.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Bayesian DN for first and second post wave plateaus. (h) BAGLASSO graphical model of the first post wave plateau.(i) BAGLASSO graphical model of the second post wave plateau.",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "5. 7-day moving average filled area line plots with standardised counts for daily new cases; deaths; tests; positive test rate; active cases; tests per active case; recoveries; hospital admissions; hospital discharges; ICU admissions and ventilated patients.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors have declared that no competing interests exist. (7) of Eq (3), allowing for different \u03bb ij 's in the target distribution Eq (5) is re-presented here for completeness.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Competing interests"
        },
        {
            "text": "To update a single column and row of \u0398 per iteration -focusing on the last column and row -let \u03a5 be a p \u00d7 p symmetric matrix with a zero main diagonal and \u03c4 in the upper off diagonal entries. Secondly, partition the matrices \u0398, S and \u03a5 as followsNotice that the conditional distribution of the last column in Eq (5) can be presented ashere D \u03c4 is the diagonalised matrix of the vector \u03c4 12 . The following change of variables \u03b2 = \u03b8 12 \u03b3 = \u03b8 22 \u2212 \u03b8 21 \u0398 \u22121 11 \u03b8 12 with Jacobian independent of (\u03b2, \u03b3), yields the following conditional distributionIt follows that (\u03b2, \u03b3 | \u0398 11 , \u03a5, S, \u03bb) \u223c GA( n 2 + 1,for a discussion on how the positive definite constraint on \u0398 is adhered to in the formulation above. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Author Contributions"
        }
    ]
}
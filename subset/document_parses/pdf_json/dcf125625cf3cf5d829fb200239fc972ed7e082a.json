{
    "paper_id": "dcf125625cf3cf5d829fb200239fc972ed7e082a",
    "metadata": {
        "title": "Development of a face mask detection pipeline for mask-wearing monitoring in the era of the COVID-19 pandemic: A modular approach",
        "authors": [
            {
                "first": "Benjaphan",
                "middle": [],
                "last": "Sommana",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Sertis Vision Lab",
                    "institution": "Mahidol University",
                    "location": {
                        "addrLine": "597/5 Sukhumvit Road, 270 Rama VI road, Thung Phaya Thai, Ratchathewi",
                        "postCode": "10110, 10400",
                        "settlement": "Watthana, Bangkok, Bangkok",
                        "country": "Thailand, Thailand"
                    }
                },
                "email": ""
            },
            {
                "first": "Ukrit",
                "middle": [],
                "last": "Watchareeruetai",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Sertis Vision Lab",
                    "institution": "Mahidol University",
                    "location": {
                        "addrLine": "597/5 Sukhumvit Road, 270 Rama VI road, Thung Phaya Thai, Ratchathewi",
                        "postCode": "10110, 10400",
                        "settlement": "Watthana, Bangkok, Bangkok",
                        "country": "Thailand, Thailand"
                    }
                },
                "email": ""
            },
            {
                "first": "Ankush",
                "middle": [],
                "last": "Ganguly",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Sertis Vision Lab",
                    "institution": "Mahidol University",
                    "location": {
                        "addrLine": "597/5 Sukhumvit Road, 270 Rama VI road, Thung Phaya Thai, Ratchathewi",
                        "postCode": "10110, 10400",
                        "settlement": "Watthana, Bangkok, Bangkok",
                        "country": "Thailand, Thailand"
                    }
                },
                "email": ""
            },
            {
                "first": "Samuel",
                "middle": [
                    "W F"
                ],
                "last": "Earp",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Sertis Vision Lab",
                    "institution": "Mahidol University",
                    "location": {
                        "addrLine": "597/5 Sukhumvit Road, 270 Rama VI road, Thung Phaya Thai, Ratchathewi",
                        "postCode": "10110, 10400",
                        "settlement": "Watthana, Bangkok, Bangkok",
                        "country": "Thailand, Thailand"
                    }
                },
                "email": ""
            },
            {
                "first": "Taya",
                "middle": [],
                "last": "Kitiyakara",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Sertis Vision Lab",
                    "institution": "Mahidol University",
                    "location": {
                        "addrLine": "597/5 Sukhumvit Road, 270 Rama VI road, Thung Phaya Thai, Ratchathewi",
                        "postCode": "10110, 10400",
                        "settlement": "Watthana, Bangkok, Bangkok",
                        "country": "Thailand, Thailand"
                    }
                },
                "email": ""
            },
            {
                "first": "Suparee",
                "middle": [],
                "last": "Boonmanunt",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Sertis Vision Lab",
                    "institution": "Mahidol University",
                    "location": {
                        "addrLine": "597/5 Sukhumvit Road, 270 Rama VI road, Thung Phaya Thai, Ratchathewi",
                        "postCode": "10110, 10400",
                        "settlement": "Watthana, Bangkok, Bangkok",
                        "country": "Thailand, Thailand"
                    }
                },
                "email": ""
            },
            {
                "first": "Ratchainant",
                "middle": [],
                "last": "Thammasudjarit",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Sertis Vision Lab",
                    "institution": "Mahidol University",
                    "location": {
                        "addrLine": "597/5 Sukhumvit Road, 270 Rama VI road, Thung Phaya Thai, Ratchathewi",
                        "postCode": "10110, 10400",
                        "settlement": "Watthana, Bangkok, Bangkok",
                        "country": "Thailand, Thailand"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to prevent spreading and contracting the virus. The ability to monitor the mask-wearing rate in the population would be useful for determining public health strategies against the virus. However, artificial intelligence technologies for detecting face masks have not been deployed at a large scale in real-life to measure the mask-wearing rate in public. In this paper, we present a two-step face mask detection approach consisting of two separate modules: 1) face detection and alignment and 2) face mask classification. This approach allowed us to experiment with different combinations of face detection and face mask classification modules. More specifically, we experimented with PyramidKey and RetinaFace as face detectors while maintaining a lightweight backbone for the face mask classification module. Moreover, we also provide a relabeled annotation of the test set of the AIZOO dataset, where we rectified the incorrect labels for some face images. The evaluation results on the AIZOO and Moxa 3K datasets showed that the proposed face mask detection pipeline surpassed the state-of-the-art methods. The proposed pipeline also yielded a higher mAP on the relabeled test set of the AIZOO dataset than the original test set. Since we trained the proposed model using in-the-wild face images, we can successfully deploy our model to monitor the mask-wearing rate using public CCTV images. *",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Since the beginning of the SARS-Cov-2 pandemic in the later part of 2019, there have been more than five million deaths from the disease globally and many more hospitalized 2 . Many countries have suffered economic problems due to repeated lockdowns and decreased consumption. As the SARS-Cov-2 virus is a respiratory virus and spreads by a mixture of large droplets and airborne particles (Jayaweera et al., 2020; Liu et al., 2020) , nonpharmaceutical interventions such as mask-wearing, handwashing and social distancing have been used to limit the spread of the virus 3 . As an intervention, mask-wearing was initially seen as a controversial issue and received varying levels of acceptance (Peeples, 2020) ; however, many countries led by example on accepting mask-wearing as a norm during the pandemic. Since then, mask-wearing has gradually become more accepted as an effective instrument in decreasing the spread of COVID-19 disease, as shown by studies demonstrating lower infection rates in areas with mask mandates (Lyu and Wehby, 2020; Van Dyke et al., 2020) . A recent meta-analysis has suggested that wearing masks could reduce the spread of the virus by up to 50% (Talic et al., 2021) . Most studies evaluating the rate of mask-wearing have used the presence of mask mandates or online questionnaires (Lyu and Wehby, 2020; Van Dyke et al., 2020) , or used one-off documentation of mask-wearing data (Elachola et al., 2020) . These have their limitations in terms of accuracy and monitoring the changes in the mask-wearing rate. Current technology in image analysis has shown that artificial intelligence (AI) can analyze and recognize faces and mask-wearing faces. Many published reports have shown that AI technologies are capable of recognizing and differentiating faces with and without masks (e.g. Fan et al., 2021; Jiang and Fan, 2020; Joshi et al., 2020; Prasad et al., 2021; Roy et al., 2020) .",
            "cite_spans": [
                {
                    "start": 390,
                    "end": 414,
                    "text": "(Jayaweera et al., 2020;",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 415,
                    "end": 432,
                    "text": "Liu et al., 2020)",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 694,
                    "end": 709,
                    "text": "(Peeples, 2020)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1025,
                    "end": 1046,
                    "text": "(Lyu and Wehby, 2020;",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1047,
                    "end": 1069,
                    "text": "Van Dyke et al., 2020)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1178,
                    "end": 1198,
                    "text": "(Talic et al., 2021)",
                    "ref_id": null
                },
                {
                    "start": 1315,
                    "end": 1336,
                    "text": "(Lyu and Wehby, 2020;",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1337,
                    "end": 1359,
                    "text": "Van Dyke et al., 2020)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1413,
                    "end": 1436,
                    "text": "(Elachola et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 1816,
                    "end": 1833,
                    "text": "Fan et al., 2021;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1834,
                    "end": 1854,
                    "text": "Jiang and Fan, 2020;",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1855,
                    "end": 1874,
                    "text": "Joshi et al., 2020;",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1875,
                    "end": 1895,
                    "text": "Prasad et al., 2021;",
                    "ref_id": null
                },
                {
                    "start": 1896,
                    "end": 1913,
                    "text": "Roy et al., 2020)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Since 2020, many researchers have explored and developed different face mask detection methods categorized into two main approaches. The first approach is two-step face mask detection, e.g., Joshi et al. (2020) , which performs face detection to locate the faces and then makes a prediction of the existence of face masks with a classification model on the detected faces. The advantage of this approach is modularity, which makes it easy to change the face detection and the mask classification modules. This modularity allows the face detector and face mask classification to be trained separately without any concerns about lacking face mask detection datasets and balancing detection and classification losses. A face detector can learn from a large-scale face detection dataset such as the WIDER Face dataset (Yang et al., 2016) while the face mask classifier can train on a crop-align face dataset with an additional task on labeling the mask class. Another approach is the single-step or end-to-end face mask detection, which exploits an object detection scheme by jointly detecting faces and predicting the presence of masks. Most of the previously published research (e.g. Deng et al., 2019; Fan et al., 2021; Jiang and Fan, 2020; Prasad et al., 2021; Roy et al., 2020) has adopted this approach. The main advantage of the end-to-end detection approach is that less resource consumption is needed compared to the two-step approach. Due to the lower computational consumption, the end-to-end detection model can be used to detect in real-time. The major drawback of this approach is the need for a face mask detection dataset to fine-tune the detection model.",
            "cite_spans": [
                {
                    "start": 191,
                    "end": 210,
                    "text": "Joshi et al. (2020)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 814,
                    "end": 833,
                    "text": "(Yang et al., 2016)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1182,
                    "end": 1200,
                    "text": "Deng et al., 2019;",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1201,
                    "end": 1218,
                    "text": "Fan et al., 2021;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1219,
                    "end": 1239,
                    "text": "Jiang and Fan, 2020;",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1240,
                    "end": 1260,
                    "text": "Prasad et al., 2021;",
                    "ref_id": null
                },
                {
                    "start": 1261,
                    "end": 1278,
                    "text": "Roy et al., 2020)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we developed a two-step face mask detection method to take advantage of existing efficient face detectors from Earp et al. (2019) and Deng et al. (2019) . As shown in Figure 1 , our proposed face mask detection consists of two main modules: 1)face detection and alignment and 2)face mask classification. Firstly, we adopted existing face detectors from Deng et al. (2019) ; Earp et al. (2019) to detect faces with landmarks. Then, the detected faces were cropped and aligned before being passed through the face mask classification model. We trained the face mask classification model with cropped and aligned face images from our internal face mask dataset, namely SertisFaceMask dataset.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 144,
                    "text": "Earp et al. (2019)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 149,
                    "end": 167,
                    "text": "Deng et al. (2019)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 368,
                    "end": 386,
                    "text": "Deng et al. (2019)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 389,
                    "end": 407,
                    "text": "Earp et al. (2019)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [
                {
                    "start": 182,
                    "end": 190,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "We summarize the key contributions of this research as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We propose a two-step face mask detection pipeline composed of face detection and face mask classification models. With the benefits of the proposed pipeline, we can exploit the existing efficient face detection models with our lightweight face mask classification model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We also provide the relabeled annotation of the test set of the AIZOO dataset (AIZOOTech) that we manually relabeled to rectify the incorrect labels for certain face images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We investigate several techniques for training the face mask classification model, such as label smoothing, face alignment, and focusing on mask region. We show that the baseline model performed better than applying these techniques on the classification task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We demonstrate that our two-step face mask detection outperformed state-of-the-art approaches (Fan et al., 2021; Jiang and Fan, 2020) on the AIZOO dataset in both original and relabeled test sets. Moreover, our proposed method also outperforms the recent approaches (Fan et al., 2021; Roy et al., 2020) on Moxa 3K dataset.",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 114,
                    "text": "(Fan et al., 2021;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 115,
                    "end": 135,
                    "text": "Jiang and Fan, 2020)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 268,
                    "end": 286,
                    "text": "(Fan et al., 2021;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 287,
                    "end": 304,
                    "text": "Roy et al., 2020)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We organize this paper as follows. We review some recent related works in Section 2. Section 3 describes datasets that we used for training and evaluation. Our proposed pipeline is presented in Section 4. Section 5 details experimental setting, evaluation metrics, and the main results. Finally, Section 6 concludes with a summary of our findings and discusses the future work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "As mentioned in Section 1, we categorize face mask detection into two main approaches, which are two-step and end-to-end face mask detection. The two-step face mask detection contains two separate modules: face detection and face mask classification, processed sequentially. The modularity of this approach allows us to easily change the face detection or the face mask classification modules. This approach also allows training each module separately. For face detection, large-scale datasets are available to train and evaluate face detection models, for example, WIDER Face (Yang et al., 2016) and Masked Faces (MAFA) datasets (Ge et al., 2017) . Even though the dataset for face mask classification is not widely available, labeling for classification tasks requires less effort than detection tasks. On the other hand, this approach consumes more time and resources than end-to-end detection with a similar backbone. Joshi et al. (2020) developed face mask detection model based on a two-step approach by employing multi-task cascaded convolutional networks (MTCNN) (Zhang et al., 2016) for face detection and MobileNetV2 for face mask classification.",
            "cite_spans": [
                {
                    "start": 577,
                    "end": 596,
                    "text": "(Yang et al., 2016)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 630,
                    "end": 647,
                    "text": "(Ge et al., 2017)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 922,
                    "end": 941,
                    "text": "Joshi et al. (2020)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1071,
                    "end": 1091,
                    "text": "(Zhang et al., 2016)",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "Due to resource limitations and real-time processing, most researchers propose a face mask detection model with an end-to-end detector. AIZOOTech proposed a lightweight face mask detection model which exploited the structure of a Single Shot Detector (SSD) (Liu et al., 2015) with the backbone of eight convolution layers. Moreover, they also proposed a face mask dataset called AIZOO,",
            "cite_spans": [
                {
                    "start": 257,
                    "end": 275,
                    "text": "(Liu et al., 2015)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "Face mask classification which included images from WIDER Face (Yang et al., 2016) and Masked Faces (MAFA) datasets (Ge et al., 2017) . Jiang and Fan (2020) proposed a face mask detection model, namely RetinaFaceMask, based on RetinaFace (Deng et al., 2019) structure. Their detection model consisted of three parts, i.e., backbone, neck, and detection head. The backbone generated feature maps, the neck performed feature aggregation, and the detection head converted feature representation to bounding boxes, landmarks, and confidence scores. They adopted ResNet and MobileNet as a backbone and applied Feature Pyramid Network (FPN) in the neck part. For a detection head, they proposed a context detection module similar to the context module in Single-Stage Headless (SSH) (Najibi et al., 2017) to form an inception block with different sizes of kernels. Furthermore, they also applied a convolutional block attention module (CBAM) (Woo et al., 2018) by cascading this attention module after the original context module. Roy et al. (2020) established face mask dataset, called Moxa3K, collected from Kaggle 4 and online sources which have 2,800 and 200 images for training and testing respectively. Moreover, they trained their face mask detector models based on popular object detection architectures, e.g., Faster R-CNN (Ren et al., 2015) , SSD (Liu et al., 2015) , YOLOv3, and YOLOv3-tiny (Redmon and Farhadi, 2018) . Prasad et al. (2021) proposed a model named maskedFaceNet, based on SSD architecture with RFB network (Liu et al., 2017) , with a progressive semi-supervised learning method to handle inadequate training samples. A progressive semi-supervised learning method for the training strategy consisted of three main phases. Firstly, the model was pre-trained on the WIDER FACE dataset (Yang et al., 2016) with grid loss (Opitz et al., 2016) and fine-tuned on the proposed face mask datasets MASKface v1 and MASK-face v2, with smooth-L1 loss. To retain the information from WIDER FACE, the authors froze the first two layers during fine-tuning model. Secondly, they used the fine-tuned model to predict face masks with unlabeled data to generate pseudo-labeled masked face datasets collected from various sources and resolutions. Finally, they used the generated pseudo-labeled images with a confidence score higher than 0.9 for fine-tuning the final model in the final phase with progressive training, where pseudolabeled images will gradually combine with the original dataset ordered by a confidence score. Fan et al. (2021) proposed a single-shot lightweight face mask detector with residual context attention module (RCAM) and synthesized Gaussian heatmap regression (SGHR). The authors employed MobileNet 0.25 as a backbone and applied FPN in the neck part. The detection head used the proposed RCAM to extract more information on the face mask region. The main difference from previous research was the use of the heatmap branch as an auxiliary task to gain more discrimination ability. In the heatmap branch, only faces with masks were considered by generating face and mask keypoints. Localization loss was fused with standard detection and classification loss.",
            "cite_spans": [
                {
                    "start": 63,
                    "end": 82,
                    "text": "(Yang et al., 2016)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 116,
                    "end": 133,
                    "text": "(Ge et al., 2017)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 136,
                    "end": 156,
                    "text": "Jiang and Fan (2020)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 238,
                    "end": 257,
                    "text": "(Deng et al., 2019)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 777,
                    "end": 798,
                    "text": "(Najibi et al., 2017)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 936,
                    "end": 954,
                    "text": "(Woo et al., 2018)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1025,
                    "end": 1042,
                    "text": "Roy et al. (2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1326,
                    "end": 1344,
                    "text": "(Ren et al., 2015)",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1351,
                    "end": 1369,
                    "text": "(Liu et al., 2015)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1396,
                    "end": 1422,
                    "text": "(Redmon and Farhadi, 2018)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1425,
                    "end": 1445,
                    "text": "Prasad et al. (2021)",
                    "ref_id": null
                },
                {
                    "start": 1527,
                    "end": 1545,
                    "text": "(Liu et al., 2017)",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1803,
                    "end": 1822,
                    "text": "(Yang et al., 2016)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1838,
                    "end": 1858,
                    "text": "(Opitz et al., 2016)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 2527,
                    "end": 2544,
                    "text": "Fan et al. (2021)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Face alignment"
        },
        {
            "text": "In this paper, we develop face mask detection based on a two-step approach similar to Joshi et al. (2020) while adding face alignment process before the classification stage and employing more efficient face detectors, e.g., RetinaFace (Deng et al., 2019) and PyramidKey (Earp et al., 2019) .",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 105,
                    "text": "Joshi et al. (2020)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 236,
                    "end": 255,
                    "text": "(Deng et al., 2019)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 271,
                    "end": 290,
                    "text": "(Earp et al., 2019)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Face alignment"
        },
        {
            "text": "This section describes the dataset we used to train and evaluate our face mask classification model and benchmark the mask detection pipeline. We created an internal dataset named SertisFaceMask to train and evaluate our face mask classification model. We compared our pipeline with stateof-the-art methods on benchmarking datasets, i.e., AIZOO and Moxa 3K datasets. Table 1 shows the details of each dataset, including the number of images in each set, data characteristics, and the problem type of each dataset.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 367,
                    "end": 374,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "SertisFaceMask dataset, which was used to train our face mask classification model, consisted of different mask types, e.g., medical, pollution, gas, and dust masks. We separated the dataset with more than 30,000 face images into three sets: training, validation, and test set with 50%, 30%, and 20%, respectively. Each set balanced the number of face images with and without masks. We collected this dataset from online sources and combined it with occluded face images from Caltech Occluded Faces in the Wild (COFW) dataset (Wu and Ji, 2017) . We used PyramidKey face detector (Earp et al., 2019) to detect faces and then subsequently cropped, aligned and resized face images to 112\u00d7112 pixels.",
            "cite_spans": [
                {
                    "start": 526,
                    "end": 543,
                    "text": "(Wu and Ji, 2017)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 579,
                    "end": 598,
                    "text": "(Earp et al., 2019)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "SertisFaceMask dataset"
        },
        {
            "text": "Since we collected this dataset from online sources, it contained some noisy images, e.g., incorrect class, animation, and obscure images, which we manually cleaned from the dataset. We annotated the dataset by using the visibility of nose as a criterion. When masks covered the nose, images were labeled as 'Mask' while masks were below by the nose, images were labeled as 'No-Mask'.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SertisFaceMask dataset"
        },
        {
            "text": "AIZOOTech provided the face mask detection dataset, called AIZOO, that combines WIDER Face (Yang et al., 2016) and Masked Faces (MAFA) dataset (Ge et al., 2017) . This dataset had two sets: training and test sets. The training set contained 6,120 images with 13,593 faces, with almost 78% of exposed faces. The test set had 1,830 images with 3,062 faces that were 2,020 faces without a mask which is about 65% of the faces in the test set. In order to classify masks that could reduce the spread of the virus, we considered only medical or pollution masks on human faces. We decided to relabel some faces that were assigned to the incorrect class or wore neither medical nor pollution masks. We manually corrected the mask class of faces as shown in Figure 2 . We found that we needed to relabel 89 face images. Among them, we changed the labels of 82 faces from 'Mask' to 'No-mask', switched five to 'Mask', and eliminated two images due to non-human faces. Moreover, we also released the relabeled annotation of the test set of the AIZOO dataset, which is available online 5 .",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 110,
                    "text": "(Yang et al., 2016)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 143,
                    "end": 160,
                    "text": "(Ge et al., 2017)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 750,
                    "end": 758,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "AIZOO dataset"
        },
        {
            "text": "The Moxa 3K dataset, created by Roy et al. (2020) , was used for training and evaluating face mask detection models. This dataset combined images from two different sources; the medical mask dataset from Kaggle and online sources. Roy et al. (2020) collected this dataset during the COVID-19 pandemic. This dataset consists of 3,000 images with different scenarios, from close-up faces to crowded scenes, using 2,800 images for training and 200 images for testing. Unlike the AIZOO dataset, the Moxa 3K dataset contained faces with masks more than exposed faces. In the training set, only 26% of the images were faces without masks, similar to the test set where about 12% of faces were without masks.",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 49,
                    "text": "Roy et al. (2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 231,
                    "end": 248,
                    "text": "Roy et al. (2020)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Moxa 3K dataset"
        },
        {
            "text": "4 Proposed method Figure 1 shows an overview of the proposed two-step face mask detection pipeline. The pipeline consists of two main modules: 1) face detection and alignment and 2) face mask classification. These two modules are trained separately and processed together during the inference.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 18,
                    "end": 26,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Moxa 3K dataset"
        },
        {
            "text": "As discussed in Section 2, we exploit several existing face detectors from RetinaFace (Deng et al., 2019) and Pyramid-Key (Earp et al., 2019) which achieved good performance on the validation set of WIDER Face dataset (Yang et al., 2016) . RetinaFace and PyramidKey have similar architecture consisting of three parts; visual backbone, neck, and detection head. The RetinaFace and PyramidKey necks include FPN and independent context modules on the top five and top six levels of pyramid features, respectively. Reti-naFace employs a multi-task loss function that combines face classification, bounding box regression, facial landmark regression, and dense regression loss. At ",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 105,
                    "text": "(Deng et al., 2019)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 122,
                    "end": 141,
                    "text": "(Earp et al., 2019)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 218,
                    "end": 237,
                    "text": "(Yang et al., 2016)",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Face detection and alignment"
        },
        {
            "text": "To predict the existence of masks on the detected faces, we construct the face mask classification model by using a MobileNetV1 1.0 (Howard et al., 2017 ) as a backbone. The model is pre-trained on the ImageNet dataset (Deng et al., 2009 ) and fine-tuned on the SertisFaceMask dataset. The MobileNet structure is based on depthwise separable convolutions consisting of 3 \u00d7 3 depthwise and pointwise convolution layers. All convolutional layers are followed by batch normalization and rectified linear unit (ReLU) (Krizhevsky et al., 2017) . The model provides the probability of the mask existence from the softmax classifier in the final layer. We adopt Binary Cross Entropy loss which is defined as:",
            "cite_spans": [
                {
                    "start": 132,
                    "end": 152,
                    "text": "(Howard et al., 2017",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 219,
                    "end": 237,
                    "text": "(Deng et al., 2009",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 513,
                    "end": 538,
                    "text": "(Krizhevsky et al., 2017)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Face mask classification"
        },
        {
            "text": "where y i and\u0177 i are ground-truth and softmax probability of i-th class (i \u2208 {0, 1}), respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Face mask classification"
        },
        {
            "text": "This section describes different configurations for our face mask detection pipeline and training techniques for the face mask classification model. We evaluate the proposed method on the test sets of the AIZOO and the Moxa 3K datasets to compare with state-of-the-art approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "For training the face mask classification model, we apply the dropout technique (Srivastava et al., 2014) with a dropout rate of 0.5. The input images are resized to 224\u00d7224 pixels with several augmentation techniques, e.g., cropping with random size and ratio; randomly jittering the brightness, contrast, and saturation; adding AlexNet-style PCA-based noise (Krizhevsky et al., 2017) ; and horizontal flipping with a probability of 0.5. We train the face mask classification model on the training set of the SertisFace-Mask dataset by using Stochastic Gradient Descent (SGD) optimizer with the momentum of 0.9, weight decay of 5 \u00d7 10 \u22124 , and a base learning rate of 10 \u22122 for 80 epochs reducing it with a factor of 0.1 at 20, 40, and 60 epochs. We set a batch size of 32. During inference, we set the face detection threshold to 0.8 with three detection scales at 320, 640, and 960, and the Non-Maximum Suppression (NMS) threshold of 0.5 for the face detection model. The NMS is a popular post-processing technique that handles the redundancy of predicted bounding boxes by keeping the one with the highest confidence score and ignoring the ones with an intersection-over-union (IOU) score greater than the NMS threshold. For face mask classification, we set the input image size to 224 \u00d7 224. The predicted class is classified as 'Mask' when the probability of the mask existence was equal to or higher than the classification threshold set to 0.95.",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 105,
                    "text": "(Srivastava et al., 2014)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 360,
                    "end": 385,
                    "text": "(Krizhevsky et al., 2017)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Experimental setup"
        },
        {
            "text": "This section investigates training the face mask classification model with several techniques: label smoothing, aligning face with only eye key points, and replacing pixellevel information on the upper half face with random noise or pixels with value zero.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training techniques"
        },
        {
            "text": "Label smoothing proposed by aims to regularize the classification layer. The main idea of this technique is to lower the confidence of hard or bad examples for preventing over-fitting and making the model more adaptable. Following the label-smoothing regularization (LSR) method, we change ground-truth label distribution as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Label smoothing"
        },
        {
            "text": "where is the smoothing factor, N is the number of classes, and \u03b4 n,y is Dirac delta, which equals 1 for n = y and 0 otherwise. We manually label hard examples based on face mask visibility to experiment with label smoothing. Examples labeled as hard consisted of side faces, transparent masks, and occluded faces. In this experiment, we experiment with two different values of of 0.1 and 0.4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Label smoothing"
        },
        {
            "text": "The occlusion caused by a face mask affects the localization ability on the nose and mouth corners which may, in turn, affect the precision of face alignment. Therefore, we also experiment with a modification in the alignment process by aligning with only two key points of the eye centers, which are not occluded while wearing a mask, instead of all five key points.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Aligning with only eye key points"
        },
        {
            "text": "The visual appearance of the face mask may be more crucial than the exposed face region for determining whether a mask covers a face in an image. Consequently, we attempt to test this hypothesis by training a model to focus only on the mask region. We extract the mask region by leveraging the five key points, i.e., eye centers, nose, and mouth corners, from the face detector. In particular, we replace the region above the nose with random noise or zero-valued pixels to ignore the upper half face information and concentrate on the mask appearance only.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Replacing the upper half face"
        },
        {
            "text": "We use precision, recall, F1-score, and mean average precision (mAP) as metrics. The precision, recall, and F1-score are computed for each class as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation metrics"
        },
        {
            "text": "where TP, FP, and FN are true positive, false positive, and false negative counts, respectively. In order to obtain the mAP, the average precision (AP) of each class needs to be measured. We employ an all-point interpolation approach (Everingham et al., 2012) to calculate the average precision of each class by measuring the area under the precision and recall curve, which can be computed as follows:",
            "cite_spans": [
                {
                    "start": 234,
                    "end": 259,
                    "text": "(Everingham et al., 2012)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation metrics"
        },
        {
            "text": "where i is the index of recall and precision values, r and p are recall and precision values, respectively, and p(r i ) is precision value at i-th recall. Once we have computed the average precision of each class, we can calculate mAP as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation metrics"
        },
        {
            "text": "where N is number of classes and AP i is average precision of i-th class. We evaluate mAP with the IOU value between ground-truth and predicted bounding box at 0.5. Table 2 presents the face mask classification results on the test set of the SertisFaceMask dataset with different training techniques. The incorporation of label smoothing produces a lower F1-score than the baseline setting. When training the model with the smoothing factor at 0.1, its performance was comparable to setting it to 0.4. The F1-score for the prediction of no-mask and mask decreases from the baseline setting by about 1.04 and 1.12 points, respectively, when we used face alignment with only eye centers. While the model performs slightly better when the region over the nose was replaced with random noise compared to zero-valued pixels, both of these replacements perform worse than the baseline setting. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 165,
                    "end": 172,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Evaluation metrics"
        },
        {
            "text": "In this section, we compare the performance of the proposed face mask detection pipeline with baseline results from a dataset provider (AIZOOTech), RetinaFaceMask (Jiang and Fan, 2020) , and SL-FMDet (Fan et al., 2021) . As shown in Table 3 , our two-step face mask detection pipeline gains a large margin of precision on both the mask and no-mask classes compared to the baseline and RetinaFaceMask results (\u223c 2.30-17.39 points). Using the smallest PyramidKey face detector with MobileNetV2 0.25 backbone, our model achieves a higher F1-score on both classes compared to a bigger backbone like ResNet from RetinaFaceMask, by 0.07 and 0.27 points on the no-mask and mask classes, respectively. Compared to SL-FMDet that reported the result as mAP, our proposed pipeline yields slightly less mAP than SL-FMDet but trades off with a simpler and more straightforward face mask detection pipeline.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 184,
                    "text": "(Jiang and Fan, 2020)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 200,
                    "end": 218,
                    "text": "(Fan et al., 2021)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 233,
                    "end": 240,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Results on AIZOO dataset"
        },
        {
            "text": "For results on the relabeled test set, both precision, recall, and F1-score of the mask class increase compared to the original test set. Moreover, we also obtain a slightly higher precision for the no-mask class but lower recall than the original annotation. With the RetinaFace face detector, our proposed detection pipeline performs better on the relabeled than the original test set in terms of mAP, improving from 93.43 to 94.47 and increasing from 93.50 to 94.58 with the largest PyramidKey face detector. Table 4 compares the performance of the proposed pipeline with several state-of-the-art approaches, i.e., SSD, YOLOv3Tiny YOLOv3, F-RCNN proposed by Roy et al. (2020) Figure 3 shows sample images from the test set of the Moxa 3K dataset with predicted mask classes on the detected faces. This figure demonstrates our pipeline's ability on the wide range of images taken in public areas. Method mAP SSD (Roy et al., 2020) 46.52 YOLOv3Tiny (Roy et al., 2020) 56.27 YOLOv3 (Roy et al., 2020) 66.84 F-RCNN (Roy et al., 2020) 60.50 SL-FMDet (Fan et al., 2021) 67.04",
            "cite_spans": [
                {
                    "start": 661,
                    "end": 678,
                    "text": "Roy et al. (2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 914,
                    "end": 932,
                    "text": "(Roy et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 950,
                    "end": 968,
                    "text": "(Roy et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "text": "(Roy et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1014,
                    "end": 1032,
                    "text": "(Roy et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1048,
                    "end": 1066,
                    "text": "(Fan et al., 2021)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 512,
                    "end": 519,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 679,
                    "end": 687,
                    "text": "Figure 3",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Results on AIZOO dataset"
        },
        {
            "text": "Face detector Face mask classifier mAP ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results of our proposed pipeline"
        },
        {
            "text": "This paper proposes a two-step face mask detection pipeline consisting of face detection with alignment and face mask classification modules. The advantage of this pipeline is that it allows us to exploit the existing efficient face detectors such as RetinaFace (Deng et al., 2019) or PyramidKey (Earp et al., 2019) . To predict the presence of the mask on detected faces, we train the face mask classification model with MobileNetV1 1.0 as a backbone. Besides introducing the two-step face mask detection pipeline, we also provide a relabeled annotation of the test set of the AIZOO dataset consisting of manually corrected class labels. Moreover, we investigated different training techniques on the face mask classification model, i.e., label smoothing, aligning with some key points, and ignoring the upper half face. The result shows that the base training procedure leads to the best-performing model. We provide the evidence to show that our proposed pipeline is superior to the state-of-the-art approaches. Our pipeline with a smaller detection backbone outperforms both the baseline and RetinaFaceMask in terms of F1-score on the test set of the AIZOO dataset. On the relabeled test set of the AIZOO dataset, recall of mask class increases by a large margin compared to the original test set, and our pipeline achieves the highest mAP at 94.58. The evaluation result on the test set of the Moxa 3K dataset shows that our proposed pipeline also gains the highest mAP among other approaches at 69.18.",
            "cite_spans": [
                {
                    "start": 262,
                    "end": 281,
                    "text": "(Deng et al., 2019)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 296,
                    "end": 315,
                    "text": "(Earp et al., 2019)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "For the future, the dataset needs to be expanded to cover a wide variety of face masks; as in its current state, the only available varieties are medical, dust, and gas masks. As the face mask classification model trained on this dataset, the hypothesis is that the model may be vulnerable to more fashionable masks, e.g., colorful and screen masks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Aizootech/facemaskdetection: Detect faces and determine whether people are wearing mask",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Aizootech",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "L.-J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Fei-Fei",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "CVPR09",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Retinaface: Single-stage dense face localisation in the wild",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yuxiang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kotsia",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zafeiriou",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Face detection with feature pyramids and landmarks. arXiv e-prints, art",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "W F"
                    ],
                    "last": "Earp",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Noinongyao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Cairns",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ganguly",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1912.00596"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Photoepidemiology to estimate face covering use in select areas in asia versus the americas and africa during the covid-19 pandemic",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Elachola",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Gozzer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "M M"
                    ],
                    "last": "Rahman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ditekemena",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Pando-Robles",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Pa",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Ebrahim",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Journal of Travel Medicine",
            "volume": "27",
            "issn": "8",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1093/jtm/taaa121"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Everingham",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "K I"
                    ],
                    "last": "Williams",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Winn",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A deep learning based light-weight face mask detector with residual context attention and gaussian heatmap to fight against covid-19",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Access",
            "volume": "9",
            "issn": "",
            "pages": "96964--96974",
            "other_ids": {
                "DOI": [
                    "10.1109/access.2021.3095191"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Detecting masked faces in the wild with lle-cnns",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ge",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2682--2690",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kalenichenko",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Weyand",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Andreetto",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ioffe",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Transmission of covid-19 virus by droplets and aerosols: A critical review on the unresolved dichotomy",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jayaweera",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Perera",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Gunawardana",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Manatunge",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Environmental Research",
            "volume": "188",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1016/j.envres.2020.109819"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Retinamask: A face mask detector. CoRR, abs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Deep learning framework to detect face masks from video footage",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Joshi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Joshi",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Kanahasabai",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kapil",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "2020 12th International Conference on Computational Intelligence and Communication Networks (CICN)",
            "volume": "",
            "issn": "",
            "pages": "435--440",
            "other_ids": {
                "DOI": [
                    "10.1109/CICN49253.2020.9242625"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Imagenet classification with deep convolutional neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Communications of the ACM",
            "volume": "60",
            "issn": "6",
            "pages": "84--90",
            "other_ids": {
                "DOI": [
                    "10.1145/3065386"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Receptive field block net for accurate and fast object detection",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "SSD: single shot multibox detector",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Erhan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Reed",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Berg",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Aerodynamic analysis of sars-cov-2 in two wuhan hospitals",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ning",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "K"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Gali",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "E"
                    ],
                    "last": "Cai",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature",
            "volume": "582",
            "issn": "7813",
            "pages": "557--560",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Community use of face masks and covid-19: Evidence from a natural experiment of state mandates in the us",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Lyu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "L"
                    ],
                    "last": "Wehby",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Health Affairs",
            "volume": "39",
            "issn": "8",
            "pages": "1419--1425",
            "other_ids": {
                "DOI": [
                    "10.1377/hlthaff.2020.00818"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "SSH: single stage headless face detector",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Najibi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Samangouei",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chellappa",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "S"
                    ],
                    "last": "Davis",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Grid loss: Detecting occluded faces",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Opitz",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Waltner",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Poier",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Possegger",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Bischof",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "What the data say about wearing face masks",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Peeples",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature",
            "volume": "586",
            "issn": "",
            "pages": "186--189",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "maskedfacenet: A progressive semi-supervised masked face detector",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Prasad",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sheng",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "2021 IEEE Winter Conference on Applications of Computer Vision (WACV)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1109/wacv48630.2021.00343"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Yolov3: An incremental improvement",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "B"
                    ],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Moxa: A deep learning based unmanned approach for real-time monitoring of people wearing medical masks",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Nandy",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ghosh",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dutta",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Biswas",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Transactions of the Indian National Academy of Engineering",
            "volume": "5",
            "issn": "3",
            "pages": "509--518",
            "other_ids": {
                "DOI": [
                    "10.1007/s41403-020-00157-z"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Journal of Machine Learning Research",
            "volume": "15",
            "issn": "56",
            "pages": "1929--1958",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Rethinking the inception architecture for computer vision",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vanhoucke",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ioffe",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shlens",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wojna",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Effectiveness of public health measures in reducing the incidence of covid-19, sars-cov-2 transmission, and covid-19 mortality: systematic review and meta-analysis",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Talic",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wild",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gasevic",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Maharaj",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ademi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Mesa-Eguiagaray",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Rostron",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Theodoratou",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Motee",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liew",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ilic",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "BMJ: British Medical Journal",
            "volume": "375",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1136/bmj-2021-068302"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Trends in county-level covid-19 incidence in counties with and without a mask mandatekansas",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "E"
                    ],
                    "last": "Van Dyke",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "M"
                    ],
                    "last": "Rogers",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pevzner",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Satterwhite",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "B"
                    ],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "J"
                    ],
                    "last": "Beckman",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "C"
                    ],
                    "last": "Hunt",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Rule",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "CBAM: convolutional block attention module",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Woo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "S"
                    ],
                    "last": "Kweon",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Robust facial landmark detection under significant head poses and occlusion",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Wider face: A face detection benchmark",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Loy",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Qiao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Signal Processing Letters",
            "volume": "23",
            "issn": "10",
            "pages": "1499--1503",
            "other_ids": {
                "DOI": [
                    "10.1109/LSP.2016.2603342"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF1": {
            "text": "The overview of two-step face mask detection pipeline.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Sample images of incorrect mask class where the green bounding box means face wearing a mask and the red bounding box means face without a mask on a test set of AIZOO dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "the same time, PyramidKey considers only the first three losses and excludes dense regression loss. BothDeng et al. (2019) andEarp et al. (2019) trained their respective approaches on the WIDER Face dataset(Yang et al., 2016).This paper experiments with several existing Pyramid-Key face detectors fromEarp et al. (2019), which proposed using MobileNetV2 0.25 , MobileNetV2 1.0 , and ResNet152 as visual backbones. The model with MobileNetV2 0.25 backbone uses only the top three layers of pyramid features while the rest used the top six pyramid features layers. Moreover, we experiment with RetinaFace face detector(Deng et al., 2019) with a ResNet50 backbone. Additionally, we use a set of five facial landmarks, i.e., eye centers, nose, and mouth corners, obtained from the face detector, to crop and align the detected faces as a pre-processing 5 https://bit.ly/3E5X1NJ",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "and SL-FMDet fromFan et al. (2021) on the Moxa 3K dataset. The table shows that our face mask detection pipeline with the PyramidKey face detectors with MobileNet backbones-MobileNetV2 0.25 and MobileNetV2 1.0 -outperform SSD and YOLOv3Tiny. The face mask detection pipeline with the smallest de-Sample images of the test set of the Moxa 3K dataset with predicted mask labels from the proposed pipeline where the green bounding box means face wearing a mask and the red bounding box means face without a mask.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Dataset comparison.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "The comparative result with the state-of-the-arts on the original and relabeled test set of the AIZOO dataset. Precision Recall F1-score Precision Recall F1-scoreResults of our proposed pipeline on the original test setResults of our proposed pipeline on the relabeled test set tector, MobileNetV2 0.25 , yields a higher mAP than SSD and YOLOv3Tiny by approximately 13.45 and 3.70 points, respectively, while inference with MobileNetV2 1.0 boosts the performance to mAP of 65.61, which is higher than MobileNetV2 0.25 by about 5.64 points. Our pipelines with the ResNet50 and ResNet152 as face detectors perform better than SL-FMDet by a large margin of about 1.75 and 2.14 points, respectively.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "The comparative result with the state-of-the-arts on the test set of the Moxa 3K dataset.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "We express our gratitude to Aubin Samacoits and Christina Kim for their input and feedback during the writing of this paper. The funding for the development of the pipeline and the use of the Sertis' face mask detection pipeline was supported by the Health Systems Research Institute (HSRI).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        }
    ]
}
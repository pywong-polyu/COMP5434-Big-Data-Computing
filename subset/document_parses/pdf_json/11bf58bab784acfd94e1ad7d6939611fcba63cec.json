{
    "paper_id": "11bf58bab784acfd94e1ad7d6939611fcba63cec",
    "metadata": {
        "title": "3D Recognition Based on Sensor Modalities for Robotic Systems: A Survey",
        "authors": [
            {
                "first": "Sumaira",
                "middle": [],
                "last": "Manzoor",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sungkyunkwan University",
                    "location": {
                        "postCode": "16419",
                        "settlement": "Suwon",
                        "country": "Korea"
                    }
                },
                "email": ""
            },
            {
                "first": "Sung-Hyeon",
                "middle": [],
                "last": "Joo",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sungkyunkwan University",
                    "location": {
                        "postCode": "16419",
                        "settlement": "Suwon",
                        "country": "Korea"
                    }
                },
                "email": ""
            },
            {
                "first": "Eun-Jin",
                "middle": [],
                "last": "Kim",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sungkyunkwan University",
                    "location": {
                        "postCode": "16419",
                        "settlement": "Suwon",
                        "country": "Korea"
                    }
                },
                "email": ""
            },
            {
                "first": "Sang-Hyeon",
                "middle": [],
                "last": "Bae",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sungkyunkwan University",
                    "location": {
                        "postCode": "16419",
                        "settlement": "Suwon",
                        "country": "Korea"
                    }
                },
                "email": ""
            },
            {
                "first": "Gun-Gyo",
                "middle": [],
                "last": "In",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sungkyunkwan University",
                    "location": {
                        "postCode": "16419",
                        "settlement": "Suwon",
                        "country": "Korea"
                    }
                },
                "email": ""
            },
            {
                "first": "Jeong-Won",
                "middle": [],
                "last": "Pyo",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sungkyunkwan University",
                    "location": {
                        "postCode": "16419",
                        "settlement": "Suwon",
                        "country": "Korea"
                    }
                },
                "email": ""
            },
            {
                "first": "Tae-Yong",
                "middle": [],
                "last": "Kuc",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sungkyunkwan University",
                    "location": {
                        "postCode": "16419",
                        "settlement": "Suwon",
                        "country": "Korea"
                    }
                },
                "email": ""
            },
            {
                "first": "E.-J",
                "middle": [
                    ";"
                ],
                "last": "Kim",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sungkyunkwan University",
                    "location": {
                        "postCode": "16419",
                        "settlement": "Suwon",
                        "country": "Korea"
                    }
                },
                "email": ""
            },
            {
                "first": "",
                "middle": [],
                "last": "Bae",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sungkyunkwan University",
                    "location": {
                        "postCode": "16419",
                        "settlement": "Suwon",
                        "country": "Korea"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [],
    "body_text": [
        {
            "text": "Today, robotic systems with social characteristics are considered an important keystone in household chores, healthcare services, and modern industrial production [1] . 3D visual recognition is the fundamental component of these social robots. Social robots [2] are autonomous robots that are currently being developed on a large scale for safe and secure robot interactions in the human-centric environment [3] . The appearance and applications of these robotic systems vary; however, recognition in the context of object and place plays a central and vital role in these systems for semantic understanding of the environment. This article starts with the impact of social robots and lists the key features of some recently developed social robots that are tailored in public, domestic, hospital, and industrial use.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 166,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 258,
                    "end": 261,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 408,
                    "end": 411,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "These robots are designed to interact and exhibit social behaviors with broad humanlike capabilities, which integrate visual recognition, knowledge representation, task planning, localization, and navigation. Among all these, we focus on a systematic review of the approaches that address the most essential robotic capability, known as visual recognition. In this direction, we present data representation methods based on sensor modalities for 3D recognition using deep learning (DL) and examine the approaches for both 3D object recognition (3DOR) and 3D place recognition (3DPR). Previous reviews, shown in Table 1 , concentrated only on 3D object recognition and did not address the 3D place recognition methods. In contrast to the previous studies, this article reviews and analyzes sensor-based data representation methods for both 3D object and place recognition (3DOPR) using state-of-the-art DL-based approaches. Moreover, we also discuss recently developed social robots.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 611,
                    "end": 618,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "This review is concentrated on 3D visual recognition approaches that have their applications in the domain of robotics, while approaches in the domain of smart environments are beyond the scope of the current survey. We aim at facilitating novice researchers and experts to overcome the challenging task of determining and utilizing the most suitable visual recognition approach for their intended robotic system, as one can quickly explore the recent research progress through this review. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Guo et al. [43] Sing et al. [44] This Survey",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 15,
                    "text": "[43]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 28,
                    "end": 32,
                    "text": "[44]",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [],
            "section": "Covered Topics"
        },
        {
            "text": "Yes [45] [46] [47] [48] [49] [50] [51] [52] [53] [54] 3D Object Recognition (3DOR) Yes Yes Yes 3D Place Recognition (3DPR) No No Yes [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] Compared to the existing survey papers, shown in Table 1 , the present review is different in the following terms, to the best of our knowledge: \u2022 We discuss the latest representative social robots that have been developed recently (Section 2). \u2022",
            "cite_spans": [
                {
                    "start": 4,
                    "end": 8,
                    "text": "[45]",
                    "ref_id": null
                },
                {
                    "start": 9,
                    "end": 13,
                    "text": "[46]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 14,
                    "end": 18,
                    "text": "[47]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 19,
                    "end": 23,
                    "text": "[48]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 24,
                    "end": 28,
                    "text": "[49]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 29,
                    "end": 33,
                    "text": "[50]",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 34,
                    "end": 38,
                    "text": "[51]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 39,
                    "end": 43,
                    "text": "[52]",
                    "ref_id": null
                },
                {
                    "start": 44,
                    "end": 48,
                    "text": "[53]",
                    "ref_id": null
                },
                {
                    "start": 49,
                    "end": 53,
                    "text": "[54]",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 133,
                    "end": 137,
                    "text": "[78]",
                    "ref_id": "BIBREF78"
                },
                {
                    "start": 138,
                    "end": 142,
                    "text": "[79]",
                    "ref_id": "BIBREF79"
                },
                {
                    "start": 143,
                    "end": 147,
                    "text": "[80]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 148,
                    "end": 152,
                    "text": "[81]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 153,
                    "end": 157,
                    "text": "[82]",
                    "ref_id": "BIBREF82"
                },
                {
                    "start": 158,
                    "end": 162,
                    "text": "[83]",
                    "ref_id": "BIBREF83"
                },
                {
                    "start": 163,
                    "end": 167,
                    "text": "[84]",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 168,
                    "end": 172,
                    "text": "[85]",
                    "ref_id": "BIBREF85"
                },
                {
                    "start": 173,
                    "end": 177,
                    "text": "[86]",
                    "ref_id": "BIBREF86"
                },
                {
                    "start": 178,
                    "end": 182,
                    "text": "[87]",
                    "ref_id": "BIBREF87"
                },
                {
                    "start": 183,
                    "end": 187,
                    "text": "[88]",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 188,
                    "end": 192,
                    "text": "[89]",
                    "ref_id": "BIBREF89"
                },
                {
                    "start": 338,
                    "end": 339,
                    "text": "\u2022",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 242,
                    "end": 249,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Representative Social Robotic Systems No No"
        },
        {
            "text": "The present study is the first article that comes up with a combined review of two robotic capabilities: 3D object recognition and 3D place recognition in a comprehensive assessment. It provides data representation modalities based on camera and LiDAR for both 3D recognition tasks using DL-based approaches (Section 3). \u2022 It reviews 14 3D object detection datasets. \u2022",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Representative Social Robotic Systems No No"
        },
        {
            "text": "The current survey presents a comparison of existing results to evaluate the performance on datasets. \u2022 It yields an analysis of selected approaches from the domain of robotics, delineates the advantages, summarizes the current main research trends, discusses the limitations, and outlines the possible future directions. \u2022 Compared to the earlier surveys, this study is more concerned with the most recent work. Therefore, it provides the reader an important opportunity to advance their understanding of state-of-the-art robotic 3D recognition methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Representative Social Robotic Systems No No"
        },
        {
            "text": "The survey has been organized in a top-down manner. The overall structure of the survey with corresponding topics and subsections is diagrammatically illustrated in Figure 1 . In Section 2, the aim is to provide fresh insight to the readers into recently developed social robots with their impact on society, use cases, sensors, tasks (i.e., recognition), and semantic functions (i.e., assisting) in public places (Section 2.1), domestic (Section 2.2), hospitals (Section 2.3), and industrial environments (Section 2.4).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 165,
                    "end": 173,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Survey Structure"
        },
        {
            "text": "In Section 3, inspired by the recognition capabilities of social robots, as described in Section 2, the article examines the sensor (camera and LiDAR) based data representation approaches used for the 3D object (Section 3.1) and place (Section 3.2) recognition applying DL-based models. In addition, it gives a brief overview of datasets (Section 4) that have been used for the evaluation of 3D recognition methods. Consequently, in Section 6, the article discusses current research challenges and future research directions, and finally we conclude the survey with a summary in Section 7. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Survey Structure"
        },
        {
            "text": "The inclusion and exclusion criteria are mainly focused on Section 3 for 3DOR and 3DPR methods. Section 2 does not involve comparison (instead it highlights the importance of visual recognition capability by giving the examples of recently developed robots from different sectors); therefore, it is not restricted to follow the same time span as Section 3. However, Section 3 performs the literature analysis for 3DOR and 3DPR methods; therefore, all studies in Section 3 are restricted to follow a specific time span based on inclusion and exclusion criteria. For 3DOR (Section 3.1) and 3DPR (Section 3.2), the inclusion criteria are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inclusion and Exclusion Criteria"
        },
        {
            "text": "\u2022",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inclusion and Exclusion Criteria"
        },
        {
            "text": "The research publications must be from 2014 to 2021. \u2022 Their domain must be a robotic system. \u2022 They must be either journal or conference publications. \u2022 They must address 3DOR or 3DPR methods using deep-learning approaches based on Camera and LiDAR sensor modalities. Table 2 represents both inclusion and exclusion criteria that were applied to perform the paper selection, and the results of the systematic approach for paper filtering process are described below.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 269,
                    "end": 276,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Inclusion and Exclusion Criteria"
        },
        {
            "text": "We conducted a systematic literature review for Section 3 to determine which DLbased models are being used for 3D object and place recognition based on sensor modalities. We used four search strings (\"Camera\" AND \"3D\" AND \"Object Recognition\", \"LiDAR\" AND \"3D\" AND \"Object Recognition\", \"Camera\" AND \"3D\" AND \"Place Recognition\", and \"LiDAR\" AND \"3D\" AND \"Place Recognition\") to extract the research articles from two key digital databases of academic journal articles that were IEEE Explorer and the ACM Digital Library. The paper selection process of this article consists of four steps as shown in Figures 2 and 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 601,
                    "end": 616,
                    "text": "Figures 2 and 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Results of the Paper Selection Process"
        },
        {
            "text": "First, the relevant articles for the survey from digital libraries using search strings were collected that correspond to the type of sensor (camera and LiDAR) and category of 3D recognition (object and place). In the second step, 329 articles in IEEE explores library and 593 articles in ACM digital library were extracted by applying the time period filter. The third step refined the 93 articles from IEEE Explorer and 144 articles from ACM Digital Library that belonged to the robotics category. We used MS Access database management software to find duplicates among these articles. For this, we ran SQL query on the database table and found that 35 articles in ACM and 21 articles In IEEE Explorer were duplicates.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results of the Paper Selection Process"
        },
        {
            "text": "After removing the duplicate articles, the fourth step involved splitting the articles that used deep-learning-based approaches and resulted in 23 articles from IEEE explorer and 51 articles from the ACM Digital Library that met the inclusion and exclusion criteria. Lastly, the selected articles based on their sensor data representation methods were arranged into 3DOR and 3DPR categories in which 17 articles from IEEE Explorer and 44 articles from ACM Digital library are related to the 3DOR task and five articles from IEEE Explorer and seven articles from ACM Digital library are related to the 3DPR task. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results of the Paper Selection Process"
        },
        {
            "text": "This section presents recently developed social robotic systems that demonstrate recognition tasks and semantic understanding to perform a function in public (Section 2.1), domestic (Section 2.2), medical (Section 2.3), and industrial (Section 2.4) environments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Representative Social Robotic Systems"
        },
        {
            "text": "Robots in public spaces indicates social robots used in places that are generally accessible for everyone, such as airports, supermarkets, libraries, and museums. Amazon launched a six-wheeled autonomous Scout delivery robot [45] in its Seattle-based research and development lab. It is commercially available in a few places in the USA, which are Atlanta, Georgia and Franklin, Tennessee after a long test run [90] . It uses an array of cameras and ultrasonic sensors for route planning and navigation on sidewalks at a walking pace and climbing up the front porch for package delivery. It has the ability of semantic task understanding, such as recognizing people and pets, detecting, and avoiding obstacles using machine learning algorithms.",
            "cite_spans": [
                {
                    "start": 225,
                    "end": 229,
                    "text": "[45]",
                    "ref_id": null
                },
                {
                    "start": 411,
                    "end": 415,
                    "text": "[90]",
                    "ref_id": "BIBREF90"
                }
            ],
            "ref_spans": [],
            "section": "Robots in Public Spaces"
        },
        {
            "text": "AIMBOT [46] is an anti-epidemic autonomous driving robot that is designed for indoor crowded public environments, including schools, hospitals, and office buildings to provide safe and efficient Covid-19 protection. It is available for commercial use. It recognizes 200 people per minute, uses infrared thermal imaging camera to measures their body temperature, detects whether individuals are wearing masks, and sends a voice reminder to the people without a mask. Table 3 lists the sensors, purpose, and tasks as well as their algorithm, appearances, semantic functions, and development status. ",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 11,
                    "text": "[46]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [
                {
                    "start": 466,
                    "end": 473,
                    "text": "Table 3",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Robots in Public Spaces"
        },
        {
            "text": "To navigate in sidewalks and climb up front porch for parcel delivery",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Semantic Functions"
        },
        {
            "text": "To provide contact-less long-distance human body temperature measurement and screening",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Semantic Functions"
        },
        {
            "text": "Yes Yes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Commercially Available"
        },
        {
            "text": "Robots in the domestic environment refer to the robots that are used at homes for household chores, entertainment, or personal assistance. At the consumer electronics show 2020, Samsung showcased a robotic chef's assistant [47] , which consists of a pair of arms that mimic human gestures to cook the meal and performs the task on voice commands. It downloads the appropriate skills and performs the tasks, such as slicing by picking up the knife, pouring the ingredients, and mixing them. It is equipped with sensors and cameras and relies on AI and computer vision algorithms for the recognition task. The prototype of the Samsung chef robot was first unveiled at KBIS 2019 [91] . It is not available commercially.",
            "cite_spans": [
                {
                    "start": 223,
                    "end": 227,
                    "text": "[47]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 676,
                    "end": 680,
                    "text": "[91]",
                    "ref_id": "BIBREF91"
                }
            ],
            "ref_spans": [],
            "section": "Robots in Domestic Environment"
        },
        {
            "text": "Amazon's Astro [48] is an Alexa-based home assistant robot that combines Alexa, computer vision, and AI software. It is a commercially available robot for home security, including a six-month free trial of Ring Protect Pro that allows saving videos in Ring's cloud storage [92] . It obeys voice commands, such as follow me or go to a specific room. It performs face recognition to deliver items to a specific person. It acts as a family companion and entertains children by playing music. It cares for elderly people by reminding them to take medicine and record their blood pressure. It also assists to take voice or video calls. It uses SLAM for mapping the environment and roaming around the house. It automatically attaches itself to the charging dock. House members can use its mobile application for remote monitoring if they are outside. Table 4 presents the sensors, usability, and tasks of domestic robots along with their algorithm, appearances, semantic functions, and development status. ",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 19,
                    "text": "[48]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 273,
                    "end": 277,
                    "text": "[92]",
                    "ref_id": "BIBREF92"
                }
            ],
            "ref_spans": [
                {
                    "start": 845,
                    "end": 852,
                    "text": "Table 4",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Robots in Domestic Environment"
        },
        {
            "text": "Robots in hospitals are used in healthcare and treatment centers for relieving medical personnel either by aiding in surgery or caring for the patients. Moxi is a robotic assistant [49] in semi-structured hospital environments that is commercially available. The Medical City Dallas Heart and Spine Hospital is the first North Texas health care provider using the Moxi robot to combat a lack of nursing personnel in hospital systems [93] . It uses AI and machine learning algorithms to reduce the cognitive workload of nurses by performing tasks that do not require interaction with patients, such as delivering supplies to patient rooms, fetching items, and removing linen bags. Table 5 illustrates its characteristics, which include the robot's sensors, purpose, and tasks, algorithm, appearances, semantic functions, and development status.",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 185,
                    "text": "[49]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 433,
                    "end": 437,
                    "text": "[93]",
                    "ref_id": "BIBREF93"
                }
            ],
            "ref_spans": [
                {
                    "start": 680,
                    "end": 687,
                    "text": "Table 5",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Robots in Hospitals"
        },
        {
            "text": "Ahn et al. [50] , developed a multi-robot system consisting of ReceptionistBot and CareBot for the hospital environment that performs the tasks of receptionist, nurse assistant, and medical server. Both ReceptionistBot and CareBot are in the prototype stage and are not available commercially. ReceptionistBot communicate with patients and obtains their personal information. If visitors want to meet the medical staff, it guides them to meet CareBot for treatment. Carebot collects data about the patient's health condition by asking questions. It assists the nurse using different healthcare devices to measure the blood pressure, pulse rate, and oxygen level of the patients. It also communicates with RoboGen, which is a secure server for managing patient information. MAiRA [51] is a multi-sensing intelligent robot that assists in complex medical procedures. This intelligent assistant is a commercially available cognitive robot [94] . It has voice recognition capability. It performs human-robot interaction in a collaborative industrial environment. It can learn from instructions given through voice commands or gestures. It can perform object detection, pose estimation, and object grasping tasks either with professionals or wholly autonomously.",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 15,
                    "text": "[50]",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 779,
                    "end": 783,
                    "text": "[51]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 936,
                    "end": 940,
                    "text": "[94]",
                    "ref_id": "BIBREF94"
                }
            ],
            "ref_spans": [],
            "section": "Robots in Hospitals"
        },
        {
            "text": "Robots in industry are used to assist in manufacturing by automating repetitive tasks, such as welding, assembly, and shipping. Handle is an autonomous mobile manipulation robot [52] developed by Boston Dynamics for moving boxes in a warehouse and unloading them from shipping containers. It relies on a 2D and 3D perception learning-based vision system to detect boxes. Table 6 enumerates the sensors, usability, tasks, algorithm, appearances, semantic functions, and development status. Handle will be available for sale in two years according to Playter the Chief Executive Officer at Boston Dynamics [96] . LARA [53] is a collaborative industrial robotic arm, developed recently. Its prototype is complete. However, it is expected to be realized soon for commercial use [97] . It is available in two sizes with 5 and 10 kg payload capacities. Its 3D vision allows detection and recognition of an object for a manipulation task.",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 182,
                    "text": "[52]",
                    "ref_id": null
                },
                {
                    "start": 604,
                    "end": 608,
                    "text": "[96]",
                    "ref_id": "BIBREF96"
                },
                {
                    "start": 616,
                    "end": 620,
                    "text": "[53]",
                    "ref_id": null
                },
                {
                    "start": 774,
                    "end": 778,
                    "text": "[97]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 371,
                    "end": 378,
                    "text": "Table 6",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Robots in Industrial Environment"
        },
        {
            "text": "Stretch [54] is a recently designed robot for autonomously moving boxes around the warehouses. Boston Dynamics expects that the robot will be commercially available from 2022 [98] . The strength of its arm makes it unique for potential entry into robotic warehouses. It is flexible and can do different tasks, such as loading, unloading boxes, and building up pallets. ",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[54]",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 175,
                    "end": 179,
                    "text": "[98]",
                    "ref_id": "BIBREF98"
                }
            ],
            "ref_spans": [],
            "section": "Robots in Industrial Environment"
        },
        {
            "text": "With the recent breakthroughs in deep learning (DL) and significant improvements in sensor technologies, 3D recognition has made great progress, which leads toward rapid development in autonomous robotic systems, including autonomous driving. In this section, we concentrate on camera and LiDAR-based data representation methods employed for both 3D object recognition (3DOR) (Section 3.1) and 3D place recognition (3DPR) (Section 3.2) using DL models. Recently developed autonomous robotic systems (as described in Section 2) are mostly equipped with both cameras and LiDAR for visual perception tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3D Recognition"
        },
        {
            "text": "LiDAR is suitable to work with real-time autonomous systems in both indoor and outdoor environments, although most of the perception approaches focus on the use of LiDAR in autonomous vehicles. However, recent trends in deep-learning-based end-toend approaches have also led researchers' interest in the innovative use of LiDAR in autonomous robots for recognition tasks that benefit from the detailed 3D PC data to detect objects accurately. The PC data provided by the LiDAR sensor retains information related to the object's position and reflection intensity as well as shape representation of different objects in complex scenes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3D Recognition"
        },
        {
            "text": "Hence, integrating this 3D PC information with DL-based recognition models is indispensable to perform precise 3D recognition. On the other hand, monocular and stereo cameras are less expensive sensors than LiDAR for 3D object detection but require postprocessing techniques to determine the size and relative distance. The detection capability and reliability of the camera and LiDAR are limited in different environments. Table 7 summarizes the advantages and limitations of both sensors. Camera-LiDAR fusion is used to overcome these issues. This section categorizes data representation methods based on sensors' modalities for 3D object recognition using deep learning in autonomous robotic systems. Compared with traditional recognition methods, the success of DL in the past ten years for robust and accurate object detection has made deep CNN the most promising method to perform 3D vision recognition tasks for robotic systems. The overall taxonomy is shown in Figure 4 , which illustrates data representation in visual sensors that include a camera (Section 3.1.1), LiDAR (Section 3.1.2), and camera-LiDAR fusion (Section 3.1.3). ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 424,
                    "end": 431,
                    "text": "Table 7",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 969,
                    "end": 977,
                    "text": "Figure 4",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "3D Recognition"
        },
        {
            "text": "This section explores the methods that perform 3DOR by estimating 3D bounding boxes (BBoxes) based on either monocular or stereo camera images as discussed in Table 8 with limitations and research gap. We first give an overview of camera-based methods and then describe their advantages and limitations in Table 9 . ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 159,
                    "end": 166,
                    "text": "Table 8",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 306,
                    "end": 313,
                    "text": "Table 9",
                    "ref_id": "TABREF7"
                }
            ],
            "section": "Camera-Based 3DOR"
        },
        {
            "text": "Uses RGB image for object detection and predicts 2D BBoxes, which are inferred to generate 3D BBoxes by re-projection or BBox regression, computationally less expensive compared to other methods",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Input image does not have depth information, which causes low localization performance and inaccurate object size estimation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Limitation(s)"
        },
        {
            "text": "CNN architectures for estimating the depth information need to be investigated to improve the detection results",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "A monocular camera is essential for the deployment of low power and low-cost systems in the real-world application of robotics or autonomous driving [99] . Therefore, researchers have shown increasing interest in monocular 3D object detection in recent years [34, [100] [101] [102] [103] [104] . Even though existing 3D detectors have achieved good accuracy, most of them do not consider the information related to occluded objects, which are partially visible. To this end, Chen et al. [55] improved 3D object detection by establishing a relationship of paired samples, which allows modeling spatial constraints for occluded objects. Its 3D detector introduced an uncertainty-aware prediction module for computing object location and object-to-object distances.",
            "cite_spans": [
                {
                    "start": 149,
                    "end": 153,
                    "text": "[99]",
                    "ref_id": "BIBREF99"
                },
                {
                    "start": 259,
                    "end": 263,
                    "text": "[34,",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 264,
                    "end": 269,
                    "text": "[100]",
                    "ref_id": "BIBREF100"
                },
                {
                    "start": 270,
                    "end": 275,
                    "text": "[101]",
                    "ref_id": "BIBREF101"
                },
                {
                    "start": 276,
                    "end": 281,
                    "text": "[102]",
                    "ref_id": "BIBREF102"
                },
                {
                    "start": 282,
                    "end": 287,
                    "text": "[103]",
                    "ref_id": "BIBREF103"
                },
                {
                    "start": 288,
                    "end": 293,
                    "text": "[104]",
                    "ref_id": "BIBREF104"
                },
                {
                    "start": 487,
                    "end": 491,
                    "text": "[55]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "(i) Monocular-based 3DOR"
        },
        {
            "text": "This method adopted a one-stage architecture by sharing the anchor-free 2D object detection approaches, consisting of one backbone and several task specific dense prediction network branches. The backbone accepted one monocular image as input while (WxHx64) size as output feature map. It had eleven output branches as shown in Figure 5 , which were divided into three parts: three for 2DOR, six for 3DOR, and two for the prediction of pairwise geometric constraints, which were estimated among adjacent objects using key points on the feature map. Li et al. [56] presented a 3D object detection method by extracting 3D information from a 2D image and generated accurate 3D BBoxes by obtaining coarse cuboids of predicted 2D boxes. In contrast to typical methods that rely on feature extraction from 2D BBoxes, it exploited 3D structural information by employing visual features and used the extracted features from surfaces to eliminate the feature ambiguity problem of 2D bounding boxes. It modified faster R-CNN for orientation prediction by including a new branch. Figure 6 shows an overview of its proposed framework in which single RGB image was passed as input, and it was processed in four steps. First, a CNN-based detector, known as 2D+O subnet, was used for extracting 2D BBoxes and orientations of the objects. Figure 6 . The proposed 3D object detection paradigm [56] consisting of a CNN based model (2D+O subnet), 3D guidance generated using the obtained output of 2D+O subnet, and extracted features utilized by the refinement model (3D subnet).",
            "cite_spans": [
                {
                    "start": 559,
                    "end": 563,
                    "text": "[56]",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 1376,
                    "end": 1380,
                    "text": "[56]",
                    "ref_id": "BIBREF56"
                }
            ],
            "ref_spans": [
                {
                    "start": 328,
                    "end": 336,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1069,
                    "end": 1077,
                    "text": "Figure 6",
                    "ref_id": null
                },
                {
                    "start": 1323,
                    "end": 1331,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "(i) Monocular-based 3DOR"
        },
        {
            "text": "In the second step, these were utilized with the prior knowledge for driving scenario and basic cuboid were generated, which were called guidance. In the third step, this guidance was projected on the image plane and features were fused as distinguishable structural information to eliminate the ambiguity. In the fourth step, another CNN called 3D subnet was used fused features as the network input to improve the guidance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(i) Monocular-based 3DOR"
        },
        {
            "text": "J\u00f6rgensen et al. [57] proposed single-stage monocular 3D (SS3D) architecture. It contained two main parts: a CNN that was used for detecting the objects by regressing a surrogate 3D representation and a 3D BBox optimizer for fitting respective 3D BBoxes. SS3D regressed 2D and 3D BBoxes simultaneously after specifying the object's center and its 2D and 3D BBox tuple contained 26 surrogate elements. Its proposed pipeline is illustrated in Figure 7 and consists of three steps. The first step is object detection with class scores and regression for 3D BBoxes' fitting, while the second step involves nonmaximum suppression for the elimination of redundant detections. Finally, 3D BBoxes were yielded through an optimizer using learning weights, and these 3D BBoxes were fitted independently and in parallel using the non-linear least squares method. Luo et al. [58] introduced a monocular 3D single stage object detector (M3DSSD) to overcome the feature mismatching issue of anchor-based monocular 3DOR methods by proposing a two-step feature alignment approach. The major components of its architecture shown in Figure 8 are a backbone network that is modified version of [105] , feature alignment, attention block, and prediction head. Its asymmetric non-local attention block (ANAB) extracts depth-wise features for representing the global information. Its feature alignment consisted of two steps to handle the misalignment of 2D and 3D BBoxes. The first step obtained the target region based on the classification confidence and allowed the respective filed of the feature map to concentrate on the anchor regions. The second step used the 2D/3D center prediction for feature offset estimation to reduce the gap between predictions and feature maps. ",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 21,
                    "text": "[57]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 863,
                    "end": 867,
                    "text": "[58]",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 1175,
                    "end": 1180,
                    "text": "[105]",
                    "ref_id": "BIBREF105"
                }
            ],
            "ref_spans": [
                {
                    "start": 441,
                    "end": 449,
                    "text": "Figure 7",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1115,
                    "end": 1123,
                    "text": "Figure 8",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "(i) Monocular-based 3DOR"
        },
        {
            "text": "Compared to the monocular camera, there are relatively fewer studies that utilize stereo vision for 3D object detection. Li et al. [59] exploited semantic and geometric information in the stereo image by proposing a stereo R-CNN based 3D object detector, which was an extension of Faster R-CNN. The stereo region proposal network, stereo R-CNN, and key points branch were three major components of its architecture as shown in Figure 9 .",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 135,
                    "text": "[59]",
                    "ref_id": "BIBREF59"
                }
            ],
            "ref_spans": [
                {
                    "start": 427,
                    "end": 435,
                    "text": "Figure 9",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "(ii) Stereo-based 3DOR"
        },
        {
            "text": "The stereo region proposal network module generated right and left RoI proposals. The stereo R-CNN module applied RoI-Align [10] on feature maps and concatenated them for object classification. It adds a stereo regression branch for accurate regression of 2D stereo boxes. The key point branch took left RoI features for detecting object key points. It performed 3D box estimation by projecting the relations between 2D right-left boxes with 3D box corners and key points. It specified accurate 3D bounding boxes and object localization by employing a dense region-based photometric alignment method. [59] , which outputs key points, stereo boxes, along with the viewpoint angle and dimensions, followed by 3D BBox estimation. Inspired by CenterNet [106] and Stereo R-CNN [59] , Shi et al. [60] proposed a 3D object detection method to recognize the target by extracting semantic and geometric features in stereo RGB images without relying on depth information. It used 2D left-right boxes and predicted four semantic key points of the object's 3D BBoxes while optimizing the position of 3D BBoxes using a photometric alignment module. Its network was built on CenterNet, which extracted the features from left and right image architecture as shown in Figure 10 using a weight-share backbone, which outputs 10 sub-branches. It performed two tasks. The first task is related to stereo 2D detection in which five sub-branches estimate the center, offset, and BBox of the left object. The second task is the stereo 3D component in which five sub-branches were used to estimate the dimension, orientation, vertices, and center distance of 3D BBoxes for left objects. ",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 128,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 601,
                    "end": 605,
                    "text": "[59]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 749,
                    "end": 754,
                    "text": "[106]",
                    "ref_id": "BIBREF106"
                },
                {
                    "start": 772,
                    "end": 776,
                    "text": "[59]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 790,
                    "end": 794,
                    "text": "[60]",
                    "ref_id": "BIBREF60"
                }
            ],
            "ref_spans": [
                {
                    "start": 1252,
                    "end": 1261,
                    "text": "Figure 10",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "(ii) Stereo-based 3DOR"
        },
        {
            "text": "LiDAR gives accurate depth information of the environment for 3DOR by discretizing the whole 3D space [107] . The major challenges toward applying DL-based approaches for LiDAR-based 3D object recognition research are the unordered, irregular, discrete, and sparse data representation of PCs, which makes it difficult to process point clouds data directly with CNN-based models. This is due to CNN models rely on convolution operation, which takes ordered, regular, and structured data. More recently, literature has emerged with different methods to address PCs data processing challenges using CNN for 3D recognition. This section divides DL-based 3D recognition methods for LiDAR point clouds into three categories: structured (ordered), unstructured (un-ordered), and graph-based representation.",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 107,
                    "text": "[107]",
                    "ref_id": "BIBREF107"
                }
            ],
            "ref_spans": [],
            "section": "LiDAR-Based 3DOR"
        },
        {
            "text": "This section discusses 2D image grid and 3D voxel grid-based representation for LiDAR-based 3DOD via deep-learning approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(i) Structured Representation for 3DOR"
        },
        {
            "text": "Much of the current literature on 3DOR pays particular attention to project discrete 3D PC data into a 2D grid representation using DL-based models. Table 10 gives a brief overview of the 2D image grid-based 3DOR method with current restrictions and research gaps. Studies along with their advantages and limitations are discussed in Table 11 . Table 10 . Methodology and Limitation(s): 2D Image Grid-based 3DOR Methods.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 149,
                    "end": 157,
                    "text": "Table 10",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 334,
                    "end": 342,
                    "text": "Table 11",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 345,
                    "end": 353,
                    "text": "Table 10",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "(a) 2D Image Grid-Based 3DOR"
        },
        {
            "text": "Projects 3D point clouds into a 2D image grid, which is passed to CNN for object detection with 2D BBoxes The 3D BBoxes are inferred from 2D BBoxes by performing position and size regression",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Projection of 3D point clouds onto a 2D image grid causes information loss, which leads to inaccurate spatial information compared to raw PC data",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Limitation(s)"
        },
        {
            "text": "Encoding of the input image by hand-engineered features could be replaced with learned representations to improve detection results Zeng et al. [61] utilized pure LiDAR PC on a 2D grid and introduced a real-time 3D detection method RT3D illustrated in Figure 11 using two sub-networks: region proposal network and classification sub-network. Its pipeline contained three major steps. First, sparse 3D point clouds were projected on a 2D grid representation for converting them into the input format of CNN. After that, height information from point data was embedded in the 2D grid for 3D object detection. Thirdly, the 2D grid information was passed to a two-stage CNN detector, which generated region proposals.",
            "cite_spans": [
                {
                    "start": 144,
                    "end": 148,
                    "text": "[61]",
                    "ref_id": "BIBREF61"
                }
            ],
            "ref_spans": [
                {
                    "start": 252,
                    "end": 261,
                    "text": "Figure 11",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "This was initialized with pre-trained ResNet-50 model [108] , while it adopted Faster-RCNN [109] techniques for the generation of region proposals on feature map and introduced pre-RoI pooling convolution techniques before RoI operations to improve the computation efficiency. Subsequently, classification and location regression for each RoI was performed to define the location, orientation, and size estimation with a pose-sensitive feature map. This addressed two problems related to the sparsity of PC: First, deleting empty anchors that contained no data on feature maps; Second: adopting automatic selection of hard examples using online hard example mining [110] to provide end-to-end efficient and effective network training.",
            "cite_spans": [
                {
                    "start": 54,
                    "end": 59,
                    "text": "[108]",
                    "ref_id": "BIBREF108"
                },
                {
                    "start": 91,
                    "end": 96,
                    "text": "[109]",
                    "ref_id": "BIBREF109"
                },
                {
                    "start": 665,
                    "end": 670,
                    "text": "[110]",
                    "ref_id": "BIBREF110"
                }
            ],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "Most PC-based 3D object detection methods use anchor-based detection methods, which have two major disadvantages. First, these methods require Non-Maximum Suppression (NMS) to filter redundant, overlapped, and imprecise bounding boxes (BBoxes), which causes non-trivial computational costs. Second, they require tricky anchor tuning, which is time-consuming. In this direction, Ge et al. [62] proposed AFDet, which is the first anchor and NMSfree PC 3D object one-stage detector with straightforward post-processing. Its 3DOR detection pipeline consisted of four major components, which were a point cloud encoder, the backbone, and necks, and it also included an anchor free detector as shown in Figure 12 . It encoded PC to image-like feature maps in birds eye view (BEV) using [111] . Then, it used a CNN with up-sampling necks, which were connected to five different heads for the prediction of object centers in the BEV plane using key point heat map and regression of 3D BBoxes. It combined the head outputs to generate detection outcomes. Every heat peak was selected by a max pooling operation during the inference, which eliminated the need for NMS. Figure 12 . 3D detection pipeline of AFDet [62] . The numbers in square brackets represent output channels of the last convolution layer, and C indicates the number of categories. Table 11 . Literature Analysis: 2D Image Grid-based 3DOR Methods.",
            "cite_spans": [
                {
                    "start": 388,
                    "end": 392,
                    "text": "[62]",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 780,
                    "end": 785,
                    "text": "[111]",
                    "ref_id": "BIBREF111"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "text": "[62]",
                    "ref_id": "BIBREF62"
                }
            ],
            "ref_spans": [
                {
                    "start": 697,
                    "end": 706,
                    "text": "Figure 12",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1159,
                    "end": 1168,
                    "text": "Figure 12",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1339,
                    "end": 1347,
                    "text": "Table 11",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "AFDet [62] Detector Category Two-stage One-stage",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 10,
                    "text": "[62]",
                    "ref_id": "BIBREF62"
                }
            ],
            "ref_spans": [],
            "section": "RT3D [61]"
        },
        {
            "text": "Outdoor Outdoor",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Environment"
        },
        {
            "text": "3D vehicle detection scenario for collision avoidance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scenario"
        },
        {
            "text": "3D object detection scenario on embedding system that is anchor free and Non-Maximum Suppression free",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scenario"
        },
        {
            "text": "Completes detection in a shorter time than the scan period of the LiDAR using pre-RoI pooling convolution and pose sensitive feature maps",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Advantage(s)"
        },
        {
            "text": "Provides anchor-free and NMS-free end-to-end 3D object detection",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Advantage(s)"
        },
        {
            "text": "Performance on the test dataset is not as good",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Limitation(s)"
        },
        {
            "text": "Many LiDAR-based 3DOR techniques use a voxel grid representation [112] . Table 12 explains the brief methodology, limitations, and the research gap, and we summarize the reviewed models with advantages and limitations in Table 13 . Table 12 . Methodology and Limitation(s): 3D Voxel Grid-based 3DOR Methods.",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 70,
                    "text": "[112]",
                    "ref_id": "BIBREF112"
                }
            ],
            "ref_spans": [
                {
                    "start": 73,
                    "end": 81,
                    "text": "Table 12",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 221,
                    "end": 229,
                    "text": "Table 13",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 232,
                    "end": 240,
                    "text": "Table 12",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Height information is not fully preserved (b) 3D Voxel Grid-Based 3DOR"
        },
        {
            "text": "Discretizes 3D point clouds into 3D voxel grid representation that preserves shape information and performs recognition using CNN or fully CNN",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Empty cells in their sparse representation make it computationally inefficient, 3D convolutions result in increased inference time",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Limitation(s)"
        },
        {
            "text": "Generating 3D region proposals could improve localization accuracy and reduce computational time LiDAR PC-based 3D vehicle detection is important for obstacle avoidance in realworld robotics applications, such as autonomous driving. The semantic context information in LiDAR-based sensors is not deeply explored in the literature. Therefore, despite significant progress, vehicle ambiguity and the varying distribution of PC across different depths are two main problems. Yi et al. [63] addressed these issues by developing free-of-charge BEV semantic masks and a depth-aware learning head in the fully convolutional network. They proposed a one-stage detection framework, SegVNet, consisting of three major components: a voxel feature encoder (VFE), semantic context encoder (SCE), and depth-aware head as shown in Figure 13 .",
            "cite_spans": [
                {
                    "start": 482,
                    "end": 486,
                    "text": "[63]",
                    "ref_id": "BIBREF63"
                }
            ],
            "ref_spans": [
                {
                    "start": 816,
                    "end": 825,
                    "text": "Figure 13",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "They introduced a VFE for voxelized feature representation of raw PC and developed a SCE for taking BEV feature maps from VFE as input and generated the semantic context encoded feature maps as output for 3D detection. SCE shared VFE feature maps with its two branches, in which, the first is adopted from [113] , while the second learns BEV semantic masks predictions. Its depth-aware head consisting of convolution layers with different kernel sizes was designed for learning distinctive depth-aware features across different depths in autonomous driving scenarios.",
            "cite_spans": [
                {
                    "start": 306,
                    "end": 311,
                    "text": "[113]",
                    "ref_id": "BIBREF113"
                }
            ],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "Many recent PC-based 3D detectors are optimized for classes, such as cars, pedestrians, and cyclists with multiple models; therefore, it requires a large number of resources to run multiple models for obtaining the desired detection results, which are not desirable for autonomous driving vehicles that have limited resources. Muramatsu et al. [64] presented their solution by developing the SECOND-DX model to support multi-class LiDAR-based 3D object detection with only a single model in realtime. This extended the [113, 114] , and [111] models to provide support for three classes: cars, pedestrians, and cyclists. It divided the PC into a 3D spatial grid and extracted fine local features using a high-resolution voxel. It contained three sub-networks in which the first [111] was used to convert points to voxel-wise representations, the second sub-network improved the spatial feature map and encoded it to a 2D feature map, and class probabilities and direction classification were performed by the last region proposal network.",
            "cite_spans": [
                {
                    "start": 344,
                    "end": 348,
                    "text": "[64]",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 519,
                    "end": 524,
                    "text": "[113,",
                    "ref_id": "BIBREF113"
                },
                {
                    "start": 525,
                    "end": 529,
                    "text": "114]",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 536,
                    "end": 541,
                    "text": "[111]",
                    "ref_id": "BIBREF111"
                },
                {
                    "start": 777,
                    "end": 782,
                    "text": "[111]",
                    "ref_id": "BIBREF111"
                }
            ],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "Feng et al. [65] proposed a LiDAR-based multi-task learning network (LidarMTL) to perform six perception tasks in a unified network for 3DOR. Its network architecture based on the voxelized Lidar point cloud is shown in Figure 14 , which voxelized the 3D space into regular voxels. It well-preserved the geometric information by proper voxel size. It used UNet architecture to add task-specific heads and trained this entire network with multi-task loss. Following [115] , they extended the encoder-decoder based [116] UNet architecture for efficient processing of 3D LiDAR points that were represented as voxels using 3D sparse convolution [113] . ",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 16,
                    "text": "[65]",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 465,
                    "end": 470,
                    "text": "[115]",
                    "ref_id": "BIBREF115"
                },
                {
                    "start": 513,
                    "end": 518,
                    "text": "[116]",
                    "ref_id": "BIBREF116"
                },
                {
                    "start": 641,
                    "end": 646,
                    "text": "[113]",
                    "ref_id": "BIBREF113"
                }
            ],
            "ref_spans": [
                {
                    "start": 220,
                    "end": 229,
                    "text": "Figure 14",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "Performance is not satisfactory for all the classes (e.g., cyclist and pedestrian.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Partial occlusion leads to false positives"
        },
        {
            "text": "The necessity of using loss weights with grid search",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Partial occlusion leads to false positives"
        },
        {
            "text": "This section focuses on Point-nets, and we analyze methods with their advantages and limitations in Table 14 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 100,
                    "end": 108,
                    "text": "Table 14",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "(ii) Unstructured Representation for 3DOR"
        },
        {
            "text": "Point-nets directly handle the irregularities by taking raw LiDAR PC data as the input. This aims at reducing the information loss in 3D space caused by projection or quantization methods. Table 15 illustrates brief methodology, limitations, and the research gaps of pointNet-based 3DOR techniques, while Table 14 gives a literature analysis of the reviewed studies. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 189,
                    "end": 197,
                    "text": "Table 15",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 305,
                    "end": 313,
                    "text": "Table 14",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "(a) PointNet-based 3DOR"
        },
        {
            "text": "Raw 3D point clouds are directly passed to CNNs for class predictions and BBox estimations without converting 3D points to 2D-image and 3D-voxel grids",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Processing the entire point cloud causes increased computational complexity Uses region proposals (RP) to restrict the number of points, however, generating RP on raw point clouds is difficult",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Limitation(s)"
        },
        {
            "text": "Processing of whole point cloud and methods to limit the number of points needs to be further investigated Most of the existing methods encode 3D PCs to 2D grid images by projection [73, 117] or 3D voxel grid [114, 118] and then apply CNN. However, the detection performance through these representations is not always optimal. Moreover, the limitation of these methods is their dependency on image detection results of 2D detectors, which do not give satisfactory performance in a large-cluttered environment. In a study, Yang et al. [66] addressed these issues by proposing an IPOD framework for 3D object detection on raw PC and provided a high recall rate. It seeded all points of cloud and object proposals without losing localization information.",
            "cite_spans": [
                {
                    "start": 182,
                    "end": 186,
                    "text": "[73,",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 187,
                    "end": 191,
                    "text": "117]",
                    "ref_id": "BIBREF117"
                },
                {
                    "start": 209,
                    "end": 214,
                    "text": "[114,",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 215,
                    "end": 219,
                    "text": "118]",
                    "ref_id": "BIBREF118"
                },
                {
                    "start": 535,
                    "end": 539,
                    "text": "[66]",
                    "ref_id": "BIBREF66"
                }
            ],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "It also extracted their local and context information, which was fed to PointNet for result generation through inference. It produced a 3D BBox from point-based object proposals and introduced the techniques for ambiguity reduction. Its network architecture shown in Figure 15 was consisted of a backbone network work based on PointNet++ [119] , a proposal feature generation module with two parts for feature map extraction, and a BBox prediction network for the prediction of object's size, shape, class, and orientation. It followed [114, 120] to train one network for cars and the other for cyclists and pedestrians. 3D object detection from raw PC has been deeply investigated compared to other 3D detection methods. In a seminal study, Zhou et al. [67] presented an FVNet framework for raw PC-based 3D object detection and front-view proposals generation. Direct learning from PC is a challenging task due to its sparse and irregular points. The FVNet circumvented this issue by projecting raw PC on a cylindrical surface for front view feature map generation and took the advantage of both 2D image grid and 3D voxel grid while retained the rich information of 3D PC. The architecture of FVNet shown in Figure 16 was composed of two sub-networks. It used a proposal generation network (PG-Net) to predict the region proposals from the generated maps.",
            "cite_spans": [
                {
                    "start": 338,
                    "end": 343,
                    "text": "[119]",
                    "ref_id": "BIBREF119"
                },
                {
                    "start": 536,
                    "end": 541,
                    "text": "[114,",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 542,
                    "end": 546,
                    "text": "120]",
                    "ref_id": "BIBREF120"
                },
                {
                    "start": 754,
                    "end": 758,
                    "text": "[67]",
                    "ref_id": "BIBREF67"
                }
            ],
            "ref_spans": [
                {
                    "start": 267,
                    "end": 276,
                    "text": "Figure 15",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1210,
                    "end": 1219,
                    "text": "Figure 16",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "Then, these maps were used for the prediction of 3D region proposals. Finally, parameter estimation network (PE-Net), which extended the PointNet [121] structure, was used for the extraction of point-wise features and regression of 3D BBox parameters. Li et al. [68] proposed density-oriented Point-Net (DPointNet) shown in Figure 17 to overcome the inhomogeneity of point clouds for 3DOR and verified its effectiveness on 3DOR by applying it to PointRCNN [122] . This network was proposed with two kinds of layers known as the SG (Sampling and Grouping) layer and several FA (Fusion and Abstraction) layers. It used the SG layer for sampling the seeds and their neighbors and several FA layers for fusion and abstraction of seeds features. The seeds from the input point cloud were sampled using farthest point sampling, and repeated random sampling was used if the neighbors were not sufficient.",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 151,
                    "text": "[121]",
                    "ref_id": "BIBREF121"
                },
                {
                    "start": 262,
                    "end": 266,
                    "text": "[68]",
                    "ref_id": "BIBREF68"
                },
                {
                    "start": 456,
                    "end": 461,
                    "text": "[122]",
                    "ref_id": "BIBREF122"
                }
            ],
            "ref_spans": [
                {
                    "start": 324,
                    "end": 333,
                    "text": "Figure 17",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "The seed neighbors were divided into several groups according to the number of FA layers. Then, the next step was performed by FA layers, which used all neighbor information from SG layer. The FA layers were designed based on three schemes to fuse and abstract information for each seed. First, the feature appending scheme was used to transform the features of all groups in FA layer. Second, the coordinate concatenation scheme, was used to adopt the 'concatenation' mechanism for fusion using coordination information. Third, the feature concatenation scheme was used to combine first and second schemes by sufficient feature extraction and feature fusion. The auxiliary heads were applied to PointRCNN for training process. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "Graph-based representation preserves the irregularity of PC. However, only a few studies have investigated graph neural networks for 3D object detection in LiDAR PC. This section first discusses recent graph-based 3DOR methods and then analyzes them with their advantages and limitations as shown in Table 16 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 300,
                    "end": 308,
                    "text": "Table 16",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "(iii) Graph representation for 3DOR"
        },
        {
            "text": "Instead of converting PC data into grid or voxel representation, Shi et al. [69] proposed Point-GNN, a graph neural network for compact representation of PC in which neighbor-hood points were linked with the graph edges. It facilitated accurate detection of multiple objects on PC using 3D BBoxes in a single shot from LiDAR PC. The points were coordinated by the auto-registration method while detection results from different vertices and integrated by box merging and scoring operations.",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 80,
                    "text": "[69]",
                    "ref_id": "BIBREF69"
                }
            ],
            "ref_spans": [],
            "section": "(iii) Graph representation for 3DOR"
        },
        {
            "text": "Existing 3D object detectors individually recognize the objects without considering their relationship in learning and inference. The overall architecture contains three components. The first is graph construction in which a voxel down-sampled point cloud was used for reducing the density of a point cloud during graph construction. The second contained a GNN of T iterations in which a graph convolutional neural network was designed to refine the vertex's state. The third was related to bounding box merging and scoring in which the merged boxes were calculated by considering the entire overlapped box cluster.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(iii) Graph representation for 3DOR"
        },
        {
            "text": "Feng et al. [70] presented a 3D relation graph network for building an object-object relation model by learning pseudo centers and direction vectors to improve the prediction accuracy. It was composed of two main parts in which 3D BBoxes were predicated through the proposal generation module, directly on the PC with PointNet++ [119] backbone. Its second part introduced the relation module for point attention pooling and exploit the object-object relationship.It also used point attention pooling for converting the point features into a uniform vector and performed relational reasoning using 3D object-object relation graph. It applied a 3D NMS post processing step for the extraction of high-quality 3D BBox candidates.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 16,
                    "text": "[70]",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 329,
                    "end": 334,
                    "text": "[119]",
                    "ref_id": "BIBREF119"
                }
            ],
            "ref_spans": [],
            "section": "(iii) Graph representation for 3DOR"
        },
        {
            "text": "3D object recognition requires both geometric and semantic information (e.g., the object's shape). However, many PC-based object detectors do not effectively capture the semantic characteristic of PCs. In this direction, Chen et al. [71] introduced the hierarchical graph network (HGNet) as shown in Figure 18 that processes raw PCs using multi-level semantics for 3D object detection. It contained three main parts, which are a graph convolution-based U-shape network called GUnet, proposal generator, and proposal reasoning module (referred to as ProRe Module).",
            "cite_spans": [
                {
                    "start": 233,
                    "end": 237,
                    "text": "[71]",
                    "ref_id": "BIBREF71"
                }
            ],
            "ref_spans": [
                {
                    "start": 300,
                    "end": 309,
                    "text": "Figure 18",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "(iii) Graph representation for 3DOR"
        },
        {
            "text": "It depicted the shape information of objects by extracting local features from geometric positions of the points. It employed a shape-attentive graph convolution, which is a Ushape network for mapping multi-level features through the voting module, and used ProRe Module to reason about proposals for BBox prediction by taking the advantage of global scene semantics. The proposal features were updated by GConv, combining the global scene semantics and including proposals' relative positions as an attention map. Wang et al. [72] overcame the inherent drawbacks of partition-based methods that limit the 3DOR of small objects by proposing the spatial-attention graph convolution (S-AT GCN), which include EdgeConv, attention, far distance feature suppression, and aggregation steps as shown in Figure 19 . For partition operation, single instance, e.g., a pedestrian was sliced, which is called the partition effect. The partition effect was used to influence the performance of 3DOR, particularly in the case of small object detection.",
            "cite_spans": [
                {
                    "start": 527,
                    "end": 531,
                    "text": "[72]",
                    "ref_id": "BIBREF72"
                }
            ],
            "ref_spans": [
                {
                    "start": 796,
                    "end": 805,
                    "text": "Figure 19",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "(iii) Graph representation for 3DOR"
        },
        {
            "text": "An extra layer called feature enhancement (FE) layer was included after partition operation. The S-AT GCN was cascaded to form FE layers, while the effectiveness of these layers was presented by adding [121] . They added the feature enhancement (FE) layer to the baseline model, point pillars [121] after partition operation and a spatial attention mechanism for GCN to extract geometric information. This enabled the network to extract more accurate foreground features. This section discusses 3D object detection based on camera-LiDAR fusion [123] using DL approaches to overcome the limitations and uncertainties of a single sensor. Camera-LiDAR fusion has become a practical approach for 3DOR [124] . The reliance on a single sensor can be risky for the accurate understanding of the surrounding environment, therefore, it is advantageous to equip robotic systems with a second sensor to achieve robust environment perception for the detection of 3D objects. To this end, sensor fusion, which leverages the data derived from multiple sensors and gives less uncertain information compared to the individual sensor, has become an emerging research area. Table 17 demonstrates the methodology and limitations along with the research gap of camera-LiDAR fusion-based 3DOR techniques. ",
            "cite_spans": [
                {
                    "start": 202,
                    "end": 207,
                    "text": "[121]",
                    "ref_id": "BIBREF121"
                },
                {
                    "start": 293,
                    "end": 298,
                    "text": "[121]",
                    "ref_id": "BIBREF121"
                },
                {
                    "start": 544,
                    "end": 549,
                    "text": "[123]",
                    "ref_id": "BIBREF123"
                },
                {
                    "start": 697,
                    "end": 702,
                    "text": "[124]",
                    "ref_id": "BIBREF124"
                }
            ],
            "ref_spans": [
                {
                    "start": 1156,
                    "end": 1164,
                    "text": "Table 17",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "(iii) Graph representation for 3DOR"
        },
        {
            "text": "Uses multi-modal CNN to fuse both LiDAR 3D point cloud and camera images Shows state-of-the-art and robust detection performance by taking advantage of both sensors",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Computationally expensive to use data from two different sensors Requires calibration between LiDAR and camera An appropriate representation of different sensor modalities is difficult and passing them to a fusion network is also challenging",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Limitation(s)"
        },
        {
            "text": "More research should be focused on improving the fusion of different sensing modalities",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "The fusion approaches can be divided into three categories. Early fusion (EF), also called data-fusion, takes inputs from multiple sensors that are first combined in the beginning and makes a new representation that is used for transformations (e.g., convolutions). Late-fusion (LF), also known as decision fusion, first transforms the sensors' inputs and then combines them. Deep-fusion (DF) or middle-fusion (MF) [125] is the combination of both EF and LF. We review some camera-LiDAR fusion methods and present their literature analysis in Table 18 . Reduces false positives and negatives due to its effective multi-modal fusion.",
            "cite_spans": [
                {
                    "start": 415,
                    "end": 420,
                    "text": "[125]",
                    "ref_id": "BIBREF125"
                }
            ],
            "ref_spans": [
                {
                    "start": 543,
                    "end": 551,
                    "text": "Table 18",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "Does not provide a multi-class detection network.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "SharedNet [77] One-stage.",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 14,
                    "text": "[77]",
                    "ref_id": "BIBREF77"
                }
            ],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "Outdoor.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "LiDAR-camerabased 3D object detection scenario with only one neural network for autonomous vehicles.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "Early, Middle.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "Achieving a good balance between accuracy and efficiency. Reduces the memory requirements and model training time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "Slightly inferior performance in case of car detection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research Gap"
        },
        {
            "text": "Fusion approaches for 3D object detection are either very complicated or rely on late-fusion. Therefore, they do not provide multi-modalities interaction at the early stages. In this direction, Chen et al. [73] proposed multi-view representation of 3D (MV3D) point cloud, which included a bird's eye view and front view of LiDAR and an image as input as shown in Figure 20 . The representation of bird's eye view was encoded by height, intensity, and density, while the complementary information was provided by the bird's eye view representation. It was used for the fusion of both LiDAR PC and RGB camera images and the prediction of 3D BBoxes.",
            "cite_spans": [
                {
                    "start": 206,
                    "end": 210,
                    "text": "[73]",
                    "ref_id": "BIBREF73"
                }
            ],
            "ref_spans": [
                {
                    "start": 363,
                    "end": 372,
                    "text": "Figure 20",
                    "ref_id": "FIGREF16"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "MV3D was composed of two sub-networks for the generation of 3D object proposals from BEV PC representation and fusion of multi-view features. It provided a deep fusion scheme after region proposal for combining region-wise features and enabled intermediate layer interaction. MV3D used 3D proposals to support different modalities and performed 3D box regression for accurate detection of objects, location, orientation, and size in 3D space. Wang et al. [74] used deep CNN for camera-LiDAR fusion architecture to detect 3D objects in the autonomous driving scenario and efficiently transformed the features between BEV and front view by developing a sparse non-homogeneous pooling layer. The main idea to transform feature maps into different views by point cloud and matrix multiplication. A fusion-based network was built The network structure of one-stage fusion-based detection network was shown in the Figure 21 , which contained two fully convolutional backbones for image and LiDAR units.",
            "cite_spans": [
                {
                    "start": 455,
                    "end": 459,
                    "text": "[74]",
                    "ref_id": "BIBREF74"
                }
            ],
            "ref_spans": [
                {
                    "start": 908,
                    "end": 917,
                    "text": "Figure 21",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "The PRN similar to many camera-based one stage detectors was used in image convolutional networks. However, region proposal was not used during the testing process. The auxiliary loss was applied to get supervision from the label and 3D proposal in the front view. It mapped two views by sparse PC and used a pooling layer to perform multi-view fusion before the proposal stage to transform the entire feature map. The architecture of its one-stage detector consists of two kinds of CNN backbone: VGG for camera-LiDAR with a feature map down-sampled four times for BEV and eight times in front view; MS-CNN [126] for camera-VoxelNet [114] with a feature map down-sampled two times for BEV and eight times in front view. Roth et al. [75] performed deep end-to-end 3D person detection with a camera and LiDAR PC using deep CNN for estimating the 3D location and extent of people in the automotive scenes. Its architecture refined 2D anchor proposals by developing a region proposal network (RPN and subsequent detection network). It extracted high-level features from camera images using VGG-like CNN, obtained PC features through Voxel Feature Encoders [114] , and performed end-to-end learning. The deep CNN learned low-level features from camera images and 3D LiDAR point clouds. It fused their high-level representations from both modalities and then passed them to the regression model as input for estimating the 3D person BBoxes. Figure 22 illustrated the network architecture, which was inspired by AVOD [120] . It adopted VGG16 network to extract the features of the image while features from the point cloud were extracted using voxel partitions. These partitions were applied by VFE layers and 3D convolutions. They size of the feature map was reduced by applying 1 \u00d7 1 convolution in RPN. The proposals were obtained by project 3D anchors on the feature map. The features from both modalities were fused after resizing and object's location was estimated by applying fully CNN. In the second stage, the best proposal were cropped and fused from full feature maps. The fully connected layers for fused crops were used for the implementation of object detection layers. It allowed end-to-end network to the 3D locations of the persons from camera image and LiDAR point cloud data. Sindagi et al. [76] extended VoxelNet [114] by introducing two fusion techniques: The point-fusion as an early-fusion scheme was employed to give a projection of PC to image feature space using a known calibration matrix, extract the features using a 2D detector, and perform point-level concatenation of image features. The voxel fusion as a late-fusion strategy was used to project non-empty 3D voxels generated by VoxelNet, extract features in 2D ROIs, and perform voxel-level concatenation of pooled features.",
            "cite_spans": [
                {
                    "start": 607,
                    "end": 612,
                    "text": "[126]",
                    "ref_id": "BIBREF126"
                },
                {
                    "start": 633,
                    "end": 638,
                    "text": "[114]",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 732,
                    "end": 736,
                    "text": "[75]",
                    "ref_id": "BIBREF75"
                },
                {
                    "start": 1152,
                    "end": 1157,
                    "text": "[114]",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 1510,
                    "end": 1515,
                    "text": "[120]",
                    "ref_id": "BIBREF120"
                },
                {
                    "start": 2304,
                    "end": 2308,
                    "text": "[76]",
                    "ref_id": "BIBREF76"
                },
                {
                    "start": 2327,
                    "end": 2332,
                    "text": "[114]",
                    "ref_id": "BIBREF114"
                }
            ],
            "ref_spans": [
                {
                    "start": 1435,
                    "end": 1444,
                    "text": "Figure 22",
                    "ref_id": "FIGREF18"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "It was a later fusion technique to handle the empty voxels. The MVX-Net effectively fused multimodal information. Its PointFusion based method is illustrated in Figure 23 in which convolutional filters of faster RCNN were used to for extracting the image feature map. The 3D points on the image were projected by calibration information and related features were appended to the 3D points. The 3D RPN and voxel feature enhancement layers were used for the processing the aggregated data and 3D detections. Wen et al. [77] proposed an early-fusion method to use both camera-LiDAR data for efficient 3DOR with single backbone network architecture. It extracted point-wise features from RGB images, which were fed into a 3D neural network. It used two strategies for reducing information loss during 3D voxel grid-based point-cloud representation. The first one was using small voxel size, while the second strategy was projecting point cloud features onto RGB images. A point feature fusion module, a voxel feature encoder module, a detection head, and a loss function were developed as the four main components of its one-stage 3D multi-class object detection model as shown in Figure 24 .",
            "cite_spans": [
                {
                    "start": 517,
                    "end": 521,
                    "text": "[77]",
                    "ref_id": "BIBREF77"
                }
            ],
            "ref_spans": [
                {
                    "start": 161,
                    "end": 170,
                    "text": "Figure 23",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1177,
                    "end": 1186,
                    "text": "Figure 24",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "The point clouds and RGB images were used as inputs and while the predictions of oriented 3D BBoxes for cars, pedestrians, and cyclists were the output. It used a point feature fusion module for the extraction of point features from the image and fused those features with the related point cloud features. High-level representation of fused pointwise features was performed by a voxel feature encoder module and 3D backbone and 3D BBoxes were classified and regressed by the detection head. Figure 24 . The pipeline of 3D object detection [77] network for the LiDAR and camera, including input, the point feature fusion module, the 3D backbone, and the detection head.",
            "cite_spans": [
                {
                    "start": 540,
                    "end": 544,
                    "text": "[77]",
                    "ref_id": "BIBREF77"
                }
            ],
            "ref_spans": [
                {
                    "start": 492,
                    "end": 501,
                    "text": "Figure 24",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Research Gap"
        },
        {
            "text": "The summary of 3DOR according to the studies reviewed in Section 3.1 and listed in Tables 9, 11 , 13, 14, 16, and 18 is presented. Current applications of 3DOR are generally categorized into two environments: outdoor and indoor, with the first category being more frequently studied (19 vs. 4 studies). The article divides these 3DOR studies according to sensor modalities that include camera-based (monocular-five studies and stereo cameras-two studies), image grid-based (two studies), 3D voxel grid-based (three studies), pointNet-based (three studies), graph-based (four studies), and camera-LiDAR fusion-based (five studies). These 3DOR methods use state-of-the-art DL-based object recognition networks that follow either one-stage (nine studies) or two-stage (14 studies) object detection pipelines.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 83,
                    "end": 95,
                    "text": "Tables 9, 11",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Summary"
        },
        {
            "text": "The advantages and limitations of 3DOR methods show that developing DL-based multi-model recognition systems is a particularly challenging task for ADV in outdoor environment because it requires a high level of accuracy and real-time performance while current models cannot generate prediction consistency over time. On the other hand, object recognition is a challenge in an indoor environment consisting of cluttered scene with many occluded objects. In addition, the fusion of multiple sensors and different feature representations as well as optimal fusion architecture for 3DOR are still open questions that require more focus on these research topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Summary"
        },
        {
            "text": "3D place recognition is a task of identifying the location in a view of a place by querying the similar images that belong to the same location in a large geo-tagged database [127] . It retrieves the database images according to the robot pose and current query image taken by the robot's sensor (i.e., camera) to find the association between query images and database images of known places. Robots and automated vehicles on the road use the place recognition approaches for accurately recognizing the locations and efficiently identifying the revisited places.",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 180,
                    "text": "[127]",
                    "ref_id": "BIBREF127"
                }
            ],
            "ref_spans": [],
            "section": "3D Place Recognition (3DPR)"
        },
        {
            "text": "Although, place recognition systems can also benefit from the existing research on object recognition by detecting the objects in the context of scene knowledge [128] . However, place recognition approaches are more concentrated on larger scale targets called the place landmarks [129] . Another major characteristic that distinguishes the place recognition from other visual recognition tasks is that it has to perform the condition-invariant recognition to a degree that many other recognition tasks do not have. Moreover, an architecture that is apt for 3DOR may not fit well into 3DPR tasks because their visual cues are different.",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 166,
                    "text": "[128]",
                    "ref_id": "BIBREF128"
                },
                {
                    "start": 280,
                    "end": 285,
                    "text": "[129]",
                    "ref_id": "BIBREF129"
                }
            ],
            "ref_spans": [],
            "section": "3D Place Recognition (3DPR)"
        },
        {
            "text": "Place recognition is an active research area and a key capability of autonomous mobile robots. However, it is still a challenging task to achieve. The recent literature on place recognition concentrates on replacing traditional handcrafted feature extractors [4, [130] [131] [132] [133] [134] [135] [136] [137] with CNN for feature extraction [138] [139] [140] [141] , which aids in the direct learning of 3D structural descriptors. Camera and LiDAR are two main sensors to perform place recognition tasks.",
            "cite_spans": [
                {
                    "start": 259,
                    "end": 262,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 263,
                    "end": 268,
                    "text": "[130]",
                    "ref_id": "BIBREF130"
                },
                {
                    "start": 269,
                    "end": 274,
                    "text": "[131]",
                    "ref_id": "BIBREF131"
                },
                {
                    "start": 275,
                    "end": 280,
                    "text": "[132]",
                    "ref_id": "BIBREF132"
                },
                {
                    "start": 281,
                    "end": 286,
                    "text": "[133]",
                    "ref_id": "BIBREF133"
                },
                {
                    "start": 287,
                    "end": 292,
                    "text": "[134]",
                    "ref_id": "BIBREF134"
                },
                {
                    "start": 293,
                    "end": 298,
                    "text": "[135]",
                    "ref_id": "BIBREF135"
                },
                {
                    "start": 299,
                    "end": 304,
                    "text": "[136]",
                    "ref_id": "BIBREF136"
                },
                {
                    "start": 305,
                    "end": 310,
                    "text": "[137]",
                    "ref_id": "BIBREF137"
                },
                {
                    "start": 343,
                    "end": 348,
                    "text": "[138]",
                    "ref_id": "BIBREF138"
                },
                {
                    "start": 349,
                    "end": 354,
                    "text": "[139]",
                    "ref_id": "BIBREF139"
                },
                {
                    "start": 355,
                    "end": 360,
                    "text": "[140]",
                    "ref_id": "BIBREF140"
                },
                {
                    "start": 361,
                    "end": 366,
                    "text": "[141]",
                    "ref_id": "BIBREF141"
                }
            ],
            "ref_spans": [],
            "section": "3D Place Recognition (3DPR)"
        },
        {
            "text": "Camera-based place recognition methods contain efficient descriptive information, but they struggle to cope with illumination and occlusion problems [142] . LiDAR-based place recognition approaches are invariant to appearance change [143] , however, rich descriptive representation is still an open research question for LiDAR-based place recognition, and it suffers from limited ranging and motion distortion issues [114, 144, 145] Therefore, fusing information from both sensors provides better solutions.",
            "cite_spans": [
                {
                    "start": 149,
                    "end": 154,
                    "text": "[142]",
                    "ref_id": "BIBREF142"
                },
                {
                    "start": 233,
                    "end": 238,
                    "text": "[143]",
                    "ref_id": "BIBREF143"
                },
                {
                    "start": 417,
                    "end": 422,
                    "text": "[114,",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 423,
                    "end": 427,
                    "text": "144,",
                    "ref_id": "BIBREF144"
                },
                {
                    "start": 428,
                    "end": 432,
                    "text": "145]",
                    "ref_id": "BIBREF145"
                }
            ],
            "ref_spans": [],
            "section": "3D Place Recognition (3DPR)"
        },
        {
            "text": "This section reviews data representation methods for 3D place recognition based on Camera and LiDAR sensors using DL models. It is subdivided as Camera-based 3DPR (Section 3.2.1), LiDAR-based 3DPR (Section 3.2.2), and Camera-LiDAR Fusion-based 3DPR (Section 3.2.3).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3D Place Recognition (3DPR)"
        },
        {
            "text": "Visual place recognition (VPR) is the problem of recognizing a place from the robot's current camera images based on the visual appearance [146, 147] . It has been around for many years. However, research in this field is growing rapidly due to recent developments in camera technologies [148] with their compatibility for DL-based techniques. In this direction, 3D depth vision cameras and event-based cameras have drawn researchers' attention. 3D depth cameras have made it possible to collect 3D data with ease. However, the limited range of depth, less accurate distance information, and training 3D data with DL-based models are the challenges still underdeveloped [149] .",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 144,
                    "text": "[146,",
                    "ref_id": "BIBREF146"
                },
                {
                    "start": 145,
                    "end": 149,
                    "text": "147]",
                    "ref_id": "BIBREF147"
                },
                {
                    "start": 288,
                    "end": 293,
                    "text": "[148]",
                    "ref_id": "BIBREF148"
                },
                {
                    "start": 670,
                    "end": 675,
                    "text": "[149]",
                    "ref_id": "BIBREF149"
                }
            ],
            "ref_spans": [],
            "section": "Camera-Based 3DPR"
        },
        {
            "text": "As DL-models rely on the networks trained only on RGB data, which lacks the depth features. In this direction, Song et al. [78] addressed these limitations using RGB-D videos for taking advantage of the richer depth and RGB information. It introduced a two-step training approach that involves weekly pre-training via patches to learn powerful depthspecific features. Its proposed CNN-RNN framework was used to model RGB-D scenes for recognition.",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 127,
                    "text": "[78]",
                    "ref_id": "BIBREF78"
                }
            ],
            "ref_spans": [],
            "section": "Camera-Based 3DPR"
        },
        {
            "text": "Inspired by the two-step CNN techniques that were trained on still images, a threestep training strategy was introduced for CNN-RCNN architecture to obtain the significant gain through the integration of depth videos. It created a joint embedding by combining convolutional and recurrent neural networks for capturing spatial and temporal information as shown in Figure 25 . LSTM blocks were used to implement the recurrent neural networks. It used independent branches for RGB and depth data. LSTMs based temporal embedding was modality specific and late fusion was performed using fully connected layer while combined architecture was trained jointly end-to-end.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 363,
                    "end": 372,
                    "text": "Figure 25",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Camera-Based 3DPR"
        },
        {
            "text": "To the best of our knowledge, there are very few studies that use an event-based camera for place recognition. Among them, Kong et al. [79] proposed Event-VPR, the first end-to-end VPR method using an event camera. These cameras work differently from the frame-based cameras because there are neuromorphic visual sensors that are inspired by the biological retina and have the advantage of low latency, low bandwidth and low power consumption [150] . The key idea of Event-VPR, as shown in Figure 26 , was to apply NetVLAD to EST voxel grid, which was generated by event streams. It selected the corresponding positive and negative of event bins and trained the network to learn the global descriptor vectors of the bins. First, it used event streams as input and divided the consecutive event stream into the bins. These bins were converted into EST voxel grid using MLP-based kernel. Then, the visual features of EST voxel grids were extracted using ResNet34 [108] . Then, feature descriptor aggregation was performed by a VLAD-based aggregated description layer, and finally the network was trained with weakly supervised training for 3DPR. ",
            "cite_spans": [
                {
                    "start": 135,
                    "end": 139,
                    "text": "[79]",
                    "ref_id": "BIBREF79"
                },
                {
                    "start": 443,
                    "end": 448,
                    "text": "[150]",
                    "ref_id": "BIBREF150"
                },
                {
                    "start": 961,
                    "end": 966,
                    "text": "[108]",
                    "ref_id": "BIBREF108"
                }
            ],
            "ref_spans": [
                {
                    "start": 490,
                    "end": 499,
                    "text": "Figure 26",
                    "ref_id": "FIGREF21"
                }
            ],
            "section": "Camera-Based 3DPR"
        },
        {
            "text": "Place recognition using LiDAR-based 3D PC is still an open issue and a harder task in large-scale dynamic environments due to the difficulty in feature extraction from raw 3D PC and global descriptor generation [151] . The article focuses on recent LiDAR pointcloud-based methods for 3D place recognition using DL-based techniques and provide their comparison in Table 19 . In contrast to image-based counterparts, most studies of 3D recognition have not dealt with LiDAR PC for place recognition due to the difficulty of its local descriptors' extraction that can later be converted into global descriptors. A recent study by Angelina et al. [80] applied DL networks and introduced PointNetVLAD to provide the solution of PC-based place recognition using NetVLAD [152] and PointNet [121] . It extracted more general global features proposing lazy triplet and quadruplet loss function while mapped 3D PC to discriminative global descriptors by training PointNETVLAD using metric learning [153] .",
            "cite_spans": [
                {
                    "start": 211,
                    "end": 216,
                    "text": "[151]",
                    "ref_id": "BIBREF151"
                },
                {
                    "start": 643,
                    "end": 647,
                    "text": "[80]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 764,
                    "end": 769,
                    "text": "[152]",
                    "ref_id": "BIBREF152"
                },
                {
                    "start": 783,
                    "end": 788,
                    "text": "[121]",
                    "ref_id": "BIBREF121"
                },
                {
                    "start": 988,
                    "end": 993,
                    "text": "[153]",
                    "ref_id": "BIBREF153"
                }
            ],
            "ref_spans": [
                {
                    "start": 363,
                    "end": 371,
                    "text": "Table 19",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "The PointNetVLAD was a combination of existing PointNet [121] and NetVLAD [152] , shown in Figure 27 for global descriptor extraction from given 3D point clouds by end-toend training and inference. Its included first block of PointNet that was cropped before maxpool aggregation layer. Its input was the same as PointNet consisting of a set of 3D points. The dimensional local feature descriptors were extracted from each input 3D point.",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 61,
                    "text": "[121]",
                    "ref_id": "BIBREF121"
                },
                {
                    "start": 74,
                    "end": 79,
                    "text": "[152]",
                    "ref_id": "BIBREF152"
                }
            ],
            "ref_spans": [
                {
                    "start": 91,
                    "end": 100,
                    "text": "Figure 27",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "These descriptors were fed to NetVLAD layer, which was designed to aggregate local image features from VGG/AlexNet into global descriptor vector. The VLAD descriptor [154] was the output of the NetVLAD layer.",
            "cite_spans": [
                {
                    "start": 166,
                    "end": 171,
                    "text": "[154]",
                    "ref_id": "BIBREF154"
                }
            ],
            "ref_spans": [],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "Place recognition and scene understanding is also an important area of research in the indoor environment. However, in contrast to the outdoor environment, there are fewer studies of place recognition from 3D PC data for the indoor environment. An autonomous robot must be aware of different places, such as rooms, hallways, and kitchens in an indoor environment to perform its task. Huang et al. [81] performed 3D PC (voxel) based scene recognition in an indoor environment by combining semantic segmentation with the multitask framework. It worked on scene recognition in indoor environment as supervised classification using neural network.",
            "cite_spans": [
                {
                    "start": 397,
                    "end": 401,
                    "text": "[81]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "The network was composed of encoder to extract feature representation from input scene and a classification head to obtain class-conditional likelihood. It explored two different options for encoder: First was the working with subsampled version of original PC networks (Pointnet [121] , Pointnet++ [119] DGCNN [155] ) while second was sparse voxel grid networks (Resnet14 [108] ). It demonstrated that multi-task learning with semantic segmentation improves the performance of scene recognition by sharing information among related tasks. The multi-task network was composed of an encoder for converting the scene into a feature representation, and two output heads, which were semantic segmentation head (top) and a classification head (bottom) for computing the class likelihood as shown in Figure 28 . For semantic segmentation, sparse Resnet14 variant with U-net style decoder was extended that mirrored the encoder with skip connections. The network weights of encoder were froze and only scene classification head was trained. Finally the network was fine-tuned with small learning rate to yield better recognition. An efficient place recognition system is invariant to illumination variation and object motion in that place [156] . Sun et al. [82] presented PC-based place recognition using CNN that was pre-trained on color images and provided robust detection to moving objects, which were also rotation and illumination invariant. The 3D place recognition system in Figure 29 shows that it first aligned the PC with its principal directions then represented it onto the cylindrical image plan. It performed feature extraction using CNN followed by the principal component analysis dimension reduction and specified a threshold to determine the trade-off between recall and precision.",
            "cite_spans": [
                {
                    "start": 280,
                    "end": 285,
                    "text": "[121]",
                    "ref_id": "BIBREF121"
                },
                {
                    "start": 299,
                    "end": 304,
                    "text": "[119]",
                    "ref_id": "BIBREF119"
                },
                {
                    "start": 311,
                    "end": 316,
                    "text": "[155]",
                    "ref_id": "BIBREF155"
                },
                {
                    "start": 373,
                    "end": 378,
                    "text": "[108]",
                    "ref_id": "BIBREF108"
                },
                {
                    "start": 1232,
                    "end": 1237,
                    "text": "[156]",
                    "ref_id": "BIBREF156"
                },
                {
                    "start": 1251,
                    "end": 1255,
                    "text": "[82]",
                    "ref_id": "BIBREF82"
                }
            ],
            "ref_spans": [
                {
                    "start": 794,
                    "end": 803,
                    "text": "Figure 28",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1477,
                    "end": 1486,
                    "text": "Figure 29",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "In its preprocessing step, it considered a 3D PC created by a Velodyne LiDAR to cover for full 360\u2022 environmental view. PCA was used to align the PC by finding the orthogonal directions and obtain more compact features. It generated the range image through the projection of PC on cylindrical plane while extracted the features by CNN using convolutional layers. It used fully connected layers to perform reshaping and pooling layer on the top of hidden layer for dimension reduction. Since one place contained one descriptor; therefore, the variance of dimension indicated its discrimination ability. For retrieval, the descriptor vector of each PC was normalized, and the cosine distance was used as similarity metric. Liu et al. [83] proposed a large-scale place description network (LPD-Net) for extracting distinct and general global feature descriptors from 3D PC. It used local features rather than isolated point positions as the network input. The network architecture was composed of three major modules to handle large scale environment as shown in Figure 30 . The adaptive local feature extraction module was used to obtain the PC distribution and the local features. The graph-based neighborhood aggregation module was used in feature and Cartesian space to learn structure information of PC. The resulting vectors were passed to NetVLAD [152] for the generation of a global descriptor.",
            "cite_spans": [
                {
                    "start": 732,
                    "end": 736,
                    "text": "[83]",
                    "ref_id": "BIBREF83"
                },
                {
                    "start": 1351,
                    "end": 1356,
                    "text": "[152]",
                    "ref_id": "BIBREF152"
                }
            ],
            "ref_spans": [
                {
                    "start": 1060,
                    "end": 1069,
                    "text": "Figure 30",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "The computational and storage complexity was reduced by extracting global descriptor to perform real-time place recognition tasks. Its feature network captured the local structure using features around each point in the local neighborhood. The raw PC data was passed as input to Transformation Net [121] , which aimed at ensuring the rotational translation invariance and the adaptive local feature extractor, which considered the statistical local distribution.",
            "cite_spans": [
                {
                    "start": 298,
                    "end": 303,
                    "text": "[121]",
                    "ref_id": "BIBREF121"
                }
            ],
            "ref_spans": [],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "The appropriate neighborhood size in different situations was selected using adaptive neighborhood structure, which were merged into feature vectors. The output of the feature network was passed to a graph network as input, and feature aggregation was performed using the kNNgraph network in the Cartesian space. It introduced the relational representation from the GNN to LPD-Net for representing the scene compositions as graph nodes, their intrinsic relationships and scene descriptors generated by GNN. Most research on place recognition [80, 157, 158] has not fully addressed the problem of 3 DoF transformation. Schaupp et al. [84] dealt with the aforementioned issue by proposing an efficient data-driven framework for extracting compact descriptors from 3D LiDAR PC using CNN, which aimed at recognizing the place and regressing the orientation between point clouds. The network was trained by a triplet loss function and a hard-negative mining scheme was applied to improve the descriptor extractor. It developed a metric global localization in the map reference frame from single scan of 3D LiDAR PC.",
            "cite_spans": [
                {
                    "start": 542,
                    "end": 546,
                    "text": "[80,",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 547,
                    "end": 551,
                    "text": "157,",
                    "ref_id": "BIBREF157"
                },
                {
                    "start": 552,
                    "end": 556,
                    "text": "158]",
                    "ref_id": "BIBREF158"
                },
                {
                    "start": 633,
                    "end": 637,
                    "text": "[84]",
                    "ref_id": "BIBREF84"
                }
            ],
            "ref_spans": [],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "For this, it used four sequential components known as point cloud projection, descriptor extraction, yaw estimation, and local point cloud registration as shown in Figure 31 . In the first step, PC projection used spherical model for PC representation and converted the LiDAR point cloud scan onto a 2D range image. In the second step, descriptor extraction was implemented for place representation and deriving orientation details using CNN.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 164,
                    "end": 173,
                    "text": "Figure 31",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "For this, 2D range images were taken as input and two compact descriptor vectors were generated, which were used to represent rotation invariant and encode it for yaw angle discrepancy between the query PC and the PC of the nearest place in the map. Finally, local registration method was applied to obtain three DoF pose estimation using planar coordinates and orientation estimate. The deep CNN architecture based on [159, 160] learned mapping from range image through encoding 3D PC onto feature vector representation to effectively perform oriented place recognition. Robust place recognition can be achieved using 3D scene structure. Ye et al. [85] represented structural information of the scene with semi-dense point clouds using DSO [132] and developed local descriptor matching to perform place recognition. It used 3D CNN like [118, 161] and generated discriminative descriptors by learning features from a 3Dvoxel grid. Its place recognition pipeline as shown in Figure 32 was composed of four main components. It used DSO [162] to acquire the information in semi-dense point cloud. It extracted the local patches from semi-dense point clouds and normalized them.",
            "cite_spans": [
                {
                    "start": 419,
                    "end": 424,
                    "text": "[159,",
                    "ref_id": "BIBREF159"
                },
                {
                    "start": 425,
                    "end": 429,
                    "text": "160]",
                    "ref_id": "BIBREF160"
                },
                {
                    "start": 649,
                    "end": 653,
                    "text": "[85]",
                    "ref_id": "BIBREF85"
                },
                {
                    "start": 741,
                    "end": 746,
                    "text": "[132]",
                    "ref_id": "BIBREF132"
                },
                {
                    "start": 837,
                    "end": 842,
                    "text": "[118,",
                    "ref_id": "BIBREF118"
                },
                {
                    "start": 843,
                    "end": 847,
                    "text": "161]",
                    "ref_id": "BIBREF161"
                },
                {
                    "start": 1034,
                    "end": 1039,
                    "text": "[162]",
                    "ref_id": "BIBREF162"
                }
            ],
            "ref_spans": [
                {
                    "start": 974,
                    "end": 983,
                    "text": "Figure 32",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "In the next step, keypoints were selected from random 5% resulting points and local cylindrical patches were extracted from them, which were chosen with the size to be as small as possible. These patches were represented using CNN-based descriptors, which contained two 3D convolutional layer, ReLU, a pooling and two fully connected layers for mapping from voxel grid to 512-dimensional descriptor. Finally, the resulting descriptors were matched to the descriptors that were stored in the database and their matches were aggregated to keyframe matches. It also used PCA to reduce the dimensionality, which resulted in efficient matching. Cramariuc et al. [86] used segment extraction combined with a matching method to perform the place recognition task in LiDAR-based 3D point clouds. It used CNN to generate descriptors for 3D PC segments and introduced a segment recognition approach based on learned descriptors, which outperformed the SegMatch descriptors [163] . It extended the structures of [164, 165] to the 3D domain for generating learning-based descriptors. It implemented place recognition task using three different CNNs as shown in Figure 33 for generating descriptors for 3D point cloud segments.",
            "cite_spans": [
                {
                    "start": 657,
                    "end": 661,
                    "text": "[86]",
                    "ref_id": "BIBREF86"
                },
                {
                    "start": 963,
                    "end": 968,
                    "text": "[163]",
                    "ref_id": "BIBREF163"
                },
                {
                    "start": 1001,
                    "end": 1006,
                    "text": "[164,",
                    "ref_id": "BIBREF164"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "text": "165]",
                    "ref_id": "BIBREF165"
                }
            ],
            "ref_spans": [
                {
                    "start": 1149,
                    "end": 1158,
                    "text": "Figure 33",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "For preprocessing, the alignment method was chosen to increase the robustness and make the extraction process less sensitive. The augmentation techniques were used to make multiple copies of the segmented data by rotating each image at different angles. Then, the segments were scaled to fit and centered inside the voxel grid. A CNN was proposed for feature extraction. Figure 34 shows the structure of descriptor extraction CNN, which tested different depths and sizes for layers and filters to keep the network small enough it could be feasible to run on the mobile robot platform. The amount of dropout in the final layers was tuned separately to ensure a correct regularization.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 371,
                    "end": 380,
                    "text": "Figure 34",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "The first approach was group-based classification. In this approach, training the CNN for segment classification was based on the groups that represent the classes. The layer before the classification was used as descriptor [166] . The closeness between the descriptors of segments of same group in the Euclidean space was loosely enforced by the classification layer. The probability of a segment belonging to a class was considered proportional to the dot product. The descriptors with small Euclidean distance were classified belonging to the same group. The candidate matches were generated by correlation between similarity and Euclidean distance between descriptors. The network was trained using SGD for minimizing the categorical cross-entropy.",
            "cite_spans": [
                {
                    "start": 224,
                    "end": 229,
                    "text": "[166]",
                    "ref_id": "BIBREF166"
                }
            ],
            "ref_spans": [],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "The second approach was training a Siamese convolutional neural network [167] in which two inputs were passed to two distinct CNNs. These two CNNs were considered as two identical descriptor extraction networks. Then, the combination of output of two networks was given to third network, which generates the final output. The advantage of Siamese over two stage detectors was that it allowed training of feature extraction simultaneously. Feature extraction and classifier were used independently during the inference process to boot the performance. It also used GSD for training to reduce the binary cross entropy of the network.",
            "cite_spans": [
                {
                    "start": 72,
                    "end": 77,
                    "text": "[167]",
                    "ref_id": "BIBREF167"
                }
            ],
            "ref_spans": [],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "The third approach was training the classifier with contrastive loss [165] for minimizing the Euclidean distance between the matching vectors while maximizing it for non-matching pairs. It recalculated the hard pairs (which had lowest Euclidean distance between their descriptors but the segments did not match and vice versa) at the end of each training epoch to increase the performance and avoid the local minima. Komorowski et al. [87] used 3D FPN [168] and sparse voxelized point cloud representation inspired by MinkowskiNet [169] to propose discriminative 3D point cloud descriptor for place recognition. The local feature extraction network and generalized mean (GeM) pooling [170] layer were the two main parts of its network architecture as shown in Figure 35 for PC-based place recognition. A set of 3D point coordinates was passed as input and quantized into a sparse, single channel tensor. It used 3D Feature Pyramid Network [168] for local feature extraction. The GeM, which was the generalization of global max and average pooling, was used for the generation of global descriptor vector.",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 74,
                    "text": "[165]",
                    "ref_id": "BIBREF165"
                },
                {
                    "start": 435,
                    "end": 439,
                    "text": "[87]",
                    "ref_id": "BIBREF87"
                },
                {
                    "start": 452,
                    "end": 457,
                    "text": "[168]",
                    "ref_id": "BIBREF168"
                },
                {
                    "start": 531,
                    "end": 536,
                    "text": "[169]",
                    "ref_id": "BIBREF169"
                },
                {
                    "start": 684,
                    "end": 689,
                    "text": "[170]",
                    "ref_id": "BIBREF170"
                },
                {
                    "start": 939,
                    "end": 944,
                    "text": "[168]",
                    "ref_id": "BIBREF168"
                }
            ],
            "ref_spans": [
                {
                    "start": 760,
                    "end": 769,
                    "text": "Figure 35",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "The network model was composed of four convolutional blocks that were used to generate sparse 3D feature maps and transposed convolution at its bottom-up and topdown parts, respectively. The top-down part was aimed at generating the upsampled feature map, which used lateral connection for concatenating with the features from the layers of bottom-up. It was intended to produce a feature map with a large respective field and high spatial resolution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "The bottom-up blocks from convolutional layer 1 to layer 3 were contained stride of two for decreasing the spatial resolution followed by residual block. batch normalization [171] layer and ReLU non-linearity were used for all layers in bottom-up blocks. Two 1x1 convolution blocks were aimed at unifying the feature maps channels of bottom-up blocks before they were concatenated in a top-down pass. ",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 179,
                    "text": "[171]",
                    "ref_id": "BIBREF171"
                }
            ],
            "ref_spans": [],
            "section": "LiDAR-Based 3DPR"
        },
        {
            "text": "This section reviews the methods that use fusion networks to generate global fusion descriptors based on camera-image and LiDAR PC for robust place recognition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "LiDAR-Camera Fusion-Based 3DPR"
        },
        {
            "text": "Xie et al. [88] presented the camera-LiDAR sensors fusion method, which robustly captures data from both sensors to solve the 3D place recognition problem. It introduced a trimmed clustering approach in 3D PC to reduce unrepresentative information for better recognition. They also built a compact neural network for robust representation of visual descriptor and 3D spatial global descriptor. It utilized deep neural network-based metric learning to minimize the distance of fused descriptors and to distinguish the similar and dissimilar places.",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 15,
                    "text": "[88]",
                    "ref_id": "BIBREF88"
                }
            ],
            "ref_spans": [],
            "section": "LiDAR-Camera Fusion-Based 3DPR"
        },
        {
            "text": "The image information and corresponding 3D PC were used as source input data. The PC data acquired form the LiDAR may vary in sizes. Deep learning based downsampling preprocess was applied to extract features from 3D source PC. It then used NN for generating compact representation of a place. CNN performed the place retrieval by learning mapping from the input data space S = (I, P) to a new space. The whole framework for place recognition in Figure 36 showed that mapping was performed by efficient feature extraction operator (blue, green and yellow blocks) and using the similarity metric for the evaluation feature descriptors (red block).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 446,
                    "end": 455,
                    "text": "Figure 36",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "LiDAR-Camera Fusion-Based 3DPR"
        },
        {
            "text": "They applied MLP and feature transform for local spatial feature extraction by mapping each 3D dimensional point into higher dimensional space. The local rotation invariant spatial features extracted by the CNN are in green block. It also introduced novel trimmed VLAD block for PC in which redundant information and environment disturbance were avoided by ignoring non-informative 3D PC clusters. It assigned the trimmed weight to meaningful clusters in partial aggregation process for obtaining the global descriptor (yellow block). It applied intra-normalization before vector concatenation, followed by L2 norm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "LiDAR-Camera Fusion-Based 3DPR"
        },
        {
            "text": "After the trimmed VLAD block, it used the fully connected layer to obtain useful features for Q-dimension compact global descriptor. Images contain many appearancebased features, which have mutual effects on the PC features. Features of camera-based images were extracted using ResNet50 [108] while the additional LiDAR sensor data was used to improve the place recognition in fused network. As a result, ResNet50 was used as image feature extractor, followed by L2 norm to make image and PC components in equal weights. Lu et al. [89] proposed a PC and image collaboration network (PIC-Net) shown in Figure 37 that fused image and PC features by attention method using DL approaches for large-scale place recognition. It mined the information of camera image with LiDAR PC and improved the place recognition performance by transforming the night image into daytime style. It used Resnet50 [108] to obtain a feature map from the image while PointNet [121] or LPD-Net [83] to extract features from PC. Then, both types of features were passed to the spatial attention layer for finding discriminative pixels and points with global channel attention layers for enhancing the features. Finally, the output of these three layers was used to generate final global features using an attention-based collaboration module.",
            "cite_spans": [
                {
                    "start": 287,
                    "end": 292,
                    "text": "[108]",
                    "ref_id": "BIBREF108"
                },
                {
                    "start": 531,
                    "end": 535,
                    "text": "[89]",
                    "ref_id": "BIBREF89"
                },
                {
                    "start": 890,
                    "end": 895,
                    "text": "[108]",
                    "ref_id": "BIBREF108"
                },
                {
                    "start": 950,
                    "end": 955,
                    "text": "[121]",
                    "ref_id": "BIBREF121"
                },
                {
                    "start": 967,
                    "end": 971,
                    "text": "[83]",
                    "ref_id": "BIBREF83"
                }
            ],
            "ref_spans": [
                {
                    "start": 601,
                    "end": 610,
                    "text": "Figure 37",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "LiDAR-Camera Fusion-Based 3DPR"
        },
        {
            "text": "The local spatial attention module shown was used in both images and point clouds for the selection of discriminative pixels and points. As shown in Figure 37 , the PointNet and LPD-Net both were used for point feature extraction, while ResNet50 (after removing the final pooling layer) was used for image feature extraction. It aimed at learning the spatial attention map of the image and PC as well as adding the attention map to the feature aggregation. NetVLAD was used for aggregating the local features. It learned the cluster centers and calculated the residual, which was weighted by learnable parameter and attention weight of correspond pixel or point.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 149,
                    "end": 158,
                    "text": "Figure 37",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "LiDAR-Camera Fusion-Based 3DPR"
        },
        {
            "text": "PCAN was replaced with a 1 \u00d7 1 \u00d7 D1 convolution layer for point cloud and 1 \u00d7 1 \u00d7 D2 convolution layer for image attention map learning. The Local channel attention module was used for learning the channel attention map to enhance the features from both PC and image branch before their fusion. For this, fully connected layer was implemented and then attention map was used to re-weight both image and PC features. The global channel attention was proposed to choose reliable features from PC and image branch. For this, global channel attention map was learned using fully connected layer for the selection of reliable features.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "LiDAR-Camera Fusion-Based 3DPR"
        },
        {
            "text": "We briefly summarize the 3DPR based on the reviewed methods in Section 3.2, which are listed in Table 19 . Applications of 3DPR were vastly more researched in outdoor environments (10 vs. indoor environment two studies) based on sensor modalities that include camera-based (two studies), LiDAR-based (eight studies), and camera-LiDAR fusion (two studies). These studies show that current DL-based approaches use convolutional techniques for place recognition [139] . Convolutional place recognition approaches for indoor and outdoor environments are an extension of object recognition techniques. However, they are more concentrated on larger scale targets called the place landmarks [129] .",
            "cite_spans": [
                {
                    "start": 459,
                    "end": 464,
                    "text": "[139]",
                    "ref_id": "BIBREF139"
                },
                {
                    "start": 684,
                    "end": 689,
                    "text": "[129]",
                    "ref_id": "BIBREF129"
                }
            ],
            "ref_spans": [
                {
                    "start": 96,
                    "end": 104,
                    "text": "Table 19",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Summary"
        },
        {
            "text": "Reliable place recognition is a challenging task due to changes in the environment and sensory ambiguity. Through the investigated studies, we found that LiDAR-based 3DPR methods were more robust to illumination, viewpoint change, and seasonal variations, which makes them competitive for outdoor 3DPR because of their longer-range capability compared to RGB-D cameras. Recent research work is more focused on DLbased applications in ADV, which shows that the integration of sensor-fusion process with recognition-based network structure for 3DPR is difficult.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Summary"
        },
        {
            "text": "However, studies show that the 3DPR task can be improved by considering the idea of using one sensor data to supervise the data of other sensors and integrating the map with sensor data for providing better environmental information to improve the detection. Furthermore, there is no optimal solution to handle the un-synchronization issue of multiple sensors. However, through the investigated studies, we found that its implicit solution can be learning from large-scale training data for landmark detection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Summary"
        },
        {
            "text": "Many public and new datasets have been developed for training the DL-based models. This section presents 3D datasets used in the studies that were reviewed in Sections 3.1 and 3.2 for 3D object and place recognition tasks in the current review. We list the datasets used by each study in Table 20 . Several methods discussed in the survey illustrate that KITTI dataset [172] published in 2012 by [173] is the most frequently used dataset for 3DOR tasks. The review shows that many 3DOR models (19 out of 23 studies) have used the KITTI dataset. This dataset has been updated many times since its first release.",
            "cite_spans": [
                {
                    "start": 369,
                    "end": 374,
                    "text": "[172]",
                    "ref_id": "BIBREF172"
                },
                {
                    "start": 396,
                    "end": 401,
                    "text": "[173]",
                    "ref_id": "BIBREF173"
                }
            ],
            "ref_spans": [
                {
                    "start": 288,
                    "end": 296,
                    "text": "Table 20",
                    "ref_id": "TABREF15"
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "Current review shows that OXford RobotCar dataset [174] published in 2017 by [175] has gained attention from several ADV studies to perform 3DPR tasks. In the current survey, 7 out of 12 3DPR studies have used the Oxford RobotCar dataset. It contains over 1000 km of recorded driving of a consistent route with over 100 repetitions. It collected almost 20 million images from six cameras, along with the LiDAR and GPS.",
            "cite_spans": [
                {
                    "start": 50,
                    "end": 55,
                    "text": "[174]",
                    "ref_id": "BIBREF174"
                },
                {
                    "start": 77,
                    "end": 82,
                    "text": "[175]",
                    "ref_id": "BIBREF175"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "A series of recent studies has also indicated that many research institutes have designed their datasets, such as the Waymo open dataset, HKUST, KAIST, and NYUD2 datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Waymo is an open dataset [176] released recently by [177] for autonomous driving vehicles. It is a large dataset consisting of 1150 scenes and each scene is spanned 20 s. It is also well-synchronized dataset with 3D BBox in LiDAR data and 2D BBox in camera images. In this review, one study [62] used the Waymo dataset for training one-stage detector to recognize the objects in outdoor environment.",
            "cite_spans": [
                {
                    "start": 25,
                    "end": 30,
                    "text": "[176]",
                    "ref_id": "BIBREF176"
                },
                {
                    "start": 52,
                    "end": 57,
                    "text": "[177]",
                    "ref_id": "BIBREF177"
                },
                {
                    "start": 291,
                    "end": 295,
                    "text": "[62]",
                    "ref_id": "BIBREF62"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "The HKUST dataset was captured by [82] for 3DPR task in their study. In this dataset, each shot is contained on a grayscale image and a point cloud. The KAIST dataset [178] was proposed by [179] to provide LiDAR and stereo images of complex urban scenes. One [88] among the reviewed studies used the KAIST dataset to perform 3DPR tasks. NYUD2 is a kinect dataset [180] that was used by one 3DPR study [78] in this survey. It was introduced by [181] with 1449 RGBD images and 26 scene classes of commercial and residential buildings. Some networks (three 3DPR studies [80, 83, 86] ) have used the in-house dataset that includes university sector, residential area, and business direct. This dataset was created by [80] using LiDAR sensors on the car driven in four regions at 10, 10, 8, and 5 km routes.",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 38,
                    "text": "[82]",
                    "ref_id": "BIBREF82"
                },
                {
                    "start": 167,
                    "end": 172,
                    "text": "[178]",
                    "ref_id": null
                },
                {
                    "start": 189,
                    "end": 194,
                    "text": "[179]",
                    "ref_id": "BIBREF179"
                },
                {
                    "start": 259,
                    "end": 263,
                    "text": "[88]",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 363,
                    "end": 368,
                    "text": "[180]",
                    "ref_id": null
                },
                {
                    "start": 401,
                    "end": 405,
                    "text": "[78]",
                    "ref_id": "BIBREF78"
                },
                {
                    "start": 443,
                    "end": 448,
                    "text": "[181]",
                    "ref_id": "BIBREF181"
                },
                {
                    "start": 567,
                    "end": 571,
                    "text": "[80,",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 572,
                    "end": 575,
                    "text": "83,",
                    "ref_id": "BIBREF83"
                },
                {
                    "start": 576,
                    "end": 579,
                    "text": "86]",
                    "ref_id": "BIBREF86"
                },
                {
                    "start": 713,
                    "end": 717,
                    "text": "[80]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "The SUN RGB-D dataset [182] used by one 3DPR [78] and two 3DOR [70, 71] studies was presented by [183] . It contains 10,355 RGB-D scene images as training set and 2860 images as testing set for 3D object detection, which is fundamental for scene understating. ISIA RGB-D dataset is proposed by [78] for use in their own study for 3DPR task. It is a video dataset to evaluate RGB scene recognition videos. It contains more than five hours of footage of the indoor environment in 278 videos. it reuses 58 categories of the MIT indoor scene database [184] .",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 27,
                    "text": "[182]",
                    "ref_id": "BIBREF182"
                },
                {
                    "start": 45,
                    "end": 49,
                    "text": "[78]",
                    "ref_id": "BIBREF78"
                },
                {
                    "start": 63,
                    "end": 67,
                    "text": "[70,",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 68,
                    "end": 71,
                    "text": "71]",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 97,
                    "end": 102,
                    "text": "[183]",
                    "ref_id": "BIBREF183"
                },
                {
                    "start": 294,
                    "end": 298,
                    "text": "[78]",
                    "ref_id": "BIBREF78"
                },
                {
                    "start": 547,
                    "end": 552,
                    "text": "[184]",
                    "ref_id": "BIBREF184"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "The multi vehicle stereo event camera dataset also called MVSEC [185] is a collection of 3D perception data that was presented by [186] for event-based cameras. It has been used by the model in [79] to perform 3D place recognition task. Its stereo event data has been collected from a car, bike, handheld, and hexacopter in both indoor and outdoor environments.",
            "cite_spans": [
                {
                    "start": 64,
                    "end": 69,
                    "text": "[185]",
                    "ref_id": "BIBREF185"
                },
                {
                    "start": 130,
                    "end": 135,
                    "text": "[186]",
                    "ref_id": "BIBREF186"
                },
                {
                    "start": 194,
                    "end": 198,
                    "text": "[79]",
                    "ref_id": "BIBREF79"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "The DDD17 dataset DDD17Dataset used in one 3DPR study [79] was introduced by [187] . It contains annotated dynamic and active-pixel vision sensors' recordings, which consist of over 12 h of video in city driving at night, daytime, and evening in different weather conditions and vehicle speed. The ScanNet dataset was reported in [188] . It has been used by two 3DOR [70, 71] and one 3DPR [81] studies in the current survey. It is an RGB-D video dataset containing 1513 scenes that are annotated with 3D camera poses. The research community has used this dataset for 3D scene understanding and semantic voxel labeling tasks.",
            "cite_spans": [
                {
                    "start": 54,
                    "end": 58,
                    "text": "[79]",
                    "ref_id": "BIBREF79"
                },
                {
                    "start": 77,
                    "end": 82,
                    "text": "[187]",
                    "ref_id": "BIBREF187"
                },
                {
                    "start": 330,
                    "end": 335,
                    "text": "[188]",
                    "ref_id": "BIBREF188"
                },
                {
                    "start": 367,
                    "end": 371,
                    "text": "[70,",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 372,
                    "end": 375,
                    "text": "71]",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 389,
                    "end": 393,
                    "text": "[81]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "The NCLT dataset [189] used by one 3DPR study [84] in this review, was documented in [190] . It is a long-term autonomy dataset for robotic research, which was collected using a Segway robot by 3D LiDAR, GPS, planar LiDAR along with proprioceptive sensors. Argoverse dataset [191] is introduced by [192] to support machine learning tasks for object detection in outdoor environment. A recent study [65] in the survey used this dataset for 3DOR task. It is mainly designed for 3D tracking and motion forecasting. Its 3D tracking dataset contains 360 \u2022 images taken from seven cameras with 3D point clouds from LiDAR while its motion forecasting dataset contains 300,000 tracked scenarios. It also includes 290 km \"HD maps\".",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 22,
                    "text": "[189]",
                    "ref_id": "BIBREF189"
                },
                {
                    "start": 46,
                    "end": 50,
                    "text": "[84]",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 85,
                    "end": 90,
                    "text": "[190]",
                    "ref_id": "BIBREF190"
                },
                {
                    "start": 275,
                    "end": 280,
                    "text": "[191]",
                    "ref_id": null
                },
                {
                    "start": 298,
                    "end": 303,
                    "text": "[192]",
                    "ref_id": "BIBREF192"
                },
                {
                    "start": 398,
                    "end": 402,
                    "text": "[65]",
                    "ref_id": "BIBREF65"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Section 4 presented 14 datasets that have been used by 35 studies. The Sun RGB-D, KITTI, and ScanNet datasets have been used for both 3DOR and 3DPR tasks. However, KITTI is the most frequently used dataset for 3DOR tasks (used by 20/23 studies), while Oxford Robot-car is a widely used dataset for scene understanding to perform 3DPR tasks (7/12 studies) in autonomous driving vehicles.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Summary"
        },
        {
            "text": "Section 5 analyzes and compares the existing results in the context of different datasets (discussed in Section 4) to present the performance of the methods that have reviewed in Sections 3.1 and 3.2 for 3DOR and 3DPR tasks. The evaluation metrics that have been used for the KITTI dataset include average precision (AP) of Intersection over Union (IoU) for both bird's eye view (AP bev ) and 3D object detection (AP 3D ) along with the average orientation similarity (AOS) [173] and average localization precision (ALP). AP, AOS, and ALP metrics are divided into easy, moderate, and hard according to difficulty levels of 3D object detection, which are height, occlusion, and truncation for all three categories: cars, pedestrians, and cyclists. The recall @ 1 %, AUC, and accuracy % are the metrics that were used to compare the performance of 3DPR tasks on different 3D detection datasets.",
            "cite_spans": [
                {
                    "start": 474,
                    "end": 479,
                    "text": "[173]",
                    "ref_id": "BIBREF173"
                }
            ],
            "ref_spans": [],
            "section": "Performance Evaluation"
        },
        {
            "text": "For performance evaluation based on the KITTI dataset, Mono Pair [55] uses 40-point interpolated average precision metric AP 40 , which is evaluated at both the bird-eye view AP bev and the 3D bounding box AP 3d . It reports AP with intersection over union (IoU) using 0.7 as thresholds for cars, pedestrians and cyclists detection. Tables 21 and 22 shows the performance of one-stage anchor-free detector of [55] on the KITTI validation and test sets for the car category, while performance for pedestrians and cyclists on the KITTI test is shown in Tables 23 and 24, respectively. It can also perform inference in real-time as 57 ms per image, which is higher than [106] .",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 69,
                    "text": "[55]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 409,
                    "end": 413,
                    "text": "[55]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 667,
                    "end": 672,
                    "text": "[106]",
                    "ref_id": "BIBREF106"
                }
            ],
            "ref_spans": [
                {
                    "start": 333,
                    "end": 349,
                    "text": "Tables 21 and 22",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "GS3D [56] evaluated the framework on the KITTI object detection benchmark and follows [193] to use two train/validation (val) splits. Its experiments were mainly focused on the car category. Tables 21 and 22 show the evaluation results of 3D detection accuracy on the KITTI for car category using the metric of AP 3D on two validation sets val 1 and val 2 . The performance on val 2 is higher than [102] for 3D object detection in autonomous driving. In [56] , researchers used the metric of Average Localization Precision (ALP) and outperformed [193] . Table 21 presents the results of [56] for car category evaluated using the metric of ALP with the results on the two validation sets val 1 /val 2 .",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[56]",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 86,
                    "end": 91,
                    "text": "[193]",
                    "ref_id": "BIBREF193"
                },
                {
                    "start": 398,
                    "end": 403,
                    "text": "[102]",
                    "ref_id": "BIBREF102"
                },
                {
                    "start": 454,
                    "end": 458,
                    "text": "[56]",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 546,
                    "end": 551,
                    "text": "[193]",
                    "ref_id": "BIBREF193"
                },
                {
                    "start": 587,
                    "end": 591,
                    "text": "[56]",
                    "ref_id": "BIBREF56"
                }
            ],
            "ref_spans": [
                {
                    "start": 191,
                    "end": 207,
                    "text": "Tables 21 and 22",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 554,
                    "end": 562,
                    "text": "Table 21",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "SS3D [57] evaluated its proposed methods primarily on the KITTI object detection benchmark. It focused on three categories car, pedestrian and cyclist, which are most relevant for autonomous vehicle applications. The metric used for [57] evaluation is the average precision (AP), where valid detection is specified if the IoU is at 0.7, in bird's-eyeview and in 3D, respectively.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[57]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 233,
                    "end": 237,
                    "text": "[57]",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [],
            "section": "Performance Evaluation"
        },
        {
            "text": "The researchers in [57] used the same validation splits and called them split-1 [194] and split-2 [195] , which divided the training data almost in half and performed the training on all three categories simultaneously. Table 21 shows AP with the 3D IoU detection criterion on validation set for the Cars class with a clear ranking Method 1 \u227a Method 2 \u227a Method 3 in terms of their performance. It also represents the results using the ALP metric. J\u00f6rgensen et al. [57] used inference on the KITTI test set and the evaluation results on test data for cars in Table 22 , while pedestrians and cyclists classes in bird's-eye-view (AP bv ) and in 3D (AP 3D ) are presented in Table 24 .",
            "cite_spans": [
                {
                    "start": 19,
                    "end": 23,
                    "text": "[57]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 80,
                    "end": 85,
                    "text": "[194]",
                    "ref_id": "BIBREF194"
                },
                {
                    "start": 98,
                    "end": 103,
                    "text": "[195]",
                    "ref_id": "BIBREF195"
                },
                {
                    "start": 464,
                    "end": 468,
                    "text": "[57]",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [
                {
                    "start": 220,
                    "end": 228,
                    "text": "Table 21",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 558,
                    "end": 566,
                    "text": "Table 22",
                    "ref_id": null
                },
                {
                    "start": 672,
                    "end": 680,
                    "text": "Table 24",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "M3DSSD [58] evaluated the proposed framework on the challenging KITTI benchmark for 3D object detection covering three main categories of objects: cars, pedestrians, and cyclists. AP scores on validation and test sets of 3D object detection and bird's eye view for cars are shown in Tables 21 and 22 , while the 3D detection performance for pedestrians and cyclists on test set at a 0.5 IoU threshold is reported in Table 24 .",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 11,
                    "text": "[58]",
                    "ref_id": "BIBREF58"
                }
            ],
            "ref_spans": [
                {
                    "start": 283,
                    "end": 299,
                    "text": "Tables 21 and 22",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 416,
                    "end": 424,
                    "text": "Table 24",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "SRCNN [59] evaluated the proposed model using Average Precision for bird's eye view (AP bv ) and 3D box (AP 3D ) on the KITTI car validation and test sets, while the results are reported in Tables 21 and 22 , respectively. It outperforms state-of-the-art monocularbased methods [34, 196] and stereo-method [197] by large margins. Specifically, for easy and moderate sets, it outperforms 3DOP [197] over 30% for both AP bev and AP 3D while for the hard set, it achieved \u223c25% improvements.",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 10,
                    "text": "[59]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 278,
                    "end": 282,
                    "text": "[34,",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 283,
                    "end": 287,
                    "text": "196]",
                    "ref_id": "BIBREF196"
                },
                {
                    "start": 306,
                    "end": 311,
                    "text": "[197]",
                    "ref_id": "BIBREF197"
                },
                {
                    "start": 392,
                    "end": 397,
                    "text": "[197]",
                    "ref_id": "BIBREF197"
                }
            ],
            "ref_spans": [
                {
                    "start": 190,
                    "end": 206,
                    "text": "Tables 21 and 22",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "CenterNet [60] used restnet18 [108] and dla-34 [105] as backbone of its three methods and showed that its methods are superior to the previous monocular-based methods. The performance on AP bev and AP 3D for car 3D localization and detection on the KITTI validation set is shown in Table 21 .",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 14,
                    "text": "[60]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 30,
                    "end": 35,
                    "text": "[108]",
                    "ref_id": "BIBREF108"
                },
                {
                    "start": 47,
                    "end": 52,
                    "text": "[105]",
                    "ref_id": "BIBREF105"
                }
            ],
            "ref_spans": [
                {
                    "start": 282,
                    "end": 290,
                    "text": "Table 21",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "RT3D [61] evaluated the proposed method on the KITTI for autonomous driving and divides the samples in training and validation sets exactly the same as [194] . The results of both 3D localization and 3D detection evaluations are obtained using Average Precision (AP loc ) and (AP 3D ), as reported in Tables 21 and 22 respectively. It is 2.5\u00d7 faster than the [114] . Its detection time of 0.089 s allows it to be deployed in real-time systems and it achieves at least 13% higher accuracy compared to [102, 194, 198] . AFDet [62] evaluated the results using average precision (AP) metric as shown in Table 21 , where the IoU threshold was 0.7 for the car class. They did not use complex post-processing process and NMS to filter out the results.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[61]",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 152,
                    "end": 157,
                    "text": "[194]",
                    "ref_id": "BIBREF194"
                },
                {
                    "start": 359,
                    "end": 364,
                    "text": "[114]",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 500,
                    "end": 505,
                    "text": "[102,",
                    "ref_id": "BIBREF102"
                },
                {
                    "start": 506,
                    "end": 510,
                    "text": "194,",
                    "ref_id": "BIBREF194"
                },
                {
                    "start": 511,
                    "end": 515,
                    "text": "198]",
                    "ref_id": "BIBREF198"
                },
                {
                    "start": 524,
                    "end": 528,
                    "text": "[62]",
                    "ref_id": "BIBREF62"
                }
            ],
            "ref_spans": [
                {
                    "start": 301,
                    "end": 317,
                    "text": "Tables 21 and 22",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 599,
                    "end": 607,
                    "text": "Table 21",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "SegV Net [63] evaluated the 3D vehicle detection results on the KITTI test dataset using AP bev and AP 3D metrics, as shown in Table 22 , while the results on validation dataset with AP 3D metric and orientation estimation (AOS) are reported in Table 21 . It outperformed LiDAR only single stage methods [111, 113] in 3D vehicle detection.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 13,
                    "text": "[63]",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 304,
                    "end": 309,
                    "text": "[111,",
                    "ref_id": "BIBREF111"
                },
                {
                    "start": 310,
                    "end": 314,
                    "text": "113]",
                    "ref_id": "BIBREF113"
                }
            ],
            "ref_spans": [
                {
                    "start": 127,
                    "end": 135,
                    "text": "Table 22",
                    "ref_id": null
                },
                {
                    "start": 245,
                    "end": 253,
                    "text": "Table 21",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "SECONDX [64] supports cars, pedestrians and cyclists' categories with a single model and outperforms other methods for all APs in three classes. Its evaluation results on the KITTI validation set are given in Tables 21 and 23 . It runs in real time without increasing memory usage and inference time compared with [120] .",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[64]",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 314,
                    "end": 319,
                    "text": "[120]",
                    "ref_id": "BIBREF120"
                }
            ],
            "ref_spans": [
                {
                    "start": 209,
                    "end": 225,
                    "text": "Tables 21 and 23",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "IPOD [66] follows AP metrics for all three classes where the IoU threshold is 0.7 for car class and 0.5 for pedestrians and cyclists classes. For evaluation on the test set, the model used train/val sets at a ratio of 4:1. The performance of the method is listed in Tables 21-24 . Yang et al. [66] showed that compared to [199] , the detection accuracy of IPOD on hard set has improved by 2.52%, and 4.14% on BEV and 3D respectively. Similarly, compared to [73, 120] it performs better in pedestrian prediction by 6.12%, 1.87%, and 1.51% on the easy, moderate, and hard levels, respectively.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[66]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 293,
                    "end": 297,
                    "text": "[66]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 322,
                    "end": 327,
                    "text": "[199]",
                    "ref_id": "BIBREF199"
                },
                {
                    "start": 457,
                    "end": 461,
                    "text": "[73,",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 462,
                    "end": 466,
                    "text": "120]",
                    "ref_id": "BIBREF120"
                }
            ],
            "ref_spans": [
                {
                    "start": 266,
                    "end": 278,
                    "text": "Tables 21-24",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "FVNet [67] presents the performance for cars category at 0.7 IoU using AP bev and AP 3D and for the pedestrians and cyclists categories at 0.5 IoU using AP 3D metric on the KITTI test dataset, as shown in Tables 22 and 24 . It achieved significant better results despite using the raw point clouds, and its inference time was 12 ms. Compared to [73] , it performs best on all three categories except the car detection in easy setting, which employs both front-view and bird's-eye-view.",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 10,
                    "text": "[67]",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 345,
                    "end": 349,
                    "text": "[73]",
                    "ref_id": "BIBREF73"
                }
            ],
            "ref_spans": [
                {
                    "start": 205,
                    "end": 221,
                    "text": "Tables 22 and 24",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "In DPointNet [68] , the dataset includes three categories of car, pedestrian, and cyclist. However, it only evaluates the car class for its rich data. Tables 21 and 22 show its performance on the KITTI validation and test sets respectively using the average precision (AP) of car class with a 0.7 IoU threshold. Li et al. [68] demonstrated that the effectiveness of proposed DPointNet on the KITTI validation set has increased from 0.4% to 0.6%, with only about 60% running time.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[68]",
                    "ref_id": "BIBREF68"
                },
                {
                    "start": 322,
                    "end": 326,
                    "text": "[68]",
                    "ref_id": "BIBREF68"
                }
            ],
            "ref_spans": [
                {
                    "start": 151,
                    "end": 167,
                    "text": "Tables 21 and 22",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "Point-GCNN [69] used the KITTI benchmark to evaluate the average precision (AP) of three types of objects: car, pedestrian and cyclist. Following [111, 114, 200] , it handles scale differences by training one network for the car and another network for both the pedestrian and cyclist. The AP results of 3D and BEV object detection on the KITTI test set for all three categories are shown in Tables 22 and 24 . It achieved good results for car detection on easy and moderate levels, for cyclist detection on moderate and hard levels while it surpasses previous approaches by 3.45. The reason of low pedestrian detection compared to its car and cyclist classes is that vertices are not dense enough to obtain more accurate bboxes.",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 15,
                    "text": "[69]",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 146,
                    "end": 151,
                    "text": "[111,",
                    "ref_id": "BIBREF111"
                },
                {
                    "start": 152,
                    "end": 156,
                    "text": "114,",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 157,
                    "end": 161,
                    "text": "200]",
                    "ref_id": "BIBREF200"
                }
            ],
            "ref_spans": [
                {
                    "start": 392,
                    "end": 408,
                    "text": "Tables 22 and 24",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "S-AT GCN [72] evaluated 3D detection results using the 3D and BEV average precession at 0.7 IoU threshold for the car class and 0.5 IoU threshold for the pedestrian and cyclist classes. The results on the KITTI validation data are reported in Tables 21 and  23 . Its method 1 indicates the results of self-attention (AT) without dimension reduction while method 2 represents the results of self-attention with dimension reduction (ATRD). Compared to method 1, the second method performrf better for car detection on all three difficulty levels, pedestrians at the hard difficulty level, and cyclists at moderate and hard difficulty levels. Wang et al. [72] described that adding feature enhancement layer with self-attention, can bring extra 1% and 2-3% improvement for its pedestrians and cyclists' detection.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 13,
                    "text": "[72]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 652,
                    "end": 656,
                    "text": "[72]",
                    "ref_id": "BIBREF72"
                }
            ],
            "ref_spans": [
                {
                    "start": 243,
                    "end": 260,
                    "text": "Tables 21 and  23",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "MV3D [73] followed [194] to split training set and validation set, each containing about half of the whole dataset. It only focused on car category and performed the evaluation on three difficulty regimes: easy, moderate, and hard. The results using AP 3D and AP loc at IOU=0.7 on validation set are shown in Table 21 . Chen et al. [73] has showed that the proposed method [73] performed better than [41] by AP loc under IoU threshold 0.7 and achieves \u223c45% higher AP loc across easy, moderate, and hard regimes. Similarly it obtained \u223c30% higher AP 3D over [41] with criteria of IoU=0.7, and reaches at 71.29% AP 3D on easy level.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[73]",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 19,
                    "end": 24,
                    "text": "[194]",
                    "ref_id": "BIBREF194"
                },
                {
                    "start": 332,
                    "end": 336,
                    "text": "[73]",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 373,
                    "end": 377,
                    "text": "[73]",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 400,
                    "end": 404,
                    "text": "[41]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 557,
                    "end": 561,
                    "text": "[41]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [
                {
                    "start": 309,
                    "end": 317,
                    "text": "Table 21",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "BEVLFVC [74] evaluated the pedestrian detection results using 3D detection average precision AP 3D on the KITTI validation dataset, as shown in Table 23 . Wang et al. described that its highest performance on validation set can be achieved by fusing [114, 126] with the proposed sparse non-homogeneous pooling layer and one-stage detection network.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[74]",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 250,
                    "end": 255,
                    "text": "[114,",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 256,
                    "end": 260,
                    "text": "126]",
                    "ref_id": "BIBREF126"
                }
            ],
            "ref_spans": [
                {
                    "start": 144,
                    "end": 152,
                    "text": "Table 23",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "D3PD [75] trained the model using different hyper parameters and evaluated the validation split using AP 3D metric for pedestrian detection, as shown in Table 23 . Roth et al. [75] illustrated that the highest performance can be obtained using concatenation feature combination in the detection network and showed that deep fusion scheme performs slightly better than early fusion scheme.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[75]",
                    "ref_id": "BIBREF75"
                },
                {
                    "start": 176,
                    "end": 180,
                    "text": "[75]",
                    "ref_id": "BIBREF75"
                }
            ],
            "ref_spans": [
                {
                    "start": 153,
                    "end": 161,
                    "text": "Table 23",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "MVX-Net [76] splits the training set into train and validation sets and does not include the samples from same sequences in both sets [73] . It evaluated the 3D car detection performance using AP metric in 3D and bird's eye view for validation and test sets as shown in Tables 21 and 22 . The experimental results show that [76] with point fusion significantly improves the score of mean average precision.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[76]",
                    "ref_id": "BIBREF76"
                },
                {
                    "start": 134,
                    "end": 138,
                    "text": "[73]",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 324,
                    "end": 328,
                    "text": "[76]",
                    "ref_id": "BIBREF76"
                }
            ],
            "ref_spans": [
                {
                    "start": 270,
                    "end": 286,
                    "text": "Tables 21 and 22",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "SharedNet [77] achieves competitive results compared with other state-of-the-art methods. The results in the KITTI validation and test dataset for three classes (cars, pedestrians, and cyclists) were evaluated on mean average precision metric. The results for car validation and test set are given in Tables 21 and 22 respectively while for pedestrian and cyclist categories on validation set are listed in Table 23 . Wen et al. [77] illustrates that the proposed model [77] competes with [199, 201] in comprehensive performance. For the cyclist class, it outperforms the [201] while in the car class, it is 2\u00d7 faster than [201] .",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 14,
                    "text": "[77]",
                    "ref_id": "BIBREF77"
                },
                {
                    "start": 429,
                    "end": 433,
                    "text": "[77]",
                    "ref_id": "BIBREF77"
                },
                {
                    "start": 470,
                    "end": 474,
                    "text": "[77]",
                    "ref_id": "BIBREF77"
                },
                {
                    "start": 489,
                    "end": 494,
                    "text": "[199,",
                    "ref_id": "BIBREF199"
                },
                {
                    "start": 495,
                    "end": 499,
                    "text": "201]",
                    "ref_id": "BIBREF201"
                },
                {
                    "start": 572,
                    "end": 577,
                    "text": "[201]",
                    "ref_id": "BIBREF201"
                },
                {
                    "start": 623,
                    "end": 628,
                    "text": "[201]",
                    "ref_id": "BIBREF201"
                }
            ],
            "ref_spans": [
                {
                    "start": 301,
                    "end": 317,
                    "text": "Tables 21 and 22",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 407,
                    "end": 415,
                    "text": "Table 23",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "SDes-Net [86] trains and tests different descriptor extraction models on real world data from the KITTI dataset. It evaluates their performance for 3DPR tasks to determine matching and non-matching pairs of segments, and to obtain the correct candidate matches. First, it compares the general accuracy of different descriptors using positive and negative pairs of segments from the test set. The experimental results show that Siamese network [167] achieves the best overall classification accuracy, which is about 80%, listed in Table 25 . The second comparison among descriptors was conducted to find the potential descriptor for generating candidate matches based on the closest neighbor in the euclidean descriptor-space. The experimental results demonstrates that the group-based classifier and feature extraction network that was trained using contrastive loss function [165] performed the best with around 50% positive matches, while the Siamese network [167] had only around 30% positive matches.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 13,
                    "text": "[86]",
                    "ref_id": "BIBREF86"
                },
                {
                    "start": 443,
                    "end": 448,
                    "text": "[167]",
                    "ref_id": "BIBREF167"
                },
                {
                    "start": 876,
                    "end": 881,
                    "text": "[165]",
                    "ref_id": "BIBREF165"
                },
                {
                    "start": 961,
                    "end": 966,
                    "text": "[167]",
                    "ref_id": "BIBREF167"
                }
            ],
            "ref_spans": [
                {
                    "start": 530,
                    "end": 538,
                    "text": "Table 25",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "OREOS [84] demonstrates the place recognition performance on NCLT and KITTI datasets for an increasing number of nearest place candidates retrieved from the map. with recall in % that is 96.7 on the KITTI dataset and 98.2 on NCLT dataset as shown in Table 25 .",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 10,
                    "text": "[84]",
                    "ref_id": "BIBREF84"
                }
            ],
            "ref_spans": [
                {
                    "start": 250,
                    "end": 258,
                    "text": "Table 25",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "CLFD-Net [88] uses KITTI and KAIST datasets for place recognition task. KITTI dataset supplies 11 scenes containing accurate odometry ground truth information. These scenes are used in experiments and referred as KITTI 00, \u00b7 \u00b7 \u00b7, KITTI 10. It has potential to be applied in the field of autonomous driving or robotic systems with a recall @1%. The performance is 98.1 for KITTI 00 scene, which is 1.7% higher than [80] , and 2.5% higher than [108] . The performance on KAIST3two scene is 95.2, which is 8.5% higher than [80] , and 6.9% higher than [108] . The overall performance of model [88] on the KITTI dataset with average recall @ 1% is higher than KAIST dataset as shown in Table 25 . Table 26 illustrates the performance of proposed network in [65] for vehicle and pedestrian detection using the standard average precision for 3D detection (AP 3D ) and on the bird's eye view (AP bv ). The AP scores are measured at IOU = 0.7 threshold for car class, and IOU = 0.5 for pedestrian class with a reasonable inference speed (30FPS). RGNet [70] and HGNet [71] used the ScanNet and Sun RGB-D datasets to perform 3DOR tasks while [81] used ScanNet dataset for 3DPR task. In [70] , the network model performs better on 15/18 classes for 3D object (i.e., chair, table, bed etc.) detection task using ScanNet dataset and evaluates the performance using mean average precession, which is given in Table 27 as model accuracy is 48.5 in terms of mAP @ 0.25. Its 3D object detection in point cloud on Sun RGB-D dataset showed the overall performance is 59.2 on 6/10 object classes with mAP @ 0.25. Table 27 .",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 13,
                    "text": "[88]",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 414,
                    "end": 418,
                    "text": "[80]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 442,
                    "end": 447,
                    "text": "[108]",
                    "ref_id": "BIBREF108"
                },
                {
                    "start": 520,
                    "end": 524,
                    "text": "[80]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 548,
                    "end": 553,
                    "text": "[108]",
                    "ref_id": "BIBREF108"
                },
                {
                    "start": 589,
                    "end": 593,
                    "text": "[88]",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 752,
                    "end": 756,
                    "text": "[65]",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "text": "[70]",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "text": "[71]",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 1131,
                    "end": 1135,
                    "text": "[81]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 1175,
                    "end": 1179,
                    "text": "[70]",
                    "ref_id": "BIBREF70"
                }
            ],
            "ref_spans": [
                {
                    "start": 681,
                    "end": 689,
                    "text": "Table 25",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 692,
                    "end": 700,
                    "text": "Table 26",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 1394,
                    "end": 1402,
                    "text": "Table 27",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 1592,
                    "end": 1600,
                    "text": "Table 27",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "RGBD-Net [78] evaluated the scene recognition results on NYUD2, SUN RGB-D and the ISIA RGB-D dataset for 3DPR task. It follows the split by [181] to recognize 27 indoor categories of NYUD2 dataset into 10 categories. Scene categories in the SUN RGB-D dataset are 40 and in the ISIA RGB-D video database are eight. It contains 60 % data of each category for training and 40 % for testing. Following [183] , it uses ther mean class accuracy for the evaluation and comparisons of results, which are shown in Table 27 .",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 13,
                    "text": "[78]",
                    "ref_id": "BIBREF78"
                },
                {
                    "start": 140,
                    "end": 145,
                    "text": "[181]",
                    "ref_id": "BIBREF181"
                },
                {
                    "start": 398,
                    "end": 403,
                    "text": "[183]",
                    "ref_id": "BIBREF183"
                }
            ],
            "ref_spans": [
                {
                    "start": 505,
                    "end": 513,
                    "text": "Table 27",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "ISR-Net [81] uses the ScanNet benchmark to present the scene classification results for place recognition (library, bedroom, kitchen, etc) and achieves an average recall of 0.70 as shown in Table 27 . It performs better on 11/1three scenes and jumps to 70.0% recall compared to [202] , which has an average recall of at most 49.8%.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[81]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 278,
                    "end": 283,
                    "text": "[202]",
                    "ref_id": "BIBREF202"
                }
            ],
            "ref_spans": [
                {
                    "start": 190,
                    "end": 198,
                    "text": "Table 27",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "In Pointnetvlad [80] , the performance on average recall at 1% is evaluated using the Oxford dataset and three in-house datasets. It achieved reasonable results, which are 80.31, 72.63, 60.27, and 65.3 for the Oxford, U.S., R.A., and B.D. datasets, respectively, as shown in Table 28 . MinkLoc3D [87] evaluated the experimental results on the Oxford dataset and three in-house datasets that were acquired using LiDARs with different characteristics. The evaluation results of place recognition model on Oxford Robot-car dataset have achieved 97.9 average recall at 1 %, which is higher than [83] . When [87] model is evaluated on three in-house datasets, its performance compared to [83] is 1.0 and 0.6 p.p. lower for U.S. and B.D. sets that is 95.0 and 88.5 respectively while 0.7 p.p. higher for R.A. set. The results are listed in Table 28 .",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 20,
                    "text": "[80]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 296,
                    "end": 300,
                    "text": "[87]",
                    "ref_id": "BIBREF87"
                },
                {
                    "start": 591,
                    "end": 595,
                    "text": "[83]",
                    "ref_id": "BIBREF83"
                },
                {
                    "start": 603,
                    "end": 607,
                    "text": "[87]",
                    "ref_id": "BIBREF87"
                },
                {
                    "start": 683,
                    "end": 687,
                    "text": "[83]",
                    "ref_id": "BIBREF83"
                }
            ],
            "ref_spans": [
                {
                    "start": 275,
                    "end": 283,
                    "text": "Table 28",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 834,
                    "end": 842,
                    "text": "Table 28",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "The experimental results of PIC-Net [89] show the performance of its optimal configuration is 98.23% on average with the recall @ 1, as shown in Table 28 , which is about 0.52% better than the direct concatenation.",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "text": "[89]",
                    "ref_id": "BIBREF89"
                }
            ],
            "ref_spans": [
                {
                    "start": 145,
                    "end": 153,
                    "text": "Table 28",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "Lpd-net [83] evaluated the network model on the three In-House datasets and achieved 96.00, 90.46 and 89.14 average recall @ 1 % for U.S., R.A., and B.D. sets, shown in Table 28 . It is trained only on the Oxford Robotcar dataset and directly test it on the In-House dataset.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[83]",
                    "ref_id": "BIBREF83"
                }
            ],
            "ref_spans": [
                {
                    "start": 169,
                    "end": 177,
                    "text": "Table 28",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "SDM-Net [85] considers ten place recognition cases and uses area under the precisionrecall curve (AUC) to evaluate the sequence pairs for representative cases. The results for all of them are reported in Table 28 . It outperforms [152] , in six out of ten cases.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[85]",
                    "ref_id": "BIBREF85"
                },
                {
                    "start": 230,
                    "end": 235,
                    "text": "[152]",
                    "ref_id": "BIBREF152"
                }
            ],
            "ref_spans": [
                {
                    "start": 204,
                    "end": 212,
                    "text": "Table 28",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "In Event-VPR [79] the performance of proposed method is evaluated on MVSEC and Oxford RobotCar datasets, and the results are listed in Table 28 . On the MVSEC dataset, two daytime and three nighttime sequences are trained together, and then each of them is tested separately. The recall @ 1 % of its model in night sequences has achieved 97.05% on average while almost the same at daytime sequences. On the Oxford RobotCar dataset, it shows the model performance for place recognition under various weather and seasons. It uses night sequences for training and performs testing on the day and night sequences. Its recall @ 1 % on Oxford Robot-car dataset is about 26.02% higher than [203] but about 7.86% lower than [152] .",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[79]",
                    "ref_id": "BIBREF79"
                },
                {
                    "start": 683,
                    "end": 688,
                    "text": "[203]",
                    "ref_id": "BIBREF203"
                },
                {
                    "start": 716,
                    "end": 721,
                    "text": "[152]",
                    "ref_id": "BIBREF152"
                }
            ],
            "ref_spans": [
                {
                    "start": 135,
                    "end": 143,
                    "text": "Table 28",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Performance Evaluation"
        },
        {
            "text": "Section 5 analyzes the performance of the 3DOR and 3DPR methods by comparing the published results based on three evaluation metrics (AP, AOS, and ALP) for 3DOR and three evaluation metrics (Recall, Accuracy, and AUC) for 3DPR tasks. It classified the results for comparison according to the datasets used by each method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Summary"
        },
        {
            "text": "Performance comparison on the KITTI car validation and test sets is presented in Tables 21 and 22 respectively. Analysis on the KITTI pedestrian and cyclist validation set is given in Table 23 and on the test set is given in Table 24 . Table 21 shows that the performance of [77] on easy while [72] on moderate and hard difficulty levels is better for AP bev (IoU @ 0.7); [63] on easy while [68] on moderate and hard levels performs better than the other methods for AP 3D (IoU @ 0.7); [61] val1 1 set surpasses all models for ALP on all three levels. Table 22 presents that [77] outperforms on easy while [69] performs better on moderate and hard sets for AP bev (IoU @ 0.7); [69] performance is higher on all three levels compare to other methods for AP 3D (IoU @ 0.7); [63] model exceeds over [67] for AOS on all three levels.",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 279,
                    "text": "[77]",
                    "ref_id": "BIBREF77"
                },
                {
                    "start": 294,
                    "end": 298,
                    "text": "[72]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 372,
                    "end": 376,
                    "text": "[63]",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 391,
                    "end": 395,
                    "text": "[68]",
                    "ref_id": "BIBREF68"
                },
                {
                    "start": 575,
                    "end": 579,
                    "text": "[77]",
                    "ref_id": "BIBREF77"
                },
                {
                    "start": 606,
                    "end": 610,
                    "text": "[69]",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 677,
                    "end": 681,
                    "text": "[69]",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 796,
                    "end": 800,
                    "text": "[67]",
                    "ref_id": "BIBREF67"
                }
            ],
            "ref_spans": [
                {
                    "start": 81,
                    "end": 97,
                    "text": "Tables 21 and 22",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 184,
                    "end": 192,
                    "text": "Table 23",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 225,
                    "end": 233,
                    "text": "Table 24",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 236,
                    "end": 244,
                    "text": "Table 21",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 552,
                    "end": 560,
                    "text": "Table 22",
                    "ref_id": null
                }
            ],
            "section": "Summary"
        },
        {
            "text": "In Table 23 , the performance analysis of pedestrian category illustrates that [66] on all three levels outperforms for AP bev (IoU @ 0.5); [77] on easy and moderate while [66] on hard level performs better for AP 3D (IoU @ 0.5). The comparison on cyclists category shows that first method of [72] on easy while its second method on moderate and hard levels gives better results using AP bev (IoU @ 0.5); first method of [72] on moderate while its second method on easy and hard levels outperforms for for AP 3D (IoU @ 0.5). Table 24 presents that, for the pedestrian category, the results of [55, 66] outperform other methods on all three levels for AP bev and AP 3D (IoU @ 0.7) and for AP bev and AP 3D (IoU @ 0.5) respectively. For cyclist category the results of [69] and third method of [57] have higher performance on all three levels when compared using AP bev and AP 3D (IoU @ 0.5) and AP bev and AP 3D (IoU @ 0.7). For 3DPR tasks, Table 25 presents that [88] has higher recall than [84] on the KITTI dataset while Constructrive and group-based methods have equally higher accuracy in [88] . Performance comparison for 3DOR task on ScanNet and Sun RGB-D datasets shows that [71] has higher mAP @0.25 compared to [70] in Table 27 . Table 28 presents that [89] on Oxford Robot-car and [83] on the In-House datasets outperform [80, 89] when evaluated with average recall @ 1 % for 3DPR task.",
            "cite_spans": [
                {
                    "start": 79,
                    "end": 83,
                    "text": "[66]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 140,
                    "end": 144,
                    "text": "[77]",
                    "ref_id": "BIBREF77"
                },
                {
                    "start": 172,
                    "end": 176,
                    "text": "[66]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 293,
                    "end": 297,
                    "text": "[72]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 421,
                    "end": 425,
                    "text": "[72]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 593,
                    "end": 597,
                    "text": "[55,",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 598,
                    "end": 601,
                    "text": "66]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 767,
                    "end": 771,
                    "text": "[69]",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 792,
                    "end": 796,
                    "text": "[57]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 963,
                    "end": 967,
                    "text": "[88]",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 991,
                    "end": 995,
                    "text": "[84]",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 1093,
                    "end": 1097,
                    "text": "[88]",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 1182,
                    "end": 1186,
                    "text": "[71]",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "text": "[70]",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "text": "[89]",
                    "ref_id": "BIBREF89"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "text": "[83]",
                    "ref_id": "BIBREF83"
                },
                {
                    "start": 1332,
                    "end": 1336,
                    "text": "[80,",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 1337,
                    "end": 1340,
                    "text": "89]",
                    "ref_id": "BIBREF89"
                }
            ],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Table 23",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 525,
                    "end": 533,
                    "text": "Table 24",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 940,
                    "end": 948,
                    "text": "Table 25",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1228,
                    "end": 1236,
                    "text": "Table 27",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 1239,
                    "end": 1247,
                    "text": "Table 28",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Summary"
        },
        {
            "text": "This section summarizes the most relevant findings on the review of social representative robots (Section 2), camera and LiDAR-based data representation of 3D recognition (Section 3) for both object (Section 3.1) and place (Section 3.2).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Research Directions"
        },
        {
            "text": "This article first highlighted the value-centric role of social robots in the society by presenting recently developed robots. These social robots are performing front-line tasks and taking complex roles in public, domestic, hospitals, and industrial settings. The semantic understanding of the environment varies depending on the domain and application scenarios of the robots. For instance, the semantic understanding task for a robot working in a factory with a human co-worker is different from those robots working at home due to different objectives. Usually, these robots are equipped with a variety of sensors, such as camera and LiDAR to perform human-like recognition tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Research Directions"
        },
        {
            "text": "Focusing on the recognition capability of social robots, It has explored camera and LiDAR-based 3D data representation methods using deep learning models for object and place recognition. Both sensors are affected by the changes in the scene lighting conditions as well as the other weather factors [204] . In addition, both object and place recognition (OPR) tasks rely on different methods of semantic understanding, which help to detect small and occluded objects in cluttered environment or objects in occluded scenes.",
            "cite_spans": [
                {
                    "start": 299,
                    "end": 304,
                    "text": "[204]",
                    "ref_id": "BIBREF204"
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Future Research Directions"
        },
        {
            "text": "Examining the existing literature on 3D recognition reveals that there are relatively fewer studies on 3D place recognition compared to 3D object recognition. Moreover, a stable model for 3D recognition has not yet been formed. In the real world, a robot's behavior strongly depends on its surrounding conditions and it needs to recognize its environment through the input scenery. However, literature search shows that up to now, little attention has been paid to LiDAR-based 3D recognition in indoor environment using DL-based approaches in contrast to outdoor recognition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Research Directions"
        },
        {
            "text": "A monocular camera is a low-cost alternative for 3DOR and depth information is calculated with the aid of semantic properties understanding from segmentation. 3D monocular object detection can be improved by establishing pairwise spatial relationships or regressing 3D representation for 3D boxes in the indoor environment, while visual features of visible surfaces for extracting 3D structural information in the outdoor environment. Compared with the monocular camera more, precise depth information can be obtained through the stereo camera by utilizing semantic and geometric information and region-based alignment methods can be used for 3D object localization. However, it can be extended to general object detection by learning 3D object shapes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Research Directions"
        },
        {
            "text": "At present, most of the 3DOR methods heavily depend on LiDAR data for accurate and precise depth information. However, LiDAR is expensive, and its perception range is relatively short. The article categorized the LiDAR-based 3DOR methods into structured, unstructured, and graph-based representations. Some 2D image grid-based methods used pre-RoI pooling convolution methods and pose-sensitive feature maps for accurate orientation and size that can be enhanced with a more advanced encoding scheme for maintaining height information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Research Directions"
        },
        {
            "text": "We reviewed 3D voxel grid-based methods that incorporate semantic information by exploiting BEV semantic masks and depth aware head and by providing multi-class support for 3D recognition. 3D object detection from raw and sparse point cloud data has been far less explored to date using DL models, compared with its 2D counterpart.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Research Directions"
        },
        {
            "text": "3D LiDAR PC-based object detection can yield improved performance by context information and Precise PC coordinates as well as generating feature maps through cylin-drical projection and combining proposal general and parameter estimation network. However, little research has looked into encoding PC using graph neural networks (GNNs) for highly accurate 3DOR. The joint learning of pseudo centers and direction vectors for utilizing multi-graphs was explored with supervised graph strategies for improving the performance. The point clouds do not well capture semantic (e.g., shape) information; however, utilizing the hierarchical graph network (HGNet) approach effectively handles this problem at multi-level semantics for 3DOR.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Research Directions"
        },
        {
            "text": "Sensor fusion methods based on camera and LiDAR for 3DOR using deep fusion schemes have gained attention. These methods rely on combing multi-view region-wise features, constructing sparse non-homogeneous pooling layer for feature transform between two views and allows fusion of these features, extracting point clouds using voxel feature encoder and utilizing anchor proposals, or integrating point and voxel fusions. In this direction, future research needs to deep multi-class detection network. Unlike 3DOR, 3DPR task based on LiDAR and camera-LiDAR fusion methods by leveraging the recent success of deep networks has remained as a less explored problem. LiDAR PC based 3DPR methods depend on metric learning and inference to extract the global descriptors from 3D PC, extraction of local structures and finding the spatial distribution of local features, representation of semi-dense point clouds-based scene, utilization of data-driven descriptor for near-by place candidates, and estimation of yaw angle for oriented recognition. Camera-LiDAR sensors fusion methods to extract fused global descriptors for 3DPR via DL approaches depends on applying a trimmed strategy on the global feature aggregation of PC or using attention-based fusion methods to distinguish discriminative features that can be improved by color normalization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Research Directions"
        },
        {
            "text": "To conclude, the present article began by enumerating the role of social robots as human assistants. Then, in the context of social robot capabilities, we focused on the recent publications related to the camera and LiDAR-based 3D data representation approaches for object and place recognition using the DL model between the years 2014 and 2021. This is the first combined study to review both 3D object and place recognition as well as recently developed social robots. We started by presenting the impact of social robots in the human-centric environment as a companion to tackle the daily problems in different (domestic, industrial, and medical) fields of life.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "We described these recent robotic systems and listed their sensors, tasks, algorithms, appearances, semantic functions, and development status. Afterward, followed by the recognition capability of these social robots, we explored 3D data representation methods for object and place recognition based on camera and LiDAR using DL-based approaches with their advantages and limitations. In addition, we reviewed 3D detection datasets and present comparisons of the existing results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "To motivate those who are interested in DL-based 3D visual recognition approaches, the current study provides information in easy-to-understand tables, in particular, by pointing out the limitations and future research areas. In addition, this study describes different 3D datasets. Moreover, in this article, we analyzed and compared the existing results in the references for different datasets. Finally, we concluded the current survey with a discussion that suggests some promising research directions for future work. Institutional Review Board Statement: Not applicable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "Acknowledgments: Thanks for the help of reviewers and editors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Informed Consent Statement: Not applicable."
        },
        {
            "text": "The authors declare no conflict of interest.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conflicts of Interest:"
        },
        {
            "text": "The list of acronyms and abbreviations used in this survey is given below: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abbreviations"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Ontology-Based Knowledge Representation in Robotic Systems: A Survey Oriented toward",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Manzoor",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "G"
                    ],
                    "last": "Rocha",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Joo",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Bae",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "J"
                    ],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Joo",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Kuc",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Applications. Appl. Sci",
            "volume": "2021",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/app11104324"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Value of social robots in services: social cognition perspective",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "\u010cai\u0107",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Mahr",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Oderkerken-Schr\u00f6der",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "J. Serv. Mark",
            "volume": "33",
            "issn": "",
            "pages": "463--478",
            "other_ids": {
                "DOI": [
                    "10.1108/JSM-02-2018-0080"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A realtime autonomous robot navigation framework for human like high-level interaction and task planning in global dynamic environment",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Joo",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Manzoor",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "G"
                    ],
                    "last": "Rocha",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "U"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Kuc",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.12942"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Distinctive image features from scale-invariant keypoints",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "G"
                    ],
                    "last": "Lowe",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Int. J. Comput. Vis",
            "volume": "60",
            "issn": "",
            "pages": "91--110",
            "other_ids": {
                "DOI": [
                    "10.1023/B:VISI.0000029664.99615.94"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Histograms of oriented gradients for human detection",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Dalal",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Triggs",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of the 2005 IEEE cOmputer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)",
            "volume": "1",
            "issn": "",
            "pages": "886--893",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Least squares support vector machine classifiers",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Suykens",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Vandewalle",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Neural Process. Lett",
            "volume": "9",
            "issn": "",
            "pages": "293--300",
            "other_ids": {
                "DOI": [
                    "10.1023/A:1018628609742"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Convolutional neural networks as a model of the visual system: Past, present, and future",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lindsay",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "J. Cogn. Neurosci",
            "volume": "33",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1162/jocn_a_01544"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Comparison of Object Recognition Approaches using Traditional Machine Vision and Modern Deep Learning Techniques for Mobile Robot",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Manzoor",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Joo",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Kuc",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 19th International Conference on Control, Automation and Systems (ICCAS)",
            "volume": "",
            "issn": "",
            "pages": "1316--1321",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Delving Into High Quality Object Detection",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Vasconcelos",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Cascade R-Cnn",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "6154--6162",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Mask r-cnn",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Gkioxari",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "2961--2969",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "You only look once: Unified, real-time object detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Divvala",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "779--788",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Single shot multibox detector",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Erhan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Reed",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "Y"
                    ],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Berg",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ssd",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "21--37",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "YOLO9000: Better, faster, stronger",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "7263--7271",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Focal loss for dense object detection",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "2980--2988",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size. arXiv 2016",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "N"
                    ],
                    "last": "Iandola",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Moskewicz",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ashraf",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "J"
                    ],
                    "last": "Dally",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Keutzer",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Squeezenet",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1602.07360"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Donahue",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "580--587",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Fast r-cnn",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1440--1448",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Yolov4: Optimal speed and accuracy of object detection",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bochkovskiy",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "Y"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "Y"
                    ],
                    "last": "Liao",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2004.10934"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "YOLOv4: Scaling Cross Stage Partial Network. arXiv 2020",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "Y"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bochkovskiy",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "Y"
                    ],
                    "last": "Liao",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2011.08036"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Optimizing the trade-off between single-stage and two-stage deep object detectors using image difficulty prediction",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Soviany",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "T"
                    ],
                    "last": "Ionescu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 20th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)",
            "volume": "",
            "issn": "",
            "pages": "209--214",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Avoiding confusing features in place recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Knopp",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sivic",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pajdla",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "748--761",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Improving Place Recognition Using Dynamic Object Detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Munoz",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dexter",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2002.04698"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Object-based Place Recognition for Mobile Robots using Laplace's Rule of Succession",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Oh",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "J"
                    ],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 12th International Conference on Computer and Automation Engineering",
            "volume": "",
            "issn": "",
            "pages": "200--204",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "A Kinect-based 3D object detection and recognition system with enhanced depth estimation algorithm",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "F"
                    ],
                    "last": "Elaraby",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hamdy",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rehan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",
            "volume": "",
            "issn": "",
            "pages": "247--252",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Object Recognition based interpolation with 3d lidar and vision for autonomous driving of an intelligent vehicle",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "S"
                    ],
                    "last": "Weon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "G"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "K"
                    ],
                    "last": "Ryu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Access",
            "volume": "8",
            "issn": "",
            "pages": "65599--65608",
            "other_ids": {
                "DOI": [
                    "10.1109/ACCESS.2020.2982681"
                ]
            }
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Intelligent Seven-DoF Robot With Dynamic Obstacle Avoidance and 3-D Object Recognition for Industrial Cyber-Physical Systems in Manufacturing Automation",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "C"
                    ],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kuo",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE 2016",
            "volume": "104",
            "issn": "",
            "pages": "1102--1113",
            "other_ids": {
                "DOI": [
                    "10.1109/JPROC.2015.2508598"
                ]
            }
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "3D Deep Object Recognition and Semantic Understanding for Visually-Guided Robotic Service",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Naguib",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "U"
                    ],
                    "last": "Islam",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "903--910",
            "other_ids": {
                "DOI": [
                    "10.1109/IROS.2018.8593985"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Dagc: Employing dual attention and graph convolution for point cloud based place recognition",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 International Conference on Multimedia Retrieval",
            "volume": "",
            "issn": "",
            "pages": "224--232",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Fast Sequence-matching Enhanced Viewpoint-invariant 3D Place Recognition",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Egorov",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Transactions on Industrial Electronics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "A Multi-View Deep Fusion Networks for Viewpoint-Free 3D Place Recognition",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Choset",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Fusionvlad",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Robot. Autom. Lett",
            "volume": "6",
            "issn": "",
            "pages": "2304--2310",
            "other_ids": {
                "DOI": [
                    "10.1109/LRA.2021.3061375"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Robots perception through 3d point cloud sensors",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "A S"
                    ],
                    "last": "Teixeira",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "B"
                    ],
                    "last": "Santos",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "De Oliveira",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "V"
                    ],
                    "last": "Arruda",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Neves",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Robot Operating System",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Kehl",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Milletari",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Tombari",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ilic",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Navab",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "205--220",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "3d bounding box estimation using deep learning and geometry",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mousavian",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Flynn",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kosecka",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "7074--7082",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Pointfusion: Deep sensor fusion for 3d bounding box estimation",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jain",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "244--253",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Scan context: Egocentric spatial descriptor for place recognition within 3d point cloud map",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "1--5",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "SRAL: Shared representative appearance learning for long-term visual place recognition",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rentschler",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Robot. Autom. Lett",
            "volume": "2",
            "issn": "",
            "pages": "1172--1179",
            "other_ids": {
                "DOI": [
                    "10.1109/LRA.2017.2662061"
                ]
            }
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Place recognition in 3D scans using a combination of bag of words and point feature based relative pose estimation",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Steder",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ruhnke",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Grzonka",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Burgard",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "volume": "",
            "issn": "",
            "pages": "1249--1255",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "M2DP: A novel 3D point cloud descriptor and its application in loop closure detection",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "231--237",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Segmatch: Segment based place recognition in 3d point clouds",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dub\u00e9",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dugas",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Stumm",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Nieto",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Siegwart",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cadena",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Vehicle detection from 3d lidar using fully convolutional network",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1608.07916"
                ]
            }
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Fusion of LiDAR and camera sensor data for environment sensing in driverless vehicles",
            "authors": [
                {
                    "first": "De",
                    "middle": [],
                    "last": "Silva",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Roche",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kondoz",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1710.06230v2"
                ]
            }
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Deep learning for 3d point clouds: A survey",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bennamoun",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "3D convolutional neural network for object recognition: A review",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "D"
                    ],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mittal",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "K"
                    ],
                    "last": "Bhatia",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Multimed. Tools Appl",
            "volume": "78",
            "issn": "",
            "pages": "15951--15995",
            "other_ids": {
                "DOI": [
                    "10.1007/s11042-018-6912-6"
                ]
            }
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Epidemic Solution",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Ubtech",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Anti",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "The Best Thing at the Booth Is This Salad-Making Chef Bot",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Swider",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Samsung",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Say Hello to Astro, Alexa on Wheels",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Seifert",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Diligent Robotics Raises $3M in sEed Funding, Launches Moxi Hospital Robot",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Demaitre",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Healthcare robot systems for a hospital environment: CareBot and ReceptionBot",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "S"
                    ],
                    "last": "Ahn",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "A"
                    ],
                    "last": "Macdonald",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)",
            "volume": "",
            "issn": "",
            "pages": "571--576",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "The Future of Robotics in Healthcare",
            "authors": [
                {
                    "first": "Heath",
                    "middle": [],
                    "last": "Europa",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Stretch Is Boston Dynamics' Take on a Practical Mobile Manipulator for Warehouses",
            "authors": [
                {
                    "first": "Evan",
                    "middle": [],
                    "last": "Ackerman",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Tai",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "12093--12102",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Gs3d: An efficient 3d object detection framework for autonomous driving",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ouyang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Sheng",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "16--20",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Monocular 3D object detection and box fitting trained end-to-end using intersection-over-union loss",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "J\u00f6rgensen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zach",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Kahl",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.08070"
                ]
            }
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Monocular 3D single stage object detector. arXiv 2021",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "M3dssd",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2103.13164"
                ]
            }
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Stereo r-cnn based 3d object detection for autonomous driving",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "7644--7652",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Stereo CenterNet based 3D Object Detection for Autonomous Driving. arXiv 2021",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Mi",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2103.11071"
                ]
            }
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "Rt3d: Real-time 3-d vehicle detection in lidar point cloud for autonomous driving",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Robot. Autom. Lett",
            "volume": "3",
            "issn": "",
            "pages": "3434--3440",
            "other_ids": {
                "DOI": [
                    "10.1109/LRA.2018.2852843"
                ]
            }
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Anchor free one stage 3d object detection. arXiv 2020",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ge",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Afdet",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.12671"
                ]
            }
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2002.05316"
                ]
            }
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "Single-model Multi-class Extension for Sparse 3D Object Detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Muramatsu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tsuji",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Carballo",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Thompson",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chishiro",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kato",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Second-Dx",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 IEEE Intelligent Transportation Systems Conference (ITSC)",
            "volume": "",
            "issn": "",
            "pages": "2675--2680",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "A Simple and Efficient Multi-task Network for",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tomizuka",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhan",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "3D Object Detection and Road Understanding. arXiv 2021",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2103.04056"
                ]
            }
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "Ipod: Intensive point-based object detector for point cloud",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1812.05276"
                ]
            }
        },
        "BIBREF67": {
            "ref_id": "b67",
            "title": "FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 12th International Congress on Image and Signal Processing",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF68": {
            "ref_id": "b68",
            "title": "A Density-Oriented PointNet for 3D Object Detection in Point Clouds. arXiv 2021",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dpointnet",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2102.03747"
                ]
            }
        },
        "BIBREF69": {
            "ref_id": "b69",
            "title": "Point-gnn: Graph neural network for 3d object detection in a point cloud",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Rajkumar",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1711--1719",
            "other_ids": {}
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "Relation graph network for 3D object detection in point clouds",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "Z"
                    ],
                    "last": "Gilani",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mian",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1912.00202"
                ]
            }
        },
        "BIBREF71": {
            "ref_id": "b71",
            "title": "A Hierarchical Graph Network for 3D Object Detection on Point Clouds",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Lei",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "Z"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "392--401",
            "other_ids": {}
        },
        "BIBREF72": {
            "ref_id": "b72",
            "title": "Spatial-Attention Graph Convolution Network based Feature Enhancement for 3D Object Detection. arXiv 2021",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lan",
                    "suffix": ""
                },
                {
                    "first": "J. S-At",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gcn",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2103.08439"
                ]
            }
        },
        "BIBREF73": {
            "ref_id": "b73",
            "title": "Multi-view 3d object detection network for autonomous driving",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1907--1915",
            "other_ids": {}
        },
        "BIBREF74": {
            "ref_id": "b74",
            "title": "Fusing bird's eye view lidar point cloud and front view camera image for 3d object detection",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tomizuka",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE Intelligent Vehicles Symposium (IV)",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF75": {
            "ref_id": "b75",
            "title": "Deep end-to-end 3d person detection from camera and lidar",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Jargot",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Gavrila",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 IEEE Intelligent Transportation Systems Conference (ITSC)",
            "volume": "",
            "issn": "",
            "pages": "521--527",
            "other_ids": {}
        },
        "BIBREF76": {
            "ref_id": "b76",
            "title": "Multimodal voxelnet for 3D object detection",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "A"
                    ],
                    "last": "Sindagi",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Tuzel",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mvx-Net",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 International Conference on Robotics and Automation (ICRA)",
            "volume": "",
            "issn": "",
            "pages": "7276--7282",
            "other_ids": {}
        },
        "BIBREF77": {
            "ref_id": "b77",
            "title": "Fast and Accurate 3D Object Detection for Lidar-Camera-Based Autonomous Vehicles Using One Shared Voxel-Based Backbone",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "H"
                    ],
                    "last": "Wen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "H"
                    ],
                    "last": "Jo",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Access",
            "volume": "9",
            "issn": "",
            "pages": "22080--22089",
            "other_ids": {
                "DOI": [
                    "10.1109/ACCESS.2021.3055491"
                ]
            }
        },
        "BIBREF78": {
            "ref_id": "b78",
            "title": "Learning effective RGB-D representations for scene recognition",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Herranz",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Image Process",
            "volume": "28",
            "issn": "",
            "pages": "980--993",
            "other_ids": {
                "DOI": [
                    "10.1109/TIP.2018.2872629"
                ]
            }
        },
        "BIBREF79": {
            "ref_id": "b79",
            "title": "Event-VPR: End-to-End Weakly Supervised Network Architecture for Event-based Visual Place Recognition",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Coleman",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kerr",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2011.03290"
                ]
            }
        },
        "BIBREF80": {
            "ref_id": "b80",
            "title": "Pointnetvlad: Deep point-cloud-based retrieval for large-scale place recognition",
            "authors": [
                {
                    "first": "Angelina",
                    "middle": [],
                    "last": "Uy",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hee Lee",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "4470--4479",
            "other_ids": {}
        },
        "BIBREF81": {
            "ref_id": "b81",
            "title": "Indoor Scene Recognition in 3D",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Usvyatsov",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Schindler",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2002.12819"
                ]
            }
        },
        "BIBREF82": {
            "ref_id": "b82",
            "title": "Point-cloud-based place recognition using CNN feature extraction",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "Y"
                    ],
                    "last": "Yeung",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Sens. J",
            "volume": "19",
            "issn": "",
            "pages": "12175--12186",
            "other_ids": {
                "DOI": [
                    "10.1109/JSEN.2019.2937740"
                ]
            }
        },
        "BIBREF83": {
            "ref_id": "b83",
            "title": "Lpd-net: 3d point cloud learning for large-scale place recognition and environment analysis",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Suo",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "H"
                    ],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF84": {
            "ref_id": "b84",
            "title": "Oriented Recognition of 3D Point Clouds in Outdoor Scenarios. arXiv 2019",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Schaupp",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "B\u00fcrki",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dub\u00e9",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Siegwart",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cadena",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Oreos",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1903.07918"
                ]
            }
        },
        "BIBREF85": {
            "ref_id": "b85",
            "title": "Place recognition in semi-dense maps: Geometric and learning-based approaches",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Cieslewski",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Loquercio",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Scaramuzza",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the British Machine Vision Conference",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF86": {
            "ref_id": "b86",
            "title": "Learning 3d segment descriptors for place recognition",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cramariuc",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dub\u00e9",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Sommer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Siegwart",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Gilitschenski",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.09270"
                ]
            }
        },
        "BIBREF87": {
            "ref_id": "b87",
            "title": "Point Cloud Based Large-Scale Place Recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Komorowski",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Minkloc3d",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1790--1799",
            "other_ids": {}
        },
        "BIBREF88": {
            "ref_id": "b88",
            "title": "Large-Scale Place Recognition Based on Camera-LiDAR Fused Descriptor",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Sensors",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/s20102870"
                ]
            }
        },
        "BIBREF89": {
            "ref_id": "b89",
            "title": "Point Cloud and Image Collaboration Network for Large-Scale Place Recognition. arXiv 2020",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pic-Net",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2008.00658"
                ]
            }
        },
        "BIBREF90": {
            "ref_id": "b90",
            "title": "Meet Scout: Amazon Is Taking Its Prime Delivery Robots to the South",
            "authors": [
                {
                    "first": "Dalvin",
                    "middle": [],
                    "last": "Brown",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF91": {
            "ref_id": "b91",
            "title": "Get a Glimpse of the Next-Generation Innovations on Display at Samsung's Technology Showcase",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Samsung News Room",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF92": {
            "ref_id": "b92",
            "title": "Amazon Astro Household Robot: Everything to Know about Price, Privacy, Battery and More",
            "authors": [
                {
                    "first": "Shelby",
                    "middle": [],
                    "last": "Brown",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF93": {
            "ref_id": "b93",
            "title": "Medical City Dallas' One-Armed, Bright-Eyed Moxi is Region's First Full-Time Nursing Robot",
            "authors": [
                {
                    "first": "Dom",
                    "middle": [],
                    "last": "Difurio",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF94": {
            "ref_id": "b94",
            "title": "Multi-Sensing Intelligent Robotic Assistant",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Neura Robotics Gmbh",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Maira",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF95": {
            "ref_id": "b95",
            "title": "A software platform for component based rt-system development: Openrtm-aist",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ando",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Suehiro",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kotoku",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the International Conference on Simulation, Modeling, and Programming for Autonomous Robots",
            "volume": "",
            "issn": "",
            "pages": "87--98",
            "other_ids": {}
        },
        "BIBREF96": {
            "ref_id": "b96",
            "title": "Iconic Boston Dynamics Robots Seek Stable Employment",
            "authors": [
                {
                    "first": "Sarah",
                    "middle": [],
                    "last": "Mcbride",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF98": {
            "ref_id": "b98",
            "title": "Meet Boston Dynamics' New Robot, Called Stretch",
            "authors": [
                {
                    "first": "Daphne",
                    "middle": [],
                    "last": "Leprince-Ringuet",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF99": {
            "ref_id": "b99",
            "title": "Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "Y"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "H"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "C"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "C F"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2624--2632",
            "other_ids": {}
        },
        "BIBREF100": {
            "ref_id": "b100",
            "title": "M3d-rpn: Monocular 3d region proposal network for object detection",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Brazil",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "9287--9296",
            "other_ids": {}
        },
        "BIBREF101": {
            "ref_id": "b101",
            "title": "Monogrnet: A geometric reasoning network for monocular 3d object localization",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "33",
            "issn": "",
            "pages": "8851--8858",
            "other_ids": {}
        },
        "BIBREF102": {
            "ref_id": "b102",
            "title": "Monocular 3d object detection for autonomous driving",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kundu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Fidler",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Urtasun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF103": {
            "ref_id": "b103",
            "title": "Disentangling monocular 3d object detection",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Simonelli",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Bulo",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Porzi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "L\u00f3pez-Antequera",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kontschieder",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1991--1999",
            "other_ids": {}
        },
        "BIBREF104": {
            "ref_id": "b104",
            "title": "Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Manhardt",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Kehl",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gaidon",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2069--2078",
            "other_ids": {}
        },
        "BIBREF105": {
            "ref_id": "b105",
            "title": "Deep Layer Aggregation",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Shelhamer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2403--2412",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2018.00255"
                ]
            }
        },
        "BIBREF106": {
            "ref_id": "b106",
            "title": "Objects as points. arXiv 2019",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kr\u00e4henb\u00fchl",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1904.07850"
                ]
            }
        },
        "BIBREF107": {
            "ref_id": "b107",
            "title": "Defense of Range View for LiDAR-based 3D Object Detection. arXiv 2021",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Rangedet",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2103.10039"
                ]
            }
        },
        "BIBREF108": {
            "ref_id": "b108",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF109": {
            "ref_id": "b109",
            "title": "Towards real-time object detection with region proposal networks",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Faster",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF110": {
            "ref_id": "b110",
            "title": "Training region-based object detectors with online hard example mining",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shrivastava",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "761--769",
            "other_ids": {}
        },
        "BIBREF111": {
            "ref_id": "b111",
            "title": "Pointpillars: Fast encoders for object detection from point clouds",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "H"
                    ],
                    "last": "Lang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vora",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Caesar",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Beijbom",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "16--20",
            "other_ids": {}
        },
        "BIBREF112": {
            "ref_id": "b112",
            "title": "Towards High Performance Voxel-based 3D Object Detection. arXiv 2020",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Voxel R-Cnn",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2012.15712"
                ]
            }
        },
        "BIBREF113": {
            "ref_id": "b113",
            "title": "Second: Sparsely embedded convolutional detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Mao",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Sensors",
            "volume": "18",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/s18103337"
                ]
            }
        },
        "BIBREF114": {
            "ref_id": "b114",
            "title": "Voxelnet: End-to-end learning for point-cloud-based 3d object detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Tuzel",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "4490--4499",
            "other_ids": {}
        },
        "BIBREF115": {
            "ref_id": "b115",
            "title": "From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "43",
            "issn": "",
            "pages": "2647--2664",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2020.2977026"
                ]
            }
        },
        "BIBREF116": {
            "ref_id": "b116",
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ronneberger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "volume": "",
            "issn": "",
            "pages": "234--241",
            "other_ids": {}
        },
        "BIBREF117": {
            "ref_id": "b117",
            "title": "Multiview random forest of local experts combining rgb and lidar data for pedestrian detection",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gonz\u00e1lez",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Villalonga",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "V\u00e1zquez",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Amores",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "L\u00f3pez",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 IEEE Intelligent Vehicles Symposium (IV)",
            "volume": "",
            "issn": "",
            "pages": "356--361",
            "other_ids": {}
        },
        "BIBREF118": {
            "ref_id": "b118",
            "title": "A 3d convolutional neural network for real-time object recognition",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Maturana",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Scherer",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Voxnet",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "922--928",
            "other_ids": {}
        },
        "BIBREF119": {
            "ref_id": "b119",
            "title": "Deep hierarchical feature learning on point sets in a metric space",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Guibas",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pointnet++",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "5099--5108",
            "other_ids": {}
        },
        "BIBREF120": {
            "ref_id": "b120",
            "title": "Joint 3d proposal generation and object detection from view aggregation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ku",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mozifian",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Harakeh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "L"
                    ],
                    "last": "Waslander",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF121": {
            "ref_id": "b121",
            "title": "Deep learning on point sets for 3d classification and segmentation",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mo",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Guibas",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pointnet",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "652--660",
            "other_ids": {}
        },
        "BIBREF122": {
            "ref_id": "b122",
            "title": "Pointrcnn: 3d object proposal generation and detection from point cloud",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "770--779",
            "other_ids": {}
        },
        "BIBREF123": {
            "ref_id": "b123",
            "title": "Camera-LiDAR Object Candidates Fusion for 3D Object Detection. arXiv 2020",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Morris",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Radha",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Clocs",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2009.00784"
                ]
            }
        },
        "BIBREF124": {
            "ref_id": "b124",
            "title": "3D Object Detection Using Frustums and Attention Modules for Images and Point Clouds",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Signals",
            "volume": "2021",
            "issn": "",
            "pages": "98--107",
            "other_ids": {
                "DOI": [
                    "10.3390/signals2010009"
                ]
            }
        },
        "BIBREF125": {
            "ref_id": "b125",
            "title": "Optimal sensor data fusion architecture for object detection in adverse weather conditions",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Pfeuffer",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Dietmayer",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 21st International Conference on Information Fusion (FUSION)",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF126": {
            "ref_id": "b126",
            "title": "A unified multi-scale deep convolutional neural network for fast object detection",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "S"
                    ],
                    "last": "Feris",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Vasconcelos",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the European conference on computer vision",
            "volume": "",
            "issn": "",
            "pages": "354--370",
            "other_ids": {}
        },
        "BIBREF127": {
            "ref_id": "b127",
            "title": "Place recognition: An overview of vision perspective",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Appl. Sci",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/app8112257"
                ]
            }
        },
        "BIBREF128": {
            "ref_id": "b128",
            "title": "Context-based vision system for place and object recognition",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Torralba",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "P"
                    ],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "T"
                    ],
                    "last": "Freeman",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Rubin",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings of the Computer Vision, IEEE International Conference on",
            "volume": "2",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF129": {
            "ref_id": "b129",
            "title": "Avoiding to Face the Challenges of Visual Place Recognition",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Mihankhah",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the SAI Intelligent Systems Conference",
            "volume": "",
            "issn": "",
            "pages": "738--749",
            "other_ids": {}
        },
        "BIBREF130": {
            "ref_id": "b130",
            "title": "Modeling the shape of the scene: A holistic representation of the spatial envelope",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oliva",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Torralba",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Int. J. Comput. Vis",
            "volume": "42",
            "issn": "",
            "pages": "145--175",
            "other_ids": {
                "DOI": [
                    "10.1023/A:1011139631724"
                ]
            }
        },
        "BIBREF131": {
            "ref_id": "b131",
            "title": "Building the gist of a scene: The role of global image features in recognition",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oliva",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Torralba",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Prog. Brain Res",
            "volume": "155",
            "issn": "",
            "pages": "23--36",
            "other_ids": {}
        },
        "BIBREF132": {
            "ref_id": "b132",
            "title": "Object retrieval with large vocabularies and fast spatial matching",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Philbin",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chum",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Isard",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sivic",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF133": {
            "ref_id": "b133",
            "title": "Video Google: A text retrieval approach to object matching in videos",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sivic",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF134": {
            "ref_id": "b134",
            "title": "Freak: Fast retina keypoint",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Alahi",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ortiz",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Vandergheynst",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "510--517",
            "other_ids": {}
        },
        "BIBREF135": {
            "ref_id": "b135",
            "title": "Surf: Speeded up robust features",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Bay",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Tuytelaars",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Proceedings of the European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "404--417",
            "other_ids": {}
        },
        "BIBREF136": {
            "ref_id": "b136",
            "title": "BRISK: Binary robust invariant scalable keypoints",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Leutenegger",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chli",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "Y"
                    ],
                    "last": "Siegwart",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 2011 International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "2548--2555",
            "other_ids": {}
        },
        "BIBREF137": {
            "ref_id": "b137",
            "title": "ORB: An efficient alternative to SIFT or SURF",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Rublee",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Rabaud",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Konolige",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Bradski",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 2011 International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "2564--2571",
            "other_ids": {}
        },
        "BIBREF138": {
            "ref_id": "b138",
            "title": "Robust visual SLAM across seasons",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Naseer",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ruhnke",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Stachniss",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Spinello",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Burgard",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF139": {
            "ref_id": "b139",
            "title": "On the performance of convnet features for place recognition",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "S\u00fcnderhauf",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shirazi",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Dayoub",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Upcroft",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Milford",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF140": {
            "ref_id": "b140",
            "title": "Robust visual semi-semantic loop closure detection by a covisibility graph and CNN features",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Cascianelli",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Costante",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Bellocchio",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Valigi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Fravolini",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "A"
                    ],
                    "last": "Ciarfuglia",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Robot. Auton. Syst",
            "volume": "92",
            "issn": "",
            "pages": "53--65",
            "other_ids": {
                "DOI": [
                    "10.1016/j.robot.2017.03.004"
                ]
            }
        },
        "BIBREF141": {
            "ref_id": "b141",
            "title": "Training a convolutional neural network for appearanceinvariant place recognition",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Gomez-Ojeda",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lopez-Antequera",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Petkov",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gonzalez-Jimenez",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1505.07428"
                ]
            }
        },
        "BIBREF142": {
            "ref_id": "b142",
            "title": "Visual place recognition: A survey",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lowry",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "S\u00fcnderhauf",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Newman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "J"
                    ],
                    "last": "Leonard",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cox",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Corke",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Milford",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Robot",
            "volume": "32",
            "issn": "",
            "pages": "1--19",
            "other_ids": {
                "DOI": [
                    "10.1109/TRO.2015.2496823"
                ]
            }
        },
        "BIBREF143": {
            "ref_id": "b143",
            "title": "Towards appearance-based methods for lidar sensors",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Mcmanus",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Furgale",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "D"
                    ],
                    "last": "Barfoot",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 2011 IEEE International Conference on Robotics and Automation",
            "volume": "",
            "issn": "",
            "pages": "1930--1935",
            "other_ids": {}
        },
        "BIBREF144": {
            "ref_id": "b144",
            "title": "Place recognition using keypoint voting in large 3D lidar datasets",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bosse",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zlot",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 2013 IEEE International Conference on Robotics and Automation",
            "volume": "",
            "issn": "",
            "pages": "2677--2684",
            "other_ids": {}
        },
        "BIBREF145": {
            "ref_id": "b145",
            "title": "SegMap: 3d segment mapping using data-driven descriptors",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dub\u00e9",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cramariuc",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dugas",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Nieto",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Siegwart",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cadena",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.09557"
                ]
            }
        },
        "BIBREF146": {
            "ref_id": "b146",
            "title": "A neurologically inspired sequence processing model for mobile robot place recognition",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Neubert",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Schubert",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Protzel",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Robot. Autom. Lett",
            "volume": "4",
            "issn": "",
            "pages": "3200--3207",
            "other_ids": {
                "DOI": [
                    "10.1109/LRA.2019.2927096"
                ]
            }
        },
        "BIBREF147": {
            "ref_id": "b147",
            "title": "Visual place recognition: A survey from deep learning perspective",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Pattern Recognit",
            "volume": "113",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1016/j.patcog.2020.107760"
                ]
            }
        },
        "BIBREF148": {
            "ref_id": "b148",
            "title": "Vpr-bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zaffar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ehsan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Milford",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Flynn",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mcdonald-Maier",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.08135"
                ]
            }
        },
        "BIBREF149": {
            "ref_id": "b149",
            "title": "Raskar, R. 3d depth cameras in vision: Benefits and limitations of the hardware",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kadambi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bhandari",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Computer Vision and Machine Learning with RGB-D Sensors",
            "volume": "",
            "issn": "",
            "pages": "3--26",
            "other_ids": {}
        },
        "BIBREF150": {
            "ref_id": "b150",
            "title": "Event-based Vision: A Survey",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Gallego",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Delbruck",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "M"
                    ],
                    "last": "Orchard",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bartolozzi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Taba",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Censi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Leutenegger",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Davison",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Conradt",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Daniilidis",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2020.3008413"
                ]
            }
        },
        "BIBREF151": {
            "ref_id": "b151",
            "title": "Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition. arXiv 2021",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hui",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2101.02374"
                ]
            }
        },
        "BIBREF152": {
            "ref_id": "b152",
            "title": "CNN architecture for weakly supervised place recognition",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Arandjelovic",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Gronat",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Torii",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pajdla",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sivic",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Netvlad",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "27--30",
            "other_ids": {}
        },
        "BIBREF153": {
            "ref_id": "b153",
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chopra",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hadsell",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)",
            "volume": "1",
            "issn": "",
            "pages": "539--546",
            "other_ids": {}
        },
        "BIBREF154": {
            "ref_id": "b154",
            "title": "All About VLAD",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Arandjelovic",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1578--1585",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2013.207"
                ]
            }
        },
        "BIBREF155": {
            "ref_id": "b155",
            "title": "Dynamic graph cnn for learning on point clouds",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Sarma",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Bronstein",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Solomon",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACM Trans. Graph",
            "volume": "38",
            "issn": "",
            "pages": "1--12",
            "other_ids": {
                "DOI": [
                    "10.1145/3326362"
                ]
            }
        },
        "BIBREF156": {
            "ref_id": "b156",
            "title": "Improving RGB-D SLAM in dynamic environments: A motion removal approach",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "Q"
                    ],
                    "last": "Meng",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Robot. Auton. Syst",
            "volume": "89",
            "issn": "",
            "pages": "110--122",
            "other_ids": {
                "DOI": [
                    "10.1016/j.robot.2016.11.012"
                ]
            }
        },
        "BIBREF157": {
            "ref_id": "b157",
            "title": "Delight: An efficient descriptor for global localisation using lidar intensities",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "P"
                    ],
                    "last": "Cop",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "V"
                    ],
                    "last": "Borges",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dub\u00e9",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA)",
            "volume": "",
            "issn": "",
            "pages": "3653--3660",
            "other_ids": {}
        },
        "BIBREF158": {
            "ref_id": "b158",
            "title": "Global localization in 3D point clouds for mobile vehicles",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Locnet",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE Intelligent Vehicles Symposium (IV)",
            "volume": "",
            "issn": "",
            "pages": "728--733",
            "other_ids": {}
        },
        "BIBREF159": {
            "ref_id": "b159",
            "title": "Very deep convolutional networks for large-scale image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1409.1556"
                ]
            }
        },
        "BIBREF160": {
            "ref_id": "b160",
            "title": "Image similarity using deep CNN and curriculum learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Appalaraju",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Chaoji",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1709.08761"
                ]
            }
        },
        "BIBREF161": {
            "ref_id": "b161",
            "title": "Learning local geometric descriptors from rgb-d reconstructions",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nie\u00dfner",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Fisher",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Funkhouser",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1802--1811",
            "other_ids": {}
        },
        "BIBREF162": {
            "ref_id": "b162",
            "title": "Direct sparse odometry",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Engel",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Koltun",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cremers",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "40",
            "issn": "",
            "pages": "611--625",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2017.2658577"
                ]
            }
        },
        "BIBREF163": {
            "ref_id": "b163",
            "title": "Segment based loop-closure for 3d point clouds. arXiv 2016",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dub\u00e9",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dugas",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Stumm",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Nieto",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Siegwart",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cadena",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Segmatch",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1609.07720"
                ]
            }
        },
        "BIBREF164": {
            "ref_id": "b164",
            "title": "Deep unified model for face recognition based on convolution neural network and edge computing",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "Z"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Harous",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "U"
                    ],
                    "last": "Hassan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "U G"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Iqbal",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mumtaz",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Access",
            "volume": "7",
            "issn": "",
            "pages": "72622--72633",
            "other_ids": {
                "DOI": [
                    "10.1109/ACCESS.2019.2918275"
                ]
            }
        },
        "BIBREF165": {
            "ref_id": "b165",
            "title": "Dimensionality reduction by learning an invariant mapping",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hadsell",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chopra",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
            "volume": "",
            "issn": "",
            "pages": "17--22",
            "other_ids": {}
        },
        "BIBREF166": {
            "ref_id": "b166",
            "title": "Deep face recognition",
            "authors": [
                {
                    "first": "O",
                    "middle": [
                        "M"
                    ],
                    "last": "Parkhi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the British Machine Vision Conference (BMVC)",
            "volume": "12",
            "issn": "",
            "pages": "41--42",
            "other_ids": {}
        },
        "BIBREF167": {
            "ref_id": "b167",
            "title": "Neural networks for fingerprint recognition",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Baldi",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chauvin",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Neural Comput",
            "volume": "5",
            "issn": "",
            "pages": "402--418",
            "other_ids": {
                "DOI": [
                    "10.1162/neco.1993.5.3.402"
                ]
            }
        },
        "BIBREF168": {
            "ref_id": "b168",
            "title": "Feature pyramid networks for object detection",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Hariharan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Belongie",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2117--2125",
            "other_ids": {}
        },
        "BIBREF169": {
            "ref_id": "b169",
            "title": "Savarese, S. 4d spatio-temporal convnets: Minkowski convolutional neural networks",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Choy",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gwak",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3075--3084",
            "other_ids": {}
        },
        "BIBREF170": {
            "ref_id": "b170",
            "title": "Fine-tuning CNN image retrieval with no human annotation",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Radenovi\u0107",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Tolias",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chum",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "41",
            "issn": "",
            "pages": "1655--1668",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2018.2846566"
                ]
            }
        },
        "BIBREF171": {
            "ref_id": "b171",
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ioffe",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "448--456",
            "other_ids": {}
        },
        "BIBREF172": {
            "ref_id": "b172",
            "title": "Welcome to the KITTI Vision Benchmark Suite",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Geiger",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF173": {
            "ref_id": "b173",
            "title": "Are we ready for autonomous driving? The kitti vision benchmark suite",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Geiger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lenz",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Urtasun",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3354--3361",
            "other_ids": {}
        },
        "BIBREF174": {
            "ref_id": "b174",
            "title": "Robot Car Dataset",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Oxford",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF175": {
            "ref_id": "b175",
            "title": "1 year, 1000 km: The Oxford RobotCar dataset",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Maddern",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pascoe",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Linegar",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Newman",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Int. J. Robot. Res",
            "volume": "36",
            "issn": "",
            "pages": "3--15",
            "other_ids": {
                "DOI": [
                    "10.1177/0278364916679498"
                ]
            }
        },
        "BIBREF176": {
            "ref_id": "b176",
            "title": "Perception Open Dataset",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Waymo",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF177": {
            "ref_id": "b177",
            "title": "Scalability in perception for autonomous driving: Waymo open dataset",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kretzschmar",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Dotiwalla",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chouard",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Patnaik",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Tsui",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chai",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Caine",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2446--2454",
            "other_ids": {}
        },
        "BIBREF179": {
            "ref_id": "b179",
            "title": "Complex uRban Dataset with Multi-Level Sensors from Highly Diverse Urban Environments",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jeong",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "S"
                    ],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Roh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. J. Robot. Res",
            "volume": "38",
            "issn": "",
            "pages": "642--657",
            "other_ids": {
                "DOI": [
                    "10.1177/0278364919843996"
                ]
            }
        },
        "BIBREF181": {
            "ref_id": "b181",
            "title": "Indoor segmentation and support inference from rgbd images",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Silberman",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hoiem",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kohli",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "746--760",
            "other_ids": {}
        },
        "BIBREF182": {
            "ref_id": "b182",
            "title": "Princeton Vision & Robotics Labs. SUNRGB-D 3D Object Detection Challenge",
            "authors": [],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF183": {
            "ref_id": "b183",
            "title": "Sun rgb-d: A rgb-d scene understanding benchmark suite",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "P"
                    ],
                    "last": "Lichtenberg",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "567--576",
            "other_ids": {}
        },
        "BIBREF184": {
            "ref_id": "b184",
            "title": "Recognizing indoor scenes",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Quattoni",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Torralba",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "413--420",
            "other_ids": {}
        },
        "BIBREF185": {
            "ref_id": "b185",
            "title": "The Multi Vehicle Stereo Event Camera Dataset",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "Z"
                    ],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF186": {
            "ref_id": "b186",
            "title": "The multivehicle stereo event camera dataset: An event camera dataset for 3D perception",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "Z"
                    ],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Thakur",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "\u00d6zaslan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pfrommer",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Daniilidis",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Robot. Autom. Lett",
            "volume": "3",
            "issn": "",
            "pages": "2032--2039",
            "other_ids": {
                "DOI": [
                    "10.1109/LRA.2018.2800793"
                ]
            }
        },
        "BIBREF187": {
            "ref_id": "b187",
            "title": "End-to-end DAVIS driving dataset. arXiv 2017",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Binas",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Neil",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "C"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Delbruck",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ddd17",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1711.01458"
                ]
            }
        },
        "BIBREF188": {
            "ref_id": "b188",
            "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "X"
                    ],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Savva",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Halber",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Funkhouser",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nie\u00dfner",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "21--26",
            "other_ids": {}
        },
        "BIBREF189": {
            "ref_id": "b189",
            "title": "The University of Michigan North Campus Long-Term Vision and LIDAR Dataset",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Carlevaris-Bianco",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ushani",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "Eustice",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF190": {
            "ref_id": "b190",
            "title": "University of Michigan North Campus long-term vision and lidar dataset",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Carlevaris-Bianco",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "K"
                    ],
                    "last": "Ushani",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "Eustice",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Int. J. Robot. Res",
            "volume": "35",
            "issn": "",
            "pages": "1023--1035",
            "other_ids": {
                "DOI": [
                    "10.1177/0278364915614638"
                ]
            }
        },
        "BIBREF192": {
            "ref_id": "b192",
            "title": "Argoverse: 3d tracking and forecasting with rich maps",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "F"
                    ],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lambert",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sangkloy",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bak",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hartnett",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Carr",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lucey",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "8748--8757",
            "other_ids": {}
        },
        "BIBREF193": {
            "ref_id": "b193",
            "title": "A Coarse-to-Fine Many-Task Network for Joint 2D and 3D Vehicle Analysis from Monocular Image",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chabot",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chaouch",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Rabarisoa",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Teuli\u00e8re",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Chateau",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Deep",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Manta",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "1827--1836",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2017.198"
                ]
            }
        },
        "BIBREF194": {
            "ref_id": "b194",
            "title": "Urtasun, R. 3d object proposals for accurate object class detection",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kundu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Berneshawi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Fidler",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "424--432",
            "other_ids": {}
        },
        "BIBREF195": {
            "ref_id": "b195",
            "title": "Data-driven 3D Voxel Patterns for object category recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Savarese",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "1903--1911",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2015.7298800"
                ]
            }
        },
        "BIBREF196": {
            "ref_id": "b196",
            "title": "Reconstructing vehicles from a single image: Shape priors for road scene understanding",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "K"
                    ],
                    "last": "Murthy",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "S"
                    ],
                    "last": "Krishna",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chhaya",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Krishna",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF197": {
            "ref_id": "b197",
            "title": "Urtasun, R. 3d object proposals using stereo imagery for accurate object class detection",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kundu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Fidler",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "40",
            "issn": "",
            "pages": "1259--1272",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2017.2706685"
                ]
            }
        },
        "BIBREF198": {
            "ref_id": "b198",
            "title": "Vehicle detection and localization on bird's eye view elevation images using convolutional neural network",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "L"
                    ],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Westfechtel",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hamada",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ohno",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tadokoro",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)",
            "volume": "",
            "issn": "",
            "pages": "102--109",
            "other_ids": {}
        },
        "BIBREF199": {
            "ref_id": "b199",
            "title": "Frustum pointnets for 3d object detection from rgb-d data",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Guibas",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "918--927",
            "other_ids": {}
        },
        "BIBREF200": {
            "ref_id": "b200",
            "title": "Sparse-to-dense 3d object detector for point cloud",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Std",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1951--1960",
            "other_ids": {}
        },
        "BIBREF201": {
            "ref_id": "b201",
            "title": "Kda3d: Key-point densification and multi-attention guidance for 3d object detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/rs12111895"
                ]
            }
        },
        "BIBREF202": {
            "ref_id": "b202",
            "title": "Self-supervised model adaptation for multimodal semantic segmentation",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Valada",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mohan",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Burgard",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Int. J. Comput. Vis",
            "volume": "128",
            "issn": "",
            "pages": "1239--1285",
            "other_ids": {
                "DOI": [
                    "10.1007/s11263-019-01188-y"
                ]
            }
        },
        "BIBREF203": {
            "ref_id": "b203",
            "title": "SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Milford",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "F"
                    ],
                    "last": "Wyeth",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 2012 IEEE International Conference on Robotics and Automation",
            "volume": "",
            "issn": "",
            "pages": "14--18",
            "other_ids": {}
        },
        "BIBREF204": {
            "ref_id": "b204",
            "title": "Comparison of camera-based and 3D LiDAR-based place recognition across weather conditions",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "\u017bywanowski",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Banaszczyk",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Nowicki",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV)",
            "volume": "",
            "issn": "",
            "pages": "886--891",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The overall structure of the survey that shows all the topics discussed in each section.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Results: ACM Digital Library paper selection based on the inclusion and exclusion criteria.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Camera and LiDAR-based Data Representation Modalities for 3D Object Recognition (3DOR).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The architecture[55] overview with eleven prediction branches divided into 2DOR, 3DOR, and pair constraints.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The pipeline of SS3D[57] for 3DOR from a single view.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "The architecture of M3DSSD[58] for monocular 3D object detection (a) Framework backbone. (b) The two-step feature alignment, classification and regression heads with ANAB for depth prediction. (c) Other regression heads.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "The architecture of Stereo R-CNN",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Network architecture of Stereo CenterNet[60] with 10 outputs and sub-branches for two tasks and the estimated 3D BBoxes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "The pipeline of RT3D[61]: (a) LiDAR-based 3D point cloud on (b) a depth map encoded with height information of points; (c) a CNN-based two-stage detector is utilized for region proposals generation and their classification on pose-sensitive feature maps; (d) visualization of detected vehicles with orientated 3D BBoxes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "The SegVNet[63] with major components VFE, SCE, and depth aware head.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "The network[65] is based on a UNet backbone with 3D sparse convolution and deconvolution to perform object detection on the Lidar BEV.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Illustration of IPOD[66] consisting of a sub-sampling network, point-based proposal generation, and the components of network architecture, which classifies and regresses the generated proposals.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "The network pipeline of FVNet[67] composed of PG-Net and PE-NET.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "The architecture with DPointNet[68] detector consisting of two stages for 3D proposal generation and proposal refinement.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "3D object detection pipeline of HGNet[71] framework with three main components: GU-net, Proposal Generator, and ProRe Module.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "The pointPillars[72] with a feature enhancement layer.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "The input features of the MV3D[73] network.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "The fusion-based one-stage object detection[74] network.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Aggregate view object detection[120] network pipeline for 3D object detection in the context of autonomous driving.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "The overview of the MVX-Net[76] PointFusion method.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "The CNN-RNN[78] architecture for video recognition.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "The pipeline of Event-VPR[79] for 3DPR.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "The architecture of our PointNetVLAD[80] network.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "The multi-task network structure[81] for scene recognition in indoor environments.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "The system overview[82] for point-cloud-based place recognition using CNN feature extraction.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF25": {
            "text": "The LPD-Net[83] for large scale place recognition.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF26": {
            "text": "The methodology[84] of oriented recognition from 3D point clouds.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF27": {
            "text": "The pipeline of[85] place recognition pipeline in semi-dense maps.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF28": {
            "text": "Three network structures [86] (a) group based CNN (b) Siamese CNN, and (c) descriptor extraction CNN trained using contrastive loss.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF29": {
            "text": "The descriptor extraction network[86] used in the three CNNs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF30": {
            "text": "The network of MinkLoc3D[87] for point-cloud-based place recognition.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF31": {
            "text": "CNN based Camera-LiDAR Fused descriptor[88] for place recognition.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF32": {
            "text": "PIC-Net[89] composed of image and point cloud branch with spatial, channel, and global attention for large-scale place recognition.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF33": {
            "text": "o A: ISIA RGB-D; B: HKUST; C: KAIST; D: NYUD2; E: Sun RGB-D; G: In-House; H: KITTI; I: Oxford Robot-car; K: NCLT; L: Argoverse; M: ScanNet; N: DDD17; O: Waymo; P: MVSEC.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF34": {
            "text": "Contributions: Conceptualization, S.M., S.-H.J. and T.-Y.K.; Methodology, S.M., S.-H.J. and T.-Y.K.; Validation, S.M., S.-H.J., E.-J.K., S.-H.B., G.-G.I., J.-W.P. and T.-Y.K.; Formal analysis, S.M. and T.-Y.K.; Investigation, S.M., S.-H.J., E.-J.K., S.-H.B., G.-G.I., J.-W.P. and T.-Y.K.; Data curation, S.M., S.-H.J., E.-J.K., S.-H.B., G.-G.I. and J.-W.P.; Writing-original draft preparation, S.M.; Writing-review and editing, S.M., S.-H.J., E.-J.K., S.-H.B., G.-G.I., J.-W.P. and T.-Y.K.; Visualization, S.M., S.-H.J., S.-H.B., E.-J.K., G.-G.I. and J.-W.P.; Supervision, T.-Y.K. All authors have read and agreed to the published version of the manuscript.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF35": {
            "text": "This research was supported by the Korea Evaluation Institute of Industrial Technology (KEIT) funded by the Ministry of Trade, Industry & Energy (MOTIE) (No. 1415168187).",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The Contributions of This Survey.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Robots in Public Spaces.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Robots in Domestic Environments.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Robots in Hospitals.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Robots in Industrial Environments.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Comparison of Camera and LiDAR Sensors.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Methodology and Limitations: Camera-based 3DOR.",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Literature Analysis: Camera-based 3D Object Detection Methods.",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "Literature Analysis: 3D Voxel Grid-based 3D Object Recognition Methods.",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "Literature Analysis: PointNet-based 3DOR Methods.",
            "latex": null,
            "type": "table"
        },
        "TABREF10": {
            "text": "Literature Analysis: Graph-based Representation for 3DOR.",
            "latex": null,
            "type": "table"
        },
        "TABREF11": {
            "text": "Methodology and Limitation(s): Camera-LiDAR fusion-based 3DOR Methods.",
            "latex": null,
            "type": "table"
        },
        "TABREF12": {
            "text": "Literature Analysis: Camera-LiDAR Fusion-based 3D Object Recognition Methods.",
            "latex": null,
            "type": "table"
        },
        "TABREF13": {
            "text": "Cont.",
            "latex": null,
            "type": "table"
        },
        "TABREF14": {
            "text": "Literature Analysis: 3D Place Recognition (3DPR) Methods.",
            "latex": null,
            "type": "table"
        },
        "TABREF15": {
            "text": "Literature Analysis: Datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF16": {
            "text": "Comparison of the Results on the KITTI Validation Dataset for the Car Category.",
            "latex": null,
            "type": "table"
        },
        "TABREF17": {
            "text": "Comparison of the Results on the KITTI Test Dataset for the Car Category.Table 23. Comparison of the Results on the KITTI Validation Dataset for the Pedestrian and Cyclist Categories. Comparison of the Results on the KITTI Validation Dataset for the Pedestrian and Cyclist Categories.",
            "latex": null,
            "type": "table"
        },
        "TABREF18": {
            "text": "Comparison of the Results on the KITTI, NCLT, and KAIST Datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF19": {
            "text": "Comparison of the Results on the Argoverse Dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF20": {
            "text": "Comparison of the Results on the ScanNet, Sun RGB-D, ISIA RGB-D, and NYUD2 Datasets.In[71], 3D object detection results with 61.3 % accuracy on the ScanNet dataset has been achieved with mAP @ 0.25 while 61.6 % on Sun RGB-D dataset for the ten most commonly used object categories ( such as bed, sofa, chair, table etc). The results are listed in",
            "latex": null,
            "type": "table"
        },
        "TABREF21": {
            "text": "Comparison of the Results on the Oxford Robot-car, MVSEC, and In-House Datasets. Oxford Robot-Car In-House U.S. R.A. B.D.",
            "latex": null,
            "type": "table"
        },
        "TABREF22": {
            "text": "Net Parameter Estimation network (PE-Net) KITTI Karlsruhe Institute of Technology and Toyota Technological Institute HKUST Hong Kong University of Science and Technology KAIST Korea Advanced Institute of Science and Technology NYUD2 New York University Dataset version 2 NCLT The University of Michigan North Campus Long-Term Vision Lidar-based multi-task learning network IPOD Intensive Point-based Object Detector for Point Cloud FVNet Front-View proposal generation Network DPointNet Density-oriented Point Network GCNN Graph Convolutional Neural Network RGNet Relation Graph Network HGNet Hierarchical Graph Network S-AT GCN Spatial Attention Graph Convolution MV3D Multi-view 3D Network MS-CNN Multi Scale Convolutional Neural Network S-AT Spatial-Attention BEVLFVC Bird's Eye View LIDAR point cloud and Front View Camera image MVX-Net Multimodal Voxelnet for 3d object detection NetVLAD Network for Vector of Locally Aggregated Descriptors DGCNN Dynamic graph Convolutional Neural Network AUC Area Under Curve AP Average Precision LPD-Net Large-scale Place Description",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
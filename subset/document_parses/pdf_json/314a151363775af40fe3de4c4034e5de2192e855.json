{
    "paper_id": "314a151363775af40fe3de4c4034e5de2192e855",
    "metadata": {
        "title": "Interpretable deep-learning models to help achieve the Sustainable Development Goals",
        "authors": [
            {
                "first": "Ricardo",
                "middle": [],
                "last": "Vinuesa",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "KTH Royal Institute of Technology",
                    "location": {
                        "settlement": "Stockholm",
                        "country": "Sweden"
                    }
                },
                "email": "es:rvinuesa@mech.kth.se"
            },
            {
                "first": "Beril",
                "middle": [],
                "last": "Sirmacek",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Smart Cities",
                    "institution": "Saxion University of Applied Sciences",
                    "location": {
                        "settlement": "Enschede",
                        "country": "The Netherlands"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "We discuss our insights into interpretable artificial-intelligence (AI) models, and how they are essential in the context of developing ethical AI systems, as well as data-driven solutions compliant with the Sustainable Development Goals (SDGs). We highlight the potential of extracting truly-interpretable models from deeplearning methods, for instance via symbolic models obtained through inductive biases, to ensure a sustainable development of AI.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Recent interest in artificial-intelligence (AI) methods has led to their application in a progressively wider range of applications, and their impact in our daily lives should not be underestimated. Despite the significant benefits of AI technology to improve our well-being [12] , there are a number of areas where AI can hinder the achievement of a sustainable future [10] . This dilemma has been recognized by Hilbert [6], and one of the main limitations of current AI technology in this context is the lack of interpretability of these models. The implications of this are nicely articulated by Rudin [11], who claims that AI models need to be inherently interpretable and not mere \"black boxes\" or provide limited and shallow explainability. Another important example of interpretability of data-driven models can be found in the context of digital contact tracing for handling the coronavirus disease-19 (COVID-19) pandemic: there should be a right to contest the decisions made by the algorithm, and interpretability would be essential in this [13] .",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 279,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 370,
                    "end": 374,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "AI algorithms have the potential to support the achievement of the Sustainable Development Goals (SDGs) of the United Nations (UN) [12] . For instance, Jean et al.",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 135,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "[8] proposed a pioneering method to identify and track regions of poverty using satellite images, via convolutional neural networks (CNNs), a study with important implications for SDG 1 (on no poverty). However, we discuss below that it is essential to add interpretability to this type of model in order to develop efficient strategies to tackle this SDG. A similar observation can be made regarding SDG 13 (on climate action), where Chantry et al.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "[2] stated that the complexity of the AI models, combined with the numerous unknown parameters, make it extremely challenging to create a robust climate model, especially for forecasting applications. Another challenge for providing a robust model was of course the difficulty of generalizing one climate model to all the areas of Earth where measurements are conducted. On the other hand, Huntingford et al. [7] highlighted the fact that AI models are typically black boxes, a fact that complicates identifying the origin of errors, the relative importance of the various parameters, and generally complicates climate-change research. They also argue that black-box AI models also generate uncertainty regarding their acceptability by government or the general public when they produce suggestions and/or make decisions. One well-known example is the medical application Stream developed by DeepMind, which was aimed at kidney-disease prediction. Since the deep-learning version of Stream was unable to provide interpretability of the decision-making process, it * Corresponding Author",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "was not approved to be used in practice. Later on, the application got approval when a simple decision-tree model (which did not provide the same high accuracy as the deep-learning model) was used instead, since it provided high interpretaibility of the decision-making process. 1 Recent advances in the context of interpretable AI (see the excellent recent review on the topic by Fan et al. [5] ) have brought mathematical techniques to provide interpretability properties for the training/test data set, model parameters, output, etc. This is therefore an excellent way to bring transparency to the black-box AI systems, which is such an important feature in the context of AI ethics. Here we would like to differentiate between methods that provide mere explanations regarding the AI results, and methods that yield a complete interpretability of those results. This is an important difference, and we advocate (aligned with the work by Rudin [11] ) for methods that provide interpretability. The ideal scenario is when the AI model is interpretable from its inception [15, 9] ; however, in many applications, particularly dealing with deep learning, the models are already trained on extensive databases and it is costly (or impossible) to reformulate them in an interpretable framework, maintaining the accuracy. Here we will discuss some explainability methods, mention their limitations, and we will propose the use of an approach for interpretability of already-trained neural networks based on symbolic models obtained through inductive biases, recently proposed by Cranmer et al. [3] in the context of physical systems.",
            "cite_spans": [
                {
                    "start": 392,
                    "end": 395,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 946,
                    "end": 950,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1072,
                    "end": 1076,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1077,
                    "end": 1079,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1590,
                    "end": 1593,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "An example of approach providing mere explanations is the family of saliency methods, which basically identify which regions of the images or what features of the input data are more relevant to the predictions of the model. Besides the simplistic approach based on assessing the prediction changes when removing certain certain features, saliency methods typically rely on game-theory concepts such as the Shapley value [1] , which quantifies the contribution of a certain feature to the predicted results. The main criticism against this type of methods [11] is the fact that they basically identify the parts of the input data the AI model is focused on, but they do not provide any interpretation for the actual outcome (for instance, the reason to place a certain image in one particular category). A similar criticism can be made against feature-analysis methods, which focus on the neural-network features to obtain improved explanatory power from the model [5] . These rely on inverting-based methods that can produce synthesized images from the feature maps [4] , again lacking complete interpretability regarding the final outcome of the AI model.",
            "cite_spans": [
                {
                    "start": 421,
                    "end": 424,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 556,
                    "end": 560,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 965,
                    "end": 968,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1067,
                    "end": 1070,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "In our view, the method proposed by Cranmer et al. [3] is preferred, since it provides a greater interpretability power to already-trained deep-learning models. Their method is based on the following four steps:",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 54,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "(i) One needs to first develop a deep-learning model with a separable internal structure and an inductive bias which is relevant to the nature of the data. Note that the inductive bias constitutes the set of assumptions made on the structure of the deep-learning model to be able to generalize beyond the data seen during training. (ii) After defining the model in (i), it is trained using the standard procedures corresponding to the chosen architecture and using the selected training database. (iii) Then, the key aspect proposed by Cranmer et al. [3] is to fit symbolic expressions to the functions composing the deep-learning model. This is done by means of a genetic algorithm which stochastically combines algebraic formulas. (iv) Finally, the internal functions are replaced by the fitted symbolic expressions.",
            "cite_spans": [
                {
                    "start": 551,
                    "end": 554,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The power of this method lies in the possibility of analytical expressions to perform the same predictions initially carried out by the black-box deep-learning model. Here it is important to clarify that the resemblance of the predictions between the new and the original models will of course depend on the quality of the fit, but the work by Cranmer et al. [3] is very encouraging in the sense that their new symbolic-based model exhibited better generalization capabilities than the original black-box model. In their case, they illustrated the use of this approach with simple physical examples and a more complex one based on dark-matter simulation data. Their work was focused on graph neural networks (GNNs). We note that an important aspect of this process is to promote sparse models through regularization terms in the loss, effectively favoring the principle of Occam's Razor.",
            "cite_spans": [
                {
                    "start": 359,
                    "end": 362,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "There are many examples where black-box AI models make decisions with important implications in the individuals, and more transparency (provided through interpretability) would be highly beneficial: facialrecognition applications [10] , decisions regarding assignment of loans or recruitment [12] , health [14] , etc. Here we illustrate the use of the methodology described above with the example of tracking poverty using satellite images and CNNs [8] . This work essentially identified features such as night-light intensity, roofing material, distance to urban areas, etc., and they predicted the average economical consumption per capita and day. The predicted-consumption values are in very good agreement with the reported ones, and their identified features can be related to around 75% of the local economic results. Adding interpretability to this model would help to really understand what is the influence of each parameter on the outcome, yielding a more robust and useful tool to track poverty and coordinate actions. In fact, this type of interpretable system may be able to shed light into the dynamics of poverty, producing a deeper understanding of the current trends, and potentially being able to predict and tackle future negative developments related to SDG 1. A schematic representation of the process, as well as its implications on the SDGs, are shown in Figure 1 . Thus, the interpretability method by Cranmer et al. [3] can provide clear understanding of the reasoning from the deep-learning models, a fact that may help both professionals and policy makers in two ways: first, being able to know how to improve the models when needed; and second, being able to design better actions aligned with the SDGs. [3] for adding interpretability to AI models based on symbolic equations. We use the example of poverty tracking via satellite images by Jean et al. [8] . Here n, r and d denote nigh-light intensity, roofing material and distance to urban areas, respectively, whereas C i and \u03b1 i are model constants. Note that a very simple symbolic model is provided as an example. This model will help to better understand the impact of the different variables on the poverty prediction, thus enhancing the actions to achieve SDG 1. Schematic representation adapted from Ref. [3] , and panels extracted from Ref. [8] with permission from the publisher (The American Association for the Advancement of Science).",
            "cite_spans": [
                {
                    "start": 230,
                    "end": 234,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 292,
                    "end": 296,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 306,
                    "end": 310,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 449,
                    "end": 452,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1442,
                    "end": 1445,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1733,
                    "end": 1736,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1882,
                    "end": 1885,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 2295,
                    "end": 2298,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 2332,
                    "end": 2335,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 1379,
                    "end": 1387,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": ""
        },
        {
            "text": "To conclude, we hope that, through this Comment piece, we will be able to influence AI researchers and policymakers towards the highest benefit for society and the environment, prioritizing interpretable AI and transparency of the employed models. If such interpretable models are achieved, they would also have chances to serve in real applications, by fitting into the Trustworthy-AI assessment guidelines provided by the European Commission. 2 ",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Explaining deep neural networks with a polynomial time algorithm for Shapley values approximation. ICML",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ancona",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "\u00d6ztireli",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gross",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Opportunities and challenges for machine learning in weather and climate modelling: hard, medium and soft AI",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chantry",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Christensen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dueben",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Palmer",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Philos. Trans. Royal Soc. A",
            "volume": "379",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Discovering symbolic models from deep learning with inductive biases",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cranmer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sanchez-Gonzalez",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Battaglia",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Cranmer",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Spergel",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ho",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "34th Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.11287"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Inverting visual representations with convolutional networks. CVPR",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dosovitskiy",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "4829--4837",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "On interpretability of artificial neural networks: A survey",
            "authors": [
                {
                    "first": "F.-L",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Transactions on Radiation and Plasma Medical Sciences",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Big data for development: A review of promises and challenges",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hilbert",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Dev. Policy Rev",
            "volume": "34",
            "issn": "",
            "pages": "135--174",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Machine learning and artificial intelligence to aid climate change research and preparedness",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Huntingford",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "S"
                    ],
                    "last": "Jeffers",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "B"
                    ],
                    "last": "Bonsall",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "M"
                    ],
                    "last": "Christensen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lees",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Environ. Res. Lett",
            "volume": "14",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Combining satellite imagery and machine learning to predict poverty",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Jean",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Burke",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "B"
                    ],
                    "last": "Davis",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "M"
                    ],
                    "last": "Lobell",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ermon",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Science",
            "volume": "353",
            "issn": "",
            "pages": "790--794",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "An interpretable framework of data-driven turbulence modeling using deep neural networks",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vinuesa",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Mi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Laima",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Phys. Fluids",
            "volume": "33",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Data deprivations, data gaps and digital divides: lessons from the COVID-19 pandemic",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Naud\u00e9",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vinuesa",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Rudin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Nat. Mach. Intell",
            "volume": "1",
            "issn": "",
            "pages": "206--215",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "The role of artificial intelligence in achieving the Sustainable Development Goals",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vinuesa",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Azizpour",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Leite",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Balaam",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Dignum",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Domisch",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fell\u00e4nder",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "D"
                    ],
                    "last": "Langhans",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tegmark",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Fuso Nerini",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nat. Commun",
            "volume": "11",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A socio-technical framework for digital contact tracing",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vinuesa",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Theodorou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Battaglini",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Dignum",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Results Eng",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Toward reliable automatic liver and tumor segmentation using convolutional neural network based on 2.5D model",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wardhana",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Naghibi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sirmacek",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Abayazid",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Int. J. CARS",
            "volume": "16",
            "issn": "",
            "pages": "41--51",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A novel evolutionary algorithm applied to algebraic modifications of the RANS stress-strain relationship",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weatheritt",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "D"
                    ],
                    "last": "Sandberg",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "J. Comput. Phys",
            "volume": "325",
            "issn": "",
            "pages": "22--37",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Schematic representation of the method proposed by Cranmer et al.",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "RV acknowledges the financial support from the Swedish Research Council (VR).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        }
    ]
}
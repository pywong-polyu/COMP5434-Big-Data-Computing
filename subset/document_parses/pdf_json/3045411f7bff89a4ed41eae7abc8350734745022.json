{
    "paper_id": "3045411f7bff89a4ed41eae7abc8350734745022",
    "metadata": {
        "title": "Sensors on the Move: Onboard Camera-Based Real-Time Traffic Alerts Paving the Way for Cooperative Roads",
        "authors": [
            {
                "first": "Olatz",
                "middle": [],
                "last": "Iparraguirre",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universidad de Navarra",
                    "location": {
                        "addrLine": "Manuel Lardizabal 13",
                        "postCode": "20018",
                        "settlement": "Tecnun, Donostia/San Sebasti\u00e1n",
                        "country": "Spain"
                    }
                },
                "email": ""
            },
            {
                "first": "Aiert",
                "middle": [],
                "last": "Amundarain",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universidad de Navarra",
                    "location": {
                        "addrLine": "Manuel Lardizabal 13",
                        "postCode": "20018",
                        "settlement": "Tecnun, Donostia/San Sebasti\u00e1n",
                        "country": "Spain"
                    }
                },
                "email": ""
            },
            {
                "first": "Alfonso",
                "middle": [],
                "last": "Brazalez",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universidad de Navarra",
                    "location": {
                        "addrLine": "Manuel Lardizabal 13",
                        "postCode": "20018",
                        "settlement": "Tecnun, Donostia/San Sebasti\u00e1n",
                        "country": "Spain"
                    }
                },
                "email": ""
            },
            {
                "first": "Diego",
                "middle": [],
                "last": "Borro",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universidad de Navarra",
                    "location": {
                        "addrLine": "Manuel Lardizabal 13",
                        "postCode": "20018",
                        "settlement": "Tecnun, Donostia/San Sebasti\u00e1n",
                        "country": "Spain"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [],
    "body_text": [
        {
            "text": "No deaths and serious injuries on European roads by 2050. This is the goal established by the European Commission (EC). Meanwhile, EU road safety targets halving these numbers by 2030 [1] . The EU has seen a substantial decrease in road fatalities in the past, but these numbers have been stagnating in recent years. The latest research studies indicate that even with the lockdown due to the COVID-19 pandemic situation, deaths on the road did not decline by the same degree as traffic volume did [2] .",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 187,
                    "text": "[1]",
                    "ref_id": null
                },
                {
                    "start": 498,
                    "end": 501,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To address this trend and meet the targets, the EC is committed to digital technologies. Future intelligent vehicles will interact with other vehicles and with the road infrastructure. This interaction is the domain of Cooperative Intelligent Systems (C-ITS) and is expected to significantly improve road safety, traffic efficiency, environmental performance and comfort driving, by helping the driver to make better decisions and adapt to the traffic situation [3] . Additionally, the EU road safety policy framework also focuses on infrastructure safety and the updating of some legislative measures.",
            "cite_spans": [
                {
                    "start": 462,
                    "end": 465,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Over the last decades, the remarkable development in technology and the increasing digitalisation made enormous advancements in the intelligent vehicles field. Its advanced electronics and mechatronics, communications and sensors made current vehicles scale-up levels of driving automation. However, there is still a long way to go before we are faced with fully autonomous and cooperative roads. The perception of the environment, given its methods for the implementation while Section 5 shows the results obtained. All these sections are structured in such a way as to separate the two applications mentioned above (TSR and fog detection). Finally, Section 6 concludes the paper by summarising the results with a global discussion focused on its application.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In recent years, there has been a lot of work related to the TSR field due to the development of machine learning methods and especially with the new deep learning techniques. Thus, there is plenty of literature and surveys around this topic. However, many methodologies got obsolete when large public databases, such as the German Traffic Sign Recognition (GTSRB) [5] and Detection (GTSDB) [6] appeared. Usually, in the literature, the TSR task is divided into two different phases: detection and classification. The first phase consists of identifying the presence of one or more signals and locating them in the image. The second step is to classify the signal according to its category and meaning.",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 368,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 391,
                    "end": 394,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Traffic Sign Recognition"
        },
        {
            "text": "The detection has been mostly covered with model-based techniques using simple features, such as colours and shapes, and they have achieved high performance for the Region of Interest (ROI) extraction. Several works are more inclined to other learning methods, such as Adaboost with Haar-like features [7] , SVM [8] , Local Binary Patterns [9] and other non-grey features, such as Aggregate Channel Features (ACF) [10, 11] . With the development of deep learning, CNN-based detectors have also appeared both to train simple features [12] and as well as end-to-end networks [13] . Regarding classification, deep learning is the method with the best performance in recent years [14] . Among the deep learning models, the convolutional neural networks (CNN) are the most popular techniques [15] .",
            "cite_spans": [
                {
                    "start": 302,
                    "end": 305,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 312,
                    "end": 315,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 340,
                    "end": 343,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 414,
                    "end": 418,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 419,
                    "end": 422,
                    "text": "11]",
                    "ref_id": null
                },
                {
                    "start": 533,
                    "end": 537,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 573,
                    "end": 577,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 676,
                    "end": 680,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 787,
                    "end": 791,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Traffic Sign Recognition"
        },
        {
            "text": "Although these learning methods mentioned above can achieve state-of-the-art results, most of them often need assistance to reach good accuracy and efficiency [16] . Many non-technical challenges jeopardise the performance of these algorithms, especially when the resolution of the images is low or there are different lighting conditions, blur, occlusions or other artefacts. Thus, it is hard to identify the best method to solve the detection problem [14] . Currently, the improvement is focused on solving both detection and classification phases together in one step [16] . Additionally, researchers are also trying to solve this issue in all kind of weather and light situations [17, 18] .",
            "cite_spans": [
                {
                    "start": 159,
                    "end": 163,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 453,
                    "end": 457,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 571,
                    "end": 575,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 684,
                    "end": 688,
                    "text": "[17,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 689,
                    "end": 692,
                    "text": "18]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Traffic Sign Recognition"
        },
        {
            "text": "In contrast to TSR methods, the field of fog detection has not been as well studied. There are only a few investigations in the last twenty years. These studies can be classified into two different approaches: The first one relies on the computation of the visibility range based on Koschimieder's law [19] . The second approach is based on the extraction of image characteristics. Some studies are focused on black and white images, most of them based on the study of grey histograms [20] . However, Liu et al. [21] address this problem by applying HSV colour space thresholding, which seems to reduce the number of parametrization compared with methods base on grey-level histograms.",
            "cite_spans": [
                {
                    "start": 302,
                    "end": 306,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 485,
                    "end": 489,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 512,
                    "end": 516,
                    "text": "[21]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Fog Detection"
        },
        {
            "text": "The traffic sign recognition research field has seen increased attention in the last years; therefore, since 2013, many new large datasets have been publicly available. This has allowed many comparative studies that have helped to improve existing algorithms. However, most of the existing datasets reached saturation, since most of the results are in the range 95-99% of the perfect solution [22, 23] Nevertheless, external non-technical challenges, such as lighting variations and weather condition changes, occlusions or damaged images, even variations in traffic signs among different countries, may decrease the system's performance [17] .",
            "cite_spans": [
                {
                    "start": 393,
                    "end": 397,
                    "text": "[22,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 398,
                    "end": 401,
                    "text": "23]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 638,
                    "end": 642,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Existing Traffic Sign Recognition Datasets"
        },
        {
            "text": "Thus, the last algorithms and the newest datasets are focused on the recognition under complex conditions. In this paper, we did a deep analysis for all publicly available datasets as of today, which is summarised in Table 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 217,
                    "end": 224,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Existing Traffic Sign Recognition Datasets"
        },
        {
            "text": "Ceit-TSR consists of 264 colour images captured from 40 different videos of driving tracks within the Basque Country (Spain). They were recorded using different mobile phones and onboard cameras located on the dashboard. The images of this dataset were specifically selected so that, in addition to the different weather and light conditions that are covered in other existing datasets, they would also include other complex conditions. Those incorporate images with motion-blur, low-resolution signs, distant signs, low contrast and windshield artefacts (reflections, raindrops, dirtiness, etc.), as is shown in Figure 1 . All images were manually annotated using the Image Labeler MATLAB tool, resulting in 418 bounding boxes that were also classified in six different categories: speed-limit, prohibitory, mandatory, caution, yield and restriction end. Finally, an extra category was added for the signs that did not fit the defined ones. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 613,
                    "end": 621,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Ceit Traffic Sign Recognition Dataset"
        },
        {
            "text": "Although there are large-scale road datasets, such as KITTI [24] , Cityscapes [25] , Mapillary Vistas [26] , ApolloScape [27] and BDD100k [28] , the availability of useful image datasets for the foggy scenes evaluation is very low. Most of the existing datasets contain few or even no foggy scenes due to the difficulty of collecting and annotating them. For example, the Mapillary Vistas database contains 10 images out of 25,000 of misty images (not dense fog). Thus, some of the existing foggy datasets are generated from synthetic images or real-world images post-processed with synthetic fog (see Table 2 for a summary). The Foggy Road Image Dataset (FRIDA) is the most popular one, which was created by Hauti\u00e8re et al. with synthetic images [29] and was later extended [30] . Among these images, six images of each scene show different kinds of fog: no fog, uniform fog, heterogeneous fog, cloudy fog, cloudy heterogeneous fog and finally a depth map image is also included.",
            "cite_spans": [
                {
                    "start": 60,
                    "end": 64,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 78,
                    "end": 82,
                    "text": "[25]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 102,
                    "end": 106,
                    "text": "[26]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 121,
                    "end": 125,
                    "text": "[27]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 138,
                    "end": 142,
                    "text": "[28]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 747,
                    "end": 751,
                    "text": "[29]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 775,
                    "end": 779,
                    "text": "[30]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [
                {
                    "start": 602,
                    "end": 609,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Existing Fog Detection Datasets"
        },
        {
            "text": "A more recent dataset derived from the Cityscapes dataset was generated by Sakaridis et al., which is called Foggy Cityscapes [31] . It constitutes a collection of images from the original dataset that are processed and automatically annotated into foggy images using a fog simulator. Same authors also generated a new dataset with real-world foggy road scenes. The Foggy Driving dataset contains 101 light fog images captured with a cell phone camera at different points of Zurich and also with images collected from the web. Later on, they extended this dataset with the collected video frames of the same city and its suburbs improving the resolution and having much wider variety of scenes and different fog levels; this last dataset is named Foggy Zurich [32] .",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 130,
                    "text": "[31]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 760,
                    "end": 764,
                    "text": "[32]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Existing Fog Detection Datasets"
        },
        {
            "text": "Finally, the SeeingThroughFog dataset [33] was developed in the context of the DENSE project. It records 10,000 km of driving in Northern Europe under different weather and illumination conditions in February and December 2019. The resulting annotations contain 5.5 k clear weather frames, 1 k captured in dense fog, 1 k captured in light fog and 4 k captured in snow/rain.",
            "cite_spans": [
                {
                    "start": 38,
                    "end": 42,
                    "text": "[33]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Existing Fog Detection Datasets"
        },
        {
            "text": "The Ceit-Foggy dataset consists of a set of 41 videos corresponding to approximately 300 km of driving through the Basque Country and Segovia. These videos were recorded in different weather conditions, as shown in Figure 2 . In total, 4480 frames have been extracted and labelled in five categories: sunny, cloudy, light fog, moderate fog and dense fog. Additionally, in some videos, if there was also rain or it was a dawn/dusk, this has also been specified. As for the Ceit-TSR dataset, these images were also recorded using different mobile phone and onboard cameras located on the dashboard. It is worth mentioning that these annotations just classify the general condition of the images; however, especially for long tracks, these conditions could change during the video. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 215,
                    "end": 223,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Ceit Fog Detection Dataset"
        },
        {
            "text": "The implemented traffic sign recognition system is composed of two modules: detection and classification (see Figure 3 ). These modules were developed using MATLAB 2017a software. As it will be explained in the following sections, the authors did the fine-tuning of both algorithms, the detector and classifier, but not for the neural network used for the classification-the latter being a publicly available pre-trained model. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 110,
                    "end": 118,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Traffic Sign Recognition"
        },
        {
            "text": "After the revision of the state-of-the-art of traffic sign detection methods, we concluded there is no clear framework that achieves the best results. Thus, we decided to implement and compare some of the most popular methods. Firstly, we tried using classic features, such as colour and shape, and modelling with the Viola Jones cascade detector. However, these alternatives were finally discarded as they were difficult to adjust and not very flexible with changing light conditions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Traffic Sign Detection"
        },
        {
            "text": "Usually, existing detectors could be improved in two ways: using more complex features or implementing more powerful learning algorithms. Since the combination of boosting and cascading has proven to be very efficient for object detection [34] , the key is to find representative characteristics at a low computational cost. In this context, a combination of record-breaking characteristics has emerged for pedestrian detection [35] and we tried to apply this method for traffic sign detection. The Aggregated Channel Features (ACF) detection framework uses an AdaBoost classifier trained with ACF features to classify image patches. The entire image is searched by using a multiscale sliding window approach. These ACF features consist of ten different channels: three from the LUV colour space, the gradient magnitude and the six oriented gradient maps (see Figure 4 ). Afterwards, the sum of every block of pixels of these channels is computed using fast features pyramids, and downscaled. Features are single-pixel lookups in the aggregated channels. Boosting is used to train and combine decision trees over these features (pixels) to accurately locate the object [35] . The channel extension offers a rich representation capacity, while the simplicity of the features allows a low computational cost. Therefore, the ACF detector was chosen to be implemented in this paper. ",
            "cite_spans": [
                {
                    "start": 239,
                    "end": 243,
                    "text": "[34]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 428,
                    "end": 432,
                    "text": "[35]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "text": "[35]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [
                {
                    "start": 860,
                    "end": 868,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Traffic Sign Detection"
        },
        {
            "text": "A pre-trained CNN model was used for the traffic sign classification, currently the most popular technique for object classification. This CNN model was trained from alexnet, applying transfer learning with the GTSRB dataset [36] . The authors of this model reported 97% accuracy in the first 50 GTSRB images.",
            "cite_spans": [
                {
                    "start": 225,
                    "end": 229,
                    "text": "[36]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Traffic Sign Classification"
        },
        {
            "text": "The input images were pre-processed before passing through the model to improve the efficiency of the CNN model. The pre-processing consists of the following minor changes:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Traffic Sign Classification"
        },
        {
            "text": "(1) Normalization of the image by subtracting the average image from the dataset. This is done to make the network less sensitive to differing background and lightening conditions. (2) Random cropping, since the alexnet requires input images of 227 \u00d7 227 pixels. For this aim, we decided to make 10 random crops of 227 \u00d7 227 on the 256 \u00d7 256 images. The 10 cropped images were classified and the bounding box with the resulting maximum score was selected. This way, the class with the highest confidence will be the final result of the classification.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Traffic Sign Classification"
        },
        {
            "text": "As it was explained in the state-of-the-art revision there are two main approaches for the detection of foggy scenes based on vision techniques: (1) measurement of the visibility range, and (2) extraction of image characteristics. The first approach was discarded for this application because there is no direct relation to the fog physical properties since several factors affect it, such as the background light, road curvature, presence of contrasted objects, etc. [37] . The high complexity of this problem could lead to the study of a solution with neural networks. However, this option was rejected given the hardware limitations as the application will run on an onboard system. So, the image feature extraction technique has been implemented. Although most of the previous works have analysed grayscale images, our work aims to study whether other colour spaces could help to get more information on the image and help to improve the results.",
            "cite_spans": [
                {
                    "start": 468,
                    "end": 472,
                    "text": "[37]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "Fog Detection"
        },
        {
            "text": "After several experimental studies in the RGB and HSV colour spaces (Extra information about colour spaces is given at Appendix A), we concluded that this information was not sufficient to properly differentiate between cloudy and foggy scenes. Therefore, we appealed to a not so popular colour space, the XYZ 1. Hence, we designed a rule-based method from scratch that can classify sunny, cloudy and foggy scenes by using XYZ features. Afterwards, the designed algorithm estimates the fog level of the foggy images by using RGB and HSV features (see Figure 5 ). ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 551,
                    "end": 559,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Fog Detection"
        },
        {
            "text": "One of the main characteristics of fog is that it blocks visibility from a certain distance on. This causes a decrease in the contrast between the object and its background so that the scene takes on a white/grey colour.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rule-Based Method"
        },
        {
            "text": "Thus, our work extracts the specific features of the images and establishes several rules to classify scenes into sunny, cloudy and foggy. These rules are summarised in Table 3 and they will be presented in detail in the following lines. First, we analysed the XYZ colour space; here, the Z channel will find cloudy scenes and the parameter defined as ZYdiff will differentiate between foggy and sunny scenes. Second, once the foggy scene is detected, our algorithm will classify the foggy scenes into light-fog, moderate-fog and dense-fog, by using the RGB and HSV colour space-based features. The grey level will provide an estimation of how dense the fog is, and the blue level to calculate and confirm how clear the sky is.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 169,
                    "end": 176,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Rule-Based Method"
        },
        {
            "text": "It is worth mentioning that, for these analyses, just pixels from the upper half of the image will be considered. In this portion of the image, we will find mostly the sky after having previously calibrated the camera position.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rule-Based Method"
        },
        {
            "text": "Z and ZYdiff calculation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rule-Based Method"
        },
        {
            "text": "In the XYZ colour space, we have analysed the Z average level of the pixels located on the upper half of the image. We observed that this channel shows a difference between cloudy scenes and the rest, since the Z average value is lower on cloudy scenes (Z < 0.35).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rule-Based Method"
        },
        {
            "text": "However, this characteristic presents similar values both for sunny and foggy scenes. Therefore, a further feature is calculated by the |Z\u2212Y| Y formula, which represents the difference of the Z and Y channels averages with respect to luminance (Y), hereinafter referred to as ZYdiff. This value is not relevant for cloudy situations but leads us to differentiate between sunny and foggy scenes, as can be seen in Figure 6 . Thus, ZYdiff is higher on sunny scenes (Z > 0.1) than in foggy ones (Z < 0.1). ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 413,
                    "end": 421,
                    "text": "Figure 6",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Rule-Based Method"
        },
        {
            "text": "The grey level allows approximating how much contrast has been lost in the image due to fog. This feature has been extracted by establishing several rules to the RGB channels. Firstly, we considered as grey pixels those RGB values enclosed in the 140-255 range. This range represents bright pixel values that are not saturated. Additionally, we also limited the difference between each channel to 20; this rule will also ensure that the saturation of the pixel is low.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Grey level estimation"
        },
        {
            "text": "Thus, this grey level would be the percentage of pixels that meet these conditions compared to the total number of pixels analysed on the upper half of the image.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Grey level estimation"
        },
        {
            "text": "The calculated greylevel seems to be a good representative of the fog level. Thus, based on experimental tests we define three thresholds that will conform the three different fog levels. This way, the light-fog scene is expected to have 20-30% of grey pixels, moderatefog conditions will oscillate between 30 and 60% and an image with more than 60% of grey pixels will be considered a dense-fog scenario.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Grey level estimation"
        },
        {
            "text": "Blue level estimation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Grey level estimation"
        },
        {
            "text": "The same approach is followed for the calculation of the blue level; but, in this case, the HSV colour space is also introduced. This specific blue level was modelled by analysing portions of different images of clear sky that allows defining the following rules that combine both colour spaces:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Grey level estimation"
        },
        {
            "text": "For RGB colour space: B > G > R && (B-R) > 40. For HSV colour space: 135 > H > 160 && S > 40 && V > 120. Therefore, the blue level would be the percentage of pixels that meet these conditions compared to the total number of pixels analysed on the upper half of the image.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Grey level estimation"
        },
        {
            "text": "This bluelevel feature will help to confirm sunny scenarios classified by the ZYdiff parameter.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Grey level estimation"
        },
        {
            "text": "In this manner, the fog detection matrix presented in Table 3 is finally constructed.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 54,
                    "end": 61,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Grey level estimation"
        },
        {
            "text": "The ACF detector was taken as good for the target application. Additionally, a CNNbased classifier was also selected for the second phase of the recognition. Although, both the detector and classifier need to be validated for the specific implementation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Traffic Sign Recognition Module"
        },
        {
            "text": "For the evaluation of the behaviour of each of the implemented techniques, the models have been created using the GTSB training set. However, for the validation, the GTSB test set and the Ceit-TSR are used as the test images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Traffic Sign Recognition Module"
        },
        {
            "text": "Since the learning process of an ACF detector is very similar to that of the cascade detector, this function also needs images with and without traffic signs. It uses as positive images all those that are passed as an argument while the negative ones are automatically generated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Detector Training"
        },
        {
            "text": "The design and selection of the parameters for the detector are crucial to achieving the optimal implementation of it; thus, we did several fine-tuning experiments. We saw that The final detector was trained with and object training size of 30 \u00d7 30 pixels. It used 5000 weak learners in 30 stages. It was tested with the GTSDB test set images, resulting in 100% precision and 76% recall; its inference time was 102 ms. On the other hand, this model achieves 96% precision and 68% recall when it is tested with the Ceit-TSR dataset. These means that the model experienced only a 4% precision decrease when dealing with detecting traffic signs in complex images (see Table 4 ). Concerning the classification of the detected signs, we implemented the CNN-based model that was trained with the GTSDB dataset. As was explained in Section 4.1.2, in this case, the model did not require any fine-tuning since it was pre-trained for this specific task, but we include additional pre-processing to the input images.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 665,
                    "end": 672,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Detector Training"
        },
        {
            "text": "Hence, to test the influence of the random cropping, we added it as a previous step before the classification. For this experiment, we used two different datasets: (1) 50 first images from GTSRB that are provided with the model and are already cropped with the ground-truthed ROIs; and (2) 50 first images from GTSRB that matches the (1) dataset, but with no crop applied-this is the original images from the dataset which retain some margin or background pixels in addition to the region of interest.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Detector Training"
        },
        {
            "text": "Thus, we see that the images that were already cropped to the ground-truthed ROI get better results. In this case, the random cropping has low influence since there was little room for improvement. However, on those input images that were not cropped, the preprocessing and random cropping leads to better results, as shown in Table 5 . Therefore, it was decided to implement the methodology that includes the normalization of the original image by the subtraction of the mean image and the 10 random crops to later pass the resulting image to the classifier. It is considered that because the detector's bounding box is sufficiently precise, the implemented classifier model achieves satisfactory results. To validate this classifier, as was done on the detection phase, we used the test set of the GTSB dataset and Ceit-TSR dataset itself. Unlike the detector, the effect of the complex images that the Ceit-TSR dataset contains is notable in the classification phase (see Table 6 ). The maximum accuracy of 92% is achieved with the test images of GTSB but there is a 23% decrease when dealing with complex-condition images. This difference in results seems to be due to the fact that the classifier confuses some categories. As Figure 7 shows, even if the general accuracy was 75%, each sign category achieves a different score. For mandatory and caution signs the accuracy is more than 90% and the yield category scores a 76.2% accuracy. However, it is observed that the system is not capable of clearly distinguishing between the prohibitory and speed limit classes since its accuracy is less than 60%. This may be due to their similarity since both have the same geometry and colours. Besides, given the complexity of the images of the Ceit-TSR dataset, the information received from them may not have sufficient quality or details to detect their differences. Thus, this effect generates false positives and negatives in the classification, causing a decrease in the average accuracy. Concerning the analysis of the restriction ends category, there are no sufficient samples to draw conclusions. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 327,
                    "end": 334,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 975,
                    "end": 982,
                    "text": "Table 6",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 1231,
                    "end": 1239,
                    "text": "Figure 7",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Detector Training"
        },
        {
            "text": "The proposal presented in this paper targets the fog detection task based on the XYZ, HSV and RGB colour spaces' feature extraction.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fog Detection Module"
        },
        {
            "text": "For the validation of this algorithm, we used the Ceit-Foggy dataset presented in Section 4.1.2. This dataset contains different scenes that allow us to validate whether our proposed algorithm can distinguish between sunny, cloudy and foggy scenes and in these last ones if we can estimate the fog level from 1-100. Additionally, due to the complexity of the real-world scenes, the proposed method also provides reliability data both for foggy and non-foggy scenes. Each event (fog/non-fog) will have the relative reliability data associated with it. The calculation of each reliability value is independent, and it is based on the repetitiveness of the data. Thus, when the fog status is stable the reliability will increase or decrease constantly and proportionally with the vehicle speed until the maximum (100) or minimum (0) is reached. However, if the fog status is oscillating, the abrupt changes will also lead to instability in the reliability of the data, which will not reach the extreme values. This way we can better detect and remove false detections that may occur due to isolated situations, such as white buildings, tunnels, veiled scenes, dirt on the lens, etc. Figure 8 presents an extended offline analysis of the images extracted from five videos that were classified in each of the possible different scenes. These graphs show the grey level, Z value (multiplied by 100 for visualization purposes) and blue level that will guide the inspection of the scene type. Additionally, reliability data, both for fog and no-fog events, are drawn to discriminate false positives or false negatives. Finally, the frame that is classified as a foggy scene is also marked with a diamond. Figure 8a shows a sunny scene where there is no point marked as a foggy scene. Z is greater than the threshold of 0.35 and the blue level is also high while the grey level is below the limit of 20%. Thus, the reliability of no-fog is maximum once the algorithm has analysed the minimum number of images. Figure 8b shows a cloudy scene where there are just two frames classified as foggy. In this case, the Z is the parameter that classifies the image as cloudy since its value is below the threshold of 0.35. It also can be observed that the blue levels are very low and grey levels are around 20%. These two false positives classified as foggy scenes at kilometre 8.5 could be easily discarded since the reliability is not high and it seems to be a punctual behaviour (\u2248200 m). Figure 8c presents a light-fog scene video. As was expected, the Z value is above the 0.35 threshold and the grey level is in the range of 20-30%. Since the ranges set for light-fog are small, there are some fluctuations and therefore the reliability of the data is around 70-100%. Figure 8d was labelled as a moderate-fog scene. This image set is a very clear example of a fog bank since at the start and end of the graph no fog is detected. During the fog bank, most of the points present a grey level value between 30 and 60%, as expected, and the Z value is also above the 0.35 limit. The reliability lines help also to confirm this fog-bank since the fog positive reliability (red line) is high during the fog period and then reversed with the non-fog reliability when leaving the fog bank. Figure 8e is the simplest case; the Z value is above 0.35, as expected, and the grey level is almost saturated due to the dense fog of the scene. Consequently, all points are marked as positives and the fog reliability is 100%.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1180,
                    "end": 1188,
                    "text": "Figure 8",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 1697,
                    "end": 1706,
                    "text": "Figure 8a",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 2001,
                    "end": 2010,
                    "text": "Figure 8b",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 2476,
                    "end": 2485,
                    "text": "Figure 8c",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 2758,
                    "end": 2767,
                    "text": "Figure 8d",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 3272,
                    "end": 3281,
                    "text": "Figure 8e",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Fog Detection Module"
        },
        {
            "text": "In this paper, two vision-based ITS applications are proposed, one for traffic sign recognition and one for fog detection that will be running in real-time for an onboard probe vehicle system. Traffic sign detection is covered with an ACF detector whereas the classification phase was approached by a CNN, achieving an overall accuracy of 92%. Fog detection instead is carried out using classic computer vision methods by image feature extraction through a combination of RGB, HSV and XYZ colour spaces. The achieved inference time in both applications allows us to make a real-time analysis for an onboard system with a performance close to the reviewed state-of-the-art methods. This real-time analysis makes it possible to convert these implementations into cooperative services that will not only support the driver but also give valuable information for other road users.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "These two C-ITS modules are already integrated on an onboard barebone industrial mini PC and are being validated on-site. In the case of TSR application, the presented module supports the operators in charge of the road maintenance to create an exhaustive traffic signal inventory and detect possible deficiencies that may require the attention of an expert. Besides, the fog detection module sends weather-related events to the server back-office and ITS-G5 beacons for communication with other vehicles. The generation of this real-time traffic alerts could enhance road safety by warning the drivers of road hazards, such as low visibility due to fog banks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "As future work, the CEIT's Sustainable Transportation and Mobility group is working on an extension of the TSR module for the inspection of road markings. Good preservation of road markings is vital for road safety and becomes even more important for the integration of future self-driving cars. Thus, this additional functionality would allow us to study the status of the paint and advise the road maintenance operators to take specific actions before the degradation is safety-critical.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future Research Direction"
        },
        {
            "text": "The integration of new modules, or the extension of them, will probably require improving the computing capacity of the onboard system. To this end, it is possible to explore the integration of 5G technology into the system, which that would allow the images to be sent to external high-performance computers and return the result with minimal latency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future Research Direction"
        },
        {
            "text": "If there were no hardware limitations, new algorithms based on deep learning could be explored for further improvements on fog detection. Moreover, the current rain module of the system, which is based on an IR sensor, could also be enhanced by data fusion with computer vision techniques. For example, a detector of the splash effect of the vehicle's wheels when there is a water film accumulated on the asphalt.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future Research Direction"
        },
        {
            "text": "Besides, further improvements could be done to the overall system by designing an interface for the visualization of the C-ITS events by using an HMI solution that could go from simply using a mobile application to augmented reality options on the windscreen. Funding: This research was funded by Basque Government (HAZITEK program) in the frame of TDATA2 project (\"TRAFIK DATA: M\u00f3dulo embarcado para monitorizaci\u00f3n de la se\u00f1alizaci\u00f3n de la carretera y condiciones meteorol\u00f3gicas con comunicaciones de servicios cooperativos\", grant number ZL-2019/00753).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future Research Direction"
        },
        {
            "text": "Informed Consent Statement: Not applicable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Institutional Review Board Statement: Not applicable."
        },
        {
            "text": "Data Availability Statement: Ceit-TSR and Ceit-Foggy datasets are available online at https:// github.com/oipa/CITS-traffic-alerts (accessed on 9 Febuary 2021).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Institutional Review Board Statement: Not applicable."
        },
        {
            "text": "The authors declare no conflict of interest.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conflicts of Interest:"
        },
        {
            "text": "In this section, there is a brief description of the three colour spaces that have been used in this work to extract characteristics from the image.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A"
        },
        {
            "text": "This colour space is the most common one since the human eye only has colour sensitive receptors for red (R), green (G) and blue (B). Thus, it is theoretically possible to decompose every visible colour into combinations of these three \"primary colours\" with different range of intensities from 0 to 255. Thus, this combination can be represented as a three dimensional coordinate plane with the values for R, G and B on each axis (see Figure A1a ). This way we can conclude that when all channels have a value of zero, no light is emitted resulting on black colour. Whereas when all three colour channels are set to their maximum the resulting colour is white.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 436,
                    "end": 446,
                    "text": "Figure A1a",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Appendix A.1. RGB Colour Space"
        },
        {
            "text": "HSV colour space is a cylindrical representation where colours of each hue (H) are arranged in a radial slice (see Figure A1b ). The hue is the colour portion of the model that is expressed in 0-360 degrees where it can be considered colours like red, yellow, green, cyan, blue and magenta each in 60-degree increments. The central axis represents the value (V) or brightness of the colour from 0 to 100 percent, where 0 is completely black and 100 is the brightest and reveals the most colour. Finally, saturation (S) describes the amount of grey in a particular colour; its minimum value (0) introduces more grey and the maximum (100) is the most similar to its primary colour. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 115,
                    "end": 125,
                    "text": "Figure A1b",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Appendix A.2. HSV Colour Space"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "COMMISSION STAFF WORKING DOCUMENT-EU Road Safety Framework 2021-2030-Next Steps towards \"Vision Zero",
            "authors": [],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "European Transport Safety Council PIN Report: Lockdowns Resulted in an Unprecedented 36% Drop in Road Deaths in the EU",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A European Strategy on Cooperative Intelligent transport Systems, a Milestone towards Cooperative, Connected and Automated Mobility",
            "authors": [],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Asociacion Espa\u00f1ola de la Carretera Auditoria del estado de las carreteras 2019-2020",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Computer: {B}enchmarking Machine Learning Algorithms for Traffic Sign Recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Stallkamp",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schlipsing",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Salmen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Igel",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Man",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Vs",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Neural Netw",
            "volume": "32",
            "issn": "",
            "pages": "323--332",
            "other_ids": {
                "DOI": [
                    "10.1016/j.neunet.2012.02.016"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Detection of traffic signs in real-world images: The German traffic sign detection benchmark",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Houben",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Stallkamp",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Salmen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schlipsing",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Igel",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proc. Int. Jt. Conf. Neural Netw",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {
                "DOI": [
                    "10.1109/IJCNN.2013.6706807"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Adaboost Detection and Forest-ECOC Classification",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Bar\u00f3",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Escalera",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Vitri\u00e0",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Pujol",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Radeva",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE Trans. Intell. Transp. Syst",
            "volume": "10",
            "issn": "",
            "pages": "113--126",
            "other_ids": {
                "DOI": [
                    "10.1109/TITS.2008.2011702"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Real-time traffic sign recognition in three stages",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zaklouta",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Stanciulescu",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Rob. Auton. Syst",
            "volume": "62",
            "issn": "",
            "pages": "16--24",
            "other_ids": {
                "DOI": [
                    "10.1016/j.robot.2012.07.019"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Rapid multiclass traffic sign detection in high-resolution images",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Intell. Transp. Syst",
            "volume": "15",
            "issn": "",
            "pages": "2394--2403",
            "other_ids": {
                "DOI": [
                    "10.1109/TITS.2014.2314711"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "An Incremental Framework for Video-Based Traffic Sign Detection, Tracking, and Recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 1918,
            "venue": "IEEE Trans. Intell. Transp. Syst",
            "volume": "18",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1109/TITS.2016.2614548"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Cascaded Segmentation-Detection Networks for Text-Based Traffic Sign Detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Liao",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Member",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Intell. Transp. Syst",
            "volume": "19",
            "issn": "",
            "pages": "209--219",
            "other_ids": {
                "DOI": [
                    "10.1109/TITS.2017.2768827"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Traffic-Sign Detection and Classification in the Wild Zhe",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "27--30",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Bin Vision-based traffic sign detection and recognition systems: Current trends and challenges",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "B"
                    ],
                    "last": "Wali",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Abdullah",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Hannan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hussain",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Samad",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Ker",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mansor",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Sensors (Switzerland)",
            "volume": "19",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/s19092093"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Robust Chinese traffic sign detection and recognition with deep convolutional neural network",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Qian",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yue",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Coenen",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 11th International Conference on Natural Computation (ICNC)",
            "volume": "",
            "issn": "",
            "pages": "791--796",
            "other_ids": {
                "DOI": [
                    "10.1109/ICNC.2015.7378092"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Machine Vision Based Traffic Sign Detection Methods: Review, Analyses and Perspectives",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Access",
            "volume": "7",
            "issn": "",
            "pages": "86578--86596",
            "other_ids": {
                "DOI": [
                    "10.1109/ACCESS.2019.2924947"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Traffic Sign Detection Under Challenging Conditions: A Deeper Look into Performance Variations and Spectral Characteristics",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Temel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Alregib",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Intell. Transp. Syst",
            "volume": "2020",
            "issn": "",
            "pages": "3663--3673",
            "other_ids": {
                "DOI": [
                    "10.1109/TITS.2019.2931429"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Challenging Environments for Traffic Sign Detection: Reliability Assessment under Inclement Conditions",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Temel",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Alshawi",
                    "suffix": ""
                },
                {
                    "first": "M.-H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Alregib",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1902.06857"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Enhanced fog detection and free-space segmentation for car navigation",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Hauti\u00e8re",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Tarel",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Halmaoui",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Br\u00e9mond",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Aubert",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Mach. Vis. Appl",
            "volume": "25",
            "issn": "",
            "pages": "667--679",
            "other_ids": {
                "DOI": [
                    "10.1007/s00138-011-0383-3"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Classification of images in fog and fog-free scenes for use in vehicles",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pavlic",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Rigoll",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ilic",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Intell. Veh. Symp. Proc",
            "volume": "2013",
            "issn": "",
            "pages": "481--486",
            "other_ids": {
                "DOI": [
                    "10.1109/IVS.2013.6629514"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "A fog level detection method based on image HSV color histogram",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Geng",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 IEEE International Conference on Progress in Informatics and Computing",
            "volume": "",
            "issn": "",
            "pages": "373--377",
            "other_ids": {
                "DOI": [
                    "10.1109/PIC.2014.6972360"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Traffic sign recognition-How far are we from the solution?",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mathias",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Timofte",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Benenson",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 2013 International Joint Conference on Neural Networks (IJCNN)",
            "volume": "",
            "issn": "",
            "pages": "4--9",
            "other_ids": {
                "DOI": [
                    "10.1109/IJCNN.2013.6707049"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "An overview of traffic sign detection and classification methods",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Saadna",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Behloul",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Int. J. Multimed. Inf. Retr",
            "volume": "6",
            "issn": "",
            "pages": "193--210",
            "other_ids": {
                "DOI": [
                    "10.1007/s13735-017-0129-8"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Geiger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lenz",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Urtasun",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3354--3361",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2012.6248074"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cordts",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Omran",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ramos",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Rehfeld",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Enzweiler",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Benenson",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Franke",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schiele",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "3213--3223",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2016.350"
                ]
            }
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Neuhold",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ollmann",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Bulo",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kontschieder",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)",
            "volume": "",
            "issn": "",
            "pages": "22--29",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "The ApolloScape Open Dataset for Autonomous Driving and Its Application",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Geng",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "42",
            "issn": "",
            "pages": "2702--2719",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2019.2926463"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xian",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Madhavan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "2633--2642",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR42600.2020.00271"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Improved visibility of road scene images under heterogeneous fog",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Tarel",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Hauti\u00e8re",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cord",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gruyer",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Halmaoui",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the IEEE Intelligent Vehicles Symposium (IV'10)",
            "volume": "",
            "issn": "",
            "pages": "478--485",
            "other_ids": {
                "DOI": [
                    "10.1109/IVS.2010"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Heterogeneous Fog To cite this version: Vision Enhancement in Homogeneous and Heterogeneous Fog",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tarel",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Hauti",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Caraffa",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Halmaoui",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gruyer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tarel",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Hauti",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Caraffa",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Halmaoui",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tarel",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Intell. Transp. Syst. Mag",
            "volume": "4",
            "issn": "",
            "pages": "6--20",
            "other_ids": {
                "DOI": [
                    "10.1109/MITS.2012.2189969"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Semantic Foggy Scene Understanding with Synthetic Data",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Sakaridis",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int. J. Comput. Vis",
            "volume": "126",
            "issn": "",
            "pages": "973--992",
            "other_ids": {
                "DOI": [
                    "10.1007/s11263-018-1072-8"
                ]
            }
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Curriculum Model Adaptation with Synthetic and Real Data for Semantic Foggy Scene Understanding",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Sakaridis",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hecker",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Int. J. Comput. Vis",
            "volume": "2020",
            "issn": "",
            "pages": "1182--1204",
            "other_ids": {
                "DOI": [
                    "10.1007/s11263-019-01182-4"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bijelic",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Gruber",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Mannan",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Kraus",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ritter",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Dietmayer",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Heide",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "11679--11689",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR42600.2020.01170"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "An Analysis of the Viola-Jones Face Detection Algorithm. Image Process. Line",
            "authors": [
                {
                    "first": "Y.-Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "4",
            "issn": "",
            "pages": "128--148",
            "other_ids": {
                "DOI": [
                    "10.5201/ipol.2014.104"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Fast Feature Pyramids for Object Detection",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dollar",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Appel",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Belongie",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Perona",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "36",
            "issn": "",
            "pages": "1532--1545",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2014.2300479"
                ]
            }
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Traffic Sign Recognition Benchmark (GTSRB) AlexNet Pycaffe Model. GitHub Repository",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jahnen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Budweiser",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "German",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Characteristics of Adverse Weather Conditions v1.0. Available online",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "RP Photonics Encyclopedia -Color Spaces",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Paschotta",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Ceit-TSR database. Sample images showing some of the challenging conditions: low contrast, fog, reflections, shadows and heavy rain.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The Ceit-Foggy dataset. Sample images showing the annotated three different fog levels; from left to right: light fog, moderate fog and heavy fog.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Methodology workflow implemented for the TSR module.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The Aggregated Channel Features (ACF). In the first row, from left to right: the original image, LUV channels, the gradient magnitude and individual representation of the HOG features at different angles of the sample sign.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Methodology workflow implemented for the fog detection module.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Studied features in three different weather scenes. From up to down-sunny, cloudy and foggy sample scenes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "the inference time was always about 200 ms per image; \u2022 up to 5000 learners or decision trees do not improve the results; \u2022 the object training size should be maximum 30 \u00d7 30 pixels, as bigger dimensions may increase the false negatives; \u2022 a confidence threshold is key to compensate for false positives and negatives.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Confusion matrix of the final solution trained with the GTSB dataset and validated with the Ceit-TSR dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Cont.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Extended offline analysis of five different videos. Each set of images presents a specific condi-tion: (a) sunny; (b) cloudy; (c) light-fog; (d) moderate-fog; and (e) dense-fog.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Contributions: Conceptualization, O.I and A.B.; investigation, methodology and software, O.I.; formal analysis, O.I., D.B. and A.B.; writing-original draft preparation, O.I.; writing-review and editing, D.B., A.B. and A.A.; supervision, D.B. and A.A.; funding acquisition and project administration, A.B. All authors have read and agreed to the published version of the manuscript.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Representation of the (a) RGB and (b) HSV colour spaces.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Summary of the existing databases for traffic sign recognition (TSR) and its principal characteristics. It is also included our database Ceit-TSR.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Summary of the existing datasets for fog detection and its principal characteristics.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "The proposed rule-based method for fog detection and fog level estimation. This method anal-yses the RGB, HSV and XYZ colour spaces.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Summary of detection results for the GTSB test set and Ceit-TSR dataset. Complex images made the model decrease its precision by 4% compared with the GTSB result.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Comparison of the random cropping influence on the classification result as a pre-processing technique.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Summary results of the classification for the GTSB and Ceit-TSR datasets. The influence of the complex images on the selected classifier is notable.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The CIE XYZ colour space encloses all colour sensations that are visible to the human eye. The human eye with normal vision has three kinds of cone cell that sense light, S, M and L, depending on their sensitivity to short-, middle-or long-wavelength light. The CIE colour model takes the luminance (as a measure for perceived brightness) as one of the three colour coordinates, calling it Y. The spectral response of the luminance is specified as the photopic luminosity function. The maximum possible Y value, e.g., for a colour image, may be chosen to be 1 or 100, for example. The Z coordinate responds mostly to shorter-wavelength light (blue colours on the visible light spectrum), while X responds both to shorter-and longer-wavelength light [38] . Figure A2 shows the used colour matching functions. ",
            "cite_spans": [
                {
                    "start": 749,
                    "end": 753,
                    "text": "[38]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [
                {
                    "start": 756,
                    "end": 765,
                    "text": "Figure A2",
                    "ref_id": null
                }
            ],
            "section": "Appendix A.3. XYZ Colour Space"
        }
    ]
}
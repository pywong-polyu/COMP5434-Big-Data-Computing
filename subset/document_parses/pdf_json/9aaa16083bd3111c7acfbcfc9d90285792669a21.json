{
    "paper_id": "9aaa16083bd3111c7acfbcfc9d90285792669a21",
    "metadata": {
        "title": "BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification",
        "authors": [
            {
                "first": "Ishani",
                "middle": [],
                "last": "Mondal",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Microsoft Research",
                    "location": {
                        "country": "India"
                    }
                },
                "email": "ishani340@gmail.com"
            }
        ]
    },
    "abstract": [
        {
            "text": "Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the everincreasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERT-MLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Recent studies have exposed the importance of biomedical NLP in the well-being of human-beings, analyzing the critical process of medical decisionmaking. However, the dialogue managing tools targeted for medical conversations (Zhang et al., 2020) , (Campillos Llanos et al., 2017) , (Kazi and Kahanda, 2019) between patients and healthcare providers in assisting diagnosis may generate certain insignificant perturbations (spelling errors, paraphrasing), which when fed to the classifier to determine the type of diagnosis required/detecting adverse drug effects/drug recommendation, might provide unreasonable performance. Insignificant perturbations might also creep in from the casual language expressed in the tweets (Zilio et al., 2020) .",
            "cite_spans": [
                {
                    "start": 226,
                    "end": 246,
                    "text": "(Zhang et al., 2020)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 249,
                    "end": 280,
                    "text": "(Campillos Llanos et al., 2017)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 283,
                    "end": 307,
                    "text": "(Kazi and Kahanda, 2019)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 721,
                    "end": 741,
                    "text": "(Zilio et al., 2020)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Thus, the classifier needs to be robust towards these perturbations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Generating adversarial examples in text is challenging compared to computer vision tasks because of (i) discrete nature of input space and (ii) preservation of semantic coherence with original text. Initial works for attacking text models relied on introducing errors at the character level or manipulating words (Feng et al., 2018) to generate adversarial examples. But due to grammatical disfluency, these seem very unnatural. Some rule-based synonym replacement strategies (Alzantot et al., 2018) , (Ren et al., 2019) have lead to more natural looking examples. (Jin et al., 2019) proposed TextFooler, as a baseline to generate adversaries for text classification models. But, the adversarial examples created by TextFooler rely heavily on word-embedding based word similarity replacement technique, and not overall sentence semantics. Recently, (Garg and Ramakrishnan, 2020) proposed BERT-MLM-based (Devlin et al., 2019) word replacements to create adversaries to better fit the overall context. Despite these advancements, there is much less attention towards making robust predictions in critical domains like biomedical, which comes with its unique challenges. (Araujo et al., 2020) has proposed two types of rule-based adversarial attacks inspired by natural spelling errors and typos made by humans and synonym replacement in the biomedical domain. Some challenges include: 1) Biomedical named entities are usually multi-word phrases such as colorectal adenoma. During token replacement, we need the entire entity to be replaced, but the MLM model (token-level replacement) fails to generate correct synonym of entity fitting in the context. So, we need a BioNER+Entity Linker (Martins et al., 2019) , (Mondal et al., 2019) to link entity to ontology for generating correct synonyms. 2) Due to several variations of representing medical entities such as Type I Diabetes could be expressed as 'Type One Diabetes', we explore numeric entity expansion strategies for generating adversaries. 3) Spelling variations (keyboard swap, modification). While we evaluate on two benchmark datasets, our method is general and is applicable for any biomedical classification datasets.",
            "cite_spans": [
                {
                    "start": 313,
                    "end": 332,
                    "text": "(Feng et al., 2018)",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 476,
                    "end": 499,
                    "text": "(Alzantot et al., 2018)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 502,
                    "end": 520,
                    "text": "(Ren et al., 2019)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 565,
                    "end": 583,
                    "text": "(Jin et al., 2019)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 849,
                    "end": 878,
                    "text": "(Garg and Ramakrishnan, 2020)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 903,
                    "end": 924,
                    "text": "(Devlin et al., 2019)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1168,
                    "end": 1189,
                    "text": "(Araujo et al., 2020)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1686,
                    "end": 1708,
                    "text": "(Martins et al., 2019)",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1711,
                    "end": 1732,
                    "text": "(Mondal et al., 2019)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we present BBAEG (Biomedical BERT-based Adversarial Example Generation) 1 , a novel black-box attack algorithm for biomedical text classification task leveraging both the BERT-MLM model for non-named entity replacements combined with NER linked synonyms for named entities to better fit the overall context. In addition to replacing words with synonyms, we explore the mechanism of generating adversarial examples using typographical variations and numeric entity modification. Our BBAEG attack beats the existing baselines by a wide margin on both automatic and human evaluation across datasets and models. To the best of our knowledge, we are the first to introduce a novel algorithm for generating adversarial examples for biomedical text whose success attack is higher than the existing baselines like TextFooler and BAE (Garg and Ramakrishnan, 2020) . The overall contributions of the paper include: 1) We explore several challenges of biomedical adversarial example generation. 2) We propose BBAEG, a biomedical adversarial example generation technique for text classification combining the power of several perturbation techniques. 3) We introduce 3 type of attacks for this purpose on two biomedical text classification datasets. 4) Through human evaluation, we show that BBAEG yields adversarial examples with improved naturalness.",
            "cite_spans": [
                {
                    "start": 840,
                    "end": 869,
                    "text": "(Garg and Ramakrishnan, 2020)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Problem Definition: Given a set of n inputs (D, Y ) = [(D 1 , y 1 ), . . .(D n , y n )] and a trained classifier M : D \u2192 Y , we assume the soft-label black-box setting where the attacker can only query the classifier for output probabilities on a given input, and has no access to the model parameters, gradients or training data. For an input of length l consisting of words w i , where 1 \u2264 i \u2264 l, (D i = [w 1 , ..., w l ], y), we want to generate an adversarial example D adv such that M (D adv ) = y. We would like D adv to be grammatically correct, semantically similar to D (Sim(D, D adv ) \u2265 \u03b1), where \u03b1 denotes the similarity threshold. 1) Named Entity Tagging: For each input instance D i (Line 1 in Algorithm), we apply sciSpacy 2 with en-ner-bc5cdr-md to extract biomedical named entities (drugs and diseases), followed by its Entity Linker (Drugs to DrugBank (Wishart et al., 2017) , Disease to MESH 3 )). After linking the NE to respective ontologies, we use pyMeshSim 4 (for disease) and DrugBank (for drugs) to obtain synonyms. In each D i of size l (w 1 , w 2 , ...[w i ...w i+2 ], ...w l ), multi-word expressions (w i ...w i+2 ) are named entities. We put them in Named Entities Set (S N E ) and other words in non-Named Entity set (S N N E ).",
            "cite_spans": [
                {
                    "start": 869,
                    "end": 891,
                    "text": "(Wishart et al., 2017)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "2) Ranking of important words: We estimate token importance I i of each w i \u2208 D, by deleting w i from D and computing the decrease in probability of predicting the correct label y (Line 2), similar to (Jin et al., 2019) . Thus, we receive a set for each token which contains the tokens in decreasing order of their importance.",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 219,
                    "text": "(Jin et al., 2019)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "3) Choosing perturbation schemes: Consider the input D i , we describe a sieve-based approach of perturbing D i . Sieves are ordered by precision, with the most precise sieve appearing first.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Sieve 1 : In the first sieve, we propose to alter the synonyms of the tokens in S N E (Line 5-9) using Ontology linking and the words in S N N E (Line 10-15) using BERT-MLM predicted tokens. This stems from the fact that synonym replacement of the non-named entities using BERT-MLM generates reasonable predictions considering the surrounding context (Garg and Ramakrishnan, 2020) . If the token is a part of S N E , replace them with the domain-specific synonyms one by one, but if the token is part of S N N E , then replace those words by the top-K BERT-MLM predictions. To achieve high semantic similarity with the original text, we filter the set of top K tokens (K is a pre-defined constant) (Line 12) predicted by BERT-MLM for the masked token, using a Sentence-Transformer (Reimers and Gurevych, 2019) based sentence similarity scorer. Additionally, we filter out predicted tokens that do not belong to the same part of speech as original token. If this sieve generates adversaries for D i , then D adv is being returned.",
            "cite_spans": [
                {
                    "start": 351,
                    "end": 380,
                    "text": "(Garg and Ramakrishnan, 2020)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Sieve 2: (Line 20-28) If the first sieve does not generate adversary, we introduce two typographical noise in the input 1) Spelling Noise-N1: Rotating 3 https://meshb.nlm.nih.gov/ 4 https://github.com/luozhhub/pyMeSHSim random p characters (Line 20) 2) Spelling Noise-N2: insertion of symbols to the beginning or end (Line 21). If this sieve generates adversaries for D i , then D adv is being returned.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Sieve 3: (Line 29-31) If Sieve 2 does not generate adversary, we replace the numeric entities by expanding the numeric digit. For example: PMD1 can be rewritten as PMD One, Covid19 as Covid nineteen. If this sieve generates adversaries for D i , then D adv is being returned.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "For each of the three sieves, among all the winning adversaries, the one which is the most similar to original text as measured by (Reimers and Gurevych, 2019) is returned. If the sieves do not generate adversaries, we return the perturbed example which causes maximum reduction in the probability of output.",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 159,
                    "text": "(Reimers and Gurevych, 2019)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "4) Final adversaries generation:"
        },
        {
            "text": "Datasets and Experimental Details: We evaluate BBAEG on two different biomedical text classification datasets: 1) Adverse Drug Event (ADE) Detection (Gurulingappa et al., 2012) and 2) Twitter ADE dataset (Rosenthal et al., 2017) for the task of classifying whether the sentence contains mention of ADE (binary).",
            "cite_spans": [
                {
                    "start": 149,
                    "end": 176,
                    "text": "(Gurulingappa et al., 2012)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 204,
                    "end": 228,
                    "text": "(Rosenthal et al., 2017)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Experimental setup"
        },
        {
            "text": "We (Beltagy et al., 2019) . We fine-tune these models on the training data (of each corpus) using Adam Optimizer (Kingma and Ba, 2015) with learning rate of 0.00002, 10 epochs and perform adversarial attack on the test data. For the BBAEG non-NER synonym attacks, we use BERT-base-uncased MLM to predict the masked tokens. We consider top K=10 synonyms from the BERT-MLM predictions and set threshold \u03b1 of 0.75 for cosine similarity between (Reimers and Gurevych, 2019) embeddings of the adversarial and input text, we set p=2 characters for rotation to introduce noise in input. For more details refer to the appendix.",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 25,
                    "text": "(Beltagy et al., 2019)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Experimental setup"
        },
        {
            "text": "Automatic Evaluation Results: We examine the success of adversarial attack using two criteria: (1) Performance Drop (Adrop): Difference between Successful challenge with clozapine in a history of pulmonary eosinophilia ailment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Successful challenge with hydrochloride in a history of pulmonary disease ailment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BAE (Using BERT-MLM):"
        },
        {
            "text": "Successful challenge with clozapinum in a history of Loeffler Syndrome ailment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BBAEG (Best Combination):"
        },
        {
            "text": "A 21-year-old patient developed rhabdomyolysis during 19th week of treatment with clozapine for schizophrenia. BBAEG (Spelling Noise-N2):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Original:"
        },
        {
            "text": "A 21-year-old patient developed rhabdomyolysis during 19th week of treatment with inoclozapine for cdschizophrenia.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Original:"
        },
        {
            "text": "A 21-year-old patient developed rhabdomyolysis during 19th week of treatment with clpazoine for schizoerhpnia.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BBAEG (Spelling Noise-N1):"
        },
        {
            "text": "A 21-year-old patient developed rhabdomyolysis during 19th week of treatment with Clozapinum for dementia Praecox.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BBAEG (Synonyms):"
        },
        {
            "text": "A twenty-one-year-old patient developed rhabdomyolysis during nineteenth week of treatment with clozapine for schizophrenia. original (accuracy on original test set) and afterattack accuracy (accuracy on the perturbed test set)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BBAEG (Number Replacement):"
        },
        {
            "text": "(2) Perturbation of input (%): Percentage of perturbed words in adversary generated. Success of attack is directly and indirectly proportional with criteria 1 and 2 respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BBAEG (Number Replacement):"
        },
        {
            "text": "Effectiveness: Table 1 shows the results of BBAEG attack on two datasets across all the models. During our experiments with HAN (general deep learning model), we observe that the attack is the most successful compared to BERT-variants, RoBERTa and the existing baselines, in terms of both the criteria (1 and 2) . Also, using BioBERT and Sci-BERT (35-45% and 40-50% accuracy drop respectively), the attack is the most successful. This stems from the fact that the vocabularies used in the datasets have already been explored during pre-training by the contextual embeddings, thus more sensitive towards small perturbations. Moreover, it has been clearly observed that unlike BERT and HAN, RoBERTa is very less susceptible to adversarial attacks (10-20% accuracy drop), per-turbing 20-25% words in the input space. We also observe that BERT-MLM-based synonym replacement techniques for non-NER, combined with multi-word NER synonym replacement using entity linking outperforms TextFooler(TF) and BAEbased approaches in terms of accuracy drop.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 15,
                    "end": 22,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 302,
                    "end": 311,
                    "text": "(1 and 2)",
                    "ref_id": null
                }
            ],
            "section": "BBAEG (Number Replacement):"
        },
        {
            "text": "Ablation Analysis: In Table 3 , we perform an ablation analysis on the different perturbation schemes and the effect of the attack using each of the sieves by making use of two fine-tuned contextual embedding model as the target model for ADE classification. Synonym replacement (S1) (average 35% accuracy drop) and character rotation (S2-1) (average 38% accuracy drop) seems to be the most promising approach for success attacks on biomedical text classification. Moreover, we conduct a deeper analysis to gain an insight of how much the synonyms of NER vs Non-NER entities contribute towards prediction change. We have found that the multi-word NERs during replacement generates natural-looking examples (compared to MLM-based entity replacement such as pulmonary eosinophillia is replaced by Loeffler Syndrome (for BBAEG) by normalizing to MESH vocabulary, while replaced by disease in BAE predictions as shown in Table 2 and they seem very unnatural. This proves that high semantic similarity does not always ensure generation of proper grammatical adversaries.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 22,
                    "end": 29,
                    "text": "Table 3",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 917,
                    "end": 924,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "BBAEG (Number Replacement):"
        },
        {
            "text": "Human Evaluation: Apart from automatic evaluation, we also perform human evaluation of our BBAEG attacks on the BERT classifier. We perform similar kind of human evaluation by two biomedical domain-experts on randomly selected 100 generated adversarial examples (from each of the different attack algorithms) on each of the two datasets. For each sample, 50 annotations were collected. Similar setup was performed by (Garg and Ramakrishnan, 2020) during evaluation. The main two criteria for evaluation of the perturbed samples are as follows:",
            "cite_spans": [
                {
                    "start": 417,
                    "end": 446,
                    "text": "(Garg and Ramakrishnan, 2020)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "BBAEG (Number Replacement):"
        },
        {
            "text": "1) Naturalness : How much the adversaries generated is semantically similar to the original text content, preserving grammatical correctness on Likert Scale (1-5)? To evaluate the naturalness of the adversarial examples, we first present the annotators with 50 different set of original data samples to understand data distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BBAEG (Number Replacement):"
        },
        {
            "text": "2) Accuracy of generated instances: on the binary classification of presence of Adverse Drug Reaction (ADR) on the adversarial examples. We enumerate the average scores of two annotators (for TextFooler (TF), BAE and our BBAEG) and present those in Table 4 . During ablation analysis, we observe that the synonym replaced perturbed samples looked more natural to the human evaluators compared to the spelling perturbed samples and number replaced entities. When considered jointly, the number replaced and synonym replaced samples seemed more natural to the annotators compared to spelling perturbed samples. This arises due to the fact that the number replaced entities when thrown to the annotators they could easily interpret the meaning correctly when given in combination with the original sample. For instance, in the examples shown in table 2, the number replaced samples (21-year old \u2192 twenty-one-year old) look more natural and easily interpretable compared to spelling perturbed samples (clozapine \u2192 clpazoine).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 249,
                    "end": 256,
                    "text": "Table 4",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "BBAEG (Number Replacement):"
        },
        {
            "text": "In this paper, we propose a new technique for generating adversarial examples combining contextual perturbations based on BERT-MLM, synonym replacement of biomedical entities, typographical errors and numeric entity expansion. We explore several classification models to demonstrate the efficacy of our method. Experiments conducted on two benchmark biomedical datasets demonstrate the strength and effectiveness of our attack. As a future work, we would like to explore more about retraining the models with the perturbed samples in order to improve model robustness.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Generating natural language adversarial examples",
            "authors": [
                {
                    "first": "Moustafa",
                    "middle": [],
                    "last": "Alzantot",
                    "suffix": ""
                },
                {
                    "first": "Yash",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "Ahmed",
                    "middle": [],
                    "last": "Elgohary",
                    "suffix": ""
                },
                {
                    "first": "Bo-Jhang",
                    "middle": [],
                    "last": "Ho",
                    "suffix": ""
                },
                {
                    "first": "Mani",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "Kai-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "2890--2896",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/D18-1316"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "On adversarial examples for biomedical nlp tasks. ArXiv, abs",
            "authors": [
                {
                    "first": "Vladimir",
                    "middle": [],
                    "last": "Araujo",
                    "suffix": ""
                },
                {
                    "first": "Andres",
                    "middle": [],
                    "last": "Carvallo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Aspillaga",
                    "suffix": ""
                },
                {
                    "first": "Denis",
                    "middle": [],
                    "last": "Parra",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Scibert: Pretrained contextualized embeddings for scientific text",
            "authors": [
                {
                    "first": "Iz",
                    "middle": [],
                    "last": "Beltagy",
                    "suffix": ""
                },
                {
                    "first": "Arman",
                    "middle": [],
                    "last": "Cohan",
                    "suffix": ""
                },
                {
                    "first": "Kyle",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Automatic classification of doctor-patient questions for a virtual patient record query task",
            "authors": [
                {
                    "first": "Sophie",
                    "middle": [],
                    "last": "Leonardo Campillos Llanos",
                    "suffix": ""
                },
                {
                    "first": "Pierre",
                    "middle": [],
                    "last": "Rosset",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Zweigenbaum",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "BioNLP 2017",
            "volume": "",
            "issn": "",
            "pages": "333--341",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/W17-2343"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "1",
            "issn": "",
            "pages": "4171--4186",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/N19-1423"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Right answer for the wrong reason: Discovery and mitigation",
            "authors": [
                {
                    "first": "Eric",
                    "middle": [],
                    "last": "Shi Feng",
                    "suffix": ""
                },
                {
                    "first": "Mohit",
                    "middle": [],
                    "last": "Wallace",
                    "suffix": ""
                },
                {
                    "first": "Pedro",
                    "middle": [],
                    "last": "Iyyer",
                    "suffix": ""
                },
                {
                    "first": "Alvin",
                    "middle": [],
                    "last": "Rodriguez",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "I"
                    ],
                    "last": "Grissom",
                    "suffix": ""
                },
                {
                    "first": "Jordan",
                    "middle": [
                        "L"
                    ],
                    "last": "Boyd-Graber",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Bae: Bert-based adversarial examples for text classification",
            "authors": [
                {
                    "first": "Siddhant",
                    "middle": [],
                    "last": "Garg",
                    "suffix": ""
                },
                {
                    "first": "Goutham",
                    "middle": [],
                    "last": "Ramakrishnan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "EMNLP",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports",
            "authors": [
                {
                    "first": "Harsha",
                    "middle": [],
                    "last": "Gurulingappa",
                    "suffix": ""
                },
                {
                    "first": "Abdul",
                    "middle": [
                        "Mateen"
                    ],
                    "last": "Rajput",
                    "suffix": ""
                },
                {
                    "first": "Angus",
                    "middle": [],
                    "last": "Roberts",
                    "suffix": ""
                },
                {
                    "first": "Juliane",
                    "middle": [],
                    "last": "Fluck",
                    "suffix": ""
                },
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Hofmann-Apitius",
                    "suffix": ""
                },
                {
                    "first": "Luca",
                    "middle": [],
                    "last": "Toldo",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Text Mining and Natural Language Processing in Pharmacogenomics",
            "volume": "45",
            "issn": "",
            "pages": "885--892",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jbi.2012.04.008"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
            "authors": [
                {
                    "first": "Kexin",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Is BERT really robust? natural language attack on text classification and entailment",
            "authors": [
                {
                    "first": "Di",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                },
                {
                    "first": "Zhijing",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                },
                {
                    "first": "Joey",
                    "middle": [
                        "Tianyi"
                    ],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Szolovits",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Automatically generating psychiatric case notes from digital transcripts of doctor-patient conversations",
            "authors": [
                {
                    "first": "Nazmul",
                    "middle": [],
                    "last": "Kazi",
                    "suffix": ""
                },
                {
                    "first": "Indika",
                    "middle": [],
                    "last": "Kahanda",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2nd Clinical Natural Language Processing Workshop",
            "volume": "",
            "issn": "",
            "pages": "140--148",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/W19-1918"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Adam: A method for stochastic optimization",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Diederik",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "authors": [
                {
                    "first": "Jinhyuk",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Wonjin",
                    "middle": [],
                    "last": "Yoon",
                    "suffix": ""
                },
                {
                    "first": "Sungdong",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Donghyeon",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Sunkyu",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Chan",
                    "middle": [],
                    "last": "Ho So",
                    "suffix": ""
                },
                {
                    "first": "Jaewoo",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "authors": [
                {
                    "first": "Yinhan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Myle",
                    "middle": [],
                    "last": "Ott",
                    "suffix": ""
                },
                {
                    "first": "Naman",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "Jingfei",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "Mandar",
                    "middle": [],
                    "last": "Joshi",
                    "suffix": ""
                },
                {
                    "first": "Danqi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Omer",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "Mike",
                    "middle": [],
                    "last": "Lewis",
                    "suffix": ""
                },
                {
                    "first": "Luke",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                },
                {
                    "first": "Veselin",
                    "middle": [],
                    "last": "Stoyanov",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Joint learning of named entity recognition and entity linking",
            "authors": [
                {
                    "first": "Pedro",
                    "middle": [
                        "Henrique"
                    ],
                    "last": "Martins",
                    "suffix": ""
                },
                {
                    "first": "Zita",
                    "middle": [],
                    "last": "Marinho",
                    "suffix": ""
                },
                {
                    "first": "Andr\u00e9",
                    "middle": [
                        "F T"
                    ],
                    "last": "Martins",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
            "volume": "",
            "issn": "",
            "pages": "190--196",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/P19-2026"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Amitava Bhattacharyya, and Mahanandeeshwar Gattu",
            "authors": [
                {
                    "first": "Ishani",
                    "middle": [],
                    "last": "Mondal",
                    "suffix": ""
                },
                {
                    "first": "Sukannya",
                    "middle": [],
                    "last": "Purkayastha",
                    "suffix": ""
                },
                {
                    "first": "Sudeshna",
                    "middle": [],
                    "last": "Sarkar",
                    "suffix": ""
                },
                {
                    "first": "Pawan",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "Jitesh",
                    "middle": [],
                    "last": "Pillai",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2nd Clinical Natural Language Processing Workshop",
            "volume": "",
            "issn": "",
            "pages": "95--100",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/W19-1912"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Sentencebert: Sentence embeddings using siamese bertnetworks",
            "authors": [
                {
                    "first": "Nils",
                    "middle": [],
                    "last": "Reimers",
                    "suffix": ""
                },
                {
                    "first": "Iryna",
                    "middle": [],
                    "last": "Gurevych",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Generating natural language adversarial examples through probability weighted word saliency",
            "authors": [
                {
                    "first": "Yihe",
                    "middle": [],
                    "last": "Shuhuai Ren",
                    "suffix": ""
                },
                {
                    "first": "Kun",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "Wanxiang",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Che",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "1085--1097",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/P19-1103"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "SemEval-2017 task 4: Sentiment analysis in Twitter",
            "authors": [
                {
                    "first": "Sara",
                    "middle": [],
                    "last": "Rosenthal",
                    "suffix": ""
                },
                {
                    "first": "Noura",
                    "middle": [],
                    "last": "Farra",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)",
            "volume": "",
            "issn": "",
            "pages": "502--518",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/S17-2088"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Drugbank 5.0: A major update to the drugbank database for",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Wishart",
                    "suffix": ""
                },
                {
                    "first": "Yannick",
                    "middle": [],
                    "last": "Djoumbou",
                    "suffix": ""
                },
                {
                    "first": "An",
                    "middle": [
                        "Chi"
                    ],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Elvis",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                },
                {
                    "first": "Ana",
                    "middle": [],
                    "last": "Marcu",
                    "suffix": ""
                },
                {
                    "first": "Jason",
                    "middle": [],
                    "last": "Grant",
                    "suffix": ""
                },
                {
                    "first": "Tanvir",
                    "middle": [],
                    "last": "Sajed",
                    "suffix": ""
                },
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "Carin",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Zinat",
                    "middle": [],
                    "last": "Sayeeda",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Nazanin Assempour",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1093/nar/gkx1037"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Hierarchical attention networks for document classification",
            "authors": [
                {
                    "first": "Zichao",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Diyi",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Dyer",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1480--1489",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/N16-1174"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Mie: A medical information extractor towards medical dialogues",
            "authors": [
                {
                    "first": "Yuanzhe",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shiwan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jiarun",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Kang",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Shengping",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ACL",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "A lexical simplification tool for promoting health literacy",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zilio",
                    "suffix": ""
                },
                {
                    "first": "Liana",
                    "middle": [
                        "Braga"
                    ],
                    "last": "Paraguassu",
                    "suffix": ""
                },
                {
                    "first": "Luis",
                    "middle": [],
                    "last": "Antonio Leiva",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hercules",
                    "suffix": ""
                },
                {
                    "first": "Laura",
                    "middle": [],
                    "last": "Ponomarenko",
                    "suffix": ""
                },
                {
                    "first": "Maria Jos\u00e9 Bocorny",
                    "middle": [],
                    "last": "Berwanger",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Finatto",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "READI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "1 https://github.com/Ishani-Mondal/BBAEG.git Algorithm 1: BBAEG Algorithm Input: D=[w1, ... w l ], label = y, target classification model M Output: Adversarial example of D = D adv 1 Initialization: D adv \u2190 D, Tag the entities in D, Named entities are in SNE and the rest in SNNE ; 2 Compute token importance Ii \u2200 wi \u2208 D; 3 for i in descending order of Ii do 4 L = {} ; 5 if (wi in SNE and (wi\u2212t..wi+t) is a NE) (wi in SNNE) then 11 D adv = D adv[1:i\u22121] [M]D adv[i+1:l] ; 12 T = top-K filtered and semantically similar tokens for M \u2208 DM ; t \u2208 T such that M (L[t]) = y then 18 Return: D adv \u2190 L[t ] where M (L[t]) = y and L[t ] has maximum similarity with D 19 else 20 N1 = Rotate p characters in wi (p \u2264 l); 21 N2 = Random insertion of symbols before/end in wi; 22 Noise = N1 + N2 ; 23 for t \u2208 N oise do 24 L[t] = D adv[1:i\u22121] [t]D adv[i+1:t \u2208 T such that M (L[t]) = y then 27 Return: D adv \u2190 L[t ] where M (L[t]) = y and L[t ] has maximum similarity with D 28 else if wi contains numeric entity then 29 t = Replace wi by num2words ; 30 L[t] = D adv[1:i\u22121] [t]D adv[i+1:l] ; 31 Return: D adv \u2190 L[t] if M (L[t]D adv \u2190 L[t ] where L[t ] causes max reduction in y probability D adv \u2190 None BBAEG Algorithm: Our proposed BBAEG algorithm consists of four steps: 1) Tagging the biomedical entities on D and prepare two classes NE (named entities) and Non-NE (non-named entities) 2) Ranking the important words for perturbation 3) Choosing perturbation schemes 4) Final adversaries generation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "use 6 classification models as M : Hierarchical Attention Model (Yang et al., 2016), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), BioBERT (Lee et al., 2019), Clinical-BERT (Huang et al., 2019), SciBERT",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Before-attack and after-attack accuracies of the models along with the % of perturbed words in the input space. Best attack and least % of perturbations are shown in bold for each dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "shows the adversaries generated by BBAEG on handpicked examples from test set of ADE corpus. The different adversaries generated by baselines and BBAEG are shown. Also, the adversaries generated using different ablation of sieves [Spellings in Blue and Number in green, synonyms by attack algorithms in red] are shown.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Ablation analysis of the sieves (S1-S3) on accuracy drop and average semantic similarities between adversaries and original text.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Human Evaluation on both the datasets.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The author would like to thank the annotators for hard work, and also the anonymous reviewers for their insightful comments and feedback.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgement"
        }
    ]
}
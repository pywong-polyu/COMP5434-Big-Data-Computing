{
    "paper_id": "3d19021a2880e9054ba979d3073d98366a564047",
    "metadata": {
        "title": "Data Placement for Multi-Tenant Data Federation on the Cloud",
        "authors": [
            {
                "first": "Ji",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Lei",
                "middle": [],
                "last": "Mo",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Sijia",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Jingbo",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "\u00b6",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Shilei",
                "middle": [],
                "last": "Ji",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Haoyi",
                "middle": [],
                "last": "Xiong",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Dejing",
                "middle": [],
                "last": "Dou",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Due to privacy concerns of users and law enforcement in data security and privacy, it becomes more and more difficult to share data among organizations. Data federation brings new opportunities to the data-related cooperation among organizations by providing abstract data interfaces. With the development of cloud computing, organizations store data on the cloud to achieve elasticity and scalability for data processing. The existing data placement approaches generally only consider one aspect, which is either execution time or monetary cost, and do not consider data partitioning for hard constraints. In this paper, we propose an approach to enable data processing on the cloud with the data from different organizations. The approach consists of a data federation platform named FedCube and a Lyapunov-based data placement algorithm. FedCube enables data processing on the cloud. We use the data placement algorithm to create a plan in order to partition and store data on the cloud so as to achieve multiple objectives while satisfying the constraints based on a multi-objective cost model. The cost model is composed of two objectives, i.e., reducing monetary cost and execution time. We present an experimental evaluation to show our proposed algorithm significantly reduces the total cost (up to 69.8%) compared with existing approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "D Ata sharing is the first step for the data-related collaborations among different organizations [1] , for example, joint modeling with data from multi-party. Meanwhile, direct sharing of raw data with collaborators is difficult due to big volume and/or ownership [2] , [3] . Data federation [4] virtually aggregates the data from different organizations, which is an appropriate solution to enable data-related collaborations without direct raw data sharing. Based on cloud service, data federation works as an intermediate layer to establish an abstract data interface. It provides a virtual data view, on which the involved organizations can collaboratively store, share and process data.",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 101,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 265,
                    "end": 268,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 271,
                    "end": 274,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 293,
                    "end": 296,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "As high efficiency and low cost make it possible to lease resources, e.g., computing, storage, and network, at a large scale, a growing number of organizations tend to outsource their data onto the cloud. With the pay-as-you-go model, cloud computing (cloud) brings convenience to the organizations to store and process a large amount of data. Cloud services bring a large number of resources at different layers. A Virtual Machine (VM) is an emulator of a computer, which can be viewed as a computing node in a network [5] . Through the data storage services, unlimited data can be stored on the cloud. Cloud providers promise to provide three features, i.e., infinite computing resources available on-demand, dynamic hardware resource provisioning in need, machines and storage paid and released as needed [6] .",
            "cite_spans": [
                {
                    "start": 520,
                    "end": 523,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 808,
                    "end": 811,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "Dynamic provisioning enables cloud tenants/users to construct scalable systems with reasonable cost on the cloud [7] . With these features, the scientific collaboration on the cloud among different organizations becomes a practical solution.",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 116,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "Despite the advantages of cloud computing, data security issue on the cloud tends to be serious. When the data is stored on the cloud, it is crucial to keep confidentiality. Only the authorized tenants/users should have access to the data [8] . Encryption is a conventional way to keep the data confidential, such as identity-based encryption [9] . In addition, the isolation techniques [10] , which provide secure execution spaces for different jobs with specific access controls, are also used to control the accessibility to the data on the cloud. A job is composed of a data processing program or a set of data processing programs to be executed on the cloud in order to generate new knowledge from the input data. During the scientific collaboration based on the data stored on the cloud, the combination of encryption algorithms and isolation techniques can be utilized to keep the confidentiality and security of the data on cloud.",
            "cite_spans": [
                {
                    "start": 239,
                    "end": 242,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 343,
                    "end": 346,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 387,
                    "end": 391,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "When using the cloud services, tenants/users have to pay for them. For instance, when tenants/users directly store their data on the cloud, they would be charged for the cloud storage service. Widely used cloud service providers, such as Amazon Web Services (AWS) cloud 1 , Microsoft Azure cloud 2 and Baidu cloud 3 , provide different data storage types, e.g., hot data storage, data storage with low frequency, cold data storage, and archive data storage, as data storage services. The cost of data storage on the cloud varies from type to type. In order to reduce the monetary cost to store and to process the data on the cloud, it is necessary to choose a proper data storage type based on a data placement algorithm. However, the job execution frequency is not well exploited while constructing the data placement algorithms for the data storage on the cloud. In addition, existing approaches cannot exploit data partitioning techniques to satisfy multiple constraints.",
            "cite_spans": [
                {
                    "start": 314,
                    "end": 315,
                    "text": "3",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "There are multiple constraints for the data processing on the cloud. For instance, when a user requires that the execution of a job should be within a time period, there is a hard time deadline. When a user has a budget limit for the data processing, there is a hard monetary budget for the execution of jobs. In addition, when the system is stable, the jobs can be continuously executed. Otherwise, there may be storage errors during the execution when the accumulated stored data exceed the storage capacity. Thus, the system stability is critical as well.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "In this paper, we propose a solution to enable data processing on the cloud for scientific collaboration among different organizations. The solution consists of a secure data processing platform named FedCube, a multi-objective cost model, and a Lyapunov-based data placement algorithm. The main contributions of this paper are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "\u2022 The FedCube platform. We propose a cloud platform, i.e., FedCube. FedCube enables secure data processing with the encrypted data stored on the cloud for collaboration among different organizations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "\u2022 A data placement problem formulation. We formulate the data placement problem based on a multiobjective cost model and constraints. The multiobjective cost model consists of monetary cost and execution time. The constraints include hard execution time deadline, hard monetary budget, and system stability constraint.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "\u2022 A Lyapunov-based data placement algorithm. We use the algorithm to create a data storage plan based on the cost model in order to reduce both monetary cost and the execution time of jobs with the consideration of constraints while exploiting data partitioning techniques.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "\u2022 An extensive experimental evaluation based on a simulation and a widely used benchmark, i.e., Wordcount, and a real-life data processing application for COVID-19 [11] . The simulation and the experiments are carried out based on a widely used cloud, i.e., Baidu cloud.",
            "cite_spans": [
                {
                    "start": 164,
                    "end": 168,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "The rest of the paper is organized as follows. Section 2 reviews related works. Section 3 presents the system design of the secure data processing platform. Section 4 presents the data placement system model, proposes a cost model, shows the hard constraints, and defines the problem. Section 5 proposes the Lyapunove-based data placement algorithm based on the cost model. Section 6 shows the experimental results. Finally, we conclude the paper in Section 7.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "Lyapunov optimization is widely used to optimize the system while ensuring system stability. For instance, Lyapunov optimization is exploited to gain profit [12] , to ensure the Quality of Service [13] and the time average sensing utility [14] . However, the aforementioned work focuses on a single objective besides the system stability and does not consider the task or data partitioning for satisfying multiple constraints. In this paper, we combine the Lyapunov optimization with multiple objectives for data placement.",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 161,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 197,
                    "end": 201,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 239,
                    "end": 243,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "RELATED WORK"
        },
        {
            "text": "Data placement is critical to both the monetary cost and the execution time of jobs. In order to reduce the execution time, data transfer can be reduced based on graph partitioning algorithm [15] . In addition, the data dependency among different jobs can be exploited to reduce the time and monetary cost to transfer data [16] . However, these methods only consider one objective, i.e., reducing execution time. They cannot be applied to place the data in different storage types on the cloud. A weighted function of multiple costs can be used to achieve multiple objectives, which can generate a Pareto optimal solution [17] , while the authors do not consider the cost to store data on the cloud or hard constraints. Load balancing algorithms [18] or dynamic provisioning algorithms [19] are proposed to generate an optimal provisioning plan in order to minimize the monetary cost while they do not consider the data storage types on the cloud. The storage type of the best performance can be selected to store data [20] while the economic storage type can be selected [21] . However, these two methods cannot address multiple objectives. In this paper, we propose an algorithm to achieve multiple objectives by placing data into various data storage types while satisfying hard constraints.",
            "cite_spans": [
                {
                    "start": 191,
                    "end": 195,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 323,
                    "end": 327,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 622,
                    "end": 626,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 746,
                    "end": 750,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 786,
                    "end": 790,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1019,
                    "end": 1023,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1072,
                    "end": 1076,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "RELATED WORK"
        },
        {
            "text": "In order to handle a multi-objective problem, there are basically two types of solutions, i.e., a priori and a posteriori [17] , [22] . In this paper, we use an a priori method, where the preference information is provided by the users, and then the best solution is produced. Our approach is based on a multi-objective scheduling algorithm focusing on minimizing a weighted sum of objectives. The advantage of such approach is that the scheduling is automatically guided by predetermined weights. In contrast, a posteriori methods produce a Pareto front of solutions without predetermined preference information [22] . Each produced solution is better than the others with respect to at least one objective, and users need to choose one from the produced solutions, which corresponds to user interference. In this paper, we assume that users can determine the value for the weight of each objective. a priori methods can enable us to produce optimal or near-optimal solutions without user interference at run-time. Finally, when the weight of each objective is positive, the minimum of the weighted cost function is already a Pareto optimal solution [23] , [24] and our proposed approach can generate a Pareto optimal or near-optimal solution with the predefined weights. Thus, we do not consider a posteriori methods in this paper.",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 126,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 129,
                    "end": 133,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 613,
                    "end": 617,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1151,
                    "end": 1155,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "RELATED WORK"
        },
        {
            "text": "Data security is of much importance to the cloud users. In order to protect data security, data accessibility is controlled by attributing different levels of permission to avoid unauthorized or malicious access to data on the cloud [25] . In addition, encryption techniques [26] , [27] and distributed data storage plan based on data partitioning [28] , [29] , [30] can be exploited. Federated learning is proposed to train a model while ensuring data privacy [31] , yet it is not applicable to the general data processing among different organizations on the cloud. In addition, secure separated data processing spaces [10] are proposed to ensure the access control and privacy of data. The separated data processing spaces are disconnected from the public network, which ensures that the confidentiality and the security of data within the local network. Out proposed platform, i.e., FedCube, not only provides different data access controls for different tenants/users but also exploits the secure separated data processing spaces to ensure the security and the confidentiality of the data.",
            "cite_spans": [
                {
                    "start": 233,
                    "end": 237,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 275,
                    "end": 279,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 282,
                    "end": 286,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 348,
                    "end": 352,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 355,
                    "end": 359,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 362,
                    "end": 366,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 461,
                    "end": 465,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 621,
                    "end": 625,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "RELATED WORK"
        },
        {
            "text": "In this section, we propose a secure data processing platform named FedCube. First, we explain the architecture of the platform. Then, we present the life cycle of users' accounts and jobs to be executed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SYSTEM DESIGN"
        },
        {
            "text": "The FedCube platform is a data federation platform that provides tenants/users with secure data processing service on the cloud. Tenants/users can upload their data onto the platform and execute the self-written programs on Baidu cloud. In addition, tenants/users can leverage the data from other organizations for their own data processing jobs, as long as they get permission from the data owners. We illustrate the architecture of the platform and explain the functionalities of each module in this section. As shown in Fig. 1 , the functional architecture of the platform consists of four modules:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 523,
                    "end": 529,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Architecture"
        },
        {
            "text": "The environment initializer creates the user account and its execution space on the coordinator node. The created user account is used for the user's security configuration, e.g., the access permission to certain data from another user. The user account is also associated with secure execution spaces for the execution of submitted jobs in the cluster. The secure execution space is a working space without a connection to any public network, which can ensure the confidentiality and the security of the data within the local network.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Environment Initializer"
        },
        {
            "text": "As shown in Fig. 2 , multiple clusters can be dynamically created by the environment initializer module when the execution of jobs is triggered. Each cluster consists of several computing nodes, i.e., VMs on the cloud. The coordinator node coordinates the execution among different clusters for all users. The user has access to the platform through the coordinator node, which is connected to the public Internet. The computing nodes in each cluster are only interconnected with the coordinator node through the local network on the cloud. Each computing node is created based on the image [32] indicated by the user, which contains necessary tools for the execution of her jobs. An image is a serialized copy of the entire state of a VM stored on the cloud [32] .",
            "cite_spans": [
                {
                    "start": 591,
                    "end": 595,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 759,
                    "end": 763,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 18,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Environment Initializer"
        },
        {
            "text": "The data storage manager creates a data storage account and storage buckets on the cloud for a user. A storage bucket is a separated storage space to store the data with its own permission strategy. The data storage account is used to transfer data between the platform and the user's devices, e.g., computer. Each account is associated with five buckets, i.e., user data bucket, user program bucket, output data bucket, download data bucket, and execution space bucket. Each account has an independent Authorization Key (AK) and Secret Key (SK), with which the tenants/users can send or retrieve the data stored in the buckets. In addition, the access permission strategy varies from bucket to bucket. For instance, the user has read and write permission on the user data bucket and the user program bucket while she only has the read permission on the download data bucket. A user can store data in the user data bucket while she can submit self-written codes to the user program bucket. The tenants/users do not have read or write permission on the output data bucket and the execution space bucket. After the execution of the program generated based on the submitted codes, the output data is stored in the output data bucket. After the confidentiality review of the output data, the output data is transferred to the download data bucket. The review is carried out by the owner of the input data of the job in order to avoid the risk that the raw data or sensitive information appears in the output data of the job. The execution space bucket is used to cache intermediate data of a job, which can be useful for the following execution in order to reduce useless repetitive execution [33] .",
            "cite_spans": [
                {
                    "start": 1689,
                    "end": 1693,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Data Storage Manager"
        },
        {
            "text": "The job execution trigger starts the execution of the job in a cluster. A user can upload the user-written codes onto the platform through a web portal. Then, she can start the execution of the program using the job execution trigger. Once the execution of the program is triggered, a cluster is created, deployed, and configured (see details in Section 3.1.1). Afterward, the execution of the job is performed in the computing nodes of the cluster. When several jobs start simultaneously in the same cluster, the job execution trigger creates the same number of execution spaces as that of jobs in order to enable parallel execution without conflict. When the input data of a program consists of the data from other data owners, the corresponding data interfaces are used in order to avoid direct raw data sharing. Let us take two tenants/users as example: User U 1 and User U 2 . A data interface (I 1 ) is defined by the data owner (User U 1 ), which is associated with the data (D 1 ) on the platform. When User U 2 gets the permission to use D 1 , the program generated based on the submitted codes of User U 2 can process the data D 1 using the Interface I 1 . The intermediate data stored in the execution bucket can also be used when the job needs the results of the previous execution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Job Execution Trigger"
        },
        {
            "text": "In the platform, we use four mechanisms to ensure the security of the data. The first mechanism is to encrypt the data before storing it on the cloud. The encryption is based on the Rijndael encryption algorithm [34] . The second mechanism is to separate the computing nodes from the public network, e.g., Internet, which ensures that no data communication is allowed between the clusters and outside devices, e.g., servers, on the cloud. The third mechanism is a uniformed data access control. When a user applies for the permission of the data owned by another user, a data access interface is provided by the data owner instead of direct raw data sharing. The last mechanism is the audition of the codes and output data by data owners, which ensures that no data is leaked from output data. Through these mechanisms, data confidentiality and security are ensured by the data interface defined by the data owner while ensuring efficient cooperation among different organizations.",
            "cite_spans": [
                {
                    "start": 212,
                    "end": 216,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Security Module"
        },
        {
            "text": "In order to present the interactions among users, the platform, and the job execution on the platform, we present the account life cycle and job life cycle. The life cycle describes the state transition of a user account or a job on the platform. We assume that there are n scientific collaborators. Each collaborator has private data, which requires keeping confidentiality and security. Through the life cycle, we present how n collaborators process the data on the platform.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Life Cycle"
        },
        {
            "text": "The account life cycle consists of three phases, i.e., account creation, data processing, and account cleanup. First, the account related to the user of the platform is created. Then, the user can process the data on the platform. Finally, when the user no longer needs the platform, the data related to the account is removed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Account Life Cycle"
        },
        {
            "text": "Account Creation Phase. When a new user needs to use the platform, we create an account and configure the platform using the environment initializer module as shown in Fig. 1 . For the n collaborators in the above scenario, we create n accounts (U t with t representing the number of the collaborator) for each scientific collaborator on the platform. First, the job execution trigger is deployed for each user in the coordinator node. Then, the data storage manager creates a storage account and five storage buckets (see details in Section 3.1.2) for each user. Afterward, the environment initializer deploys the security module for each user. The security module contains the encryption and decryption information for each user. Please note that the encryption and decryption information is different for different users.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 168,
                    "end": 174,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Account Life Cycle"
        },
        {
            "text": "Data Processing Phase. After the account creation, data processing jobs can be carried out on the platform. Before processing the data, each user uploads her own data and the data interface file to the user data bucket. As shown in Fig. 3 , if User U i needs to exploit the data from another Users U j , User U i can apply for the permission. Once User U i gets the permission from User U j , the user also gets the necessary information, e.g., the mock data, to access the data using the corresponding data interface. The mock data contains the data schema of the raw data and some randomly generated examples, while the raw data is never shared with the users. User U i may use the data from several other tenants/users at the same time. Then, User U i can submit the codes to process data. In order to process data, User U i triggers the execution of a job related to the submitted codes (see details in Section 3.2.2), which corresponds to the execution of the job (j i with i representing the number of the execution) on the platform. During the execution of a job, the intermediate data generated from different execution of the job can be directly used. After the execution and the review of the output data, user U i can download the output data of Job j i from the user download bucket.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 232,
                    "end": 238,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Account Life Cycle"
        },
        {
            "text": "Account Cleanup Phase. When the user no longer needs the platform, the corresponding data, storage buckets, and accounts are removed from the platform by the environment initializer module.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Account Life Cycle"
        },
        {
            "text": "Initialization Phase. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Account Life Cycle"
        },
        {
            "text": "The job life cycle consists of four phases, i.e., initialization, data synchronization, job execution, and finalization. The initialization phase [35] is to prepare the environment to execute a job on the platform. The preparation contains three steps: provisioning, deployment, and configuration. First, VMs are provisioned to the job as computing nodes. There are two cases where existing VMs can be provisioned to the job. The first case is that there are enough live computing nodes on the platform corresponding to the execution of the same or the other jobs of the same user. The second case is that there are enough live computing nodes for the programs of other tenants/users, and all the related tenants/users allow sharing computing nodes. Otherwise, the environment initializer module dynamically creates new VMs as computing nodes, which contain necessary tools for the execution of the job. Then, in order to execute the job, a proper execution space is deployed on the allocated VMs. In order to enable data access, the execution space is configured in each node. For instance, the AK and SK files are transferred into the computing nodes in order to enable data synchronization.",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 150,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Job Life Cycle"
        },
        {
            "text": "Data Synchronization Phase. During the data synchronization phase [36] , the data storage module synchronizes the data or data interfaces stored on the cloud. In addition, the scripts or the files corresponding to the submitted codes are also transferred to the execution space created in the initialization phase.",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 70,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Job Life Cycle"
        },
        {
            "text": "Job Execution Phase. The execution phase [35] is the period to execute jobs in the execution space of corresponding VMs. The execution frequency of each job can be dynamically monitored by the platform to compute the cost of data storage. The data, including newly generated intermediate data, is dynamically placed with appropriate storage types with small cost according to the method presented in Section 5. As shown in Fig. 3 , after synchronizing the data from buckets, the program corresponding to the submitted codes processes the input data. The execution can be performed in a single computing node or multiple computing nodes in order to reduce the overall execution time. After the execution, the output data is transferred to the output bucket of the user. Once the data is reviewed and approved by the data owners of the input data, it is encrypted by the security module and is transferred to the download bucket to be accessed by the user.",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 45,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [
                {
                    "start": 423,
                    "end": 429,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Job Life Cycle"
        },
        {
            "text": "Finalization Phase. In the finalization phase [37] , the data storage manager uploads the encrypted intermediate of the job. Afterward, the environment initializer module removes the corresponding execution space(s). If a node does not contain any execution space, the node is released, i.e., removed, by the environment initializer, in order to reduce the monetary cost to rent the corresponding VMs.",
            "cite_spans": [
                {
                    "start": 46,
                    "end": 50,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Job Life Cycle"
        },
        {
            "text": "In this section, we first present the system model for data placement. Then, we propose a cost model based on two costs, i.e., monetary cost and execution time. Afterward, we present the data placement constraints, i.e., hard execution time constraints and the hard monetary budget constraints. Finally, we define the problem to address in the paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MULTI-OBJECTIVE COST MODEL AND PROB-LEM FORMULATION"
        },
        {
            "text": "The system model for data placement is shown in Fig. 4 . In the FedCube platform, we assume that the execution of jobs generates intermediate data at time slot t, which may be used as input data in the following time slots, e.g., t+x with x > 0. Then, the intermediate data should be placed with other input data. Each job has a queue to store the generated intermediate data, and we consider N data storage spaces, which correspond to N storage types with diverse data access speeds and diverse prices to store data. Each data set can be placed to one or multiple data storage types. In order to place a data set to multiple storage types, a data set can be partitioned into several chunks, and each chunk is placed to a data storage type. We assume that the valid time of data set d i placed at storage type s j is T max(i,j) . If data set d i is not accessed by any job within T max(i,j) , the data set will be removed from the storage space of the platform. When there is a data set generated during the execution of a job or when a job is executed, all the input data is placed again. When the input data is being replaced, its original corresponding storage type is kept until a newly placed storage type is associated.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 48,
                    "end": 54,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Data Placement System Model"
        },
        {
            "text": "Inspired by [17] , we propose a multi-objective cost model. The cost model is composed of monetary cost and time cost (i.e., the execution time of a job). In order to find a storage plan, we need a cost model to estimate the cost of storing the input data for the execution of jobs. The cost model is generally implemented in the data storage module and under a specific execution environment. In the case of this paper, the execution environment is the FedCube platform. The origin of parameters mentioned in this section is summarized in Table 1 . We assume that there are K jobs on the platform.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 16,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 540,
                    "end": 547,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Cost Model"
        },
        {
            "text": "The total cost to execute a set of jobs with a data placement plan at time slot t is defined as the sum of the total cost of all the jobs:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Model"
        },
        {
            "text": "where P lan[t] represents a data placement plan of the data sets related to the set of jobs at time slot t, and j k TABLE 1 Description of parameters. \"Abbreviation\" represents the abbreviation of the parameters. \"Origin\" represents where the value of the parameter comes from. UD: that the parameter value is defined by users; Measure: that the parameter value is estimated by the user with the job in a cloud environment; Execution: measured during the execution of job in cloud; cloud: the parameter value is obtained from the cloud provider represents the k th job. In the rest of this paper, the total cost represents the normalized cost to execute a set of jobs with a data placement plan per time unit. P lan[t] is a matrix of data placement variables, which can be expressed by the following formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Model"
        },
        {
            "text": "where p i,j [t] represents that data set d i is placed to storage type s j , m represents the number of input and intermediate data sets, and n represents the number of storage types. When p i,j [t] = 0, data set d i is not placed to storage type s j ; when p i,j [t] = 1, data set d i is directly placed to storage type s j ; When 0 \u2264 p i,j [t] \u2264 1, data set d i is partitioned and the part corresponding to p i,j [t] is placed to storage type s j . The total cost to execute a job is defined by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Model"
        },
        {
            "text": "where T n (job k , P lan[t]) and M n (job k , P lan[t]) are the normalized time cost and monetary cost, respectively, and they can be defined by Formulas (4) and (9); job k represents the k th job and P lan[t] represents the data placement plan at time slot t; w t and w m represents the importance of the execution time and the monetary cost of the job. Defined by the user, w t and w m should be positive values that meet the constraints:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Model"
        },
        {
            "text": "represents the average frequency of the job execution, which can be dynamically measured according to the history execution before the job execution, e.g., daily, monthly, quarterly and yearly. Since the time cost and monetary cost are normalized, neither of them has a unit. Please note that the time cost refers to the execution time of Jobs once while the hard monetary budget is related to the budget per time period, e.g., a day or a month.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Model"
        },
        {
            "text": "The normalized time cost is defined by the following formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Cost"
        },
        {
            "text": "where T(j, P lan[t]) represents the total execution time of the job and DT k represents the expected execution time (set by the user) of Job job k . Please note that the execution time T(j, P lan[t]) represents the time to execute job k once. The desired execution time could be larger or smaller than the real execution time Time(j, P lan[t]) while it should be larger than a limit defined by the FedCube platform, e.g., 1/20 * SET k with SET k representing the sequential execution time of Job job k with one computing node, in order to avoid the unfairness among users. The total execution time consists of three parts, which are defined by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Cost"
        },
        {
            "text": "where InitT(job k ) represents the Time to Initialize the computing nodes for Job job k ; DTT(job k , P lan[t]) represents the Time to Transfer the Data from the cloud storage service to computing nodes; ET(job k ) represents the Execution Time of Job job k . The initialization of the computing nodes for Job job k can be specified by the user or realized by the platform, which is out of the scope of this paper while the time can be calculated based on Job job k , e.g., n k \u00b7 AIT with n k representing the number of computing nodes and AIT representing the average time to initialize a computing node. The data transfer time can be calculated based on the size of the input data of Job job k and the data placement plan as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Cost"
        },
        {
            "text": "where speed j represents the speed to transfer data from data storage type j to computing nodes. As explained in Section 4.1, N represents the total number of storage types on the FedCube platform. According to the Amdahl's law [38] , the execution time of Job j can be estimated by the following formula [17] when the impact of data communication can be ignored with n computing nodes:",
            "cite_spans": [
                {
                    "start": 228,
                    "end": 232,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 305,
                    "end": 309,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Time Cost"
        },
        {
            "text": "where \u03b1 k 1 represents the percentage of the workload that can be executed in parallel; n is the number of computing nodes, which is configured by users while it should be less than a limit, e.g., 20, defined by the FedCube platform; WL(j) represents the workload of a job which can be measured by the number of FLOP (FLoat-point Operations) [39] . CSP is the average computing performance of each computing node, which can be measured by the number of FLOPS (FLoating-point Operations Per Second).",
            "cite_spans": [
                {
                    "start": 342,
                    "end": 346,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "Time Cost"
        },
        {
            "text": "Normalized monetary cost is defined by the following formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Monetary Cost"
        },
        {
            "text": "where Money(job k , P lan[t]) is the financial cost to rent VMs as computing nodes on the cloud. Please note that the monetary cost Money(job k , P lan[t]) represents the total monetary cost to execute job k within a time period, e.g., a month. DM k represents the expected execution monetary cost of Job Job k , which can be configured by the user while it should be larger than or equal to a limit defined by the FedCube platform, i.e., SMC k with SMC k representing the monetary cost of Job job k with one computing node, in order to avoid the unfairness among users. DM k can be bigger or smaller than the real monetary cost Money(job k , t). Money(job k , P lan[t]) can be estimated based on the following formula: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Monetary Cost"
        },
        {
            "text": "where VMP(job k ) represents the average monetary cost of a VM for the execution of Job j; n k represents the number of computing nodes to execute the job; T(job k , P lan[t]) and InitT(job k ) are defined in Formula (5). We allocate the storage monetary cost of a data set to the jobs based on the workload. DSM(j, P lan[t]) is defined by the following formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Monetary Cost"
        },
        {
            "text": "1. \u03b1 k can be obtained by measuring the execution time of executing the job k twice with different numbers of computing nodes [17] . For instance, assume that we have t 1 for m 1 computing nodes and t 2 for m 2 computing nodes,",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 130,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Monetary Cost"
        },
        {
            "text": "where WL(job k ) represents the workload of job job k ; dataset(j) represents the data sets that job j uses; job(i) represents the jobs that takes data i as input data; SP(s i ) represents the monetary cost to store the data with the storage type s i , which is defined in the data placement plan plan[t], on the cloud; size(d i ) represents the size of the input data d i . DAM(job k , P lan[t]) is defined by the following formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Monetary Cost"
        },
        {
            "text": "where RP j represents the monetary cost to read data d i from the cloud storage service; size(d i ) represents the size of the input data d i of Job job k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Monetary Cost"
        },
        {
            "text": "In this section, we present the constraints of data placement. First, we present the hard execution time and monetary budget constraints for each job. Then, we present the system stability constraint based on Lyapunov optimization. We assume that there are hard time deadline and hard monetary budget for each job, which can be formulated as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Placement Constraints"
        },
        {
            "text": "where T(job k , P lan[t]) and M(job k , P lan[t]) are defined in Formulas 5 and 10 respectively, T DL k represents the hard execution time deadline, M B k represents the hard monetary cost Budget, and Jobs represents the set of jobs in the system. For storage spaces, as shown in the right part of Fig. 4 , we use S j (t) to denote the set of data sets placed in the data storage space of Type j. Therefore, the dynamic set is defined as follows:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 298,
                    "end": 304,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Data Placement Constraints"
        },
        {
            "text": "where r j (t) represents the data to be removed because of time limit and m i=1 p i,j [t] represents the newly placed data sets to data storage type s j .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Placement Constraints"
        },
        {
            "text": "For jobs shown in the middle part of Fig. 4 , we use J i to denote the set of data sets generated from the execution of Job i. We have the following job data storage set defined as follows: (17) where data k represents the set of input data sets of Job k, and G k [t] represents the newly generated intermediate data of Job k.",
            "cite_spans": [
                {
                    "start": 190,
                    "end": 194,
                    "text": "(17)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 37,
                    "end": 43,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Data Placement Constraints"
        },
        {
            "text": "We exploit the Lyapunov optimization technique [40] by considering both the set of data sets placed in the data storage spaces and the job data storage sets. Let D(t) = (S j (t), J i (t), j \u2208 {1, . . . , n}, i \u2208 {1, . . . , k}, t \u2208 {1, 2, . . . , }) denote all the data sets in time slot t. We have the following constraint in order to ensure the stability of the system:",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 51,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Data Placement Constraints"
        },
        {
            "text": "The problem we address in the paper is a data placement problem, i.e., how to choose a storage type to store the data in order to reduce the expected total cost, which consists of the monetary cost and the execution time of jobs, while satisfying constraints, on the cloud. A job can be executed multiple times because the user-defined codes are updated, or the parameters are updated [41] . As shown in Table 2 , different data storage types of storage services on the cloud correspond to different prices. The storage type with higher expected data access frequency, e.g., Standard, has a higher price and higher data access speed. The total cost to execute a job once differs with different data placement plans. Thus, the problem we address in this paper is how to find an optimal data placement plan of all the data sets in order to reduce the expected total cost to execute the jobs with different execution frequencies based on a cost model. We define the expected total cost as:",
            "cite_spans": [
                {
                    "start": 385,
                    "end": 389,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [
                {
                    "start": 404,
                    "end": 411,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Problem Definition"
        },
        {
            "text": "E{Cost(J obs, P lan[t])}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "(19) Then, the problem addressed in this paper can be formulated as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "s.t.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "Formulas (14), (15) , and (18).",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 19,
                    "text": "(15)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "where Jobs represents the set of jobs in the system. The data placement problem is a typical NP-hard problem [42] . Let us separate the data placement variables into two parts, i.e., p i,j and p i,j . p i,j is a continuous variable between 0 and 1, which represents the partitioning of a data set. p i,j is a 0-1 integer, which is the scheduling decision. Then, we have p i,j = p i,j * p i,j . Then, the problem defined in Formula 19 is a Mixed Integer Linear Programming (MILP) problem, which is a proven NP-hard problem [43] , [44] , [45] . In this case, the exhaustive search for an optimal solution for p i,j increases exponentially and the complexity is O(N M ), which cannot be solved within a polynomial time, with N representing the number of storage types and M represents the number of data sets.",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 113,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 522,
                    "end": 526,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 529,
                    "end": 533,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 536,
                    "end": 540,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "In this section, we present a near-optimal data placement approach based on Lyapunov optimization. Lyapunov optimization is widely used to achieve optimization objectives while ensuring the system stability [40] , [46] . In order to exploit Lyapunov optimization techniques, we first construct a Lyapunov function and propose a Lyapunov-based algorithm (LNODP) to perform the data placement while ensuring system stability. Then, we propose a greedy approach to perform the near-optimal data placement while satisfying hard deadlines. The greedy approach consists of three algorithms, i.e., near-optimal data planning (NOD Planning), near-optimal data placement, and data placement (NOD Placement) with partitioning (NOD Partitioning). LNODP exploits NOD Planning to generate a near-optimal data placement plan; NOD Planning takes advantage of NOD Placement to choose optimal data storage type when the hard constraints can be satisfied, and NOD Placement uses NOD partitioning to generate a plan to partition the data in order to satisfy hard constraints when one data storage type does not work.",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 211,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 214,
                    "end": 218,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                }
            ],
            "ref_spans": [],
            "section": "NEAR-OPTIMAL DATA PLACEMENT"
        },
        {
            "text": "We define a Lyapunov function L(t) as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lyapunov Optimization based Data Placement"
        },
        {
            "text": "This function represents the data sets to be placed. Then, we can define the derivative of the Lyapunov function as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lyapunov Optimization based Data Placement"
        },
        {
            "text": "We use the expectation to address the randomness of the intermediate data generated by the execution of jobs and the data placement actions. As to solve the problem defined in Formula (20) requires the global information the FedCube system, which is hard to predict or gather, we transform the problem defined in Formula (20) to the following objective function, which is a greedy conversion with limited local information:",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 188,
                    "text": "(20)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 321,
                    "end": 325,
                    "text": "(20)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Lyapunov Optimization based Data Placement"
        },
        {
            "text": "where T(job k , P lan[t]) and M(job k , P lan[t]) are defined in Formulas 5 and 10 respectively, T DL k represents the hard execution time deadline, M B k represents the hard monetary cost Budget, and the parameter \u03c9 \u2265 0 represents the importance of the expected total cost compared with the stability of the system.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lyapunov Optimization based Data Placement"
        },
        {
            "text": "The objective function has the following upper bound when t = 1: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1."
        },
        {
            "text": "with L defined in Formula (28), C defined in Formula (30) , and C defined in Formula (31) .",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 57,
                    "text": "(30)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 85,
                    "end": 89,
                    "text": "(31)",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Theorem 1."
        },
        {
            "text": "Proof. First, we focus on the data stored in the job data queue with the assumption that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1."
        },
        {
            "text": "Then, we have the similar results for the data storage spaces with the assumption that N i\u2208data k ,j=1 p i,j [t] \u2264 data max , where data max represents the maximum number of data sets for any job, and G k [t] \u2264 G max k :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1."
        },
        {
            "text": "With Formulas (25) and (26), we have: while not all data sets \u2208 D are placed and iter < T do 4:",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 18,
                    "text": "(25)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Theorem 1."
        },
        {
            "text": "for each Data set d i in D do 6: for j \u2208 N do 7:",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 32,
                    "text": "6:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "10: else 11: Set p i,j [t + 1] = 0 12: end if 13: end for 14: end for 15: end while 16: iter \u2190 iter + 1 17: end for 18: Update J k (t) and S j (t)",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 12,
                    "text": "11:",
                    "ref_id": null
                },
                {
                    "start": 46,
                    "end": 49,
                    "text": "13:",
                    "ref_id": null
                },
                {
                    "start": 58,
                    "end": 61,
                    "text": "14:",
                    "ref_id": null
                },
                {
                    "start": 70,
                    "end": 73,
                    "text": "15:",
                    "ref_id": null
                },
                {
                    "start": 84,
                    "end": 87,
                    "text": "16:",
                    "ref_id": null
                },
                {
                    "start": 116,
                    "end": 119,
                    "text": "18:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "The cost model presented in Section 4.2 can be rewritten as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "Algorithm 2 Near-Optimal Data Planning Input: D: A set of data sets; P lan: data placement plan in Time slot t. Output: P lan * : The near-optimal data placement plan of each data d in data set D with the minimum cost. 1: for each Data d in D do 2: cost before \u2190 calculateCost(P lan)",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 248,
                    "text": "2:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "According to Formula (1) 3: P lan \u2190 getNearOptimalPlacement(d, P lan) 4: if Cost(Plan ) < Cost(Plan) then 5: P lan * \u2190 P lan 6: end if 7: end for Finally, we can take the expectation and add the total cost, i.e., cost(P lan[t]) to both sides of Formula (27) and hence Theorem 1 is proven.",
            "cite_spans": [
                {
                    "start": 25,
                    "end": 27,
                    "text": "3:",
                    "ref_id": null
                },
                {
                    "start": 70,
                    "end": 72,
                    "text": "4:",
                    "ref_id": null
                },
                {
                    "start": 106,
                    "end": 108,
                    "text": "5:",
                    "ref_id": null
                },
                {
                    "start": 125,
                    "end": 127,
                    "text": "6:",
                    "ref_id": null
                },
                {
                    "start": 253,
                    "end": 257,
                    "text": "(27)",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "In order to solve the problem defined in Formula (20), we minimize the upper bound of Theorem 24. As the status of Time slot t can be observed in the system, we only need to minimize the following element:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "with C i,j defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "where Jobs i represents the set of jobs that process Data set d i .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "We design a Lyapunov-based approach to minimize Formula (32), as shown in Algorithm 1. First, we sort the data sets based on C i,j in descent order in order to minimize the cost of the data set corresponding to high costs first (Line 1). Then, for each data set, we use Algorithm 2 to find an optimal data placement plan (Line 4). For each combination of {i, j} (Line 5), if C i,j \u2264 0 (Line 8), we will update p i,j [t + 1] = p * i,j (Line 9), otherwise, we will set p i,j [t + 1] = 0 (Line 11). Please note that the data set is placed with a data placement plan that meets reasonable constraints based on Algorithm 2 when C i,j \u2264 0. Otherwise, the placement plan remains idle and will be set with a proper data placement plan in later time intervals. When the data placement plan of a data set is idle, the execution of related jobs is postponed until the placement plan is set in order to meet the constraints. Please note that when the data in the system exceed the capacity of the system or the given constraints are not reasonable, the algorithm may not generate a proper data placement plan that meets all the constraints.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "Based on the multi-objective cost model, we propose a greedy algorithm to generate a near-optimal data placement Algorithm 3 Near-Optimal Data Placement Input: d: A data set;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Near-Optimal Data Placement Algorithm"
        },
        {
            "text": "Jobs: A set of jobs that process Data set d; StorageT ypeList: The list of storage types; P lan: a data placement plan. Output: P lan * : The near-optimal data placement plan of Data set d. 1: P lan * \u2190 P lan 2: j * = getOptimalType(P lan, d, StorageT ypeList) 3: T ypesF orT imeConstraints \u2190 getTypesForTimeConstraint(P lan, d, Jobs) 4: T ypesF orM onetaryConstraints \u2190 getTypesForMonetaryConstraint(P lan, d, jobs) 5: AvailableT ypes \u2190 T ypesF orT imeConstraints \u2229 T ypesF orM onetaryConstraints 6: if j * \u2208 AvailableT ypes then 7: For j \u2208 [1, N ] and p i,j \u2208 P lan * , set p i,j = 1, j = j * 0, j = j * 8: else 9: P lan * \u2190 dataPlacementWithPartitioning(d, P lan, Jobs, T ypesF orT imeConstraints, 10: T ypesF orM onetaryConstraints) 11: end if plan while reducing the expected total cost to execute a set of jobs on the FedCube platform as shown in Algorithm 2. In the algorithm, for each Data set d, we first calculate the total cost based on the cost model (Line 2). Then, we generate a near-optimal data placement plan by replacing Data set d while keeping the other data sets based on Algorithm 3 (Line 3). Afterward, if the new data placement plan can reduce the total cost according to the cost model, we update the data placement plan if the new data placement plan corresponds to a smaller total cost (Lines 4 -5).",
            "cite_spans": [
                {
                    "start": 531,
                    "end": 533,
                    "text": "7:",
                    "ref_id": null
                },
                {
                    "start": 614,
                    "end": 616,
                    "text": "9:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Near-Optimal Data Placement Algorithm"
        },
        {
            "text": "Algorithm 3 replaces Data set d in order to reduce the total cost. First, we choose an optimal data storage type j * based on the data placement plan by trying each data storage type in storageT ypeList (Line 2). Then, we choose a set of possible storage type candidates that meet both the hard time deadline constraint and hard monetary budget constraint (Lines 3 -4). If the chosen data storage type j * is within the set of storage type candidates, we will update the data storage placement. If not, we will exploit Algorithm 4 to place the data set with data partitioning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Near-Optimal Data Placement Algorithm"
        },
        {
            "text": "Algorithm 4 generates a near-optimal data placement plan with the consideration of data partitioning while meeting the two constraints, i.e., the hard time deadline constraint and the hard monetary budget constraint. First, if any of the set of available data storage type candidates for the hard time deadline constraint or hard monetary budget constraint is an empty set, we consider that the two constraints cannot be met (Lines 2 and 3 ). If not, first, we choose an optimal type (j 1 for the time constraint and j 2 for the monetary constraint) within the set of candidates for each constraint by trying each storage type (Lines 5 and 6). We define a possible area as the range of parts of the data set to be placed at Type j 1 while meeting both the two constraints. We can calculate the possible area for Algorithm 4 Data Placement With Partitioning Input: d: A set of data;",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 425,
                    "end": 439,
                    "text": "(Lines 2 and 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Near-Optimal Data Placement Algorithm"
        },
        {
            "text": "Jobs: A set of job that process Data set d; StorageT ypeList: The list of storage types; T ypesF orT imeConstraints: A set of storage types that only meet the hard execution time constraint; T ypesF orM onetaryConstraints: A set of storage types that only meet the hard monetary budget constraint; P lan: a data placement plan. Output: P lan * : The near-optimal data placement plan of each data d; F easibility: If there is a data placement plan that meets the two constraints 1: P lan * \u2190 P lan[t] 2: if T ypesF orT imeConstraints = \u2205 or T ypesF orM onetaryConstraints = \u2205 then 3: F easibility = F alse 4: else 5: j 1 \u2190 getOptimalTypeForTimeConstraint(P lan, d, Jobs, T ypesF orT imeConstraints) for j \u2208 [1, N] do 9: possibleArea \u2190 possibleArea \u2229 getArea(P lan, d, j 1 , j 2 , Jobs) 10: end for 11: if possibleArea = \u2205 then 12: F easibility = F alse 13: else 14: p \u2190 getOptimalPart(d, plan, Jobs, possibleArea) 15: For j \u2208 [1, N ] and p i,j \u2208 P lan * , set 16: ",
            "cite_spans": [
                {
                    "start": 580,
                    "end": 582,
                    "text": "3:",
                    "ref_id": null
                },
                {
                    "start": 613,
                    "end": 615,
                    "text": "5:",
                    "ref_id": null
                },
                {
                    "start": 716,
                    "end": 718,
                    "text": "9:",
                    "ref_id": null
                },
                {
                    "start": 785,
                    "end": 788,
                    "text": "10:",
                    "ref_id": null
                },
                {
                    "start": 797,
                    "end": 800,
                    "text": "11:",
                    "ref_id": null
                },
                {
                    "start": 826,
                    "end": 829,
                    "text": "12:",
                    "ref_id": null
                },
                {
                    "start": 852,
                    "end": 855,
                    "text": "13:",
                    "ref_id": null
                },
                {
                    "start": 861,
                    "end": 864,
                    "text": "14:",
                    "ref_id": null
                },
                {
                    "start": 913,
                    "end": 916,
                    "text": "15:",
                    "ref_id": null
                },
                {
                    "start": 959,
                    "end": 962,
                    "text": "16:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Near-Optimal Data Placement Algorithm"
        },
        {
            "text": "end if 18: end if each job and the intersection of the area for all the related jobs (Lines 7 -10). Given a related job job k of a data set and two data storage types (j 1 , j 2 ), we can calculate the possible area based on Formulas (1) -(13), (14) and (15) , and the calculated area is: max{0, a} \u2264 p i,j1 \u2264 min{b, 1} when c > 0, or max{a, b} \u2264 p i,j1 \u2264 1 when c < 0, with:",
            "cite_spans": [
                {
                    "start": 245,
                    "end": 249,
                    "text": "(14)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 254,
                    "end": 258,
                    "text": "(15)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Near-Optimal Data Placement Algorithm"
        },
        {
            "text": "where AIT represents the average initialization time, ET represents the execution time, which can be calculated based on Formula 7, SP represents the storage price, RP represents the read price. Finally, if the final possible area is an empty set, we consider that the two constraints cannot be met (Lines 11 and 12 ). If not, we calculate the optimal data partitioning by choosing a boundary of the area that corresponds to a smaller total cost and update the data placement plan (Lines 14 and 16).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 299,
                    "end": 315,
                    "text": "(Lines 11 and 12",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Near-Optimal Data Placement Algorithm"
        },
        {
            "text": "Let us assume that we have M input data, N data storage types, and each input data is related to K jobs on average. Then, the search space for the problem we address is O(N M ), which is the complexity of the bruteforce method. The complexity of ActGreedy algorithm [17] is Although the complexity of LNODP is slightly bigger than that of ActGreedy, Economic, or Performance, it can generate near-optimal data placement plans while satisfying hard constraints. LNODP can generate a near-optimal result while satisfying the hard constraints in most cases. However, there are two cases where LNODP cannot generate a data placement plan to satisfy hard constraints for a job. First, when there is no data storage type to store all the input data of a job while satisfying both the hard time deadline and the hard monetary budget. Second, when there is no combination of two storage types that can satisfy both the hard time deadline and the hard monetary budget. In these two cases, the user should reset the hard constraints of the job in order to use LNODP to generate data placement plans.",
            "cite_spans": [
                {
                    "start": 266,
                    "end": 270,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Algorithm Analysis"
        },
        {
            "text": "In order to analyze the worst case guarantee of the LNODP algorithm, we focus on the case when the data is scheduled according to the near-optimal data plan as explained in Line 9 of Algorithm 1. The case explained in Line 11 of Algorithm 1 is ignored as the data is not scheduled in this case. The problem addressed in Algorithm 2 is a scheduling problem when there is an optimal solution according to Algorithms 3 and 4. When the data can be scheduled without being partitioned, the solution is equal to the solution generated by a greedy algorithm. As the cost function of the combination problem is monotone, Algorithm 4 can generate an optimal combination of the chosen storage types by Algorithm 3. Thus, when the data needs to be partitioned while scheduling, the solution is also equal to the solution generated by a greedy algorithm. As the scheduling problem while minimizing a cost function is a typical submodular problem as explained in [47] , the worst case guarantee of the LNODP algorithm becomes the worst case guarantee of a greedy algorithm for a submodular, which is e\u22121 e * f * , where e is the base of the natural logarithm and f * represents the optimal solution [48] . ",
            "cite_spans": [
                {
                    "start": 950,
                    "end": 954,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 1186,
                    "end": 1190,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                }
            ],
            "ref_spans": [],
            "section": "Algorithm Analysis"
        },
        {
            "text": "In this section, we first present the simulation to compare the execution time of our proposed Lyapunov-based Near-Optimal Data Placement (LNODP) algorithm and the bruteforce method. We consider four storage types, i.e., Standard, Low frequency, Cold, and Archive, in our proposed algorithm. These four storage types are provided by the storage service on the Baidu cloud. Then, we compare the total cost of four storage methods: LNODP, brute-force, Performance [20] , and Economic [21] . The brute-force method is to search the minimum cost in the entire searching space, which means that the result of brute-force is the optimal solution. The Performance method [20] uses the storage type that corresponds to the highest data transfer speed. Economic [21] uses the storage type that corresponds to the smallest price to store data. In addition, we compare our algorithm with a simple adapted greedy algorithm, i.e., ActGreedy [17] , to show that our algorithm can address multiple hard constraints while ActGreedy only reduces the total cost without considering the hard constraints. Then, we present the comparison of the total cost among the four storage methods using a widely used data processing benchmark, i.e., Wordcount on Hadoop [49] , and a reallife data processing program for the correlation analysis of COVID-19 [11] (COVID-19-Correlation), which is selected from recent work related to COVID-19 [11] , [50] , [51] . In the experimentation, we consider five execution frequencies (daily, semimonthly, monthly, quarterly, and yearly) for Wordcount and COVID-19-Correlation.",
            "cite_spans": [
                {
                    "start": 462,
                    "end": 466,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 482,
                    "end": 486,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 664,
                    "end": 668,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 753,
                    "end": 757,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 928,
                    "end": 932,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1411,
                    "end": 1415,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1418,
                    "end": 1422,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "EXPERIMENTATION"
        },
        {
            "text": "In this section, we compare our proposed algorithm with the brute-force method in terms of the execution time and the total cost. We take 15 data sets with the average size being 5.5 GB as the input data of jobs. We execute fifteen jobs to process the input data. Each job is associated with different data sets, including Wordcount, Grep, etc. Each job is with different frequencies and different settings such as DT , w t . The data sets include DBLP XML files [52] and some data sets from Baidu. The DBLP XML file contains the metadata, e.g., the name of authors, publishers, of computer-based English articles. The comparison experiment results are shown in Fig. 5 . In order to generate a data placement plan for six data sets with fifteen jobs, the execution time of the greedy algorithm is shorter than 0.0001s, while that of LNODP is 0.08s. When the number of the data sets augments, the execution time of the brute-force method increases exponentially. When the number of data sets becomes 15, the execution time of the brute-force method is 67839s, while that of LNODP remains within 0.0001s. Fig. 6 presents the comparison among four methods: LNODP, brute-force, Performance, and Economic. LNODP corresponds to the same total cost as that of the brute-force method, which is up to 8.2% and 30.6% smaller than that of Performance and Economic, respectively. The simulation experiment shows that the result of our proposed algorithm is as same as the brute-force method, which means the result of our proposed algorithm is the optimal solution in these situations.",
            "cite_spans": [
                {
                    "start": 463,
                    "end": 467,
                    "text": "[52]",
                    "ref_id": "BIBREF51"
                }
            ],
            "ref_spans": [
                {
                    "start": 662,
                    "end": 668,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 1103,
                    "end": 1109,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Simulation"
        },
        {
            "text": "Hadoop [53] is a framework for parallel big data processing on a cluster of commodity servers. Hadoop contains two components, i.e., HDFS [54] and MapReduce. HDFS is a distributed file system with a master-slave architecture.",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 11,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 138,
                    "end": 142,
                    "text": "[54]",
                    "ref_id": "BIBREF53"
                }
            ],
            "ref_spans": [],
            "section": "Wordcount"
        },
        {
            "text": "MapReduce is a programming model and implementation for parallel data processing in a distributed environment. MapReduce contains two phases, i.e., Map and Reduce. In the Map phase, the input data is processed, and key-value pairs are generated. In the reduce phase, the key-value pairs of the same Key are processed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Wordcount"
        },
        {
            "text": "Wordcount is a widely used benchmark, which counts the frequency of each word in the input files. Wordcount contains two steps, i.e., Map and Reduce. In the Map step, < word, 1 > is generated for each work in the input data. Then, the number of < word, 1 > is counted for each work in the Reduce step. Finally, the frequency of each word is calculated and stored in HDFS.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Wordcount"
        },
        {
            "text": "We deploy Hadoop on three computing nodes based on the platform. Each node is a VM with one CPU core and 4 GB RAM. We use DBLP 2019 XML files of 6.04 GB as the input data. We set DT as 1200 seconds and DM as 1 dollar. First, we set the hard time deadline as 2000 seconds and 10 dollars. Fig. 7 shows that our proposed algorithm, i.e., LNODP, significantly outperforms the baseline approach. When the frequency is daily, the total cost corresponding to different approaches is shown in Fig. 7(a) . Compared with Economic, LNODP can reduce the total cost by 42.2%, 25.2%, and 8.7% when \u03c9 t is 0, 0.5, and 0.9 respectively. When the frequency is quarterly, LNODP can reduce the total cost by 44.3% compared with Performance when the \u03c9 t is 0 as Fig. 7(b) shows. LNODP can generate an optimal storage plan, which significantly outperforms (the total cost is 31.7% and 7.2% smaller) Economic when \u03c9 t is 0.5 and 0.9, respectively. Fig. 7 (c) presents the efficiency of our proposed algorithm when the frequency is yearly. Compared with Performance, our algorithm can reduce the total cost by 69.8%, 60.4% and 27.4% when \u03c9 t is 0, 0.5 and 0.9, respectively. Fig. 7 presents that our algorithm can reduce the total cost by up to 69.8% compared with Performance and up to 42.2% compared with Economic. As the execution frequency of the job decreases, the advantage of our algorithm becomes significant. The comparison of Fig. 7 (a), 7(b) and 7(c) indicates that as the importance of time cost becomes bigger, i.e., \u03c9 t , increases, the advantage of our proposed algorithm becomes significant as well. This experiment also shows that the result of our algorithm can generate the optimal solution as the brute-force method. Table 3 presents the execution with a strict hard execution time constraint and a hard monetary budget constraint, i.e., 1420 seconds and 6.5 dollars. The existing methods, e.g., ActGreedy, Performance, Economic, cannot meet both the two constraints, while LNODP can place the data with data partitioning while satisfying the two hard constraints with small total cost. In addition, we find that the weight of objectives only impacts the total cost, which has no impact on the satisfaction of the constraints.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 287,
                    "end": 293,
                    "text": "Fig. 7",
                    "ref_id": null
                },
                {
                    "start": 485,
                    "end": 494,
                    "text": "Fig. 7(a)",
                    "ref_id": null
                },
                {
                    "start": 742,
                    "end": 751,
                    "text": "Fig. 7(b)",
                    "ref_id": null
                },
                {
                    "start": 926,
                    "end": 932,
                    "text": "Fig. 7",
                    "ref_id": null
                },
                {
                    "start": 1152,
                    "end": 1158,
                    "text": "Fig. 7",
                    "ref_id": null
                },
                {
                    "start": 1413,
                    "end": 1419,
                    "text": "Fig. 7",
                    "ref_id": null
                },
                {
                    "start": 1714,
                    "end": 1721,
                    "text": "Table 3",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Wordcount"
        },
        {
            "text": "In addition, the average execution time of LNODP, Performance, Economic, and Brute-force are 2.79 * 10 \u22124 , 4.26 * 10 \u22125 , 4.14 * 10 \u22125 , 2.98 * 10 \u22124 , respectively. While LN-ODP corresponds can generate good data placement plans, the execution time remains quite acceptable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Wordcount"
        },
        {
            "text": "Since the coronavirus disease (COVID-19) has become a global emergency, we reproduced the data processing program for the correlation among COVID-19-related search activities, human mobility, and the number of confirmed cases in Mainland China presented in [11] . The data involved in [11] includes the number of confirmed cases in each city (dataset c ), the volume of COVID-19-related search activities in each city (dataset s ), inflows and outflows for each city (dataset m ) and the population in each city (dataset p ). dataset m is the inflow and outflow data of inter-city population with the transitions of the inter-city mobility categorized by the origin and destination pairs. dataset s includes the keywords and phrases related to the epidemic from January to March. The total amount of these data sets is 1.134 GB.",
            "cite_spans": [
                {
                    "start": 257,
                    "end": 261,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 285,
                    "end": 289,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "COVID-19"
        },
        {
            "text": "The data processing for the COVID-19-related correlation analysis consists of the following three steps. First, the data is selected using a filter operation. Then, a join operator is used to generate the features for each city, i.e., the number of confirmed cases, the inflows, the outflows, the search volumes, the population. Afterward, the correlation between any two features is calculated for each city. The experimental results are shown in Fig. 8 . We set DT as 600 seconds and DM as 0.5 dollars.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 448,
                    "end": 454,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF10"
                }
            ],
            "section": "COVID-19"
        },
        {
            "text": "First, we set the hard execution time constraint as 800 seconds and the hard monetary budget constraint as 2 dollars. Fig. 8 shows that our proposed algorithm, i.e., LNODP, significantly outperforms Performance (up to 65.1%) when the size of the input data of the job is smaller than that of Wordcount. When the frequency is daily, the total costs of different approaches are shown in Fig. 8(a) . When \u03c9 t is 0 and \u03c9 m is 1, our algorithm can reduce the total cost by 30.5% compared with Economic. When \u03c9 t increases to 0.5, our algorithm can reduce the total cost by 3.6% compared with Economic. When the importance of time, i.e., \u03c9 t , increases to 0.7, our algorithm can outperform Economic, and the total cost can be reduced by 2.5%. Fig. 8 (b) presents the total cost of different approaches when the frequency is quarterly. When the user only considers the importance of money, our algorithm can reduce the total cost by 35.7% compared with Performance. With the increase of \u03c9 t , our algorithm can reduce the total cost by 3.5% and 1.9% compared with Economic when \u03c9 t is 0.5 and 0.7 respectively. When the frequency is yearly, the execution results are shown in Fig. 8(c) . The most significant result is that our algorithm can reduce the total cost by 65.1% compared with Performance when \u03c9 t is 0 and \u03c9 m is 1. When \u03c9 t is 0.5 and 0.7, our algorithm can reduce the total cost by 14.9% and 6.5% compared with Performance and Economic, respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 118,
                    "end": 124,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF10"
                },
                {
                    "start": 385,
                    "end": 394,
                    "text": "Fig. 8(a)",
                    "ref_id": "FIGREF10"
                },
                {
                    "start": 738,
                    "end": 744,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF10"
                },
                {
                    "start": 1170,
                    "end": 1179,
                    "text": "Fig. 8(c)",
                    "ref_id": "FIGREF10"
                }
            ],
            "section": "COVID-19"
        },
        {
            "text": "From Fig. 8 , we find that our proposed algorithm, i.e., LNODP, significantly outperforms the Performance method (up to 65.1%) and the Economic method (up to 30.5%), when the frequency of the job execution is high and when the size of the input data of the job is big. Table 4 presents the execution with a strict hard execution time constraint and a hard monetary budget constraint, i.e., 722 seconds and 1.9 dollars. The existing methods, e.g., ActGreedy, Performance, Economic, cannot meet both the two constraints. However, LNODP can place the data with data partitioning while satisfying the two hard constraints with a small total cost. We find that the weight of objectives only impacts the total cost while having no impact on the satisfaction of the constraints.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 5,
                    "end": 11,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF10"
                },
                {
                    "start": 269,
                    "end": 276,
                    "text": "Table 4",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "COVID-19"
        },
        {
            "text": "In addition, the average execution time of LNODP, Performance, Economic, and Brute-force are 2.01 * 10 \u22124 , 2.01 * 10 \u22125 , 2.01 * 10 \u22125 , 2.04 * 10 \u22124 , respectively. The execution time of LNODP is quite acceptable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19"
        },
        {
            "text": "When organizations outsource their data onto the cloud, it is critical to choose a proper data placement strategy to reduce its expected total cost. In this paper, we proposed a solution to enable data processing on the cloud with the data from different organizations. The approach consists of three parts: a data federation platform with secure data sharing and secure data computing, a multi-objective cost model, and a Lyapunov-based near-optimal data placement algorithm. The cost model consists of monetary cost and execution time. The Lyapunov-based near-optimal algorithm delivers a solution to the problem. We carried out extensive experiments to validate our proposed approach. The experimental results indicate that our proposed algorithm outperforms the baseline approaches up to 69.8% and that our algorithm can generate the same optimal solution as the brute-force method within a short execution time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CONCLUSION"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Data sharing in group work",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Greif",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "K"
                    ],
                    "last": "Sarin",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "ACM Transactions on Office Information Systems",
            "volume": "5",
            "issn": "2",
            "pages": "187--211",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Efficient scheduling of scientific workflows using hot metadata in a multisite cloud",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Pineda-Morales",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pacitti",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Costan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Valduriez",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Antoniu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mattoso",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "volume": "31",
            "issn": "10",
            "pages": "1940--1953",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "The EU General Data Protection Regulation (GDPR): A Practical Guide",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Voigt",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Von",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bussche",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Target and (astro-)wise technologies data federations and its applications",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Valentijn",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Begeman",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Belikov",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Boxhoorn",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Astroinformatics, ser. Proceedings of the International Astronomical Union",
            "volume": "12",
            "issn": "S325",
            "pages": "333--340",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Parallelization of Scientific Workflows in the Cloud",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pacitti",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Valduriez",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mattoso",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "INRIA",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Above the clouds: A berkeley view of cloud computing",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fox",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Griffith",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Joseph",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Katz",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Konwinski",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Patterson",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rabkin",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Stoica",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A brief history of cloud application architectures",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kratzke",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Applied Sciences",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "On data center demand response: A cloud federation approach",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Moghaddam",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Manshaei",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Saad",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Goudarzi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Access",
            "volume": "7",
            "issn": "",
            "pages": "101--829",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Anonymous hierarchical identity-based encryption (without random oracles)",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Boyen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Waters",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Annual International Cryptology Conference",
            "volume": "",
            "issn": "",
            "pages": "290--307",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Customizable isolation in transactional workflow",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Guabtni",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Charoy",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Godart",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Interoperability of Enterprise Software and Applications",
            "volume": "",
            "issn": "",
            "pages": "197--202",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Understanding the collective responses of populations to the covid-19 pandemic in mainland china",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "An",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dou",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "A stochastic control approach to maximize profit on service provisioning for mobile cloudlet platforms",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
            "volume": "48",
            "issn": "4",
            "pages": "522--534",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "An integrated top-down and bottomup task allocation approach in social sensing based edge computing systems",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "Y"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE INFOCOM Conference on Computer Communications",
            "volume": "",
            "issn": "",
            "pages": "766--774",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Dynamic task assignment in crowdsensing with location awareness and location diversity",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Gan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE INFOCOM Conference on Computer Communications",
            "volume": "",
            "issn": "",
            "pages": "2420--2428",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Distributed data placement to minimize communication costs via graph partitioning",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Golab",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hadjieleftheriou",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Karloff",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Saha",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 26th International Conference on Scientific and Statistical Database Management",
            "volume": "",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "An optimized data storage strategy by computational performance and monetary cost with data importance in the cloud",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "21st IEEE International Conference on Computer Supported Cooperative Work in Design",
            "volume": "",
            "issn": "",
            "pages": "433--438",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Multi-objective scheduling of scientific workflows in multisite clouds",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pacitti",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Valduriez",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Oliveira",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mattoso",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Future Generation Computer Systems",
            "volume": "63",
            "issn": "",
            "pages": "76--95",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Dynamic load balancing algorithm for balancing the workload among virtual machine in cloud computing",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "C"
                    ],
                    "last": "Sharma",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Procedia computer science",
            "volume": "115",
            "issn": "",
            "pages": "322--329",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "On minimizing the resource consumption of cloud applications using process migrations",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Tziritas",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "U"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Loukopoulos",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lalis",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Journal of Parallel and Distributed Computing",
            "volume": "73",
            "issn": "12",
            "pages": "1690--1704",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Cost-efficient storage for on-demand video streaming on cloud",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Darwich",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ismail",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darwich",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Bayoumi",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "CoRR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Feeding the pelican: Using archival hard drives for cold storage racks",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Black",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Donnelly",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Harper",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ogus",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "I T"
                    ],
                    "last": "Rowstron",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "USENIX Workshop on Hot Topics in Storage and File Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Multi-objective job placement in clusters",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Blagodurov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fedorova",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Vinnik",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dwyer",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hermenier",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. for High Performance Computing, Networking, Storage and Analysis",
            "volume": "",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Survey of multi-objective optimization methods for engineering",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "T"
                    ],
                    "last": "Marler",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Arora",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Structural and multidisciplinary optimization",
            "volume": "26",
            "issn": "",
            "pages": "369--395",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Optimality and non-scalar-valued performance criteria",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zadeh",
                    "suffix": ""
                }
            ],
            "year": 1963,
            "venue": "IEEE transactions on Automatic Control",
            "volume": "8",
            "issn": "1",
            "pages": "59--60",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Cloud computing and data security threats taxonomy: A review",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Farsi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ali",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "A"
                    ],
                    "last": "Wagan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kharabsheh",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Intelligent and Fuzzy Systems",
            "volume": "38",
            "issn": "3",
            "pages": "2517--2527",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Security in the cloud",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Anthes",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Communications of the ACM",
            "volume": "53",
            "issn": "11",
            "pages": "16--18",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Securegbm: Secure multi-party gradient boosting",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Huan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 IEEE International Conference on Big Data (Big Data",
            "volume": "",
            "issn": "",
            "pages": "1312--1321",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "A secured cost-effective multi-cloud storage in cloud computing",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Kandah",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Computer Communications Workshops (INFOCOM WKSHPS)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Mp2sda: Multiparty parallelized sparse discriminant learning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bian",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Huan",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD)",
            "volume": "14",
            "issn": "3",
            "pages": "1--22",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Multiparty sparse discriminant learning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bian",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE International Conference on Data Mining (ICDM)",
            "volume": "",
            "issn": "",
            "pages": "745--750",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Mcmahan",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Moore",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ramage",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hampson",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "A"
                    ],
                    "last": "Arcas",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), ser. Proceedings of Machine Learning Research",
            "volume": "54",
            "issn": "",
            "pages": "1273--1282",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Managing security of virtual machine images in a cloud environment",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Ammons",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Bala",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ning",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "ACM Workshop on Cloud Computing Security",
            "volume": "",
            "issn": "",
            "pages": "91--96",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Adaptive caching for data-intensive scientific workflows in the cloud",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Heidsieck",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Oliveira",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pacitti",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Pradal",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Tardieu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Valduriez",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. Conf. on Database and Expert Systems Applications (DEXA), ser",
            "volume": "11707",
            "issn": "",
            "pages": "452--466",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "The rijndael algorithm",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "IEEE Potentials",
            "volume": "23",
            "issn": "2",
            "pages": "36--38",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "The anatomy of mapreduce jobs, scheduling, and performance challenges",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bardhan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "A"
                    ],
                    "last": "Menasc\u00e9",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "39th International Computer Measurement Group Conference",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "A comparative review of job scheduling for mapreduce",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yoo",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Sim",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE International Conference on Cloud Computing and Intelligence Systems",
            "volume": "",
            "issn": "",
            "pages": "353--358",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "The batch sheduling model for dynamic multiitem, multilevel production in an assembly job-shop with parrallel machines",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Surjandari",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rachman",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dhini",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Journal of Technology",
            "volume": "1",
            "issn": "",
            "pages": "84--96",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Reevaluating amdahl's law in the multicore era",
            "authors": [
                {
                    "first": "X.-H",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Journal of Parallel and Distributed Computing",
            "volume": "70",
            "issn": "2",
            "pages": "183--188",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Evaluating grasp-based cloud dimensioning for comparative genomics: a practical approach",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Coutinho",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Drummond",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Frota",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Oliveira",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ocana",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Int. Conf. on Cluster Computing",
            "volume": "",
            "issn": "",
            "pages": "371--379",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Lyapunov functions: An optimization theory perspective",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Polyak",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Shcherbakov",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IFAC-PapersOnLine",
            "volume": "50",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Parallel low discrepancy parameter sweep for public health policy",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chunduri",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ghaffari",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "S"
                    ],
                    "last": "Lahijani",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Srinivasan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Namilae",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int. Symposium on Cluster, Cloud and Grid Computing (CCGRID)",
            "volume": "",
            "issn": "",
            "pages": "291--300",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Datum: Managing data purchasing and data placement in a geo-distributed data market",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "London",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ziani",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wierman",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE/ACM Transactions on Networking",
            "volume": "26",
            "issn": "2",
            "pages": "893--905",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Controllable qos for imprecise computation tasks on dvfs multicores with time and energy constraints",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kritikakou",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Sentieys",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
            "volume": "8",
            "issn": "4",
            "pages": "708--721",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Non-convex mixed-integer nonlinear programming: A survey",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Burer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Letchford",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Surveys in Operations Research and Management Science",
            "volume": "17",
            "issn": "",
            "pages": "97--106",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Computers and intractability: a guide to the theory of np-completeness (michael r. garey and david s. johnson)",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hartmanis",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "Siam Review",
            "volume": "24",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "A fair task assignment strategy for minimizing cost in mobile crowdsensing",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Luan",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE International Conference on Parallel and Distributed Systems (ICPADS)",
            "volume": "",
            "issn": "",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "On submodular search and machine scheduling",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fokkink",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lidbetter",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "A"
                    ],
                    "last": "V\u00e9gh",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Mathematics of Operations Research",
            "volume": "44",
            "issn": "4",
            "pages": "1431--1449",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "An analysis of approximations for maximizing submodular set functions-I",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "L"
                    ],
                    "last": "Nemhauser",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "A"
                    ],
                    "last": "Wolsey",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Fisher",
                    "suffix": ""
                }
            ],
            "year": 1978,
            "venue": "Mathematical programming",
            "volume": "14",
            "issn": "1",
            "pages": "265--294",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Hadoop: The Definitive Guide",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "White",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Analysis of collective response reveals that covid-19-related activities start from the end of 2019 in mainland china",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dou",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "An investigation of containment measures against the covid-19 pandemic in mainland china",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "An",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dou",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2007.08254"
                ]
            }
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Dblp: computer science bibliography",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Apache hadoop",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "The hadoop distributed file system",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shvachko",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kuang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Radia",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chansler",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "IEEE Symposium on Mass Storage Systems and Technologies (MSST)",
            "volume": "",
            "issn": "",
            "pages": "1--10",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The functionality architecture of the FedCube platform.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Infrastructure architecture of the FedCube platform.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Job Execution Workflow.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "System model for data placement.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Algorithm 1 Lyapunov-based Near-Optimal Data Placement Input: D: A set of data sets; T : Maximum number of iterations; T : Maximum number of iterations for generating data placement plans; P lan[t]: data placement plan in Time slot t. Output: P lan[t + 1]: data placement plan in Time slot t + 1. 1: D \u2190 sort(D) 2: for t \u2208 T do 3:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "O(M * K * N ). Then, the complexity of LNODP is O(T * M * K * N ) (when there is no need to execute Algorithm 4) or O(T * M * K 2 * N ) (when Algorithm 4 is executed for each job), which is much smaller than O(N M ) when N M \u22121 > M * K 2 * T (this is a general case). Please note that we do not reduce the complexity of the problem but reduce the complexity of the solution. The complexity of Economic and Performance (see details in Section 6) is O(M * M ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Execution time of Greedy and Brute-force",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Comparison among four methods",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "shows the result of the execution time of different methods.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Total cost of COVID-19-Correlation.",
            "latex": null,
            "type": "figure"
        },
        "TABREF2": {
            "text": "The monetary cost to store data on the cloud with different storage types, i.e., Standard, Low frequency, Cold and Achieve.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Results for hard execution time constraint and hard monetary budget constraint. Frequency: yearly. Hard time deadline: 1420; hard monetary budget: 6.5. The time unit is second and the monetary unit is yuan.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Results for hard execution time constraint and hard monetary budget constraint. Frequency: yearly. Hard time deadline: 722; hard monetary budget: 1.9. The time unit is second and the monetary unit is yuan.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
{
    "paper_id": "c88a95c759e550ae1fda0b4c4ed96b66b3a18909",
    "metadata": {
        "title": "Markov decision processes with observation costs",
        "authors": [
            {
                "first": "Christoph",
                "middle": [],
                "last": "Reisinger",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Jonathan",
                "middle": [],
                "last": "Tam",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "We present a framework for a controlled Markov chain where the state of the chain is only given at chosen observation times and of a cost. Optimal strategies therefore involve the choice of observation times as well as the subsequent control values. We show that the corresponding value function satisfies a dynamic programming principle, which leads to a system of quasivariational inequalities (QVIs). Next, we give an extension where the model parameters are not known a priori but are inferred from the costly observations by Bayesian updates. We then prove a comparison principle for a larger class of QVIs, which implies uniqueness of solutions to our proposed problem. We utilise penalty methods to obtain arbitrarily accurate solutions. Finally, we perform numerical experiments on three applications which illustrate our framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In this article, we consider a controlled Markov chain problem, where a cost is incurred to reveal the state of the chain at a given moment in time. We assume that any changes to the control can only occur at these observation times. Hence, in addition to searching for the optimal action, the user also seeks for the optimal intervals between successive observations of the state.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "A continuous stream of information is often an assumption taken for granted in both fully observable or partially observable control problems. However, in many practical applications, due to technical and labour difficulties, it can be expensive or impractical to obtain such measurements. Examples include the virological state of patients [11, 20, 21] , environmental measurements on river sediments [25] and biological growth dynamics of organisms [27] . In consumer spending, it is desirable to purchase a good product with a low searching cost [14] . Such searches are sequential by nature, so that the assumption of continuous observations does not apply.",
            "cite_spans": [
                {
                    "start": 341,
                    "end": 345,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 346,
                    "end": 349,
                    "text": "20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 350,
                    "end": 353,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 402,
                    "end": 406,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 451,
                    "end": 455,
                    "text": "[27]",
                    "ref_id": null
                },
                {
                    "start": 549,
                    "end": 553,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To the best of our knowledge, the earliest works on observation controls are [3, 4] . A Brownian motion is to be stopped upon the exit of a given set, but each inspection of the Brownian motion comes with a cost. It was shown that the problem reduces to a free boundary problem and the corresponding value function satisfies a quasi-variational inequality (QVI), with further analysis demonstrating the well-posedness of the problem. More recently, Dyrssen and Ekstr\u00f6m incorporated observation costs in hypothesis testing for the drift of a diffusion, and characterises the value function as the unique fixed point of an associated operator [12] . Other works concerning observation controls are motivated by specific applications: Winkelmann et al. explored the optimal diagnosis and treatment scheduling for HIV-1 patients, based on the trade off between treatment cost against productivity loss across different countries [11, 20, 21] ; Yoshioka et al. focused on a variety of environmental management problems, including the modelling of replenishing sediment storage in rivers, monitoring algae population dynamics and biological growth of fishery resources [24, 25, 26, 27] . The phenomenon of sporadically observing the state process is also modelled in mathematical finance under the term rational inattention, where portfolio adjustments are assumed to occur infrequently with a utility cost proportional to the users' assets [1, 2, 10] .",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 80,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 81,
                    "end": 83,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 641,
                    "end": 645,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 925,
                    "end": 929,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 930,
                    "end": 933,
                    "text": "20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 934,
                    "end": 937,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "text": "[24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1168,
                    "end": 1171,
                    "text": "25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1172,
                    "end": 1175,
                    "text": "26,",
                    "ref_id": null
                },
                {
                    "start": 1176,
                    "end": 1179,
                    "text": "27]",
                    "ref_id": null
                },
                {
                    "start": 1435,
                    "end": 1438,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1439,
                    "end": 1441,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1442,
                    "end": 1445,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our goal in this article is to systematically develop a framework for the observation control problems described above, utilising first principles to establish dynamic programming for the value function, and exploring suitable numerical approximations to the solutions of the resulting systems of QVIs. To avoid intuition being obscured by technical complications, we shall focus on discrete-time Markov chains, which often serve as valid approximations for diffusions in continuous time [16] .",
            "cite_spans": [
                {
                    "start": 488,
                    "end": 492,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In our model, we consider an underlying controlled Markov chain X = {Xn}n on a probability space (\u2126, F, P), with a corresponding controlled observation filtration the estimation and control problem can be separated. This is a common procedure in solving partially observable control models [8, 17] . In general, the conditional distribution is characterised by the Zakai or Kushner-Stratonovich equations using standard filtering techniques [5, 13] . We will show in Section 2 that in our case, each realisation of \u00b5n only depends on the last observation time k, state of the chain x and control of the chain i. This gives rise to a higher dimensional value function than in the standard case due to the presence of k and i. More specifically, in the finite horizon case, the reward functional that we aim to optimise depends on the tuple (m, (k, x, i), \u03b1), where m represents the current time, and \u03b1 is a double sequence representing the observation times and control of the chain. Some analysis of the value function as set up above was seen in [3, 4] , but their applications were limited to optimal stopping problems concerning Brownian motion. Other existing works on the observation control model assume stationarity in the problem. This leads to a reward functional that only depends on the triplet (x, i, \u03b1) [12, 21, 24, 25, 26, 27] . Whilst this formulation gives an overall lower dimensional problem, the setup assumes that the user is in possession of accurate and updated information of the state process at initialisation. This excludes solving for scenarios with unintended large gaps between observations, which can lead to qualitatively different optimal actions. For example, lockdowns imposed due to the COVID-19 pandemic have led to delayed consultations for patients, who are more likely to appear in a worsened state upon the time of diagnosis. We adopt the former, more general formulation to include such incidences into our model. The qualitative behaviour of the value function under these scenarios is demonstrated in our numerical experiments in Section 5.",
            "cite_spans": [
                {
                    "start": 290,
                    "end": 293,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 294,
                    "end": 297,
                    "text": "17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 441,
                    "end": 444,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 445,
                    "end": 448,
                    "text": "13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1047,
                    "end": 1050,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1051,
                    "end": 1053,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1316,
                    "end": 1320,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1321,
                    "end": 1324,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1325,
                    "end": 1328,
                    "text": "24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1329,
                    "end": 1332,
                    "text": "25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1333,
                    "end": 1336,
                    "text": "26,",
                    "ref_id": null
                },
                {
                    "start": 1337,
                    "end": 1340,
                    "text": "27]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Having set up the initial framework and established dynamic programming in Section 2, we obtain a system of discrete quasi-variational inequalities (QVIs). In Section 4, we investigate the well-posedness of such systems. We can rewrite the QVIs in the more abstract form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "where M : R N\u00d7L\u00d7d \u2192 R N\u00d7L : (Mu) n l = ((An\u016b q ) l \u2212 c), q \u2208 {1, . . . , N } is some fixed index, c is a constant, each An \u2208 R L\u00d7L is a strictly substochastic matrix, and Fi : R N\u00d7L \u2192 R N\u00d7L satisfies a monotonicity property specified in Section 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The system of QVIs (1.3) share a similar structure to that of a monotone interconnected obstacle system, which typically feature in optimal switching problems [18] . The well-posedness of interconnected obstacle systems is treated in [19] , which establishes existence via penalisation [22, 23] . The use of penalty methods over traditional policy iteration is due to the fact that invertibility of the matrices is not guaranteed under the presence of the obstacle operator. The extra matrix An appearing in the obstacle operator M represents the (discounted) transition matrix over the unobserved period of time. We show a modified proof of the comparison principle appearing in [19] , taking into account the effect of the matrix An on the coupling in (1.3). The same modifications apply for the penalty approximations, which gives us the well-posedness of the observation control problem. We also note that although the system of Bellman-type equations arising from the lower dimensional formulation can be solved via a fixed-point method by bounding the total number of observations [12] , it is not applicable to our model due to the presence of the extra time lag variable k in the value function.",
            "cite_spans": [
                {
                    "start": 159,
                    "end": 163,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 234,
                    "end": 238,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 286,
                    "end": 290,
                    "text": "[22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 291,
                    "end": 294,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 680,
                    "end": 684,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1087,
                    "end": 1091,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Lastly, as a further novel extension, we incorporate parameter uncertainty into the model dynamics and consider sequential updates to the control in Section 3. In the literature, the adoption of Bayesian principles with observation control has been explored in the context of reinforcement learning [6] : an algorithm for solving the optimal policy and estimator in parallel was proposed, but an equivalent Bellman-type equation was not established. Along similar ideas on the tradeoff between cost and information, Cohen et al. consider a cost constraint that arises from the tracking error relative to the optimal Bayesian estimator, and relates the optimal band of inaction to a two-sided hypothesis test [9] .",
            "cite_spans": [
                {
                    "start": 299,
                    "end": 302,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 708,
                    "end": 711,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Here, we consider the case where the transition probability of the Markov chain X depends on an unknown parameter \u03b8. In general, the standard approach in Bayesian control problems is to reformulate the problem into a partially observable optimal control problem [15] , and then proceed to the separation of estimation and control as before. We apply the same principles to the observation control problem and derive a dynamic programming equation involving the 'prior' and 'posterior' distributions of \u03b8. In particular, the reward functional is now augmented to be dependent on the tuple (m, (k, x, i), \u03c0, \u03b1), where \u03c0 is a measure over the parameter space. Whilst the resulting equations are often infinite dimensional in nature, this can be reduced back to the finite dimensional case if one considers conjugate distributions for \u03c0. We demonstrate the solution of a model problem on a random walk involving beta conjugate priors in Section 5 and investigate its corresponding numerical properties.",
            "cite_spans": [
                {
                    "start": 262,
                    "end": 266,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The main contributions of our article are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We expand on the probabilistic approach to the observation control problem, and extend the results first seen in [3, 4] . We subsequently derive a system of discrete QVIs that the value function satisfies with dynamic programming.",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 117,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 118,
                    "end": 120,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-Following the approach of [19] , we prove a comparison principle for the system of discrete QVIs and establish an approximation to the solutions via penalisation and semismooth Newton methods [7] . This also provides well-posedness to the observation control problem.",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 31,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 193,
                    "end": 196,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We provide a novel Bayesian parametric setup of the observation control problem. This incorporates a distribution over the unknown parameters in the value function, which is dynamically updated after each observation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We demonstrate the numerical performance of our model with three numerical experiments, one of which is a time-discretised version of the HIV-treatment problem appearing in [21] . We show that our model extends the original one by solving for scenarios with large observation gaps, and that the optimal control coincides in scenarios that are covered by both models.",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 178,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The rest of this paper is organised as follows. Section 2 sets out the framework for the Markov chain observation control model and establishes the corresponding set of discrete QVIs. A model problem with an explicit solution is also provided to illustrate the setup. The Bayesian extension is outlined in Section 3. In Section 4 we prove a comparison principle for a class of discrete QVIs which subsumes the QVIs obtained in Section 2. The penalty method is also introduced as an approximation for the QVI. Finally we provide numerical experiments for our observation control model in Section 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we establish the framework for the observation control problem for Markov chains. We will consider in the following both the finite and infinite horizon problem. We introduce the notion of observation filtrations in the spirit of [12] , as well as the corresponding class of observation controls. We then formulate the control problem and derive the relevant equations for the value function with the use of the dynamic programming principle. We conclude the section with a model problem that yields a closed form solution.",
            "cite_spans": [
                {
                    "start": 247,
                    "end": 251,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Observation control model for Markov chains"
        },
        {
            "text": "Let X = X(\u03b1) be a controlled Markov chain on a probability space (\u2126, F, P), taking values in a state space S. We consider the discrete time case and write X = (Xn) n\u2208N . The control \u03b1 is in general an adapted process with values in a finite control set I := {1, . . . , d}, and determines the distribution of the Markov chain via its transition kernel: P = P (i) = Pi = (pxy(i))x,y\u2208S, where i \u2208 I. We will use the notation p (n) xy (i) to denote the entries of P n , i.e. the n-step transition probabilities. We also assume that the transition kernel is time-homogeneous and is known a priori.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "In the observation control model, access to the realisation of each Xn will come with a cost. Thus one would expect that observations would generally not occur at every time point, so that the information available to us is restricted. It is then natural to consider a filtration that is no finer than the natural filtration generated by X. Hence, we define the following notion of an observation filtration, which is based on that appearing in [12] .",
            "cite_spans": [
                {
                    "start": 445,
                    "end": 449,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Proposition 2.1. Let X be a Markov chain on (\u2126, F, P). Let \u03c4 = (\u03c4 k ) \u221e k=1 be a strictly increasing sequence of random times (in the sense that \u03c4 k (\u03c9) < \u03c4 k+1 (\u03c9) for every \u03c9 \u2208 \u2126). Denote also \u03c40 for the initial time of the chain. Define for all n \u2265 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Then the sequence (F X,\u03c4 n )n forms a filtration.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Proof. See appendix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "By construction, each \u03c4 k is a stopping time with respect to F X,\u03c4 . In the context of the observation control model, we would like to determine observation times based only upon information amassed from prior observations up to the current time. This motivates the following definition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Definition 2.2. We say that \u03c4 is an observation sequence for X if each \u03c4 k is F X,\u03c4 \u03c4 k\u22121measurable. In this case F X,\u03c4 is called an X-observation filtration.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "For the rest of this article, we will only consider \u03c4 which are observation sequences. Such sequences are also predictable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "is an observation sequence for X, then each \u03c4 k is a predictable stopping time with respect to F X,\u03c4 , i.e. for each k, {\u03c4 k = n} \u2208 F X,\u03c4 n\u22121 for all n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Proof. See appendix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Since the influx of information is decided exogenously, the observation sequence should form part of the control. We define the set of admissible controls as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Definition 2.4. Let \u03c4 be an observation sequence for X. For each \u03c4 k , let \u03b9 k be an I-valued, F X,\u03c4 \u03c4 k -measurable random variable. An admissible control is a double sequence of the form \u03b1 = (\u03c4n, \u03b9n)n. The set of admissible controls is denoted by A. Define also the corresponding piecewise constant process I = (In)n of the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Note that double sequence controls are often seen in optimal switching problems (see, for example, [18, Chapter 5.3] ), but here the process is in addition adapted to the observation filtration. The following proposition justifies the intuitive idea that the observation filtration indeed contains no more information than the natural filtration.",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 103,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 104,
                    "end": 116,
                    "text": "Chapter 5.3]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Proposition 2.5. Let F X be the natural filtration generated by X. If the sequence \u03c4 is predictable (with respect to F X,\u03c4 ), then F X,\u03c4 is coarser than F X , i.e., F X,\u03c4 n \u2286 F X n for every n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Proof. See appendix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "At any given time n \u2265 1, we will often consider the most recent observation that occurred before time n. We employ the following notation when considering quantities related to such observations. Definition 2.6. Let \u03c4 be an observation sequence for X. For any n \u2265 0, define the random variables\u03c4",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Similarly, define also\u03b9n = \u03b9\u03c4 n = In andXn = X\u03c4 n .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Thus one can view the tilde notation (\u03c4n,\u03b9n,Xn) as representing 'the most recent data available'. We can see that the random variable\u03c4n is F X,\u03c4 n -measurable, since for m \u2264 n,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Although\u03c4n is not a F X,\u03c4 -stopping time, so that F X,\u03c4 \u03c4n is not well-defined, we can still utilise the Markov property of the underlying chain X to obtain the relation in the following lemma. Proof. This follows from the decomposition of the conditional expectation across the countable events that generate F X,\u03c4 m and the Markov property of X.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "To formulate the observation control problem, we will have to define the conditional distribution of Xn given its past observations. Definition 2.8. Let P(S) be the set of probability measures over S. Define the P(S)-valued (controlled) process",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Then \u00b5n is the conditional distribution of Xn given its past observation history up to and including time n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "We can characterise the random measures (\u00b5n)n as follows. As S is countable, it suffices to consider the singleton sets {y} for any y \u2208 S. By Lemma 2.7, \u00b5n({y}) = P(Xn = y |\u03c4n,\u03b9n,Xn) = k<n x\u2208S i\u2208I \u00bd {\u03c4n=k,Xn=x,\u03b9n=i} P(Xn = y |\u03c4n = k,Xn = x,\u03b9n = i) = k<n x\u2208S i\u2208I \u00bd {\u03c4n=k,Xn=x,\u03b9n=i} p (n\u2212k) xy (i).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "(2.7)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "The events {\u03c4n = k,Xn = x,\u03b9n = i} partition the sample space, and \u00b5n restricted to each event is constant. Therefore, each realisation \u00b5n(\u03c9) can be identified by some k \u2208 {0, . . . , n}, x \u2208 S, and i \u2208 I. In view of this, we will use the notation \u00b5 k,x,i n to represent a realisation \u00b5n(\u03c9) in the sequel.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "By (2.4) and the predictability of \u03c4 , we also have \u00bd {\u03c4 n+1 =n+1} \u2208 F X,\u03c4 n , so that, given any admissible control \u03b1 \u2208 A, at time n, we know whether an observation should be made at time n + 1. Hence, given the initial condition \u00b5n = \u00b5 k,x,i n , any admissible control \u03b1 will either hav\u1ebd \u03c4n+1 = n + 1 (a new observation occurs) or\u03c4n+1 = k (no new observation is made). In the case that\u03c4n+1 = k, then we simply have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "On the other hand, if\u03c4n+1 = n + 1, then for each y \u2208 S, there exists a unique jy \u2208 I such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "As a final note for this subsection, recall that the set of measures P(S) can be identified as the dual of the set of bounded functions on S (denoted B(S)). As such, for any f \u2208 B(S), we can also write",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "This identification will be used in formulating the observation control problem in the following sections.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Set-up and preliminaries"
        },
        {
            "text": "Let us define the reward functional we aim to maximise. Let f : N \u00d7 S \u00d7 I \u2192 R be a function that represents the running reward to the user. We assume for simplicity that the observation cost c obs is constant and independent of the control. The general objective in the observation control problem is to maximise a reward functional of the form Remark 2.9. One can incorporate switching costs {gij }i,j\u2208S into the problem, to formulate an optimal switching problem but with observation costs. This can be done by considering a variable observation cost cij = gij + c obs .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "Through the law of total expectation, we can rewrite this functional as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "Now, by treating \u00b5 as our new state process, define the reward functional",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "This is now a fully observable control problem, so a standard application of the Dynamic Programming Principle establishes the following Bellman-type equation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "The proof of Proposition 2.10 is standard and can be found, for example, in [18, Chapter 3] . We can expand upon (2.15) to obtain the following set of finite-dimensional equations.",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 80,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 81,
                    "end": 91,
                    "text": "Chapter 3]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": ").",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "Proof. Given the initial condition \u00b5m = \u00b5 k,x,i m we always have Im = i, so expanding the first term in the right side of (2. 15 ) gives",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 128,
                    "text": "15",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "(2.17)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "Next, choosing \u03b1 \u2208 A such that\u03c4m+1 = k leads to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "On the other hand, choosing \u03b1 \u2208 A such that\u03c4m+1 = m + 1 leads to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "Combining the above together, we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": ") .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finite horizon"
        },
        {
            "text": "Equality is achieved by noting that any optimal control \u03b1 * must either have\u03c4m+1 = k or \u03c4m+1 = m + 1. The QVI (2.21) follows from rearranging.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(2.20)"
        },
        {
            "text": "We can interpret the value function v(m, \u00b5 k,x,i m ) as the optimal value obtainable from time m, given the latest available data (k, x, i). In the QVI (2.21), the first part of the minimum represents the region of no observation, and the latter part represents an observation, and subsequently adjusting to the optimal regime j * .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(2.20)"
        },
        {
            "text": "Remark 2.12. Equation (2.16) can also be expressed as a discrete quasi-variational inequality (QVI): for all 0 \u2264 k \u2264 m \u2264 N , x \u2208 S, and i \u2208 I,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(2.20)"
        },
        {
            "text": "We will show later in Section 4 that the value function uniquely solves the above QVI by proving a comparison principle.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(2.20)"
        },
        {
            "text": "In the infinite horizon case, we remove the time dependence in the reward function f so that the problem is stationary. The reward functional now reads",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Infinite horizon"
        },
        {
            "text": "where \u03b3 \u2208 [0, 1) represents the discount factor. By the stationarity of f , we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Infinite horizon"
        },
        {
            "text": "Thus without loss of generality, we can set \u00b5 x,i m = \u00b5 0,x,i m and consider instead the quantities",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Infinite horizon"
        },
        {
            "text": "Under this formulation, m represents the time elapsed since the previous observation, with (x, i) being the observed state and regime at the time origin. In order for (2.25) to be well-defined, we assume additionally that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Infinite horizon"
        },
        {
            "text": "for any \u03b3 \u2208 [0, 1), x \u2208 S, and i \u2208 I. Following the same arguments as in Proposition 2.11, we obtain another Bellman-type equation for the infinite horizon case:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Infinite horizon"
        },
        {
            "text": "Proposition 2.13. The infinite horizon value function satisfies for all m \u2265 0, x \u2208 S, and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Infinite horizon"
        },
        {
            "text": "which can be explicitly written as the finite",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Infinite horizon"
        },
        {
            "text": "where j * \u2208 I is the regime such that v(0, \u00b5 z,j * m+1 ) = maxj\u2208I v(0, \u00b5 z,j m+1 ). Remark 2.14. Just as in the finite horizon problem, (2.29) can also be written in QVI form , extra boundary conditions have to be imposed to make a closed system and to ensure uniqueness of solutions. This will be detailed in Sections 4 and 5. We first work out a case with a explicit solution in the subsection below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Infinite horizon"
        },
        {
            "text": "To illustrate the framework, we present a model problem involving a two-state Markov chain and give an explicit solution. We assume the following setup:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "-the transition matrix",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "where p \u2208 (0, 1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "This model problem can be interpreted, e.g., as finding an optimal interval for repairing an appliance that is prone to breaking down over time. The profit function f gives a reward of 1 when the state and control values are the same, and zero otherwise. By construction, if the control remains constant, the chain eventually arrives at the undesirable absorbing state. It is clear that given the knowledge of Xm+1, the optimal regime is simply Im+1 = Xm+1. Hence it is sufficient to only consider the cases where x = i for x \u2208 S and i \u2208 I. At each time m, the possible controls are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "-do not make an observation (and make no regime changes); -make an observation, and set Im+1 = Xm+1. The optimal control is then the control that maximises the value function. Let us now solve for the infinite horizon problem. For ease of notation, we write (x, i) in place of \u00b5 x,i m . First consider the case (x, i) = (0, 0). Choose some arbitrary time m \u2265 0. If it is optimal to not make an observation, then by Proposition 2.13, v m; (0, 0) = p m + \u03b3v m + 1; (0, 0) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "(2.33)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "If, instead, it is optimal to make an observation, then Im+1 = Xm+1 and applying Proposi- Furthermore, note that (x, i) = (1, 1) will also lead to the same set of equations. Therefore, to further simplify the notation, we can drop the x and i arguments in the value function to obtain the one-dimensional equation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "Now if T is the first optimal observation time (where by convention T = \u221e if the optimal control is to never make an observation), the value function is further determined by the recurrence relations",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "otherwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "(2.37)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "Solving the above for v(0), we obtain the explicit solution",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "from which v(n) for n \u2265 1 can be calculated from (2.37). For this model problem, due to the symmetry of the chain, the optimal interval between observations is constant. We can see that (2.38) is actually a geometric series, where the first term T k=0 \u03b3 k p k \u2212 \u03b3 T c obs is the expected returns across the optimal observation interval, and the common ratio \u03b3 T +1 is the discount factor over the whole interval.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model problem"
        },
        {
            "text": "In this section, we expand upon the formulation of the observation control problem in Section 2 by incorporating Bayesian parameter uncertainty. In a variation of the setup from before, suppose now that the transition matrices {Pi}i\u2208I depend on an unknown parameter \u03b8, taking values in the parameter space \u0398. For simplicity, we shall focus on the case where \u0398 \u2286 R.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "More specifically, we take the following setup. Take the sample space",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "with F the canonical product \u03c3-algebra. For \u03c9 = (\u03b8, x0, i0, x1, i1, . . .) \u2208 \u2126, define the process X = (Xn)n by Xn(\u03c9) = xn. Let {P i,\u03b8 } i\u2208,\u03b8\u2208\u0398 be a family of transition kernels, with p (n) xy|\u03b8 (i) denoting the corresponding n-step transition probabilities. Then, for any x0 \u2208 S, \u03c10 \u2208 P(\u2126) and fixed control sequence i = (in) \u221e n=0 , there exists a probability measure P x 0 ,\u03c1 0 ,i such that for any x1, . . . , xn+1 \u2208 S, the following relation holds:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "where (\u03c1n)n is defined recursively by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "For ease of notation, we will write P in place of P x 0 ,\u03c1 0 ,i . In the context of the observation control model, we are concerned with the conditional distribution",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "(we write \u03bdn here so as to distinguish from \u00b5n in the previous section). Note that due to the presence of \u03b8, Lemma 2.7 does not apply to the process \u03bd = (\u03bdn)n here. However, one can partition the sample space across the observation history as follows. Let H n denote the observation history up to time n, i.e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "H n (\u03c9) = (\u03c4 k (\u03c9), X\u03c4 k (\u03c9), I\u03c4 k (\u03c9)) k\u2208H n (\u03c9) ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "where H n (\u03c9) = {j \u2265 1 : \u03c4j (\u03c9) \u2264 n}, so that H n is of random length at most n. Next, for each n, define the collection of sets Cn by Cn = {(tj , xt j , it j ) 0\u2264j\u2264k : k \u2264 n, tj \u2265 0 increasing, xt j \u2208 S, it j \u2208 I}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "(3.5)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "Then, by conditioning on \u03b8 we obtain for any y \u2208 S,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "Now, conditional on the value of \u03b8, Lemma 2.7 does apply, so that P Xn = y \uf8e6 \uf8e6 \u03b8, H n = P(Xn = y | \u03b8,\u03c4n,Xn,\u03b9n).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "Hence, when dynamically updating the problem, the values of \u03bdn can be characterised by the values of\u03c4n,Xn,\u03b9n, and the conditional distribution P(d\u03b8 | F X,\u03c4 n ). That is, for every \u03c9 \u2208 \u2126, there exists some k \u2208 N, x \u2208 S, i \u2208 I and \u03c0 \u2208 P(\u0398) such that for any y \u2208 S",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "(3.8)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "In line with the previous section, we shall use the notation \u03bd k,x,i,\u03c0 n for a realisation \u03bdn(\u03c9). Let us now establish the transition kernel for the process \u03bd. First denote the parameter process (\u03c0n)n such that \u03c0n(d\u03b8) = P(d\u03b8 | F X,\u03c4 n ). Then, by Bayes' Theorem,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "Hence, in analogy with the transition kernel for \u00b5, when conditioned on \u03bdn = \u03bd k,x,i,\u03c0n n , either \u03c4n+1 = n + 1 or\u03c4n+1 = k. If\u03c4n+1 = k, then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "(3.10)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "Otherwise,\u03c4n+1 = n + 1 and for each y \u2208 S there exists a unique jy \u2208 I such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "xy|\u03b8 (i) \u03c0n(d\u03b8), j = jy and (3.9) holds; 0, otherwise. Thus, when dynamically solving for the observation control problem at each time m, \u03c0m acts as the prior distribution for \u03b8, from which the optimal action is computed. Any new observation occurring has an associated likelihood, from which the posterior \u03c0m+1 is calculated. Subsequently \u03c0m+1 acts as the prior for time m + 1. As (3.12) and (3.13) now form a fully observable control problem with state process \u03bd, we can once again state the Dynamic Programming Principle below, which can be expanded using the transition probabilities (3.11). ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bayesian formulation"
        },
        {
            "text": "and j * \u2208 I is the regime such that v(m + 1, \u03bd m+1,z,j * ,\u03c0 \u2032 m+1 ) = maxj\u2208I v(m + 1, \u03bd m+1,z,j,\u03c0 \u2032 m+1 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "17)"
        },
        {
            "text": "Proof. The proof is similar to that of Proposition 2.11. First, (3.14) is obtained through standard dynamic programming. Next,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "17)"
        },
        {
            "text": "Then, any optimal control \u03b1 \u2208 A conditioned on \u03bdm = \u03bd k,x,i,\u03c0 m must have either\u03c4m+1 = k or \u03c4m+1 = m. If\u03c4m+1 = k, then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "17)"
        },
        {
            "text": "Alternatively, if\u03c4m+1 = m + 1, then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "17)"
        },
        {
            "text": "). (3.20)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "17)"
        },
        {
            "text": "Combining both equations then gives us the result.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "17)"
        },
        {
            "text": "For the infinite horizon case, we can once again invoke stationarity to obtain v(m, \u03bd k,x,i,\u03c0 m ) = v(m \u2212 k, \u03bd 0,x,i,\u03c0 m\u2212k ), (3.21) so that by setting \u03bd x,i,\u03c0 m = \u03bd 0,x,i,\u03c0 m , we arrive at the corresponding DPP for the infinite horizon problem below. \u03c0 \u2032 (d\u03b8) = \u03c0(d\u03b8) \u00b7 p (m+1) xy|\u03b8 (i)/p \u03c0,(m+1) xy (i), (3.25) and j * \u2208 I is the regime such that v(0, \u03bd z,j * ,\u03c0 \u2032 m+1 ) = maxj\u2208I v(0, \u03bd z,j,\u03c0 \u2032 m+1 ). Whilst the DPP here, even in reduced form, is typically an infinite dimensional equation, it can be reduced to finite dimensions if one uses conjugate distributions. One such example involving random walks is given in Section 5.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 132,
                    "text": "(3.21)",
                    "ref_id": null
                },
                {
                    "start": 307,
                    "end": 313,
                    "text": "(3.25)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "17)"
        },
        {
            "text": "In this section, we consider a class of discrete QVIs and prove a comparison principle, as well as introducing a penalisation scheme which approximates the QVI. We will show that the QVI obtained via dynamic programming in the observation control model falls under this class of QVIs. We follow an approach similar to [19] , which considers the case A = Id below.",
            "cite_spans": [
                {
                    "start": 318,
                    "end": 322,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "For this section we employ the following notation for indexing: n \u2208 {1, . . . , N } for the time domain, l \u2208 {1, . . . , L} for the spatial domain, and i \u2208 {1, . . . , d} for the control. Let u \u2208 R N\u00d7L\u00d7d , and write u n i,l for its components. Define\u016b \u2208 R N\u00d7L by\u016b n l = maxj\u2208I u n j,l . We also write\u016b n to represent the vector (\u016b n l ) l . The class of discrete QVIs of interest can be stated as follows: where q \u2208 {1, . . . , N } is some fixed index, c is a constant and each An \u2208 R L\u00d7L is a strictly substochastic matrix, i.e., all elements are non-negative and all row sums are less than 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "-Fi : R N\u00d7L \u2192 R N\u00d7L satisfies the following property: there exists a constant \u03b2 > 0 such that for any",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "Note that in general the c in (4.2) can also depend on i, n and l (this is indeed the case for (2.30), see Remark 4.7), but we shall only consider a constant c for the proofs in this section for ease of notation. The fixed index q can be arbitrary; the idea is that the operator M only couples the solution u at a cross-section of values along the time domain. In the following, we adapt the argument in [19] to prove a comparison principle of the QVI (4.1). The main ingredient in the proof is to bound the coupling terms An(\u016b q \u2212v q ) by their maximum. If we extend the problem to an infinite domain, it is not obvious that this maximum is achieved. However, recall from Remark 2.15 that additional spatial boundary conditions have to be imposed for a closed system. This reduces the system of QVIs back into the form of (4.1). The comparison principle is shown in the proposition below. Let \u03b3 < 1 be the maximum of the row sums of Am. Then by combining both inequalities above we obtain",
            "cite_spans": [
                {
                    "start": 404,
                    "end": 408,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "This implies that there exist some ju, jv \u2208 I and l * \u2208 {1, . . . , L} such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "which is a contradiction by the maximality of M . Hence, we must have Fj (uj) m k \u2264 0, but then since v is a supersolution we have by the monotonicity property",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "so that M \u2264 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "Now we present a penalty approximation to the QVI (4.1). Consider the following penalised problem. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "where the penalisation function \u03c0 : R \u2192 R is continuous, non-decreasing with \u03c0| (\u2212\u221e,0] = 0 and \u03c0| (0,\u221e) > 0, and is applied elementwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "We show that for each fixed \u03c1, (4.10) satisfies a comparison principle. This implies uniqueness for Problem 4.3. The argument follows similarly to the approach in [19] and Proposition 4.2.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 167,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "Proposition 4.4. For any penalty parameter \u03c1 \u2265 0 and any c \u2265 0, if u \u03c1 = (u \u03c1 i )i\u2208I (resp., v \u03c1 = (v \u03c1 i )i\u2208I) satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "Fi(u \u03c1 i ) \u2212 \u03c1 \u03c0 (Mu \u03c1 \u2212 u \u03c1 ) \u2264 0 (resp., \u2265 0), (4.11) then u \u03c1 \u2264 v \u03c1 .",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 55,
                    "text": "(4.11)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "Proof. As in the previous proposition, let M := max i,l,n (u \u03c1,n i,l \u2212 v \u03c1,n i,l ) =: u \u03c1,m j,k \u2212 v \u03c1,m j,k . Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "Hence by rearranging we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "It then follows from the sub/super-solution properties of u \u03c1 and v \u03c1 that Theorem 4.6. For any fixed c \u2265 0, the solution to(4.10) converges monotonically from below to a function u \u2208 R d\u00d7N\u00d7L as \u03c1 \u2192 \u221e. Moreover u solves the discrete QVI (4.1) if c > 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "Remark 4.7. The QVI (2.30), derived from the infinite horizon problem of the observation control model, is an instance of the QVI (4.1). Specifically, we have L = |S| as well as the following:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "Moreover, it is straightforward to see that Fi satisfies the monotonicity condition (4.3). For uniqueness of solutions to hold, additional boundary conditions have to be imposed to close the system. This will be demonstrated in the numerical experiments section below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison principle and penalisation"
        },
        {
            "text": "In this section, we apply our observation cost framework to three numerical experiments. Sections 5.1 and 5.2 analyse two infinite horizon problems. The roadmap for both is as follows: we first set up the discrete QVIs arising from the problem, which is then approximated by the penalised problem. As in [19] , we will employ the penalty function \u03c0(x) = x + . The solution of the penalised problem is in turn approximated iteratively with semismooth Newton methods [23] . Formally speaking, starting with an initialisation v (0) to the penalised problem",
            "cite_spans": [
                {
                    "start": 304,
                    "end": 308,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 465,
                    "end": 469,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Numerical experiments"
        },
        {
            "text": "we obtain the next iterate by solving for",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical experiments"
        },
        {
            "text": "where L \u03c1 denotes the generalised derivative of the function G \u03c1 . For each example, we examine the numerical performance of the penalty method and Newton iterations, as well as the effects of the observation cost on the qualitative behavior of the solutions. Section 5.3 considers the Bayesian formulation of the observation control problem over a finite horizon. The solutions are obtained through backwards recursion from the terminal conditions. We examine the impact that the extra parameter uncertainty has on the optimal trajectories.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical experiments"
        },
        {
            "text": "Consider an integer-valued random walk (Xn) n\u2208N with two regimes, representing a positive and negative drift respectively, so that the control space is I = {+1, \u22121}. The probability of each step is parametrised by \u03b8. Specifically, for any x \u2208 S = N,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "We also adopt the following reward function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "The mass of this reward function f is concentrated around the origin, so naturally, the optimal regime is one that reverts the process back towards the origin. For this example, we consider the infinite horizon problem. Recall that the discrete QVI (2.30) reads: for all m \u2265 0, x \u2208 S, and i \u2208 I,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "Note that there exists a path from x to y over m units of time if and only if m \u2265 |y \u2212 x| and m \u2261 y (mod 2). If S x m denotes the set of states that can be reached from x after m units of time, then the transition probabilties are given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "where r = (m + y \u2212 x)/2. Hence, in full, the QVI reads:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "In general, r depends on x, y and m, but we suppress the subscript for ease of notation. To close the system to ensure a unique solution, we enforce the following time and spatial boundary conditions. We impose a reflecting boundary at x = \u00b1L, where L is suitably large. In particular, so that the QVI (5.5) for the states x = \u00b1L will use the transition probabilities (5.9) instead.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "For the time boundary, we enforce an observation at some large N > 0. The terminal condition then reads (for \u2212L < x < L):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "where here r \u2032 = (N + y \u2212 x)/2. The analogous equations hold for the spatial boundary x = \u00b1L, but with the transition probabilities (5.9). These terminal conditions can be interpreted as the largest possible interval between two observations. We now proceed to solve the penalised problem for the system (5.8), with boundary conditions (5.9) and (5.10), through the use of semismooth Newton methods. To initialise the iteration, we solve for the uncoupled system",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "with the spatial boundary transition probabilities (5.9) and time boundary",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "(5.12) The system (5.11) corresponds to the penalised equation with \u03c1 = 0. The uncoupled time boundary condition is equivalent to enforcing an observation but with no switching (i.e., assuming that v = vi in each equation for vi). The iteration terminates once a relative tolerance threshold of 10 \u22128 is reached. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift"
        },
        {
            "text": "We investigate the numerical performance of our described methods for the case \u03b8 = 0.75, \u03b3 = 0.99, L = 50 and N = 500, across different cost parameters c obs . Computations are performed using MATLAB R2019b. The numerical solutions are shown in Table 5 .1. Row (a) shows that the number of Newton iterations required to reach the tolerance threshold is independent from the size of the penalty parameter \u03c1. Fewer iterations are required for more extreme values of c obs , but the overall number of iterations remains low across different observation costs. Row (b) shows the increments v \u03c1 \u2212 v 2\u03c1 \u221e. The values clearly demonstrate a first-order convergence of the penalisation error with respect to the penalty parameter \u03c1, which is in line with the theoretical results presented in [19, Theorem 3.9, 4.2] . We now discuss the qualitative behaviour of the solution. It is clear that if the chain is observed to be at a positive state, then the control should be switched to i = \u22121 for a negative drift and vice versa. Table 5 .2 lists the optimal observation time gap for selected states across different observation costs c obs . As the problem is symmetric by construction, it is sufficient to only examine the behavior for the positive states. In general, the optimal observation time increases as c obs increases. A longer unobserved period of time then leads to a lower average reward. This is illustrated in Figure 5 .3, where the function n \u2192 v n \u22121,30 is plotted for various values of c obs . In the absence of an observation cost, i.e., for c obs = 0, the optimal observation time equals the magnitude of the last observed state, as there is no need to observe until it is possible for the walk to cross the origin again. ",
            "cite_spans": [
                {
                    "start": 783,
                    "end": 787,
                    "text": "[19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 788,
                    "end": 800,
                    "text": "Theorem 3.9,",
                    "ref_id": null
                },
                {
                    "start": 801,
                    "end": 805,
                    "text": "4.2]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 245,
                    "end": 252,
                    "text": "Table 5",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 1018,
                    "end": 1025,
                    "text": "Table 5",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 1414,
                    "end": 1422,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Random walk with drift"
        },
        {
            "text": "In this subsection, we implement our observation control framework in an extension to an HIVtreatment scheduling problem that appeared in [21] . As alluded to in the introduction, the stationarity of the reward function in the original model (see (5.13 ) below) implicitly assumes that the observer is given the state of the underlying process at initialisation. However, in practice many scenarios of interest do not satisfy this assumption. Our model formulation extends the above by allowing in addition that the user can approach the problem at initialisation with outdated or sub-optimal information. We demonstrate that such initial conditions can lead to different qualitative behaviours in the value function through time. We also examine the numerical performance of the penalty method when applied to the system of QVIs for this larger system, compared to that in Section 5.1.",
            "cite_spans": [
                {
                    "start": 138,
                    "end": 142,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 247,
                    "end": 252,
                    "text": "(5.13",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "Let us briefly describe the original problem in [21] here. A continuous-time Markov chain is used to model virus levels of HIV-positive patients over time. With two types of treatment available, the control space is I = {0, 1, 2} (where 0 represents no treatment given). Four virus strains are considered: WT denotes the wild type (susceptible to both treatments), R1 and R2 denotes strains that are each resistant to Treatment 1 and Treatment 2 respectively, and HR denotes the strain that is highly resistant to both. The level of each strain is represented by the states 'none' (0), 'low' (l), 'medium' (m), and 'high' (h). Therefore, the state space for the Markov chain is S = {0, l, m, h} 4 \u222a { * }, where the asterisk represents patient death. Note in particular that * is an absorbing state. The goal in the original model is to then minimise a cost functional J : S \u00d7 I \u2192 R of the form:",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 52,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "\u03c4 j e \u2212\u03b3s c(Xs, \u03b9(X\u03c4 j )) ds + e \u2212\u03b3\u03c4 j+1 c obs , (5.13) where the cost function c : S \u00d7 I \u2192 R is a linear combination of the productivity loss resulting from each patient's condition and their received treatment. To adapt the model above for our framework, we first discretise the Markov chain, choosing each step to represent one day. We then take the model parameters from the original paper [21, Section 3] , which provides the transition rate matrices {Qi}i\u2208I and the cost function c(x, i). The transition matrices {Pi}i\u2208I are then given by Pi = e Q i (as the time unit in [21] is one day). For illustration purposes, a sparse plot of the transition matrix P0 is shown in Figure 5 .4. As our framework takes the form of a maximisation problem, we choose f = \u2212c for the reward function. We can now formulate our problem in terms of the following QVI:",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 55,
                    "text": "(5.13)",
                    "ref_id": null
                },
                {
                    "start": 394,
                    "end": 398,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 399,
                    "end": 409,
                    "text": "Section 3]",
                    "ref_id": null
                },
                {
                    "start": 577,
                    "end": 581,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 676,
                    "end": 684,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "x + c obs = 0. (5.14)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "We now follow the same procedure in Section 5.1 to obtain a numerical solution. Note that for this problem, the spatial domain is finite and we also have a natural spatial boundary arising from the absorbing death state * , that is, for all m \u2265 0 and i \u2208 I,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "where a is a constant representing the average GDP loss due to patient death [20, 21] . A time boundary is once again enforced at some large time N > 0, which can be interpreted as a mandatory observation at time N . Explicitly, this reads",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 81,
                    "text": "[20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 82,
                    "end": 85,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "We now solve the associated penalised problem with semismooth Newton methods. As in Section 5.1, we choose the initial guess to be the solution to the penalised problem with \u03c1 = 0, with uncoupled time boundary conditions. The iterations terminate once a relative tolerance threshold of 10 \u22128 is reached. The numerical experiments are performed on MATLAB R2019b. Table 5 .6 shows the numerical solution for different values of N and c obs across different penalty parameters \u03c1. Row (a) shows that much like the random walk experiment in Section 5.1, the number of iterations remains constant with respect to \u03c1. However, the number of Newton iterations required to reach the target threshold is much higher, approximately 20 iterations. Figure 5 .5 illustrates the gap between the initial guess and the final solution. We see that the disagreement occurs only on one side of the free boundary. This is due to the nature of our initial guess, where solving the penalised problem for \u03c1 = 0 is equal to restricting the system to only one region (in this case, the region of no observations). Despite the large gap between the two curves, we see that the iterate obtained after one step is significantly closer to the true solution, which indicates that much of the convergence in the solutions is achieved in the first few Newton iterations.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 362,
                    "end": 369,
                    "text": "Table 5",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 735,
                    "end": 743,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "Row (b) in Table 5 .6 shows the increments v \u03c1 \u2212 v 2\u03c1 \u221e. Reassuringly, for this more complicated system, we still see a clear first-order convergence of the penalisation error with respect to the penalty parameter \u03c1. Even for small values of \u03c1, the successive increments were within O(1) (in comparison to the magnitude of the solution which is of O(10 6 )). This shows that the penalty approximation is very effective for small penalty parameters, and that it works well when extended to the class of QVIs that we introduced in Section 4.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 11,
                    "end": 18,
                    "text": "Table 5",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "We now analyse the behaviour of the value function when plotted as a function against time. The top-left graph of Figure 5 .7 depicts an instance where the patient is under a stable condition. Here the observation region is [15, N ] . There are limited benefits of frequently paying a high observation cost when it is unlikely that the patient's condition will deteriorate over a short period of time. On the other hand, if the patient has a high chance of mortality, i.e. a high probability of reaching the absorbing state * , then it is optimal to observe as soon as possible. The top-right graph illustrates the case when the last observed state contains a low amount of the R2 strain but with no treatment given. The observation region here is [0, 53]. The disparity between the optimal actions can be attributed to the negative reward associated with the absorbing state. For the latter case, with the parameters for the transition matrix giving a mortality rate of approximately 3% after 53 days, making an observation unlikely to improve subsequent rewards, if any. The optimal control allows for a more effective resource allocation by focusing on higher quality samples during data collection. The solutions of the value function under these scenarios were not available in the original model.",
            "cite_spans": [
                {
                    "start": 224,
                    "end": 232,
                    "text": "[15, N ]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 114,
                    "end": 122,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "To examine the behaviour around the decision boundaries, we plot the central finite difference terms (v n+1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "i,x \u2212 v n\u22121 i,x )/2\u2206n in the bottom row of Figure 5 .7, underneath their respective graphs of the value function. If we consider the plots as a discretisation of a continuous value function, we see that there is much bigger variation within the observation region. Critically, there is non-smoothness across the boundary in the bottom-left graph. This suggests that the solution in continuous-time is C 2 in time within each decision region, but only C 1 across the boundary. This is in line with theoretical results on the regularity of viscosity solutions in optimal stopping and switching problems [18, Chapter 5].",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 43,
                    "end": 51,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Extension of an HIV-treatment model"
        },
        {
            "text": "In this subsection, we consider a random walk with drift, as set up in Section 5.1, but with the additional assumption that the true value of the drift parameter \u03b8 is unknown to the user. To avoid complications with boundary conditions and infinite parameter domains, we shall only consider the finite horizon problem. Using the notation in Section 3, for a fixed value of \u03b8, the n-step transition probabilities are given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift: Bayesian formulation"
        },
        {
            "text": "where r = 1 2 (n + y \u2212 x). As remarked at the end of Section 3, we shall choose the prior from a family of beta distributions to obtain conjugacy in the parameter process. This reduces the \u03c1 10 3 2 \u00d7 10 3 4 \u00d7 10 3 8 \u00d7 10 3 16 \u00d7 10 3 32 \u00d7 10 3 N = 150, c obs = 200 (a) 18 is the probability mass function of the Beta-binomial distribution and B(a, b) is the Beta function. The posterior distribution \u03c0n = \u03c0 \u2032 0 is then",
            "cite_spans": [
                {
                    "start": 268,
                    "end": 270,
                    "text": "18",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Random walk with drift: Bayesian formulation"
        },
        {
            "text": "Beta(a + r, b + n \u2212 r), \u03b90 = +1, Beta(a + n \u2212 r, b + r), \u03b90 = \u22121.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift: Bayesian formulation"
        },
        {
            "text": "(5.20)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift: Bayesian formulation"
        },
        {
            "text": "Since the parameter process can now be characterised by the parameters of the Beta distribution, if \u03c0 \u223c Beta(a, b) then we write v(m; (k, x, i); (a, b)) := v(m, \u03bd k,x,i,\u03c0 m ). As both parameters can generally be any non-negative value, it is not feasible to obtain a solution of the value function for all values of (a, b). However, for a given prior, the required values of the Beta distribution parameters for calculation are limited via the relation in (5.20) . We can then recursively calculate the value function using (3.15) from the time horizon N . Recalling that the reward function f (y) = 1/(|y| + 1) is independent of the control \u03b1, we have the set of terminal",
            "cite_spans": [
                {
                    "start": 456,
                    "end": 462,
                    "text": "(5.20)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Random walk with drift: Bayesian formulation"
        },
        {
            "text": "where S x k is the (finite) set of states that can be reached from x after k units of time, as defined in Section 5.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift: Bayesian formulation"
        },
        {
            "text": "For our experiment, we choose three different initial parameter pairs for the prior \u03c00 as well as varying the observation cost. Here we assume that the true value of \u03b8 = 0.3 and set a time horizon of N = 50. We evaluate their performances under the optimal strategy for the same path realisations. The trajectories are sampled as follows. A realisation of the path is generated via a sequence of uniform random variables Un \u223c U [0, 1] to represent the walk at each time step. If \u03b9n = +1, we take an upwards step if Un \u2264 \u03b8, and a downwards step otherwise; the opposite applies to \u03b9n = \u22121, with switching only allowed at the optimal observation times. An illustration of a particular optimal sequence of actions is depicted in Figure 5 .8.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 725,
                    "end": 733,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Random walk with drift: Bayesian formulation"
        },
        {
            "text": "We now examine the effects of parameter uncertainty on the value function, recorded in Table 5 .9. The inaccuracy of the prior \u03c00 increases further down the table. When comparing the values of row (a) column-wise, i.e. for a fixed observation cost, there is a small increase in the number of observations as the prior moves away from the true value. The effects of the choice in prior is more clearly seen in row (b), where the average profit sees a more substantial decrease for \u03c00, \u223c Beta(5, 2), whose mass is concentrated towards [0.5, 1]. The misalignment of the prior and the ground truth is also reflected in row (c), where the credible intervals are generally widest in the bottom row of the table, reflecting a bigger uncertainty over the parameter value. In particular, {\u03c4 k = n} \u2208 F X,\u03c4 \u03c4 k\u22121 for any n. As the sequence \u03c4 is strictly increasing, we have the lower bound \u03c4n \u2265 n, so without loss of generality assume k \u2264 n. Now {\u03c4 k = n} \u2286 {\u03c4j \u2264 n \u2212 1} for all 0 \u2264 j \u2264 k \u2212 1. Hence,",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 87,
                    "end": 94,
                    "text": "Table 5",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Random walk with drift: Bayesian formulation"
        },
        {
            "text": "so that \u03c4 k is predictable as required.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift: Bayesian formulation"
        },
        {
            "text": "Proof of Proposition 2.5. We prove the claim by induction. Assume without loss of generality that \u03c40 = 0. Since \u03c40 is deterministic, trivially that F X,\u03c4 0 = F X 0 . Now for any n \u2265 1, recall that the strictly increasing nature of \u03c4 means we have where the second inclusion follows from the predictability of \u03c4 , and the third is the induction assumption.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random walk with drift: Bayesian formulation"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Optimal inattention to the stock market",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Abel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Eberly",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Panageas",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Am. Econ. Rev",
            "volume": "97",
            "issn": "2",
            "pages": "244--249",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Optimal inattention to the stock market with information costs and transactions costs",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Abel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Eberly",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Panageas",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "National Bureau of Economic Research",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Optimal inspections in a stochastic control problem with costly observations",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "F"
                    ],
                    "last": "Anderson",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                }
            ],
            "year": 1977,
            "venue": "Math. Oper. Res",
            "volume": "2",
            "issn": "2",
            "pages": "155--190",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Optimal inspections in a stochastic control problem with costly observations",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "F"
                    ],
                    "last": "Anderson",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                }
            ],
            "year": 1978,
            "venue": "II. Math. Oper. Res",
            "volume": "3",
            "issn": "1",
            "pages": "67--81",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Fundamentals of Stochastic Filtering. Stochastic Modelling and Applied Probability",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bain",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Crisan",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Active measure reinforcement learning for observation cost minimization",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bellinger",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Coles",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Crowley",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Tamblyn",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.12697"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Some convergence results for howard's algorithm",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Bokanowski",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Maroso",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zidani",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "SIAM J. Numer. Anal",
            "volume": "47",
            "issn": "4",
            "pages": "3001--3026",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Controlled diffusion processes",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "S"
                    ],
                    "last": "Borkar",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Probab. Surv",
            "volume": "2",
            "issn": "",
            "pages": "213--244",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Switching cost models as hypothesis tests",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "N"
                    ],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Henckel",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "D"
                    ],
                    "last": "Menzies",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Muhle-Karbe",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Zizzo",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Econ. Lett",
            "volume": "175",
            "issn": "",
            "pages": "32--35",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Transactions costs and portfolio choice in a discrete-continuoustime setting",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Duffie",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "S"
                    ],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "J. Econ. Dyn. Control",
            "volume": "14",
            "issn": "1",
            "pages": "35--51",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Optimal treatment strategies in the context of 'treatment for prevention' against HIV-1 in resource-poor settings",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Duwal",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Winkelmann",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Sch\u00fctte",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Von Kleist",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "PLoS Comput. Biol",
            "volume": "11",
            "issn": "",
            "pages": "1--30",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Sequential testing of a Wiener process with costly observations",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Dyrssen",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ekstr\u00f6m",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Sequential Anal",
            "volume": "37",
            "issn": "1",
            "pages": "47--58",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Optimal control for partially observed diffusions",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "H"
                    ],
                    "last": "Fleming",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pardoux",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "SIAM J. Control Optim",
            "volume": "20",
            "issn": "2",
            "pages": "261--285",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Search for information on multiple products",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ke",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Villas-Boas",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Manage. Sci",
            "volume": "62",
            "issn": "",
            "pages": "2--2016",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Stochastic Systems: Estimation, Identification, and Adaptive Control",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Varaiya",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Classics in Applied Mathematics. Society for Industrial and Applied Mathematics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Numerical Methods for Stochastic Control Problems in Continuous Time",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "J"
                    ],
                    "last": "Kushner",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "G"
                    ],
                    "last": "Dupuis",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "On some recent aspects of stochastic control and their applications",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Probab. Surv",
            "volume": "2",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Continuous-Time Stochastic Control and Optimization with Financial Applications",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "A penalty scheme for monotone systems with interconnected obstacles: Convergence and error estimates",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Reisinger",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "SIAM J. Numer. Anal",
            "volume": "57",
            "issn": "4",
            "pages": "1625--1648",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Markov Decision Processes with Information Costs",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Winkelmann",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Markov control processes with rare state observation: Theory and application to treatment scheduling in HIV-1",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Winkelmann",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Sch\u00fctte",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "V"
                    ],
                    "last": "Kleist",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Commun. Math. Sci",
            "volume": "12",
            "issn": "5",
            "pages": "859--877",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "A penalty method for the numerical solution of Hamilton-Jacobi-Bellman (HJB) equations in finance",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Witte",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Reisinger",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "SIAM J. Numer. Anal",
            "volume": "49",
            "issn": "1",
            "pages": "213--231",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Penalty methods for the solution of discrete HJB equations-continuous control and obstacle problems",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Witte",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Reisinger",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "SIAM J. Numer. Anal",
            "volume": "50",
            "issn": "2",
            "pages": "595--625",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Analysis and computation of an optimality equation arising in an impulse control problem with discrete and costly observations",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yoshioka",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tsujimura",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "J. Comput. Appl. Math",
            "volume": "366",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "A hybrid stochastic river environmental restoration modeling with discrete and costly observations",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yoshioka",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tsujimura",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hamagami",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yoshioka",
                    "suffix": ""
                }
            ],
            "year": 1964,
            "venue": "Optimal Control Appl. Methods",
            "volume": "41",
            "issn": "6",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "X\u03c3 (\u00bd {\u03c4 j \u2264n} \u03c4j, \u00bd {\u03c4 j \u2264n} X\u03c4 j ) : j \u2265 0 , (1.1) where \u03c4 = (\u03c4 k ) \u221ek=0 represents a controlled sequence of observation points. By reformulating the control problem with the conditional distribution \u00b5 = (\u00b5n)n as the state process, where \u00b5n(dx) := P Xn \u2208 dx",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "For any bounded and continuous function f and n \u2265 m we have E f (Xn)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "be interpreted as the profit accumulated over the time interval {0, . . . , N }, minus the observation cost for every inspection of the chain.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "1 illustrates the chain for the case p = 0.9.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Illustration of the two-state Markov chain. Left: i = 0; Right: i = 1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "34) where in the last equality we use the fact that by symmetry of the problem, v m; (0, 0) = v m; (1, 1) . Combining both equations, we obtain v m; (0, 0) = max{p m + \u03b3v m + 1; (0, 0) , p m \u2212 c obs + \u03b3v 0; (0, 0) }. (2.35)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "f (m, Xm, Im) \u2212 \u00bd {\u03c4 m+1 =m+1} c obs + v(m + 1, \u03bdm+1)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "f (Xm, Im) \u2212 \u00bd {\u03c4 m+1 =m+1} c obs + \u03b3v(m + 1, \u03bdm+1)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Find u = (u1, . . . , u d ) \u2208 R N\u00d7L\u00d7d such that min {Fi(ui), ui \u2212 Mu} = 0, i \u2208 I = {1, . . . , d}, M : R N\u00d7L\u00d7d \u2192 R N\u00d7L is defined by (Mu) n l = ((An\u016b q ) l \u2212 c) , (4.2)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Suppose that c > 0, and u = (ui)i\u2208I (resp. v = (vi)i\u2208I) satisfies min {Fi(ui), ui \u2212 Mu} \u2264 0 (resp. \u2265 0), i \u2208 I;(4.4)then u \u2264 v. Proof. Let M := max i,l,n (u n i,l \u2212 v n i,l ) =: u m j,k \u2212 v m j,k . Since u is a subsolution, we have Fj (uj) \u2264 0 or uj \u2212 Mu \u2264 0. First suppose that uj \u2264 (Mu). Then u m j,k \u2264 (Am\u016b q ) k \u2212 c. (4.5)By the assumption that v is a supersolution, v m j,k \u2265 (Amv q ) k \u2212 c.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Let \u03c1 \u2265 0 be the penalty parameter. Find u",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "The monotonicity assumption of F then gives us that M \u2264 0.The following lemma and theorem have been proven in[19, Section 2]. The proof extends to our setting given Propositions 4.2 and 4.4, and follows from the monotone properties of the functions Fi. Lemma 4.5 gives well-posedness for Problem 4.3, as well as establishing a bound, independent of the penalty parameter, to the penalised solutions u \u03c1 . Theorem 4.6 proves the convergence of the solutions to the penalised equation (4.10) towards the solutions of the QVI (4.1). This gives existence of solutions to (4.1) via construction.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Suppose u \u03c1 is the solution to (4.10) with parameter \u03c1 \u2265 0 with costs c \u2265 0. Then we have the bound u \u03c1 \u2264 F (0) /\u03b3.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "pL,L(+1) = \u03b8, pL,L\u22121(+1) = 1 \u2212 \u03b8, pL,L(\u22121) = 1 \u2212 \u03b8, pL,L\u22121(\u22121) = \u03b8; p\u2212L,\u2212L(+1) = 1 \u2212 \u03b8, p\u2212L,\u2212L+1(+1) = \u03b8, p\u2212L,\u2212L(\u22121) = \u03b8, p\u2212L,\u2212L+1(\u22121) = 1 \u2212 \u03b8, (5.9)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Difference in total reward obtained when altering the observation cost c obs . Each line shows the graph of n \u2192 v n \u22121,30 . The cross indicates the optimal observation time.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Sparsity pattern of the transition matrix P0 (the pattern is the same across all control states). The state space is encoded as {1, . . . , 256}, by considering the state vectors [WT, R1, R2, HR] as a base-4 string in reverse order (for example, [h, 0, l, l] corresponds to 83). The death state * is represented by 256.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "Convergence of the Newton iterates towards the solution. The lines show the graphs of n \u2192 v n 0,4 for the initial guess v (0) , first iterate v (1) and true solution v, where the state [WT, R1, R2, HR] = [0, 0, l, 0] is encoded as 4 in base 4. The cross indicates the boundary between the observation regions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "15) to a finite dimensional equation. Then, if \u03c00 \u223c Beta(a, b) and the subsequent observation occurs at time n, a standard calculation shows that p \u03c0 0 ,(n) xy (+1) = g(r | n, a, b), p \u03c0 0 ,(n) xy (\u22121) = g(n \u2212 r | n, a, b),",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "The value function exhibits two qualitatively different decay modes depending on the starting states x. Left: a stable condition with the correct treatment. Right: a worse condition with no treatment. The top row shows the mappings n \u2192 v n i,x . The bottom row plots the corresponding central finite difference terms.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "Left: sample realisation of the controlled random walk along the optimal trajectory. Right: prior and posterior distribution of \u03b8; the grey lines indicate 'intermediate posteriors' obtained from earlier observations.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "obs = 0.1 c obs = 0.25 c obs = 0.5 c obs = 0.75 (a) 24.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF25": {
            "text": "X(\u00bd {\u03c4 j \u2264n} \u03c4j , \u00bd {\u03c4 j \u2264n} X\u03c4 j ) : j \u2265 0 = \u03c3 (\u00bd {\u03c4 j \u2264n, \u03c4 j \u2264n+1} \u03c4j , \u00bd {\u03c4 j \u2264n} X\u03c4 j ) : j \u2265 0 \u2286 \u03c3 (\u00bd {\u03c4 j \u2264n} , \u00bd {\u03c4 j \u2264n+1} \u03c4j , \u00bd {\u03c4 j \u2264n} X\u03c4 j ) : j \u2265 0 \u2286 \u03c3 (\u00bd {\u03c4 j \u2264n+1} \u03c4j , \u00bd {\u03c4 j \u2264n+1} X\u03c4 j ) : j \u2265 0 = F Proof of Proposition 2.3. By definition, \u03c4 k \u2208 F X,\u03c4 \u03c4 k\u22121 where F X,\u03c4 \u03c4 k\u22121 = \u03c3{\u03c40, X\u03c4 0 , \u03c41, X\u03c4 1 , . . . , \u03c4 k\u22121 , X\u03c4 k\u22121 }. (A.2)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF26": {
            "text": "X(\u00bd {\u03c4 j \u2264n} \u03c4j, \u00bd {\u03c4 j \u2264n} X\u03c4 j ) : 0 \u2264 j \u2264 n . {\u03c4 j =k} X k , (A.6)and the fact that for any two random variables Y and Z, \u03c3(Y + Z), \u03c3(Y Z) \u2286 \u03c3(Y, Z), we obtain X\u03c3{\u00bd {\u03c4 1 =n} , . . . , \u00bd {\u03c4n=n} , Xn}\u2286 \u03c3 F X,\u03c4 n\u22121 \u222a Xn \u2286 \u03c3 F",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Remark 2.15. In general, (2.21) and (2.30) have to be solved numerically. In the case of (2.21), this can be done via backwards induction in m. For (2.30)",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "List of optimal observation times across various states x and costs c obs .",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 5.6: Numerical results for the HIV-treatment problem. Line (a): number of Newton iterations. Line (b): the increments v \u03c1 \u2212 v 2\u03c1 .",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5.9: Numerical results for the parameter uncertainty problem. Line (a): average number of observations. Line (b): average profit (N = 50). Line (c): average credible interval width (HDI 95%). [26] H. Yoshioka, Y. Yaegashi, M. Tsujimura, and Y. Yoshioka. Cost-efficient monitoring of continuous-time stochastic processes based on discrete observations. Appl. Stoch. Models Bus. Ind., 37(1):113-138, 2021. [27] H. Yoshioka, Y. Yoshioka, Y. Yaegashi, T. Tanaka, M. Horinouchi, and F. Aranishi. Analysis and computation of a discrete costly observation model for growth estimation and management of biological resources. Comput. Math. Appl., 79(4):1072-1093, 2020. Proof of Proposition 2.1. Note that",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Jonathan Tam has been supported by the EPSRC Centre for Doctoral Training in Mathematics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        }
    ]
}
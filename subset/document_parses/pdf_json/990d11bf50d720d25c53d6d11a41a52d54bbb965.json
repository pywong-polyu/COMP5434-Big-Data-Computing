{
    "paper_id": "990d11bf50d720d25c53d6d11a41a52d54bbb965",
    "metadata": {
        "title": "Deep Neural Networks Guided Ensemble Learning for Point Estimation in Finite Samples",
        "authors": [
            {
                "first": "Tianyu",
                "middle": [],
                "last": "Zhan",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Haoda",
                "middle": [],
                "last": "Fu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Jian",
                "middle": [],
                "last": "Kang",
                "suffix": "",
                "affiliation": {},
                "email": "jiankang@umich.edu."
            }
        ]
    },
    "abstract": [
        {
            "text": "As one of the most important estimators in classical statistics, the uniformly minimum variance unbiased estimator (UMVUE) has been adopted for point estimation in many statistical studies, especially for small sample problems. Moving beyond typical settings in the exponential distribution family, it is usually challenging to prove * Tianyu Zhan is an employee of AbbVie Inc. the existence and further construct such UMVUE in finite samples. For example in the ongoing Adaptive COVID-19 Treatment Trial (ACTT), it is hard to characterize the complete sufficient statistics of the underlying treatment effect due to pre-planned modifications to design aspects based on accumulated unblinded data. As an alternative solution, we propose a Deep Neural Networks (DNN) guided ensemble learning framework to construct an improved estimator from existing ones. We show that our estimator is consistent and asymptotically reaches the minimal variance within the class of linearly combined estimators. Simulation studies are further performed to demonstrate that our proposed estimator has considerable finite-sample efficiency gain. In the ACTT on COVID-19 as an important application, our method essentially contributes to a more ethical and efficient adaptive clinical trial with fewer patients enrolled.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Identifying the potential uniformly minimum variance unbiased estimator (UMVUE) of an unknown parameter is one of the most fundamental and important problems in statistics. It provides uniformly no larger variance than any other unbiased estimators in the parameter space considered (Lehmann and Casella, 2006) . However, its existence and characterization are usually challenging to investigate when one moves beyond exponential families. For example in the ongoing Adaptive COVID-19 Treatment Trial (ACTT), adaptive clinical trials are appealing to accommodate uncertainty with limited knowledge of the treatment profiles by allowing prospectively planned modifications to design aspects based on accumulated unblinded data (Bretz et al., 2009; Chen et al., 2010 Chen et al., , 2014 National Institutes of Health, 2020a) . One is interested in an unbiased estimator of the underlying treatment effect to have an accurate assessment of the efficacy of the study drug, but traditional estimators are often biased (Bretz et al., 2009) . Although several methods (Shen, 2001; Stallard et al., 2008) have been proposed to estimate the bias, its correction in adaptive design is still a less well-studied phenomenon, as acknowledged by the Food and Drug Administration European Medicines Agency (2007)]. Moving beyond, the next question is how to identify a more efficient unbiased estimator of the treatment effect, which further contributes to a more ethical and efficient adaptive clinical trial with fewer patients enrolled.",
            "cite_spans": [
                {
                    "start": 283,
                    "end": 310,
                    "text": "(Lehmann and Casella, 2006)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 726,
                    "end": 746,
                    "text": "(Bretz et al., 2009;",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 747,
                    "end": 764,
                    "text": "Chen et al., 2010",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 765,
                    "end": 784,
                    "text": "Chen et al., , 2014",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 785,
                    "end": 822,
                    "text": "National Institutes of Health, 2020a)",
                    "ref_id": null
                },
                {
                    "start": 1013,
                    "end": 1033,
                    "text": "(Bretz et al., 2009)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1061,
                    "end": 1073,
                    "text": "(Shen, 2001;",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1074,
                    "end": 1096,
                    "text": "Stallard et al., 2008)",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Since the complete sufficient statistics can be hard to characterize or do not even exist in many problems (Lehmann and Casella, 2006) , we consider an alternative perspective by constructing a better estimator in finite samples from existing unbiased estimators. This is motivated by the spirit of ensemble learning to build a prediction model by combining the strengths of a collection of simpler base models (Biau, 2012; Bradic et al., 2016; Katzfuss et al., 2016; McDermott and Wikle, 2017; Biau et al., 2019; Tian and Feng, 2021) . For example, the XGBoost algorithm (Chen and Guestrin, 2016) is one of the most powerful algorithms in machine learning literature as a scalable end-to-end tree boosting system .",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 134,
                    "text": "(Lehmann and Casella, 2006)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 411,
                    "end": 423,
                    "text": "(Biau, 2012;",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 424,
                    "end": 444,
                    "text": "Bradic et al., 2016;",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 445,
                    "end": 467,
                    "text": "Katzfuss et al., 2016;",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 468,
                    "end": 494,
                    "text": "McDermott and Wikle, 2017;",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 495,
                    "end": 513,
                    "text": "Biau et al., 2019;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 514,
                    "end": 534,
                    "text": "Tian and Feng, 2021)",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 572,
                    "end": 597,
                    "text": "(Chen and Guestrin, 2016)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this article, we propose a novel Deep Neural Networks (DNN) guided ensemble learning framework to provide point estimation on the parameters of interest. DNN is becoming more popular in biomedical fields in recent years due to its strong functional representation (She et al., 2014; Brahma et al., 2015; Liang et al., 2018; Lu et al., 2018; Rava and Bradic, 2020; Bai et al., 2020; Chao et al., 2020; Chen et al., 2020; Wu et al., 2020; Yuan et al., 2020; Gao and Wang, 2021) . In this article, we leverage DNN to approximate the optimal weights of linearly integrating unbiased estimators to achieve minimum variance in finite sample size. We show that the bias of our estimator is asymptotically zero, and the mean squared error (MSE) converges to the optimal variance within the class of linearly combined estimators. Simulations demonstrate that our proposed estimator achieves considerable finite-sample efficiency gain, for example in the scale-uniform distribution considered in Section 5.1 where the Cram\u00e9r-Rao bound is not satisfied, and in the regression model for analyzing heterogeneous data in Section 5.2. We further apply our method to the ACTT on COVID-19 to provide a more accurate estimate on the underlying treatment effect and to consistently achieve higher power of detecting a promising treatment effect than several alternatives in the context of hypothesis testing. This makes our method appealing in practice, because a more ethical trial with fewer patients enrolled can be implemented to deliver a safe and efficacious drug to patients more efficiently.",
            "cite_spans": [
                {
                    "start": 267,
                    "end": 285,
                    "text": "(She et al., 2014;",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 286,
                    "end": 306,
                    "text": "Brahma et al., 2015;",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 307,
                    "end": 326,
                    "text": "Liang et al., 2018;",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 327,
                    "end": 343,
                    "text": "Lu et al., 2018;",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 344,
                    "end": 366,
                    "text": "Rava and Bradic, 2020;",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 367,
                    "end": 384,
                    "text": "Bai et al., 2020;",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 385,
                    "end": 403,
                    "text": "Chao et al., 2020;",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 404,
                    "end": 422,
                    "text": "Chen et al., 2020;",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 423,
                    "end": 439,
                    "text": "Wu et al., 2020;",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 440,
                    "end": 458,
                    "text": "Yuan et al., 2020;",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 459,
                    "end": 478,
                    "text": "Gao and Wang, 2021)",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The remainder of this article is organized as follows. In Section 2, we introduce our framework of constructing an ensemble estimator with improved efficiency. Next we propose 4 an algorithm based on DNN to approximate the optimal weight parameters in Section 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In Section 4, we provide the upper bounds of the bias and the MSE of our estimator. Three experiments including the ACTT on COVID-19 are conducted in Section 5 to demonstrate our superior finite sample performance. Concluding remarks are provided in Section 6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our parameter of interest is \u03b8 under an open and bounded \u0398 \u2286 R. For illustration, \u03b8 is considered as a scalar quantity, but our proposed method can be readily applied to a vector as considered in the regression problem at Section 5.2. Let x = (x 1 , \u00b7 \u00b7 \u00b7 , x n ) be independent and identically distributed (i.i.d.) random variables given on the probability",
            "cite_spans": [],
            "ref_spans": [],
            "section": "An ensemble estimator"
        },
        {
            "text": "where \u2126 x is a compact set in R and P x = p(x; \u03b8, \u03c9) is the probability function. The nuisance parameters \u03c9 is of d \u2212 1 dimension with an open and bounded support \u2126 \u2286 R d\u22121 and d is an integer larger than 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "An ensemble estimator"
        },
        {
            "text": "for all \u03b8 \u2208 \u0398 (Lehmann and Casella, 2006) . Without being further specified, the expectation E(\u00b7) is with respect to P x . If there exists such an unbiased estimator T (x) satisfying (1), then the estimand \u03b8 is U-estimable. An unbiased estimator is the uniformly minimum variance unbiased estimator (UMVUE) if it has no larger variance than any other unbiased estimators of \u03b8 for all \u03b8 \u2208 \u0398 (Lehmann and Casella, 2006) . Despite attractive features of UMVUE, the existence and characterization are usually challenging to investigate when one moves beyond exponential families.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 41,
                    "text": "(Lehmann and Casella, 2006)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 390,
                    "end": 417,
                    "text": "(Lehmann and Casella, 2006)",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "An ensemble estimator"
        },
        {
            "text": "In this article, we propose an alternative approach to construct an improved unbiased estimator with smaller variance by ensembling two existing ones via Deep Neural Networks (DNN). Let T 1 (x) and T 2 (x) be two unbiased estimators of \u03b8. We construct U (x) by a linear combination of them,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "where w \u2208 R. The optimal weight w {opt} is the one that minimizes the variance of U (x; w)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "for w \u2208 R,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "Since both T 1 (x) and T 2 (x) are unbiased for \u03b8 and w {opt} is a constant with respect to observed data x, then U x; w {opt} is also unbiased. The construction in (3) ensures that U x; w {opt} has the smallest variance among all U (x; w) in (2) for w \u2208 R. We provide the variance reduction in the following Proposition 1 with proof in the Supplemental Materials Section 1. For simplicity, \"(x)\" is removed from the notations of T 1 (x) and T 2 (x).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "Proposition 1 The variance reduction \u039b(w) of estimating \u03b8 by U x; w {opt} with w {opt} in (3) as compared with U (x; w) in (2) is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "Note that this variance improvement \u039b(w) is non-negative with \u039b w {opt} = 0. In some problems where (3) is free from \u03b8 and \u03c9 or can be evaluated in a closed form, the solution of w {opt} is straightforward -for example, on estimating the mean of a normal distribution with known coefficient of variation based on two unbiased estimators from the sample mean and the sample variance (Khan, 2015) . In general, w {opt} in (3) is a function of \u03b8, \u03c9 and sample size n, but does not necessarily have an analytic solution. For many problems, we do not have closed forms of the distributions of T 1 and T 2 , and thus the direct computation is not feasible. For some other problems, T 1 and T 2 themselves do not have closed forms, and making the computation even harder.",
            "cite_spans": [
                {
                    "start": 382,
                    "end": 394,
                    "text": "(Khan, 2015)",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "While it is usually feasible to empirically estimate w {opt} given underlying \u03b8 and \u03c9, our goal is to estimate w {opt} and further construct improved statistics based on observed data",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "x with given sample size n. We further denote \u03c6 = (\u03b8, \u03c9). In the next Section 3, we introduce our proposed algorithm for approximating w {opt} (\u03c6) by DNN from x.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "We first provide a short review on Deep Neural Networks (DNN) in Section 3.1, and then demonstrate in Section 3.2 that there exists a DNN structure which can well approximate the underlying w {opt} (\u03c6) to a desired level of accuracy. In the next section 3.3, we illustrate our DNN-based algorithm to estimate w {opt} (\u03c6).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A DNN-based algorithm to approximate w {opt}"
        },
        {
            "text": "Deep learning is a specific subfield of machine learning with a major application to approximate a function y = w(\u03c6) (Goodfellow et al., 2016) . We restrict our attention to the so-called deep feedforward networks or feedforward neural networks, which define a mapping function y = w(\u03c6; \u03b7) and learn the value of parameters \u03b7 that result in the best function approximation, where \u03b7 denotes a stack of the weights and bias parameters in the neural networks with dimension d \u03b7 .",
            "cite_spans": [
                {
                    "start": 117,
                    "end": 142,
                    "text": "(Goodfellow et al., 2016)",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Review on Deep Neural Networks (DNN)"
        },
        {
            "text": "In earlier years, it has been shown that a shallow neural network with sigmoid activation function can approximate any continuous function to a desired accuracy with sufficiently large number of nodes (Cybenko, 1989) , and then interest shifted towards deeper networks with a better generalizability (Liang et al., 2018; Lu et al., 2018; Chen et al., 2020; Rava and Bradic, 2020; Wu et al., 2020) . The upper bounds were also investigated on the approximation error of Lipschitz-continuous functions (Bach, 2017; Xu and Wang, 2018; Chen et al., 2019) , and functions in Sobolev spaces (Yarotsky, 2017) .",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 216,
                    "text": "(Cybenko, 1989)",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 300,
                    "end": 320,
                    "text": "(Liang et al., 2018;",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 321,
                    "end": 337,
                    "text": "Lu et al., 2018;",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 338,
                    "end": 356,
                    "text": "Chen et al., 2020;",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 357,
                    "end": 379,
                    "text": "Rava and Bradic, 2020;",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 380,
                    "end": 396,
                    "text": "Wu et al., 2020)",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 500,
                    "end": 512,
                    "text": "(Bach, 2017;",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 513,
                    "end": 531,
                    "text": "Xu and Wang, 2018;",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 532,
                    "end": 550,
                    "text": "Chen et al., 2019)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 585,
                    "end": 601,
                    "text": "(Yarotsky, 2017)",
                    "ref_id": "BIBREF53"
                }
            ],
            "ref_spans": [],
            "section": "Review on Deep Neural Networks (DNN)"
        },
        {
            "text": "Consider a motivating example of a DNN with two hidden layers in Figure 1 . The input parameter \u03c6 has a dimension d = 2 on the left, with a scaler output y on the right. We follow the notations in Anthony and Bartlett (2009) to characterize the complexity of a DNN structure. In this simple architecture, there are 6 computation units from the two hidden layers, a total of 18 weights parameters, and 7 bias parameters. Therefore, the dimension of \u03b7 is d \u03b7 = 25. We further define n (l) as the depth of DNN and n (w) as the total number of computation unites, weights and bias parameters. In Figure 1 we have n (l) = 4 and n (w) = 31.",
            "cite_spans": [
                {
                    "start": 197,
                    "end": 224,
                    "text": "Anthony and Bartlett (2009)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [
                {
                    "start": 65,
                    "end": 73,
                    "text": "Figure 1",
                    "ref_id": null
                },
                {
                    "start": 592,
                    "end": 600,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Review on Deep Neural Networks (DNN)"
        },
        {
            "text": "We utilize DNN to construct a mapping function w : A.2 E (T 1 ) 2 ; \u03c6 , E (T 2 ) 2 ; \u03c6 and E (T 1 T 2 ; \u03c6) are Lipschitz continuous on \u03a6 for some constants c 1 , c 2 and c 12 , respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "A.3 T 1 and T 2 have finite second moments bounded by b 1 and b 2 , respectively, for \u03c6 \u2208 \u03a6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "Remarks: Condition A.1 specifies that the parameter space \u03a6 is open and bounded with a continuously differentiable boundary (Evans, 2010) . Condition A.2 requires that the second",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 137,
                    "text": "(Evans, 2010)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "for some constant C and every x, y \u2208 U . This condition is weaker than differentiation but stronger than continuity. Consider an example where x of size n follow a normal distribution with mean zero and variance \u03c3 2 , and T 1 is the sample mean with E (T 1 ) 2 = \u03c3 2 /n. It can be shown that C = 1/n satisfies the above definition for every \u03c3 2 \u2208 U \u2286 R + . This condition is usually satisfied by T 1 and T 2 in common statistical models. These two base statistics are required to have finite second moments in Condition A.3. The fourth condition A.4",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "requires that the variance of T 1 \u2212 T 2 is lower bounded by a positive constant. A trivial counterexample is that the variance of T 1 \u2212 T 2 becomes zero when T 1 = T 2 . We provide more discussion on how to choose T 1 and T 2 in practice in Section 4.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "In the following Proposition 2, we show that under those four regularity conditions, there exists a DNN w(\u03c6; \u03b7 0 ) with finite n (l) and n (w) that can well approximate w {opt} with the uniform maximum error defined by,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "Proposition 2 Under regularity conditions A.1 -A.4, for a given dimension d and an error tolerance d \u2208 (0, 1), there exists a DNN w(\u03c6; \u03b7 0 ) with underling \u03b7 0 and ReLU activation function that is capable of expressing w {opt} with the uniform maximum error",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "The DNN has a finite number of layers n (l) , finite total number of computation unites, weight and bias parameters n (w) , which satisfy",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "The proof is provided in the Supplemental Materials Section 2. Our contribution is to show that the objective function w {opt} in (3) is Lipschitz continuous for \u03c6 \u2208 \u03a6 under those four regularity conditions. Therefore, it belongs to a Sobolev space",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "is the respective weak derivative, and \"ess sup\" is the essential supremum (Evans, 2010) . The norm w W 1,\u221e (\u03a6) in (6) is denoted as c d . Furthermore, the upper bounds on n (l) and n (w) of approximating functions in Sobolev spaces are obtained from Theorem 1 in Yarotsky (2017).",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 88,
                    "text": "(Evans, 2010)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Approximation error bound of DNN"
        },
        {
            "text": "In the previous section, we have shown that there exists a DNN w(\u03c6; \u03b7 0 ) that can well approximate w {opt} (\u03c6) with a controlled uniform maximum error in (5) in Proposition 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A DNN-based algorithm"
        },
        {
            "text": "The next question is how to estimate \u03b7 0 by \u03b7 to construct a learnable DNN w(\u03c6; \u03b7) using data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A DNN-based algorithm"
        },
        {
            "text": "Step 1 of Algorithm 1, we construct input data of DNN as {\u03c6 m } M m=1 , and the output label as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "working multivariate probability function P \u03c6 is usually set as some flat distributions to let simulated {\u03c6 m } M m=1 spread within the support \u03a6. In the remainder of this article, we draw each of the d elements in \u03c6 m for m = 1, \u00b7 \u00b7 \u00b7 , M from d separate uniform distributions under its corresponding support in \u03a6. The output label is w(\u03c6 m ) as an estimate of the underlying w {opt} (\u03c6 m ), whose functional form is usually unknown. It can be obtained from the numerical integration method if the joint distribution of T 1 and T 2 is known, or it can be estimated by the sparse grid method in a high-dimensional setting (Shen and Yu, 2010; Zhang et al., 2015) , or by Monte Carlo samples. For a general demonstration, we obtain",
            "cite_spans": [
                {
                    "start": 622,
                    "end": 641,
                    "text": "(Shen and Yu, 2010;",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 642,
                    "end": 661,
                    "text": "Zhang et al., 2015)",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "where x i of size n are drawn from the distribution function p(x; \u03c6 m ), for i = 1, \u00b7 \u00b7 \u00b7 , N .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "Given the underlying parameters \u03c6 m , it is usually feasible to compute w(\u03c6 m ) in (7) as a consistent estimator of w {opt} (\u03c6 m ) in (3). However, one is more interested in estimating",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "and the working model is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "where e m converges in probability to zero as M goes to infinity, and e m , for m = 1, \u00b7 \u00b7 \u00b7 , M , are assumed to be i.i.d. random errors with zero mean and finite variance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "In statistics, fitting a neural network can be viewed as a nonlinear regression problem to find the least squared estimator \u03b7 of \u03b7 0 (White, 1990; Shen et al., 2019) , where \u03b7 is given by",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 146,
                    "text": "(White, 1990;",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 147,
                    "end": 165,
                    "text": "Shen et al., 2019)",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "and H is a compact subset of R d\u03b7 ; and recall that d \u03b7 is the dimension of \u03b7. There are many challenges in obtaining \u03b7 and further studying its properties. The structure of DNN w(\u03c6; \u03b7 0 ) in (9) which satisfies the approximation error bound in Proposition 2 is usually unknown. The identifiability of \u03b7 0 is questionable if the structure of w(\u03c6; \u03b7) in (9) is arbitrarily complex (White, 1990 properties of a least squared estimator under sub-Gaussian errors e m in (9) with one hidden layer and sigmoid activation function. Furthermore, since the loss function in (10) is usually non-convex, identifying \u03b7 is a challenging and active field in machine learning (Goodfellow et al., 2016) . We utilize the RMSProp for the DNN fitting at Step 2 and 3, as it has been shown to be an effective and practical optimization algorithm (Hinton et al., 2012; Goodfellow et al., 2016) .",
            "cite_spans": [
                {
                    "start": 380,
                    "end": 392,
                    "text": "(White, 1990",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 661,
                    "end": 686,
                    "text": "(Goodfellow et al., 2016)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 826,
                    "end": 847,
                    "text": "(Hinton et al., 2012;",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 848,
                    "end": 872,
                    "text": "Goodfellow et al., 2016)",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "It is important to select a proper DNN structure by cross-validation at",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "Step 2 and L 2 regularization methods, on the over-saturated DNN structure to decrease validation MSE while keeping the training MSE below a certain tolerance, for example 10 \u22125 . Several candidates around this sub-optimal structure can be proposed, and the final structure at",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "Step 3 is selected by cross-validation with the smallest validation MSE from this candidate pool to obtain the fitted DNN w(\u03c6; \u03b7).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "For a generic conclusion, we consider the approximation error of w(\u03c6; \u03b7) as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "where d is the specified tolerance in Proposition 2, and w is the approximation error of estimating w(\u03c6; \u03b7 0 ) by w(\u03c6; \u03b7). To accommodate a more general distribution on e m in addition to Shen et al. (2019) 's work, we consider an alternative perspective on this problem to adopt existing results on non-linear regression (Jennrich, 1969; Wu, 1981 (7).",
            "cite_spans": [
                {
                    "start": 188,
                    "end": 206,
                    "text": "Shen et al. (2019)",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 322,
                    "end": 338,
                    "text": "(Jennrich, 1969;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 339,
                    "end": 347,
                    "text": "Wu, 1981",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "Step 2. Conduct cross validation to select a proper DNN structure class F with the training datasets of size 80% \u00d7 M and the validation datasets of size 20% \u00d7 M .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "Step 3. Utilize the RMSProp algorithm to train DNN with the selected structure to get \u03b7. Compute w(\u03c6; \u03b7) as an approximating function of w {opt} (\u03c6).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "At"
        },
        {
            "text": "In Section 4.1, we illustrate how to construct the ensemble estimator U (x; w) following the DNN training in Algorithm 1. Its bias and MSE are further studied at Section 4.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Point estimation of \u03b8"
        },
        {
            "text": "After obtaining w(\u03c6; \u03b7) as an estimate of w {opt} (\u03c6) from Algorithm 1, we are now ready to construct the ensemble estimator. We denote the variance of T 1 and T 2 in (2) based on x of size n as V 1 and V 2 , respectively. Suppose that V 1 and V 2 can be decomposed as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Construct the ensemble estimator U (x; w)"
        },
        {
            "text": "where r and t are positive constants, and the leading terms c (n) r (\u03b8, \u03c9) and c (n) t (\u03b8, \u03c9) are positive as well. For example, if T 1 (x) is the sample mean of x drawn from a Normal distribution with mean \u00b5 and variance \u03c3 2 , then V 1 = \u03c3 2 /n with c (n) r (\u00b5, \u03c3) = \u03c3 2 and r = 1. Without loss of generality, we assume that V 1 \u2264 V 2 , which means that T 1 (x) is a more precise unbiased estimator as compared with T 2 (x) at the current sample size n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Construct the ensemble estimator U (x; w)"
        },
        {
            "text": "Suppose that there exists an unbiased or consistent estimator \u03c9(x) of the nuisance parameters \u03c9. For an observed data vector x, we can use (T 1 , \u03c9) to estimate \u03c6 = (\u03b8, \u03c9), and therefore w (T 1 , \u03c9; \u03b7) approximates w {opt} (\u03c6). Following Algorithm 2, we plug w (T 1 , \u03c9; \u03b7) to equation (2), and compute the ensemble estimator of \u03b8 as U [x; w (T 1 , \u03c9; \u03b7)].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Construct the ensemble estimator U (x; w)"
        },
        {
            "text": "Algorithm 2 Point estimate of \u03b8 based on observed data x",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Construct the ensemble estimator U (x; w)"
        },
        {
            "text": "Step 1. Compute the two base estimators T 1 and T 2 of \u03b8, and \u03c9 of \u03c9.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Construct the ensemble estimator U (x; w)"
        },
        {
            "text": "Step 2. Use w (T 1 , \u03c9; \u03b7) to estimate w {opt} .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Construct the ensemble estimator U (x; w)"
        },
        {
            "text": "Step 3. Construct the ensemble estimator of \u03b8 as U [x; w (T 1 , \u03c9; \u03b7)].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Construct the ensemble estimator U (x; w)"
        },
        {
            "text": "For illustrating purposes, we assume that \u03c9(x) is an unbiased estimator of the nuisance parameter \u03c9. The following results can be generalized to cases where \u03c9(x) is consistent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "We first introduce two additional conditions before discussing the bias and MSE of the ensemble estimator, B.1 The maximum element-wise variance of \u03c9(x) is denoted as V \u03c9 , and it is finite with the following form,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "where s and c B.2 The first order partial derivative \u2202 w(\u03c6; \u03b7)/\u2202\u03c6 and \u2202 w(\u03c6; \u03b7)/\u2202\u03b7 are upper bounded at c \u03c6 and c \u03b7 , respectively, for \u03c6 \u2208 \u03a6 and \u03b7 \u2208 H.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "The notation of V \u03c9 in Condition B.1 is analog to V 1 for T 1 in (12) ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "where V 1 is defined in (12), V 2 in (13), and V \u03c9 in (14). The mean squared error (MSE) of",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "We provide some remarks on the upper bound of the absolute bias in (15). The first part can be arbitrarily small by increasing the training data size M and the number of Monte Carlo samples N in Algorithm 1 and choosing a sufficiently small d in Proposition 2 as discussed in Section 3.3. We further denote",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "as the maximum finite variance of T 1 , T 2 and \u03c9. The second part shrinks as the sample size n of x increases based on V in (20). As further demonstrated in the following three experiments, this bias is relatively small in our evaluated finite-sample settings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "On the mean squared error (MSE) of U (x; w [T 1 , \u03c9; \u03b7]), we compute its MSE improvement as compared with the component T 1 . Note that T 1 has a smaller variance than the other component T 2 as specified in Section 4.1. The reduction of MSE is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "The MSE improvement of U (x; w [T 1 , \u03c9; \u03b7]) is lowered bounded by (21) In practical problems where UMVUE is unknown or does not exist, we suggest applying our proposed method to combine two estimators with a relatively small correlation to obtain a more precise estimator with reduced MSE. If there are more than two candidate unbiased estimators, we suggest using the two with the smallest empirical variances to get a smaller V in (20). This construction also ensures that U with the underlying w {opt} (\u03c6) is at least as accurate as either one of them. As a generalization, one can also iteratively apply our algorithm to identify a better statistic with more than two base estimators if necessary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bias and MSE of U (x; w)"
        },
        {
            "text": "In this section, we evaluate the performance of our proposed ensemble estimator in three examples. Section 5.1 considers the scale-uniform distribution, and Section 5.2 assesses a regression model for analyzing heterogeneous data to show our finite sample efficiency gain.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "In Section 5.3, we apply our analysis method to the Adaptive COVID-19 Treatment Trial (ACTT) to make it more efficient and ethical.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "We use U nif to denote the Uniform distribution, and consider the scale-uniform distribution U nif [1\u2212k]\u03b8, [1+k]\u03b8 with the parameter of interest \u03b8 and a known design parameter k \u2208 (0, 1) (Galili and Meilijson, 2016) . This type of distribution has wide applications, for example the product inventory management in economics (Wanke, 2008) and the inverse transform sampling (Vogel, 2002) .",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 215,
                    "text": "(Galili and Meilijson, 2016)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 325,
                    "end": 338,
                    "text": "(Wanke, 2008)",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 374,
                    "end": 387,
                    "text": "(Vogel, 2002)",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "Scale-uniform family of distributions"
        },
        {
            "text": "We are interested in making inference on \u03b8 using sample x = (x 1 , \u00b7 \u00b7 \u00b7 , x n ) of size n with the support \u2126 x = {x \u2208 R : p x (x; \u03b8, k) > 0}, where p x (x; \u03b8, k) denotes the probability",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scale-uniform family of distributions"
        },
        {
            "text": "Since the support \u2126 x is not the same for all \u03b8 \u2208 \u0398 with \u0398 as an open interval in R, this distribution family does not satisfy the usual differentiability assumptions leading to the Cram\u00e9r-Rao bound and efficiency of maximum likelihood estimators (MLEs; Lehmann and Casella (2006) , Galili and Meilijson (2016) ).",
            "cite_spans": [
                {
                    "start": 254,
                    "end": 280,
                    "text": "Lehmann and Casella (2006)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 283,
                    "end": 310,
                    "text": "Galili and Meilijson (2016)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Scale-uniform family of distributions"
        },
        {
            "text": "We apply the proposed method to construct a more efficient estimator of \u03b8 based on existing ones.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scale-uniform family of distributions"
        },
        {
            "text": "As a starting point, we utilize the Rao-Blackwell theorem to construct the first base unbiased estimator T 1 . The minimal sufficient statistic for \u03b8 is x (1) , x (n) , where x (1) = min(x) and x (n) = max(x). Since x 1 is unbiased for \u03b8, then an improved unbiased estimator based on the Rao-Blackwell theorem is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scale-uniform family of distributions"
        },
        {
            "text": "The second base estimator T 2 is set as \u03b8 M ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scale-uniform family of distributions"
        },
        {
            "text": "which is the unbiased corrected version of the MLE \u03b8 M LE = x (n) /(k + 1) (Galili and Meilijson, 2016) , Utilizing our proposed method, we ensemble T 1 = \u03b8 RB and T 2 = \u03b8 M in (2) to construct a better estimator U \u03b8 RB , \u03b8 M with a smaller variance. Suppose we are interested in \u03b8 \u2208 \u0398 = (0.2, 10) as an open interval in R with finite data size n = 2 or 10. For a given n, we simulate M = 10 3 training input data for DNN with varying \u03b8 \u223c U nif (0.2, 10) and the known parameter k at either 0.1 or 0.9 to accommodate the scenarios considered at Table 1 for evaluating performance. Note that the above training data sample spaces can be set wider as needed. The input data of DNN is \u03c6 = (\u03b8, k), and the output label w(\u03c6) in (7) is evaluated by N = 10 6 Monte Carlo samples. In cross-validation, we consider 4 candidate DNN structures: 2 hidden layers with 40 nodes per layer, 2 hidden layers with 20 60 nodes per layer, 3 hidden layers with 40 nodes per layer, 3 hidden layers with 60 nodes per layer, and select the structure with the smallest validation MSE for final DNN training.",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 103,
                    "text": "(Galili and Meilijson, 2016)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [
                {
                    "start": 545,
                    "end": 552,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Scale-uniform family of distributions"
        },
        {
            "text": "We use a dropout rate of 0.1, number of training epochs at 10 3 , and a batch size of 100 in the training process to obtain a fitted DNN w(\u03c6; \u03b7). The number of simulation iterations for testing at Table 1 is 10 6 . The above setup parameters of training DNN are utilized throughout this article if not specified otherwise.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 197,
                    "end": 204,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Scale-uniform family of distributions"
        },
        {
            "text": "Under all scenarios considered in Table 1 , the relative bias of U \u03b8 RB , \u03b8 M is less than 10 \u22123 (results not shown). To further evaluate the efficiency gain of our method, we compute the relative efficiency of U \u03b8 RB , \u03b8 M versus three existing estimators: \u03b8 RB in (22), \u03b8 M in (23) and \u03b8 E as the sample mean. The relative efficiency of two estimators is defined as the inverse ratio of their variances. The ensemble estimator U \u03b8 RB , \u03b8 M is uniformly more efficient than three comparators as demonstrated by all ratios larger than 1. Within the same n, one observes that \u03b8 RB is more efficient than \u03b8 M when k = 0.1, and vice versa when k = 0.9. Our U \u03b8 RB , \u03b8 M learns their advantages under different k's and shows a consistently better performance.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 34,
                    "end": 41,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Scale-uniform family of distributions"
        },
        {
            "text": "Aggregating and analyzing heterogeneous data is one of the most fundamental challenges in scientific data analysis . For observable X \u2208 R d and a discrete variable Z \u2208 Z, a general mixture model assumes, for a distribution F with parameters \u03b8 z in the sub-population z . The variable Z can be known in some applications, for example on synthesizing control information from multiple historical clinical trials (Neuenschwander et al., 2010) ; or it can be latent in general (Fan et al., 2014) .",
            "cite_spans": [
                {
                    "start": 410,
                    "end": 439,
                    "text": "(Neuenschwander et al., 2010)",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 473,
                    "end": 491,
                    "text": "(Fan et al., 2014)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Regression model for analyzing heterogeneous data"
        },
        {
            "text": "In this motivating simulation study, we consider the following Gaussian regression model where the variance of the dependent variable is proportional to the square of its expected value (Amemiya, 1973; Ramanathan, 2002) ,",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 201,
                    "text": "(Amemiya, 1973;",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 202,
                    "end": 219,
                    "text": "Ramanathan, 2002)",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "Regression model for analyzing heterogeneous data"
        },
        {
            "text": "where x i is a vector of covariates for subject i, and \u03b8 is a vector of unknown parameters. This type of model has wide applications in economics and operational research, for example understanding the influence of customer demographics on the rent paid (Anderson and Jaggia, 2009), and modeling efficiency scores in censoring data generating process (McDonald, 2009) .",
            "cite_spans": [
                {
                    "start": 351,
                    "end": 367,
                    "text": "(McDonald, 2009)",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Regression model for analyzing heterogeneous data"
        },
        {
            "text": "Challenges exist in this problem to find an efficient unbiased estimator of \u03b8 in finite samples. The minimal sufficient statistics consisting of sample mean and sample variance are not complete for \u03b8 \u2208 \u0398 (Khan, 2015) . When x i \u03b8 is relatively small, the Fisher information matrix can be ill-conditioned (Amemiya, 1973) , which introduces bias in the maximum likelihood estimator (MLE). As robust alternatives, Amemiya (1973) considers the following two unbiased estimators,",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 216,
                    "text": "(Khan, 2015)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 304,
                    "end": 319,
                    "text": "(Amemiya, 1973)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 411,
                    "end": 425,
                    "text": "Amemiya (1973)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Regression model for analyzing heterogeneous data"
        },
        {
            "text": "where \u03b8 L is the least square estimator and \u03b8 W is the weighted least square estimators. To avoid extreme values in practice, we upper bound the weight 1/ x i \u03b8 L 2 by 10 5 . Matrix n i=1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regression model for analyzing heterogeneous data"
        },
        {
            "text": "x i x i is assumed to be positive definite and x i is bounded for i = 1, \u00b7 \u00b7 \u00b7 , n. We utilize our proposed method to assemble \u03b8 W as T 1 and \u03b8 L as T 2 to get a DNN-based estimator U \u03b8 W , \u03b8 L with smaller variance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regression model for analyzing heterogeneous data"
        },
        {
            "text": "In this simulation study, we consider that \u03b8 is a four-dimensional vector with \u03b8 1 as intercept and \u03b8 2 , \u03b8 3 and \u03b8 4 as coefficients. Further denote U 1 , U 2 , U 3 and U 4 as the elements in our ensemble estimator: U \u03b8 W , \u03b8 L = (U 1 , U 2 , U 3 , U 4 ). The parameter space for \u03b8 1 , \u03b8 2 , \u03b8 3 and \u03b8 4 are considered at \u0398 = (\u22121.5, 1.5). Covariates x i , for i = 1, \u00b7 \u00b7 \u00b7 , n, is simulated from a uniform distribution with a lower bound \u22122 and an upper bound 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regression model for analyzing heterogeneous data"
        },
        {
            "text": "A moderate sample size n = 100 is evaluated in this study. In Algorithm 2, we simulate M = 10 3 training input data for DNN as \u03c6 = (\u03b8 1 , \u03b8 2 , \u03b8 3 , \u03b8 4 ) with varying \u03b8 1 , \u03b8 2 , \u03b8 3 and \u03b8 4 from uniform distributions within \u0398. The number of Monte Carlo samples is N = 10 5 when computing w(\u03c6) in (7). In the testing stage, we evaluate different patterns of \u03b8 with three magnitudes at 0.2, 0.6 and 1.2 at Table 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regression model for analyzing heterogeneous data"
        },
        {
            "text": "The absolute relative bias of our estimator is less than 0.02 across all scenarios. Table   2 shows the finite sample efficiency, where \u03b8 W is generally more efficient than \u03b8 L and can be less efficient on estimating \u03b8 4 in some cases. Our proposed estimator U \u03b8 W , \u03b8 L is consistently more efficient than its two components \u03b8 W and \u03b8 L under all scenarios (relative efficiencies are larger than one).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 84,
                    "end": 93,
                    "text": "Table   2",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Regression model for analyzing heterogeneous data"
        },
        {
            "text": "In this section, we apply our method to the Adaptive COVID-19 Treatment Trial (ACTT) to evaluate the safety and efficacy of remdesivir from Gilead Inc. in hospitalized adults diagnosed with COVID-19 (National Institutes of Health, 2020a). Adaptive clinical trials are appealing under the COVID-19 pandemic with limited knowledge on treatment profiles under evaluation, because they are capable of accommodating uncertainty during study conduction. As acknowledged by regulatory agencies (Food and Drug Administration, 2019; European Medicines Agency, 2007) , the bias correction in adaptive design is still a less well-studied phenomenon. Our proposed method not only provides a solution for this problem to have an accurate understanding of the treatment effect, but also improves finite Relative efficiency versus \u03b8 W sample efficiency of such estimators to make adaptive designs more efficient and ethical.",
            "cite_spans": [
                {
                    "start": 487,
                    "end": 523,
                    "text": "(Food and Drug Administration, 2019;",
                    "ref_id": null
                },
                {
                    "start": 524,
                    "end": 556,
                    "text": "European Medicines Agency, 2007)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Adaptive COVID-19 Treatment Trial (ACTT)"
        },
        {
            "text": "For illustrative purposes, we consider the sample size reassessment adaptive design with a binary endpoint of achieving hospital discharge at Day 14 (National Institutes of Health, 2020b; Gilead Inc., 2020). Let \u03b8 1 be the response rate in the placebo, and \u03b8 2 be that from the treatment. The objective is to estimate the treatment effect \u03b8 = \u03b8 2 \u2212\u03b8 1 based on binary data from two groups. The underlying true \u03b8 1 = 0.47 and \u03b8 2 = 0.59 are assumed based on the preliminary interim results in National Institutes of Health (2020b).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "We consider a two-stage adaptive design, where n (1) subjects are randomized to the treatment group and n (1) subjects to the control group in the first stage. After evaluating unblinded interim data from those 2 \u00d7 n (1) subjects, a Data and Safety Monitoring Board (DSMB) makes sample size adjustments based on the following rule,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "is a vector of observed binary data of size n (h) for group j, j = 1, 2 at stage h, h = 1, 2, and n",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "(2) min , n",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "max and \u03b8 min are pre-specified design features. Basically, n (2) in the second stage will be decreased to n (2) min if a promising treatment effect larger than a clinically meaningful difference \u03b8 min is observed, but increased to n (2) max otherwise. Other adaptive rules can also be applied (Bretz et al., 2009) . Due to the pre-specified adjustment of n (2) based on the first stage data, it is challenging to determine the existence or to characterize the functional form of the complete sufficient statistics of \u03b8. The empirical treatment difference \u03b8 (x 2 ) \u2212 \u03b8 (x 1 ) is even a biased estimator of \u03b8 (Bretz et al., 2009) ",
            "cite_spans": [
                {
                    "start": 294,
                    "end": 314,
                    "text": "(Bretz et al., 2009)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 608,
                    "end": 628,
                    "text": "(Bretz et al., 2009)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "is the pooled data from two stages in group j, j = 1, 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "An unbiased estimator of \u03b8 can be constructed by the following weighted average of the treatment differences from two stages based on the conditional invariance principle (Bretz et al., 2009) ,",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 191,
                    "text": "(Bretz et al., 2009)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "where k \u2208 [0, 1] is a constant, and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "is an unbiased estimator of \u03b8 based on data at stage h, for h = 1, 2. The pre-specified weight k can be chosen to minimize the variance of \u03b8(k) in the study design stage given a working value of the true treatment effect \u03b8, but may lead to efficiency loss when observed data deviate. Using our proposed method, we ensemble T 1 = \u03b8(0.5) and T 2 = \u2206 (1) to deliver a more accurate unbiased estimator within a neighborhood of the underlying \u03b8.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "We consider \u03b8 1 \u2208 (0.2, 0.7) and \u03b8 \u2208 (\u22120.2, 0.3) as our parameter spaces, and n (1) = 100, n (2) min = 50, n",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "(2) max = 250 and \u03b8 min = 0.16 as design features in (26). Following Algorithm 2, we simulate M = 10 3 training input data for DNN with varying \u03b8 and \u03b8 1 from uniform distributions within their corresponding supports. The input data vector for DNN is \u03c6 = (\u03b8 1 , \u03b8). The performance of our DNN based estimator U \u03b8(0.5), \u2206 (1) is compared with three unbiased estimators \u03b8(0.2), \u03b8(0.5) and \u03b8(0.8) in (27) with k = 0.2, 0.5, and 0.8, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "In the first block of Table 3 , these 4 scenarios cover varying magnitudes of \u03b8 1 around its true value 0.47, and with \u03b8 2 = \u03b8 1 demonstrating no treatment effect. The next three blocks consider varying placebo rate \u03b8 1 and varying treatment effect \u03b8. Under all scenarios evaluated, our ensemble estimator has a relatively small bias \u2264 0.001. Among the three comparators, \u03b8(0.2) is more accurate when \u03b8 = 0, and \u03b8(0.5) is preferable when \u03b8 > 0. Our estimator is consistently more efficient than them, supported by the relative efficiency.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 22,
                    "end": 29,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "We then plot the power of rejecting the one-sided null hypothesis H 0 : \u03b8 \u2264 0 at a type I error rate \u03b1 = 0.05 under \u03b8 1 = 0.47 and varying treatment effect \u03b8 in Figure   2 . The critical values of rejecting H 0 are computed at 0.064 for our method, 0.064 for \u03b8(0.2), 0.068 for \u03b8(0.5), and 0.094 for \u03b8(0.8) by the grid search method to control validating type I error rates not exceeding 5% when \u03b8 1 = \u03b8 2 = 0.42, 0.5, 0.58, 0.66. Our proposed method has consistently higher power of detecting a promising treatment effect than the other three estimators. Therefore, a more efficient and more ethical adaptive clinical trial can be implemented based on our proposed method to evaluate treatment options to cure COVID-19. Table 3 : Small bias of U \u03b8(0.5), \u2206 (1) and its high relative efficiency compared with three unbiased estimators in the ACTT on COVID-19. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 161,
                    "end": 171,
                    "text": "Figure   2",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 720,
                    "end": 727,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Relative efficiency versus \u03b8"
        },
        {
            "text": "In this article, we propose a novel DNN-based ensemble learning method to improve finite sample efficiency of point estimation. As a critical application in the ACTT on COVID-19, our method is more efficient and has a higher power of detecting a promising treatment effect than several alternatives. The proposed method can contribute to a more ethical and efficient adaptive clinical trial with fewer patients enrolled.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Our construction in (2) is to get the best linear unbiased estimator when the optimal weight is known. In practice when the weight is to be estimated from data, we show that the bias approaches zero as sample size increases. This construction on correcting bias is preferred in many applications, for example in understanding the treatment effect of the study drug relative to placebo in the ACTT on COVID-19. Our method can be generalized to minimize other measures such as MSE, Bayes risk, et cetera. For instance, the Ridge estimator can be combined to reduce MSE by introducing a tolerable bias. One can also iteratively apply our method to integrate more than two base estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "There are some potential limitations of our method. The DNN-based approach requires additional training and computational time to obtain the estimator. It takes approximately 4 hours to simulate training and validation data to reproduce Table 3 in the case study.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 237,
                    "end": 244,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "However, the well-trained DNNs can be saved in files before observing current data. As illustrated in our shared code, one can instantly compute the weight parameter and construct the ensemble estimator with available functional form of DNNs. A future work is to make statistical inference of the parameters of interest based on the ensemble estimator.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Regression analysis when the variance of the dependent variable is proportional to the square of its expectation",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Amemiya",
                    "suffix": ""
                }
            ],
            "year": 1973,
            "venue": "Journal of the American Statistical Association",
            "volume": "68",
            "issn": "344",
            "pages": "928--934",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Rent-to-own agreements: Customer characteristics and contract outcomes",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Anderson",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jaggia",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Journal of Economics and Business",
            "volume": "61",
            "issn": "1",
            "pages": "51--69",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Neural network learning: Theoretical foundations",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Anthony",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "L"
                    ],
                    "last": "Bartlett",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Breaking the curse of dimensionality with convex neural networks",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Bach",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "The Journal of Machine Learning Research",
            "volume": "18",
            "issn": "1",
            "pages": "629--681",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Efficient variational inference for sparse deep learning with theoretical guarantee",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2011.07439"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Analysis of a random forests model",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Biau",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "The Journal of Machine Learning Research",
            "volume": "13",
            "issn": "1",
            "pages": "1063--1095",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Neural random forests",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Biau",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Scornet",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Welbl",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Sankhya A",
            "volume": "81",
            "issn": "2",
            "pages": "347--386",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Randomized maximum-contrast selection: Subagging for large-scale regression",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bradic",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Electronic Journal of Statistics",
            "volume": "10",
            "issn": "1",
            "pages": "121--170",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Why deep learning works: A manifold disentanglement perspective",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "P"
                    ],
                    "last": "Brahma",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "She",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE transactions on neural networks and learning systems",
            "volume": "27",
            "issn": "",
            "pages": "1997--2008",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Adaptive designs for confirmatory clinical trials",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Bretz",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Koenig",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Brannath",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Glimm",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Posch",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Statistics in Medicine",
            "volume": "28",
            "issn": "8",
            "pages": "1181--1217",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Directional pruning of deep neural networks",
            "authors": [
                {
                    "first": "S.-K",
                    "middle": [],
                    "last": "Chao",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xing",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.09358"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Theoretical investigation of generalization bound for residual networks",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Mo",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IJCAI",
            "volume": "",
            "issn": "",
            "pages": "2081--2087",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Bayesian clinical trials. Frontiers of Statistical Decision Making and Bayesian Analysis",
            "authors": [
                {
                    "first": "M.-H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "K"
                    ],
                    "last": "Dey",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "257--284",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Bayesian design of superiority clinical trials for recurrent events data with applications to bleeding and transfusion events in myelodyplastic syndrome",
            "authors": [
                {
                    "first": "M.-H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "G"
                    ],
                    "last": "Ibrahim",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Biometrics",
            "volume": "70",
            "issn": "4",
            "pages": "1003--1013",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "XGBoost: A scalable tree boosting system",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Guestrin",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "785--794",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Nonlinear variable selection via deep neural networks",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Computational and Graphical Statistics",
            "volume": "",
            "issn": "",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Approximation by superpositions of a sigmoidal function",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cybenko",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "Mathematics of Control, Signals and Systems",
            "volume": "2",
            "issn": "4",
            "pages": "303--314",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Reflection paper on methodological issues in confirmatory clinical trials planned with an adaptive design",
            "authors": [],
            "year": 2007,
            "venue": "European Medicines Agency",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Partial differential equations",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "C"
                    ],
                    "last": "Evans",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Challenges of big data analysis",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "National Science Review",
            "volume": "1",
            "issn": "2",
            "pages": "293--314",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Heterogeneity adjustment with applications to graphical model inference",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Electronic Journal of Statistics",
            "volume": "12",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Curse of heterogeneity: Computational barriers in sparse mixture models and phase retrieval",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1808.06996"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "An example of an improvable Rao-Blackwell improvement, inefficient maximum likelihood estimator, and unbiased generalized bayes estimator",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Galili",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Meilijson",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "The American Statistician",
            "volume": "70",
            "issn": "1",
            "pages": "108--113",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Theoretical investigation of generalization bounds for adversarial learning of deep neural networks",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Journal of Statistical Theory and Practice",
            "volume": "15",
            "issn": "2",
            "pages": "1--28",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Gilead Announces Results From Phase 3 Trial of Investigational Antiviral Remdesivir in Patients With Severe COVID-19",
            "authors": [
                {
                    "first": "Gilead",
                    "middle": [],
                    "last": "Inc",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Deep learning",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Courville",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Neural networks for machine learning",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Swersky",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Coursera",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Asymptotic properties of non-linear least squares estimators",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "I"
                    ],
                    "last": "Jennrich",
                    "suffix": ""
                }
            ],
            "year": 1969,
            "venue": "The Annals of Mathematical Statistics",
            "volume": "40",
            "issn": "2",
            "pages": "633--643",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Understanding the ensemble kalman filter",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Katzfuss",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Stroud",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "K"
                    ],
                    "last": "Wikle",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "The American Statistician",
            "volume": "70",
            "issn": "4",
            "pages": "350--357",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "A remark on estimating the mean of a normal distribution with known coefficient of variation",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Khan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Statistics",
            "volume": "49",
            "issn": "3",
            "pages": "705--710",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Theory of point estimation",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "L"
                    ],
                    "last": "Lehmann",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Casella",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Deep advantage learning for optimal dynamic treatment regime",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Statistical theory and related fields",
            "volume": "2",
            "issn": "",
            "pages": "80--88",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Deeppink: reproducible feature selection in deep neural networks",
            "authors": [
                {
                    "first": "Y",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lv",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "S"
                    ],
                    "last": "Noble",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "An ensemble quadratic echo state network for non-linear spatio-temporal forecasting",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "L"
                    ],
                    "last": "Mcdermott",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "K"
                    ],
                    "last": "Wikle",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Stat",
            "volume": "6",
            "issn": "1",
            "pages": "315--330",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Using least squares and tobit in second stage DEA efficiency analyses",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Mcdonald",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "European Journal of Operational Research",
            "volume": "197",
            "issn": "2",
            "pages": "792--798",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Adaptive COVID-19 Treatment Trial (ACTT)",
            "authors": [],
            "year": 2020,
            "venue": "National Institutes of Health",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "NIH Clinical Trial Shows Remdesivir Accelerates Recovery from Advanced COVID-19",
            "authors": [],
            "year": 2020,
            "venue": "National Institutes of Health",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Summarizing historical information on controls in clinical trials",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Neuenschwander",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Capkun-Niggli",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Branson",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Spiegelhalter",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Clinical Trials",
            "volume": "7",
            "issn": "1",
            "pages": "5--18",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Introductory econometrics with applications",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ramanathan",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Deephazard: neural network for time-varying risks",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rava",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bradic",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2007.13218"
                ]
            }
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Learning topology and dynamics of large recurrent neural networks",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "She",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Transactions on Signal Processing",
            "volume": "62",
            "issn": "22",
            "pages": "5881--5891",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Efficient spectral sparse grid methods and applications to highdimensional elliptic problems",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "SIAM Journal on Scientific Computing",
            "volume": "32",
            "issn": "6",
            "pages": "3228--3250",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "An improved method of evaluating drug effect in a multiple dose clinical trial",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Statistics in Medicine",
            "volume": "20",
            "issn": "13",
            "pages": "1913--1929",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Asymptotic Properties of Neural Network Sieve Estimators",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Sakhanenko",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.00875"
                ]
            }
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Estimation following selection of the largest of two normal means",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Stallard",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Todd",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Whitehead",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Journal of Statistical Planning and Inference",
            "volume": "138",
            "issn": "6",
            "pages": "1629--1638",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Rase: Random subspace ensemble classification",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Journal of Machine Learning Research",
            "volume": "22",
            "issn": "45",
            "pages": "1--93",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Computational methods for inverse problems",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Vogel",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "23",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Boosting algorithms for estimating optimal individualized treatment rules",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "P.-L",
                    "middle": [],
                    "last": "Loh",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2002.00079"
                ]
            }
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "The uniform distribution as a first practical approach to new product inventory management",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "F"
                    ],
                    "last": "Wanke",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "International Journal of Production Economics",
            "volume": "114",
            "issn": "2",
            "pages": "811--819",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary mappings",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "White",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Neural Networks",
            "volume": "3",
            "issn": "5",
            "pages": "535--549",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Asymptotic theory of nonlinear least squares estimation. The Annals of Statistics",
            "authors": [
                {
                    "first": "C.-F",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 1981,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "501--513",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Statistical insights into deep neural network learning in subspace classification",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lv",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Stat",
            "volume": "9",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Understanding weight normalized deep neural networks with rectified linear units",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "130--139",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Error bounds for approximations with deep ReLU networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yarotsky",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Neural Networks",
            "volume": "94",
            "issn": "",
            "pages": "103--114",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Deep learning from a statistical perspective",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Qu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Stat",
            "volume": "9",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "A hyperspherical adaptive sparse-grid method for high-dimensional discontinuity detection",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Webster",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gunzburger",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Burkardt",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "SIAM Journal on Numerical Analysis",
            "volume": "53",
            "issn": "3",
            "pages": "1508--1536",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "FDA; Food and Drug Administration (2019)] and the European Medicines Agency [EMA;",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": ". Shen et al. (2019) considered a sieve as a sequence of function classes indexed by size M , and further established the consistency and asymptotic",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "(\u03b8, \u03c9) are positive.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "and V 2 for T 2 in (13). Condition B.2 can be checked empirically based on the fitted DNN w(\u03c6; \u03b7) obtained in Algorithm 1. Next, we provide upper bounds on the absolute bias and MSE of our estimator U [x; w (T 1 , \u03c9; \u03b7)] in the following Theorem 1. Theorem 1 Under the aforementioned conditions A.1 -A.4, B.1, B.2, and (11), the absolute bias of U (x; w [T 1 , \u03c9; \u03b7]) is upper bounded at,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": ". The first term var(T 1 \u2212 T 2 ) 1 \u2212 w {opt} 2 is non-negative and represents the underlying potential improvement with the unknown w {opt} as compared with T 1 based on Proposition 1. For M 1 , it can be sufficiently small by our algorithm design following the similar argument of absolute bias on w and d from S 2 in (19). The next term M 2 decreases as sample size increases per V in (20). The last term M 3 decreases as the covariance between T 1 and T 2 decreases. By Condition A.4 in Section 3.2, the denominator E[T 2 (T 2 \u2212 T 1 )] in M 3 is lower bounded by c L /2. If T 1 and T 2 are highly correlated such that M 2 + M 3 > 1 \u2212 w {opt} 2 , then there will be no reduction of MSE by using our ensemble estimator. In an extreme scenario where T 1 is the known UMVUE, then w {opt} is equal to a constant of 1 with c d = 0. The MSE improvement of our method can be negative, because the lower bound of (21) is \u2212M 1 .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Consistently higher power of U \u03b8(0.5), \u2206 (1) than \u03b8(0.2), \u03b8(0.5) and \u03b8(0.8) to detect a promising treatment effect \u03b8 in the ACTT on COVID-19.",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": ")",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "(Goodfellow et al., 2016). By increasing the number of layers and number of nodes in DNN, the empirical MSE from the training dataset usually decreases by containing more complex structures. However, the MSE in the validation dataset or the MSE from the Jackknife method is subject to increasing with poor performance at generalization tasks. Then one can further implement certain regulation approaches, for example dropout techniques or L 1",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "). In the Supplemental Materials Section 3, we show that w can be expressed as O p M \u22121/2 with four additional regularity conditions. Since M and N are our design parameters, they can be chosen sufficiently large to control the approximation error. One may substitute",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 1: Standard deviation (SD) of U \u03b8 RB , \u03b8 M and its high relative efficiency versus two base components \u03b8 RB and \u03b8 M , and the empirical mean \u03b8 E .",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "High relative efficiency of U \u03b8 W , \u03b8 L versus two base components \u03b8 W and \u03b8 L .",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "U \u03b8(0.5), \u2206 (1)",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This manuscript was supported by AbbVie. AbbVie participated in the review and approval of the content. Tianyu Zhan is employed by AbbVie Inc., Haoda Fu is employed by Eli Lilly and Company, and Jian Kang is Professor in the Department of Biostatistics at the University of Michigan, Ann Arbor. All authors may own AbbVie stock.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        },
        {
            "text": "Supplementary Materials are available online including the R code and a help file to replicate all simulation studies.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supplementary Materials"
        }
    ]
}
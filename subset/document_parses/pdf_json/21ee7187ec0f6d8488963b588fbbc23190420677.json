{"paper_id": "21ee7187ec0f6d8488963b588fbbc23190420677", "metadata": {"title": "EmotionGIF-Yankee: A Sentiment Classifier with Robust Model Based Ensemble Methods", "authors": [{"first": "Wei-Yao", "middle": [], "last": "Wang", "suffix": "", "affiliation": {"laboratory": "Advanced Database System Laboratory", "institution": "National Chiao Tung University", "location": {"settlement": "Hsinchu", "country": "Taiwan"}}, "email": ""}, {"first": "Kai-Shiang", "middle": [], "last": "Chang", "suffix": "", "affiliation": {"laboratory": "Advanced Database System Laboratory", "institution": "National Chiao Tung University", "location": {"settlement": "Hsinchu", "country": "Taiwan"}}, "email": ""}, {"first": "Yu-Chien", "middle": [], "last": "Tang", "suffix": "", "affiliation": {"laboratory": "", "institution": "National Chiao Tung University", "location": {"settlement": "Hsinchu", "country": "Taiwan"}}, "email": ""}]}, "abstract": [{"text": "This paper provides a method to classify sentiment with robust model based ensemble methods. We preprocess tweet data to enhance coverage of tokenizer. To reduce domain bias, we first train tweet dataset for pre-trained language model. Besides, each classifier has its strengths and weakness, we leverage different types of models with ensemble methods: average and power weighted sum. From the experiments, we show that our approach has achieved positive effect for sentiment classification. Our system reached third place among 26 teams from the evaluation in SocialNLP 2020 EmotionGIF competition.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Natural language is often indicative of one's emotion. Hence, detecting emotions in textual conversations has been one of popular topics in the field of natural language processing (NLP) sentiment domain. Sentiment classifier can help researchers study such information on user's feeling. There are various tasks of sentiment classification, for example, Riloff et al. (2005) presents an information extraction (IE) system that automatically uses filtering extractions to improve subjectivity classification. On opinion extraction, Zhai et al. (2011) extracts different opinion feature, including sentiment-words, substrings, and key-substringgroups, to help improve sentiment classification performance. In recent years, Hazarika et al. (2018) proposes a multi-modal emotion detection framework, interactive conversational memory network (ICON), to extract multi-modal features for emotion detection.", "cite_spans": [{"start": 355, "end": 375, "text": "Riloff et al. (2005)", "ref_id": "BIBREF13"}, {"start": 532, "end": 550, "text": "Zhai et al. (2011)", "ref_id": "BIBREF16"}, {"start": 722, "end": 744, "text": "Hazarika et al. (2018)", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "Introduction"}, {"text": "In SocialNLP 2020 EmotionGIF, the challenge is to use tweet text and reply to recommend exactly 6 categories. In this paper, we propose an architecture to apply to the shared task. We preprocess original tweet data to pre-trained language model, then fine-tune to multi-label classification model. To build comprehensive emotion classifier, we design an ensemble scheme to get higher performance.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "The shared task includes a first-of-its-kind dataset of 40,000 two-turn Twitter threads. Each thread contains 5 columns which are idx, text, reply, categories, and mp4.", "cite_spans": [], "ref_spans": [], "section": "Dataset"}, {"text": "Here are the explanations of 5 columns:", "cite_spans": [], "ref_spans": [], "section": "Dataset"}, {"text": "\u2022 idx: a unique identifier of each tweet", "cite_spans": [], "ref_spans": [], "section": "Dataset"}, {"text": "\u2022 text: the text of the original tweet", "cite_spans": [], "ref_spans": [], "section": "Dataset"}, {"text": "\u2022 reply: the text content of the response tweet", "cite_spans": [], "ref_spans": [], "section": "Dataset"}, {"text": "\u2022 categories: the categories of the response GIF, containing 1 to 6 categories out of a list of 43 categories", "cite_spans": [], "ref_spans": [], "section": "Dataset"}, {"text": "\u2022 mp4: the hash file name of the response GIF", "cite_spans": [], "ref_spans": [], "section": "Dataset"}, {"text": "The dataset is split into three JSON files, traingold, dev-unlabeled, and test-unlabeled. First including 32,000 threads is training data, and the others including 4,000 threads are validation data, and testing data. The difference between train-gold and dev-unlabeled, test-unlabeled is that the former consists of all the 5 columns while the latter two only consist of 3 columns, idx, text, and reply. Figure 1 is the subset of the correlation table which contains the frequency of co-appearance of any two categories. The figure illustrates the correlation between different categories and we can observe that some categories have strong connection while some categories have weak connection. ", "cite_spans": [], "ref_spans": [{"start": 404, "end": 412, "text": "Figure 1", "ref_id": "FIGREF0"}], "section": "Dataset"}, {"text": "Our study can be mainly divided into three topics, including multi-label classification, pre-trained models, and ensemble methods.", "cite_spans": [], "ref_spans": [], "section": "Related Work"}, {"text": "Multi-label classification is a generalization of multiclass classification. Nowadays the multi-label classification is increasingly used in many fields of NLP, such as semantic scene classification and sentiment classification. There are two main categories of multi-label classification approaches: problem transformation (PT) methods, and algorithm adaptation (AA) methods. Generally speaking, problem transformation methods will transform the multi-label classification problem into one or more single-label classification problem (Zhang and Zhou, 2014) , while algorithm adaptation methods usually use those algorithms having been adapted to multi-label task and needing no problem transformation. For problem transformation methods, a good strategy to achieve the goal is Classifier Chains (CC) (Read et al., 2011) , which classifies whether the original multi-label problem belongs to a label or not in chain structure, and is able to capture the interdependencies between the labels. And for algorithm adaptation methods, Multi-label k-Nearest Neighbors (MLkNN) (Zhang and Zhou, 2007) is one of the most popular. It relies on the maximum a posteriori (MAP) principle on training the k-Nearest Neighbors (kNN), which is a well known traditional machine learning algorithm, to determine which label subset each instance belongs to. Due to its promising results and simplicity, it has been applied to many practical tasks of text classification.", "cite_spans": [{"start": 535, "end": 557, "text": "(Zhang and Zhou, 2014)", "ref_id": "BIBREF17"}, {"start": 801, "end": 820, "text": "(Read et al., 2011)", "ref_id": "BIBREF12"}, {"start": 1070, "end": 1092, "text": "(Zhang and Zhou, 2007)", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Multi-label Classification"}, {"text": "Most of the existing multi-label classification approaches solve the emotion classification by training the model on a large dataset. The idea is to find informative features which can reflect the emotion expressed in the text, so with this approach most studies aim to find efficient features leading to better performance (Jabreel and Moreno, 2016) . Also, deep learning models are introduced to solve the multi-label classification problem, and have been proved that such models are able to extract high-level features from raw data. For instance, Baziotis et al. (2018) , the winner of SemEval-2018 Task 1 competition: Affect in Tweets, proposes a Bi-LSTM architecture with attention mechanism. They leverage a set of word2vec word embeddings trained on a dataset of 550 million tweets.", "cite_spans": [{"start": 324, "end": 350, "text": "(Jabreel and Moreno, 2016)", "ref_id": "BIBREF5"}, {"start": 551, "end": 573, "text": "Baziotis et al. (2018)", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Multi-label Classification"}, {"text": "Pre-trained models have been widely applied in a variety of NLP systems and achieve dramatically performance for downstream tasks. There are three major advantages for pre-trained models. First of all, since they are unsupervised learning, there will be unlimited corpus can be trained. Secondly, a strength pre-trained language model can generate deep contextual word representation which means a word token can have several representation in different sentences. Hence, through fine-tuning we improve downstream tasks more efficiently. Last but not least, using pre-trained models can reduce huge architecture engineering. This allows us don't need to design a deep learning network by ourselves and pre-train with massive cost.", "cite_spans": [], "ref_spans": [], "section": "Pre-trained Models"}, {"text": "BERT (Devlin et al., 2018) , Bidirectional Encoder Representations from Transformers, is one of state-of-the-art (SOTA) pre-trained model. There are two main tasks in pre-training stages. At the first task, called Masked LM (MLM), is to replace 15% of the words in each sequence to a [MASK] token and model need to predict these masked tokens. Encoder learns contextual representations during this stage. Second task, Next Sentence Prediction (NSP), the model takes pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original documents. In details, 50% of the inputs will be a pair in original documents in training, while the other 50% a random sentence from the corpus is chosen as the second sentence.", "cite_spans": [{"start": 5, "end": 26, "text": "(Devlin et al., 2018)", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Pre-trained Models"}, {"text": "There are some variant models based on BERT like RoBERTa (Liu et al., 2019) and DistilBERT (Sanh et al., 2019) . DistilBERT, distilled BERT, reduces the size of a BERT model by 40%, while retaining 97% of its language understanding and being 60% faster. DistilBERT removes half number of layers on token-type embeddings and the pooler. Instead of focusing on efficiency, RoBERTa, robustly optimized BERT approach, finds BERT undertrained that is why they study carefully to modify key hyperparameters to improve performance. Since there is different discrepancy about whether to remove NSP (Devlin et al., 2018; Lample and Conneau, 2019; Yang et al., 2019; Joshi et al., 2020) , RoBERTa do some experiments and find that remove NSP can slighlty improve downstream tasks. Furthermore, RoBERTa uses bytes instead of unicode as the base subword units (Radford et al., 2019) . Using bytes makes model learn larger subword vocabulary.", "cite_spans": [{"start": 57, "end": 75, "text": "(Liu et al., 2019)", "ref_id": "BIBREF10"}, {"start": 91, "end": 110, "text": "(Sanh et al., 2019)", "ref_id": "BIBREF14"}, {"start": 590, "end": 611, "text": "(Devlin et al., 2018;", "ref_id": "BIBREF1"}, {"start": 612, "end": 637, "text": "Lample and Conneau, 2019;", "ref_id": "BIBREF8"}, {"start": 638, "end": 656, "text": "Yang et al., 2019;", "ref_id": "BIBREF15"}, {"start": 657, "end": 676, "text": "Joshi et al., 2020)", "ref_id": "BIBREF6"}, {"start": 848, "end": 870, "text": "(Radford et al., 2019)", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Pre-trained Models"}, {"text": "In general, supervised learning can be defined as finding hypotheses (classifier) that are closed to the true function which can represent all the data points in training data. However, learning algorithms that only output one hypothesis would face three major problems, statistical, computational, and representational. Fortunately, ensemble methods construct a set of classifiers and then classify new data points by taking a vote of their predictions which could usually address the three problems just mentioned (Dietterich, 2000) . The first problem is that learning algorithms may give same accuracy with different hypotheses. By constructing an en-semble out of all of these accurate classifiers, the algorithm can use a simple and fair voting mechanism to reduce the risk of choosing the wrong classifier. The second problem is that many learning algorithms implement local search which may stop even if the best solution found by the algorithm is not optimal. An ensemble constructed by running the local search from many different starting points may provide a better approximation to the true unknown function than any of the individual classifiers. The third problem is that in most applications of machine learning, the true function cannot be represented by any of the hypotheses. By forming weighted sums of hypotheses, it may be possible to expand the space of representable functions. Hagen et al. (2015) introduces an approach with ensemble methods on twitter sentiment detection. Their ensemble method is a voting scheme on the actual classifications of the individual classifiers rather than averaging confidences. Their system proves a strong baseline in the SemEval 2015 evaluation.", "cite_spans": [{"start": 516, "end": 534, "text": "(Dietterich, 2000)", "ref_id": "BIBREF2"}, {"start": 1402, "end": 1421, "text": "Hagen et al. (2015)", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Ensemble Methods"}, {"text": "These studies motivate us to transfer Emo-tionGIF task into multi-label classification problem since this task needs to infer most possible 6 categories of each tweet. To reduce huge architecture engineering, we adopt pre-trained models then focus on preprocessing and postprocessing stages such as ensemble methods to achieve better performance on the competition. In addition to solving those problems mentioned previously, each classifier has its strengths and weakness, if we can combine different types of classifiers to leverage others forte to cover its own drawbacks, we can obtain highly accurate classifiers by combining less accurate ones. By combining these three techniques, we could build a robust system on EmotionGIF task.", "cite_spans": [], "ref_spans": [], "section": "Ensemble Methods"}, {"text": "The main goal of the present work is to predict 6 most possible categories for each tweet in Emo-tionGIF task. We propose an architecture as in Figure 3 which includes three stages: preprocessing, model framework, and ensemble methods.", "cite_spans": [], "ref_spans": [{"start": 144, "end": 152, "text": "Figure 3", "ref_id": null}], "section": "Methodology"}, {"text": "Tweet data don't have same structure as formal corpus (e.g. Wikipedia). There are multiple methods to clean up original tweet data. We perform some methods to normalize data, including five steps, but we do not convert to lower case. Here are the main five steps to normalize tweet dataset. We do these steps in order:", "cite_spans": [], "ref_spans": [], "section": "Preprocessing"}, {"text": "1. Transform weird punctuation such as and .", "cite_spans": [], "ref_spans": [], "section": "Preprocessing"}, {"text": "2. Transform apostrophes to original words. For example, hasn't will be converted to has not.", "cite_spans": [], "ref_spans": [], "section": "Preprocessing"}, {"text": "3. Mapping unknown punctuation which not in tokenizer's vocabulary. For example, \u03b2 is unknown in RoBERTa tokenizer, this will be transformed to word beta.", "cite_spans": [], "ref_spans": [], "section": "Preprocessing"}, {"text": "4. Demojize: convert emoji symbols into their corresponding meanings. Also, if there are duplicate emojis, we will only retain one emoji to represent these duplicate emojis.", "cite_spans": [], "ref_spans": [], "section": "Preprocessing"}, {"text": "5. Detweetize and more words conversion: some words in dataset are in tweet style, which means these words are seldom seen in formal corpus. We replace these words by manually into common representations. Like idk will be replaced with I don't know. Moreover, there are many recent trends like COVID which haven't been seen in tokenizers before. Therefore, we transform these words to common words like virus which can be tokenized correctly in tokenizers.", "cite_spans": [], "ref_spans": [], "section": "Preprocessing"}, {"text": "Model framework is composed of two parts: enhanced pre-trained language model and fine-tuned multi-label classification model. Pre-trained model trains on formal corpus like Wikipedia instead of tweet dataset. To avoid our model overfitting and domain bias on the training data, we use provided 32,000 training set to further train on pre-trained language model. The enhanced language model understands more about tweet style sentences.", "cite_spans": [], "ref_spans": [], "section": "Model Framework"}, {"text": "In EmotionGIF task, we treat as multi-label classification problem. Hence, we use enhanced pretrained model to fine-tune to multi-label classification model in downstream task.", "cite_spans": [], "ref_spans": [], "section": "Model Framework"}, {"text": "To properly handle multi-label classification, we select BCEWithLogitsLoss as our loss function. BCEWithLogitsLoss combines a sigmoid layer and the BCELoss, and takes advantage of the log-sumexp trick for numerical stability as Equation (1) and Equation (2).", "cite_spans": [], "ref_spans": [], "section": "Model Framework"}, {"text": "where N is the batch size,", "cite_spans": [], "ref_spans": [], "section": "Model Framework"}, {"text": "(2) Our goal aims to get better performance instead of efficiency, we use RoBERTa-base, BERT-basecased, and BERT-base-uncased to individually train language model and fine-tune to multi-label classification model. Since RoBERTa and BERT use different input formats, and our dataset has pair of sequences text and reply in each tweet, we convert input sentences based on corresponding models. BERT format is to add a special token [CLS] at first and add [SEP] between sentences and the end. RoBERTa format is to add <s> at first and add </s> between sentences and the end. An example of representation is as Table 1 .", "cite_spans": [], "ref_spans": [{"start": 607, "end": 614, "text": "Table 1", "ref_id": "TABREF1"}], "section": "Model Framework"}, {"text": "Since each classifier has its strengths and weakness, if we can combine different types of classifiers to leverage others forte to cover its own drawbacks, we can obtain highly accurate classifiers by combining less accurate ones. To attain the desired results, we combine three different types of models, RoBERTa-base, BERT-base-cased, and BERTbase-uncased. On account of different dropout weights in each training, the performance of each trained model may have a big gap compared with each others. By training 10 same type of models with different dropout weights and averaging their predictions, we can lower the risk of using single model with bad performance.", "cite_spans": [], "ref_spans": [], "section": "Ensemble Methods"}, {"text": "After training and averaging three types of model, we use Equation (3) Figure 4 : Visualization of equation y = x n with n = 1/4, 1/3, 1/2, 1, 2, 3, 4 P i for i = 1, 2, 3 are average predict scores from RoBERTa-base, BERT-base-cased and BERTbase-uncased respectively and w i for i = 1, 2, 3 are the weights corresponding to each model. To choose a reasonable N, we look into the property of power function. Figure 4 shows that the further away the probability is from 1, then the faster the probability is closer to 0 and vice versa. The probabilities that remain the highest at the end are the probabilities whose relative agreement (weighted down by the probability and the power) coming from each ensemble model is the highest (Laurae). We take advantage of the power weighted sum to enhance performance of model.", "cite_spans": [], "ref_spans": [{"start": 71, "end": 79, "text": "Figure 4", "ref_id": null}, {"start": 407, "end": 415, "text": "Figure 4", "ref_id": null}], "section": "Ensemble Methods"}, {"text": "In EmotionGIF, we only have ground truth labels in training data. We use dev-unlabeled as our validation data. That is, we fine-tune hyperparameters based on validation data and use best models from tuning to predict testing data, test-unlabeled. In this section, our system gives some reasonable results from experiments. The source code for this paper is available as a Github repository 1 .", "cite_spans": [], "ref_spans": [], "section": "Experiment"}, {"text": "For both pre-trained language model and multilabel classification model, we use Adam (Kingma and Ba, 2014) as optimizer with epsilon 1e-8, learning rate 4e-5. Gradient accumulation steps and warmup ratio are 1 and 0.06. Max sequence length, number of epochs, and batch size are set to 113, 4, and 16. For pre-trained language model, we set block size to 96. For multi-label classification model, early stopping is used, which means beam search is stopped when number of beam sentences finished per batch. Early stopping patience is set to 3. Early stopping metric is eval loss and early stopping metric should be minimized. Most of these configurations are default arguments in Simple Transformers 2 .", "cite_spans": [], "ref_spans": [], "section": "Experimental Setup"}, {"text": "In order to achieve ensemble methods, we train 10 of RoBERTa-base, 5 of BERT-base-cased and 5 of BERT-base-uncased. All of these models have been trained with above configurations.", "cite_spans": [], "ref_spans": [], "section": "Experimental Setup"}, {"text": "The metric that will be used to evaluate entries is Mean Recall at k, with k=6 (MR@6). Table 2 shows an example how we evaluate our predictions. For each output, we will predict 6 categories out of a list of 43 categories as Prediction in Table 2 and calculate how many categories (N) that our predicted categories are identical to the answer. The MR@6 is N divided by the total amount of Answer. The final result is the average of the MR@6 for all Twitter threads.", "cite_spans": [], "ref_spans": [{"start": 87, "end": 94, "text": "Table 2", "ref_id": "TABREF2"}, {"start": 239, "end": 246, "text": "Table 2", "ref_id": "TABREF2"}], "section": "Evaluation Metric"}, {"text": "Prediction MR@6 agree, thank you, thumbs up oops, scared, thank you, you got this, do not want, agree 1/3 ", "cite_spans": [], "ref_spans": [], "section": "Answer"}, {"text": "To check preprocessing methods, we use RoBERTabase tokenizer coverage to validate shown in Table  3 . From Table 4 shows first 6 out-ofvocab (OOV) tokens. Although there are still some unknown tokens, we can observe that some words like medium-dark might be able to be tokenized into expected tokens like medium, -, and dark.", "cite_spans": [], "ref_spans": [{"start": 91, "end": 99, "text": "Table  3", "ref_id": "TABREF3"}, {"start": 107, "end": 114, "text": "Table 4", "ref_id": "TABREF6"}], "section": "Preprocessing Analysis"}, {"text": "Exploratory data analysis (EDA) is a initial investigations on data so as to discover pattern or to check assumption with the help of statistics. Through EDA, we find that convert word to lower case may cause unexpected tokens from tokenizer. For example, word Hug can be correctly tokenized when at different positions, while word hug cannot be tokenized as we expected. That is, hug will be tokenized into h and ug. Hence we don't convert all words into lower case in EmotionGIF task.", "cite_spans": [], "ref_spans": [], "section": "Preprocessing Analysis"}, {"text": "The experiment results of validation data are shown in Table 5 Table 6 is our system predict on testing set. Ensemble models achieve about 0.5662 MAR@6 score, while only using single type of model only gets 0.5404. This indicates single type of model may be slightly worse in testing data. Applying ensemble methods does solve this problem. Overall, our proposed system successfully outperforms with either using original pre-trained language model to fine-tune or EmotionGIF official baseline. Our approach achieves high MAR@6 score both on validation data and testing data in this competition.", "cite_spans": [], "ref_spans": [{"start": 55, "end": 62, "text": "Table 5", "ref_id": "TABREF7"}, {"start": 63, "end": 70, "text": "Table 6", "ref_id": "TABREF8"}], "section": "Evaluation Results"}, {"text": "In this work, we propose an system architecture combining with preprocessing, model framework, and ensemble models for EmotionGIF task. We intently convert some words to our desired format and increase the coverage of words recognized by tokenizer. Based on preprocessing data, We apply multi-label classification and pre-trained model when training models to make our work more sophisticated. Besides, we also show that ensemble models with power weighted sum outperform any single model with same parameters we trained.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "In Section 2, we observe that there is an imbalance between categories. However, in the present work, we don't deal with it. Furthermore, we consider to replace multi-label classification with ranking classification due to its property of dependency in future work. The probabilities of multi-label classification are treated as independent, so there is no correlation among categories while ranking classification is the opposite. Since the category would have some connection with each other as Table 1 shown, we assume that it would be better to let our model regard the dependency between categories as critical.", "cite_spans": [], "ref_spans": [{"start": 497, "end": 504, "text": "Table 1", "ref_id": "TABREF1"}], "section": "Conclusion"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Ntua-slp at semeval-2018 task 1: Predicting affective content in tweets with deep attentive rnns and transfer learning", "authors": [{"first": "Christos", "middle": [], "last": "Baziotis", "suffix": ""}, {"first": "Nikos", "middle": [], "last": "Athanasiou", "suffix": ""}, {"first": "Alexandra", "middle": [], "last": "Chronopoulou", "suffix": ""}, {"first": "Athanasia", "middle": [], "last": "Kolovou", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1804.06658"]}}, "BIBREF1": {"ref_id": "b1", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "authors": [{"first": "Jacob", "middle": [], "last": "Devlin", "suffix": ""}, {"first": "Ming-Wei", "middle": [], "last": "Chang", "suffix": ""}, {"first": "Kenton", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Kristina", "middle": [], "last": "Toutanova", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1810.04805"]}}, "BIBREF2": {"ref_id": "b2", "title": "Ensemble methods in machine learning", "authors": [{"first": "G", "middle": [], "last": "Thomas", "suffix": ""}, {"first": "", "middle": [], "last": "Dietterich", "suffix": ""}], "year": 2000, "venue": "International workshop on multiple classifier systems", "volume": "", "issn": "", "pages": "1--15", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Webis: An ensemble for twitter sentiment detection", "authors": [{"first": "Matthias", "middle": [], "last": "Hagen", "suffix": ""}, {"first": "Martin", "middle": [], "last": "Potthast", "suffix": ""}, {"first": "Michel", "middle": [], "last": "B\u00fcchner", "suffix": ""}, {"first": "Benno", "middle": [], "last": "Stein", "suffix": ""}], "year": 2015, "venue": "Proceedings of the 9th international workshop on semantic evaluation", "volume": "", "issn": "", "pages": "582--589", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Icon: Interactive conversational memory network for multimodal emotion detection", "authors": [{"first": "Devamanyu", "middle": [], "last": "Hazarika", "suffix": ""}, {"first": "Soujanya", "middle": [], "last": "Poria", "suffix": ""}, {"first": "Rada", "middle": [], "last": "Mihalcea", "suffix": ""}, {"first": "Erik", "middle": [], "last": "Cambria", "suffix": ""}, {"first": "Roger", "middle": [], "last": "Zimmermann", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "2594--2604", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Sentirich: Sentiment analysis of tweets based on a rich set of features", "authors": [{"first": "Mohammed", "middle": [], "last": "Jabreel", "suffix": ""}, {"first": "Antonio", "middle": [], "last": "Moreno", "suffix": ""}], "year": 2016, "venue": "CCIA", "volume": "", "issn": "", "pages": "137--146", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Spanbert: Improving pre-training by representing and predicting spans", "authors": [{"first": "Mandar", "middle": [], "last": "Joshi", "suffix": ""}, {"first": "Danqi", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Yinhan", "middle": [], "last": "Liu", "suffix": ""}, {"first": "S", "middle": [], "last": "Daniel", "suffix": ""}, {"first": "Luke", "middle": [], "last": "Weld", "suffix": ""}, {"first": "Omer", "middle": [], "last": "Zettlemoyer", "suffix": ""}, {"first": "", "middle": [], "last": "Levy", "suffix": ""}], "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "volume": "8", "issn": "", "pages": "64--77", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Adam: A method for stochastic optimization", "authors": [{"first": "P", "middle": [], "last": "Diederik", "suffix": ""}, {"first": "Jimmy", "middle": [], "last": "Kingma", "suffix": ""}, {"first": "", "middle": [], "last": "Ba", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1412.6980"]}}, "BIBREF8": {"ref_id": "b8", "title": "Crosslingual language model pretraining", "authors": [{"first": "Guillaume", "middle": [], "last": "Lample", "suffix": ""}, {"first": "Alexis", "middle": [], "last": "Conneau", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1901.07291"]}}, "BIBREF9": {"ref_id": "b9", "title": "Reaching the depths of (power/geometric) ensembling when targeting the auc metric", "authors": [{"first": "", "middle": [], "last": "Laurae", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Roberta: A robustly optimized bert pretraining approach", "authors": [{"first": "Yinhan", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Myle", "middle": [], "last": "Ott", "suffix": ""}, {"first": "Naman", "middle": [], "last": "Goyal", "suffix": ""}, {"first": "Jingfei", "middle": [], "last": "Du", "suffix": ""}, {"first": "Mandar", "middle": [], "last": "Joshi", "suffix": ""}, {"first": "Danqi", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Omer", "middle": [], "last": "Levy", "suffix": ""}, {"first": "Mike", "middle": [], "last": "Lewis", "suffix": ""}, {"first": "Luke", "middle": [], "last": "Zettlemoyer", "suffix": ""}, {"first": "Veselin", "middle": [], "last": "Stoyanov", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1907.11692"]}}, "BIBREF11": {"ref_id": "b11", "title": "Language models are unsupervised multitask learners", "authors": [{"first": "Alec", "middle": [], "last": "Radford", "suffix": ""}, {"first": "Jeffrey", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Rewon", "middle": [], "last": "Child", "suffix": ""}, {"first": "David", "middle": [], "last": "Luan", "suffix": ""}, {"first": "Dario", "middle": [], "last": "Amodei", "suffix": ""}, {"first": "Ilya", "middle": [], "last": "Sutskever", "suffix": ""}], "year": 2019, "venue": "OpenAI Blog", "volume": "1", "issn": "8", "pages": "", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Classifier chains for multi-label classification", "authors": [{"first": "Jesse", "middle": [], "last": "Read", "suffix": ""}, {"first": "Bernhard", "middle": [], "last": "Pfahringer", "suffix": ""}, {"first": "Geoff", "middle": [], "last": "Holmes", "suffix": ""}, {"first": "Eibe", "middle": [], "last": "Frank", "suffix": ""}], "year": 2011, "venue": "Machine learning", "volume": "85", "issn": "3", "pages": "", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Exploiting subjectivity classification to improve information extraction", "authors": [{"first": "Ellen", "middle": [], "last": "Riloff", "suffix": ""}, {"first": "Janyce", "middle": [], "last": "Wiebe", "suffix": ""}, {"first": "William", "middle": [], "last": "Phillips", "suffix": ""}], "year": 2005, "venue": "AAAI", "volume": "", "issn": "", "pages": "1106--1111", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "authors": [{"first": "Victor", "middle": [], "last": "Sanh", "suffix": ""}, {"first": "Lysandre", "middle": [], "last": "Debut", "suffix": ""}, {"first": "Julien", "middle": [], "last": "Chaumond", "suffix": ""}, {"first": "Thomas", "middle": [], "last": "Wolf", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1910.01108"]}}, "BIBREF15": {"ref_id": "b15", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "authors": [{"first": "Zhilin", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Zihang", "middle": [], "last": "Dai", "suffix": ""}, {"first": "Yiming", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Jaime", "middle": [], "last": "Carbonell", "suffix": ""}, {"first": "R", "middle": [], "last": "Russ", "suffix": ""}, {"first": "Quoc V", "middle": [], "last": "Salakhutdinov", "suffix": ""}, {"first": "", "middle": [], "last": "Le", "suffix": ""}], "year": 2019, "venue": "Advances in neural information processing systems", "volume": "", "issn": "", "pages": "5753--5763", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Exploiting effective features for chinese sentiment classification", "authors": [{"first": "Zhongwu", "middle": [], "last": "Zhai", "suffix": ""}, {"first": "Hua", "middle": [], "last": "Xu", "suffix": ""}, {"first": "Bada", "middle": [], "last": "Kang", "suffix": ""}, {"first": "Peifa", "middle": [], "last": "Jia", "suffix": ""}], "year": 2011, "venue": "Expert Systems with Applications", "volume": "38", "issn": "", "pages": "9139--9146", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "A review on multi-label learning algorithms", "authors": [{"first": "M", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Z", "middle": [], "last": "Zhou", "suffix": ""}], "year": 2014, "venue": "IEEE Transactions on Knowledge and Data Engineering", "volume": "26", "issn": "8", "pages": "1819--1837", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Ml-knn: A lazy learning approach to multi-label learning. Pattern recognition", "authors": [{"first": "Min-Ling", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Zhi-Hua", "middle": [], "last": "Zhou", "suffix": ""}], "year": 2007, "venue": "", "volume": "40", "issn": "", "pages": "2038--2048", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Figure 2 is the distribution of 43 categories in training data. The figure shows that there is an imbalance between categories. arXiv:2007.02259v1 [cs.CL] 5 Jul 2020 Subset of correlation table", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Distribution of 43 categories", "latex": null, "type": "figure"}, "TABREF1": {"text": "An example of representation", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Model </td><td>Text Reply </td><td>Representation\n</td></tr><tr><td>BERT </td><td>Don't forget to Hydrate! </td><td>[CLS] Don't forget to Hydrate! [SEP] [SEP]\n</td></tr><tr><td>RoBERTa </td><td>Don't forget to Hydrate! </td><td>&lt;s&gt; Don't forget to Hydrate! &lt;/s&gt;&lt;/s&gt; &lt;s&gt;\n</td></tr></table></body></html>"}, "TABREF2": {"text": "An example of evaluation metric", "latex": null, "type": "table"}, "TABREF3": {"text": "Coverage of RoBERTa-base tokenizer", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>Train </td><td>Validation </td><td>Test\n</td></tr><tr><td>Steps </td><td>text </td><td>reply </td><td>text </td><td>reply </td><td>text </td><td>reply\n</td></tr><tr><td>0 </td><td>77.681% </td><td>73.164% </td><td>77.472% </td><td>72.608% </td><td>78.101% </td><td>72.828%\n</td></tr><tr><td>1 </td><td>77.684% </td><td>73.166% </td><td>77.473% </td><td>72.615% </td><td>78.102% </td><td>72.828%\n</td></tr><tr><td>2 </td><td>80.319% </td><td>75.973% </td><td>80.157% </td><td>75.556% </td><td>80.592% </td><td>75.643%\n</td></tr><tr><td>3 </td><td>92.560% </td><td>89.777% </td><td>92.546% </td><td>89.144% </td><td>92.731% </td><td>89.830%\n</td></tr><tr><td>4 </td><td>93.929% </td><td>91.692% </td><td>93.334% </td><td>91.166% </td><td>93.455% </td><td>91.763%\n</td></tr><tr><td>5 </td><td>93.919% </td><td>92.086% </td><td>93.921% </td><td>91.497% </td><td>94.056% </td><td>92.158%\n</td></tr></table></body></html>"}, "TABREF4": {"text": "", "latex": null, "type": "table"}, "TABREF5": {"text": ". The proposed system and baseline are evaluated based on the MAR@6. To verify our system architecture, we first", "latex": null, "type": "table"}, "TABREF6": {"text": "First 6 out-of-vocab tokens fine-tune multi-label classification model(MLC)   for each type of pre-trained base language model directly. MLC RoBERTa , MLC BERT-cased , and MLC BERT-uncased all outperform baseline provided by EmotionGIF official. This explains that directly fine-tune pre-trained model has great performance over baseline. Also, MLC RoBERTa is around 3% higher than BERT models. This shows that RoBERTa does better optimization as compared to BERT. In order to prevent domain bias, we train tweet dataset on pre-trained language model (LM) to enhance it's knowledge. Adding language model with RoBERTa (LM RoBERTa + MLC RoBERTa ) improves about 1.2% on MAR score which implies training tweet data on language model can slightly reach higher performance than directly use formal pre-trained language model. To reduce affect of dropout weights, we train several models and average them to maintain balance performance. Motivated from Laurae, we apply power weighted sum in our system with power 1.8 and weights are 3.0, 1.8, and 0.8 corresponding to RoBERTa, BERTcased, and BERT-uncased. With these ensemble methods, our system reaches 0.5619, which improves about 2% performance on validation set.", "latex": null, "type": "table"}, "TABREF7": {"text": "Validation results", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Model </td><td>MAR@6\n</td></tr><tr><td>Official majority baseline </td><td>0.4009\n</td></tr><tr><td>MLCBERT-cased </td><td>0.5021\n</td></tr><tr><td>MLCBERT-uncased </td><td>0.5023\n</td></tr><tr><td>MLCRoBERTa </td><td>0.5293\n</td></tr><tr><td>LMRoBERTa + MLCRoBERTa </td><td>0.5414\n</td></tr><tr><td>Ensemble models </td><td>0.5619\n</td></tr></table></body></html>"}, "TABREF8": {"text": "Testing results", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Model </td><td>MAR@6\n</td></tr><tr><td>Official majority baseline </td><td>0.4065\n</td></tr><tr><td>LMRoBERTa-base + MLCRoBERTa-base </td><td>0.5404\n</td></tr><tr><td>Ensemble models </td><td>0.5662\n</td></tr></table></body></html>"}, "TABREF9": {"text": "Table 4: First 6 out-of-vocab tokens", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Out-of-vocab tokens pensive </td><td>Count\n</td></tr><tr><td>251\n</td></tr><tr><td>dependable </td><td>247\n</td></tr><tr><td>Scotty </td><td>203\n</td></tr><tr><td>backhand </td><td>194\n</td></tr><tr><td>6pm </td><td>163\n</td></tr><tr><td>medium-dark </td><td>152\n</td></tr></table></body></html>"}}, "back_matter": []}
{"paper_id": "e2996632a493b878adbd74746e43ab21f208ae26", "metadata": {"title": "Natural Language Generation Using Transformer Network in an Open-Domain Setting", "authors": [{"first": "Deeksha", "middle": [], "last": "Varshney", "suffix": "", "affiliation": {"laboratory": "", "institution": "Indian Institute of Technology Patna", "location": {"settlement": "Patna", "country": "India"}}, "email": ""}, {"first": "Asif", "middle": [], "last": "Ekbal", "suffix": "", "affiliation": {"laboratory": "", "institution": "Indian Institute of Technology Patna", "location": {"settlement": "Patna", "country": "India"}}, "email": ""}, {"first": "Ganesh", "middle": ["Prasad"], "last": "Nagaraja", "suffix": "", "affiliation": {"laboratory": "", "institution": "Samsung Research India", "location": {"settlement": "Bangalore", "country": "India"}}, "email": ""}, {"first": "Mrigank", "middle": [], "last": "Tiwari", "suffix": "", "affiliation": {"laboratory": "", "institution": "Samsung Research India", "location": {"settlement": "Bangalore", "country": "India"}}, "email": "mrigank.k@samsung.com"}, {"first": "Abhijith", "middle": [], "last": "Athreya", "suffix": "", "affiliation": {}, "email": "abhijith@psu.edu"}, {"first": "Mysore", "middle": [], "last": "Gopinath", "suffix": "", "affiliation": {"laboratory": "", "institution": "Samsung Research India", "location": {"settlement": "Bangalore", "country": "India"}}, "email": ""}, {"first": "Pushpak", "middle": [], "last": "Bhattacharyya", "suffix": "", "affiliation": {"laboratory": "", "institution": "Indian Institute of Technology Patna", "location": {"settlement": "Patna", "country": "India"}}, "email": ""}]}, "abstract": [{"text": "Prior works on dialog generation focus on task-oriented setting and utilize multi-turn conversational utterance-response pairs. However, natural language generation (NLG) in the open-domain environment is more challenging. The conversations in an open-domain chit-chat model are mostly single-turn in nature. Current methods used for modeling single-turn conversations often fail to generate contextually relevant responses for a large dataset. In our work, we develop a transformerbased method for natural language generation (NLG) in an open-domain setting. Experiments on the utterance-response pairs show improvement over the baselines, both in terms of quantitative measures like BLEU and ROUGE and human evaluation metrics like fluency and adequacy.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Conversational systems are some of the most important advancements in the area of Artificial Intelligence (AI). In conversational AI, dialogue systems can be either an open-domain chit-chat model or a task-specific goal-oriented model. Task-specific systems focus on particular tasks such as flight or hotel booking, providing technical support to users, and answering non-creative queries. These systems try to generate a response by maximizing an expected reward. In contrast, an open-domain dialog system operates in a non-goal driven casual environment and responds to the all kinds of questions. The realization of rewards is not straightforward in these cases, as there are many factors to model in. Aspects such as understanding the dialog context, acknowledging user's personal preferences, and other external factors such as time, weather, and current events need consideration at each dialog step.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "In recent times, there has been a trend towards building end-to-end dialog systems such as chat-bots which can easily mimic human conversations. [19, 22, 25] developed systems using deep neural networks by training them on a large amount of multi-turn conversational data. Virtual assistants in open-domain settings usually utilize single-turn conversations for training the models. Chitchat bots in such situations can help humans to interact with machines using natural language, thereby allowing humans to express their emotional states.", "cite_spans": [{"start": 145, "end": 149, "text": "[19,", "ref_id": "BIBREF18"}, {"start": 150, "end": 153, "text": "22,", "ref_id": "BIBREF21"}, {"start": 154, "end": 157, "text": "25]", "ref_id": "BIBREF24"}], "ref_spans": [], "section": "Introduction"}, {"text": "In dialogue systems, generating relevant, diverse, and coherent responses is essential for robustness and practical usages. Generative models tend to generate shorter, inappropriate responses to some questions. The responses range from invalid sentences to generic ones like \"I don't know\". The reasons for these issues include inefficiency of models in capturing long-range dependencies, generation of a large number of out-of-vocabulary (OOV) words, and limitations of the maximum likelihood objective functions for training these models. Transformer models have become an essential part of most of the state-of-the-art architectures in several natural language processing (NLP) applications. Results show that these models capture long-range dependencies efficiently, replacing gated recurrent neural network models in many situations.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "In this paper, we propose an efficient end-to-end architecture based on the transformer network for natural language generation (NLG) in an open-domain dialogue system. The proposed model can maximize contextual relevancy and diversity in generated responses.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Our research reported here contributes in three ways: (i) we build an efficient end-to-end neural architecture for a chit-chat dialogue system, capable of generating contextually consistent and diverse responses; (ii) we create a singleturn conversational dataset with chit-chat type conversations on several topics between a human and a virtual assistant; and (iii) empirical analysis shows that our proposed model can improve the generation process when trained with enough data in comparison to the traditional methods like retrieval-based and neural translation-based.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Conversational Artificial Intelligence (AI) is currently one of the most challenging problems of Artificial Intelligence. Developing dialog systems that can interact with humans logically and can engage them in having long-term conversations has captured the attention of many AI researchers. In general, dialog systems are mainly of two types -task-oriented dialog systems and open-domain dialog systems. Task-oriented dialog systems converse with the users to complete a specific task such as assisting customers to book a ticket or online shopping. On the other hand, an open-domain dialog system can help users to share information, ask questions, and develop social etiquette's through a series of conversations.", "cite_spans": [], "ref_spans": [], "section": "Related Work"}, {"text": "Early works in this area were typically rule-based or learning-based methods [12, 13, 17, 28] . Rule-based methods often require human experts to form rules for training the system, whereas learning-based methods learn from a specific algorithm, which makes it less flexible to adapt to the other domains. Data from various social media platforms like Twitter, Reddit, and other community question-answering (CQA) platforms have provided us with a large number of human-to-human conversations. Data-driven approaches developed by [6, 16] can be used to handle such problems. Retrieval based methods [6] generate a suitable response from a predefined set of candidate responses by ranking them in the order of similarity (e.g., by matching the number of common words) against the input sentence. The selection of a random response from a set of predefined responses makes them static and repetitive. [16] builds a system based on phrase-based statistical machine translation to exploit single turn conversations. [30] presented a deep learning-based method for retrieval-based systems. A brief review of these methods is presented by [2] .", "cite_spans": [{"start": 77, "end": 81, "text": "[12,", "ref_id": "BIBREF11"}, {"start": 82, "end": 85, "text": "13,", "ref_id": "BIBREF12"}, {"start": 86, "end": 89, "text": "17,", "ref_id": "BIBREF16"}, {"start": 90, "end": 93, "text": "28]", "ref_id": "BIBREF27"}, {"start": 530, "end": 533, "text": "[6,", "ref_id": "BIBREF5"}, {"start": 534, "end": 537, "text": "16]", "ref_id": "BIBREF15"}, {"start": 599, "end": 602, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 899, "end": 903, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 1012, "end": 1016, "text": "[30]", "ref_id": "BIBREF29"}, {"start": 1133, "end": 1136, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Related Work"}, {"text": "Lately, generation based models have become quite popular. [19, 22, 23, 25] presented several generative models based on neural network for building efficient conversational dialog systems. Moreover, several other techniques, for instance generative adversarial network (GAN) [10, 29] and conditional variational autoencoder (CVAE) [3, 7, 18, 20, 32, 33] are also implemented for dialog generation.", "cite_spans": [{"start": 59, "end": 63, "text": "[19,", "ref_id": "BIBREF18"}, {"start": 64, "end": 67, "text": "22,", "ref_id": "BIBREF21"}, {"start": 68, "end": 71, "text": "23,", "ref_id": "BIBREF22"}, {"start": 72, "end": 75, "text": "25]", "ref_id": "BIBREF24"}, {"start": 276, "end": 280, "text": "[10,", "ref_id": "BIBREF9"}, {"start": 281, "end": 284, "text": "29]", "ref_id": "BIBREF28"}, {"start": 332, "end": 335, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 336, "end": 338, "text": "7,", "ref_id": "BIBREF6"}, {"start": 339, "end": 342, "text": "18,", "ref_id": "BIBREF17"}, {"start": 343, "end": 346, "text": "20,", "ref_id": "BIBREF19"}, {"start": 347, "end": 350, "text": "32,", "ref_id": "BIBREF31"}, {"start": 351, "end": 354, "text": "33]", "ref_id": "BIBREF32"}], "ref_spans": [], "section": "Related Work"}, {"text": "Conversations generated from retrieval-based methods are highly fluent, grammatically correct, and are of good quality as compared to dialogues generated from the generative methods. Their high-quality performance is subjected to the availability of an extensive repository of human-human interactions. However, responses generated by neural generative models are random in nature but often lack grammatical correctness. Techniques that can combine the power of both retrieval-based methods and generative methods can be adapted in such situations. On the whole hybrid methods [21, 27, 31, 34] first find some relevant responses using retrieval techniques and then leverages them to generate contextually relevant responses in the next stage.", "cite_spans": [{"start": 577, "end": 581, "text": "[21,", "ref_id": "BIBREF20"}, {"start": 582, "end": 585, "text": "27,", "ref_id": "BIBREF26"}, {"start": 586, "end": 589, "text": "31,", "ref_id": "BIBREF30"}, {"start": 590, "end": 593, "text": "34]", "ref_id": "BIBREF33"}], "ref_spans": [], "section": "Related Work"}, {"text": "In this paper, we propose a novel method for building an efficient virtual assistant using single-turn open-domain conversational data. We use a self-attention based transformer model, instead of RNN based models to get the representation of our input sequences. We observe that our method can generate more diverse and relevant responses.", "cite_spans": [], "ref_spans": [], "section": "Related Work"}, {"text": "Our goal is to generate contextually relevant responses for single-turn conversations. Given an input sequence of utterance U = u 1 , u 2 , ..., u n composed of n words we try to generate a target response Y = y 1 , y 2 , ..., y m .", "cite_spans": [], "ref_spans": [], "section": "Problem Statement"}, {"text": "We use pre-trained GLoVE [15] 1 embeddings to initialize the word vectors. GLoVE utilizes two main methods from literature to build its vectors: global matrix factorization and local context window methods. The GloVe model is trained on the non-zero entries of a global word to word co-occurrence matrix, which computes how frequently two words can occur together in a given corpus. The embeddings used in our model are trained on Common Crawl dataset with 840B tokens and 2.2M vocab. We use 300-dimensional sized vectors.", "cite_spans": [{"start": 25, "end": 29, "text": "[15]", "ref_id": "BIBREF14"}], "ref_spans": [], "section": "Word Embeddings"}, {"text": "We formulate our task of response generation as a machine translation problem. We define two baseline models based on deep learning techniques to conduct our experiments. First, we build a neural sequence to sequence model [23] based on Bi-Directional Long Short Term Memory (Bi-LSTM) [5] cells. The second model utilizes the attention mechanism [1] to align input and output sequences. We train these models using the Glove word embeddings as input features.", "cite_spans": [{"start": 223, "end": 227, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 285, "end": 288, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 346, "end": 349, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Baseline Models"}, {"text": "To build our first baseline, we use a neural encoder-decoder [23] model. The encoder, which contains RNN cells, converts the input sequence into a context vector. The context vector is an abstract representation of the entire input sequence. The context vector forms the input for a second RNN based decoder, which learns to output the target sequence one word at a time. Our second baseline uses an attention layer [1] between the encoder and decoder, which helps in deciding which words to focus on the input sequence in order to predict the next word correctly.", "cite_spans": [{"start": 61, "end": 65, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 416, "end": 419, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Baseline Models"}, {"text": "The third model, which is our proposed method, is based on the transformer network architecture [24] . We use Glove word embeddings as input features for our proposed model. We develop the transformer encoder as described in [24] to obtain the representation of the input sequence and the transformer decoder to generate the target response. Figure 1 shows the proposed architecture. The input to the transformer encoder is both the embedding, e, of the current word, e(u n ), as well as positional encoding PE(n) of the nth word:", "cite_spans": [{"start": 96, "end": 100, "text": "[24]", "ref_id": "BIBREF23"}, {"start": 225, "end": 229, "text": "[24]", "ref_id": "BIBREF23"}], "ref_spans": [{"start": 342, "end": 350, "text": "Figure 1", "ref_id": "FIGREF0"}], "section": "Proposed Model"}, {"text": "There are a total of N x identical layers in a transformer encoder. Each layer contains two sub-layers -a Multi-head attention layer and a position-wise feedforward layer. We encode the input utterances and target responses of our dataset using multi-head self-attention. The second layer performs linear transformation over the outputs from the first sub-layer. A residual connection is applied to each of the two sub-layers, followed by layer normalization. The following equations represent the layers:", "cite_spans": [], "ref_spans": [], "section": "Proposed Model"}, {"text": "where M 1 is the hidden state returned by the first layer of multi-head attention and F 1 is the representation of the input utterance obtained after the first feed forward layer. The above steps are repeated for the remaining layers:", "cite_spans": [], "ref_spans": [], "section": "Proposed Model"}, {"text": "where n = 1, ..., N x . We use c to denote the final representation of the input utterance obtained at N x -th layer:", "cite_spans": [], "ref_spans": [], "section": "Proposed Model"}, {"text": "Similarly, for decoding the responses, we use the transformer decoder. There are N y identical layers in the decoder as well. The encoder and decoder layers are quite similar to each other except that now the decoder layer has two multihead attention layers to perform self-attention and encoder-decoder attention, respectively.", "cite_spans": [], "ref_spans": [], "section": "Proposed Model"}, {"text": "R y = [y 1 , ..., y m ]", "cite_spans": [], "ref_spans": [], "section": "Proposed Model"}, {"text": "y m = e(y m ) + P E(m) (10)", "cite_spans": [], "ref_spans": [], "section": "Proposed Model"}, {"text": "To make prediction of the next word, we use Softmax to obtain the words probabilities decoded by the decoder.", "cite_spans": [], "ref_spans": [], "section": "Proposed Model"}, {"text": "In this section, we present the details of the datasets used in our experiments, along with a detailed overview of the experimental settings.", "cite_spans": [], "ref_spans": [], "section": "Datasets and Experiments"}, {"text": "Our dataset comprises of single-turn conversations from ten different domains -Data About User, Competitors, Emotion, Emergency, Greetings, About Bixby, Entertainment, Sensitive, Device, and Event. Professional annotators with a linguistics background and relevant expertise created this dataset. The total dataset comprises of 184,849 utterance and response pairs with an average of 7.31 and 14.44 words for utterance and response, respectively. We first split the data into a train and test set in a 95:5 ratio. We then use 5% of the training data for preparing the validation set. The dataset details are given in Table 2 . Some examples from the dataset are shown in Table 1 .", "cite_spans": [], "ref_spans": [{"start": 617, "end": 624, "text": "Table 2", "ref_id": null}, {"start": 671, "end": 678, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Datasets"}, {"text": "We use two different types of models for our experiments -recurrent and transformer-based sequence-to-sequence generative models. All data loading, model implementations, and evaluation were done using the OpenNMT 2 [9] as the code framework.", "cite_spans": [], "ref_spans": [], "section": "Experimental Setup"}, {"text": "We train a seq2seq model where the encoder and decoder are parameterized as LSTMs [5] . We also experiment with the seq2seq model with an attention mechanism [1] between the decoder and the encoder outputs. The encoder and decoder LSTMs have 2 layers with 512-dimensional hidden states with a dropout rate of 0.1.", "cite_spans": [{"start": 82, "end": 85, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 158, "end": 161, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Recurrent Models."}, {"text": "The layers of both encoder and decoder are set to 6 with 512-dimensional hidden states with a dropout of 0.1. There are 8 multihead attention heads and 2048 nodes in the feed-forward hidden layers. The dimension of word embedding is empirically set to 512. We use Adam [8] for optimization. When decoding the responses, the beam size is set to 5.", "cite_spans": [{"start": 269, "end": 272, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Transformer Model."}, {"text": "Automatic Evaluation: We use the standard metrics like BLEU [14] , ROUGE [11] and perplexity for the automatic evaluation of our models. Perplexity is reported on the generated responses from the validation set. Lower perplexity indicates better performance of the models. BLEU and ROUGE measure the ngram overlap between a generated response and a gold response. Higher BLEU and ROUGE scores indicate better performance.", "cite_spans": [{"start": 60, "end": 64, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 73, "end": 77, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "To qualitatively evaluate our models, we perform human evaluation on the generated responses. We sample 200 random responses from our test set for the human evaluation. Given an input utterance, target response, and predicted response triplet, two experts with post-graduate exposure were asked to evaluate the predicted responses based on the given two criteria:", "cite_spans": [], "ref_spans": [], "section": "Human Evaluation:"}, {"text": "1. Fluency: The predicted response is fluent in terms of the grammar. 2. Adequacy: The predicted response is contextually relevant to the given utterance.", "cite_spans": [], "ref_spans": [], "section": "Human Evaluation:"}, {"text": "We measure fluency and adequacy on a 0-2 scale with '0' indicating an incomplete or incorrect response, '1' indicating acceptable responses and '2' indicating a perfect response. To measure the inter-annotator agreement, we compute the Fleiss kappa [4] score. We obtained a kappa score of 0.99 for fluency and a score of 0.98 for adequacy denoting \"good agreement.", "cite_spans": [{"start": 249, "end": 252, "text": "[4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Human Evaluation:"}, {"text": "In this section we report the results for all our experiments. The first two experiments (seq2seq & seq2seq attn) are conducted with our baseline models. Our third experiment (c.f Fig. 1 ) is carried out on our proposed model using word embeddings as the input sequences. Table 3 and Table 4 show the automatic and manual evaluation results for both the baseline and the proposed model.", "cite_spans": [], "ref_spans": [{"start": 180, "end": 186, "text": "Fig. 1", "ref_id": "FIGREF0"}, {"start": 272, "end": 279, "text": "Table 3", "ref_id": "TABREF1"}, {"start": 284, "end": 291, "text": "Table 4", "ref_id": "TABREF2"}], "section": "Results and Analysis"}, {"text": "Our proposed model has lower perplexity and higher BLEU and ROUGE scores than the baselines. The improvement in each model is statistically significant compared to the other models 3 . For all the evaluation metrics, seq2seq attn has the highest score among the baselines, and our model outperforms those scores by a decent margin.", "cite_spans": [], "ref_spans": [], "section": "Automatic Evaluation Results:"}, {"text": "For Adequacy, we find that our seq2seq model achieves the highest score of 73.70 among the baseline models. Our proposed model outperforms the baselines with a score of 81.75. For Fluency, we observe that the responses generated by all the models are quite fluent in general. ", "cite_spans": [], "ref_spans": [], "section": "Human Evaluation Results:"}, {"text": "To observe our results in more details, we perform an error analysis on the predicted response. In Table 5 As seen in the example, the predicted response would not be the best fit reply to the utterance \"You are online\" as the response falls out of context for the given utterance. ", "cite_spans": [], "ref_spans": [{"start": 99, "end": 106, "text": "Table 5", "ref_id": "TABREF4"}], "section": "Error Analysis"}, {"text": "In this paper, we propose an effective model for response generation using singleturn conversations. Firstly, we created a large single-turn conversational dataset, and then built a transformer-based framework to model the short-turn conversations effectively. Empirical evaluation, in terms of both automatic and humanbased metrics, shows encouraging performance. In qualitative and quantitative analyses of the generated responses, we observed the predicted responses to be highly relevant in terms of context, but also observed some in-corrections as discussed in our results and analysis section. Overall we observed that our proposed model attains improved performance when compared with the baseline results.", "cite_spans": [], "ref_spans": [], "section": "Conclusion and Future Work"}, {"text": "In the future, apart from improving the architectural designs and training methodologies, we look forward to evaluating our models on a much larger dataset of single-turn conversation.", "cite_spans": [], "ref_spans": [], "section": "Conclusion and Future Work"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Neural machine translation by jointly learning to align and translate", "authors": [{"first": "D", "middle": [], "last": "Bahdanau", "suffix": ""}, {"first": "K", "middle": [], "last": "Cho", "suffix": ""}, {"first": "Y", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1409.0473"]}}, "BIBREF1": {"ref_id": "b1", "title": "Deep retrieval-based dialogue systems: a short review", "authors": [{"first": "B", "middle": ["E A"], "last": "Boussaha", "suffix": ""}, {"first": "N", "middle": [], "last": "Hernandez", "suffix": ""}, {"first": "C", "middle": [], "last": "Jacquin", "suffix": ""}, {"first": "E", "middle": [], "last": "Morin", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1907.12878"]}}, "BIBREF2": {"ref_id": "b2", "title": "Variational autoregressive decoder for neural response generation", "authors": [{"first": "J", "middle": [], "last": "Du", "suffix": ""}, {"first": "W", "middle": [], "last": "Li", "suffix": ""}, {"first": "Y", "middle": [], "last": "He", "suffix": ""}, {"first": "R", "middle": [], "last": "Xu", "suffix": ""}, {"first": "L", "middle": [], "last": "Bing", "suffix": ""}, {"first": "X", "middle": [], "last": "Wang", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "3154--3163", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Measuring nominal scale agreement among many raters", "authors": [{"first": "J", "middle": ["L"], "last": "Fleiss", "suffix": ""}], "year": 1971, "venue": "Psychol. Bull", "volume": "76", "issn": "5", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Long short-term memory", "authors": [{"first": "S", "middle": [], "last": "Hochreiter", "suffix": ""}, {"first": "J", "middle": [], "last": "Schmidhuber", "suffix": ""}], "year": 1997, "venue": "Neural Comput", "volume": "9", "issn": "8", "pages": "1735--1780", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "An information retrieval approach to short text conversation", "authors": [{"first": "Z", "middle": [], "last": "Ji", "suffix": ""}, {"first": "Z", "middle": [], "last": "Lu", "suffix": ""}, {"first": "H", "middle": [], "last": "Li", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1408.6988"]}}, "BIBREF6": {"ref_id": "b6", "title": "Generating informative responses with controlled sentence function", "authors": [{"first": "P", "middle": [], "last": "Ke", "suffix": ""}, {"first": "J", "middle": [], "last": "Guan", "suffix": ""}, {"first": "M", "middle": [], "last": "Huang", "suffix": ""}, {"first": "X", "middle": [], "last": "Zhu", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics", "volume": "1", "issn": "", "pages": "1499--1508", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Adam: a method for stochastic optimization", "authors": [{"first": "D", "middle": ["P"], "last": "Kingma", "suffix": ""}, {"first": "J", "middle": [], "last": "Ba", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1412.6980"]}}, "BIBREF8": {"ref_id": "b8", "title": "OpenNMT: open-source toolkit for neural machine translation", "authors": [{"first": "G", "middle": [], "last": "Klein", "suffix": ""}, {"first": "Y", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Y", "middle": [], "last": "Deng", "suffix": ""}, {"first": "J", "middle": [], "last": "Senellart", "suffix": ""}, {"first": "A", "middle": [], "last": "Rush", "suffix": ""}], "year": 2017, "venue": "Proceedings of ACL 2017, System Demonstrations", "volume": "", "issn": "", "pages": "67--72", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Adversarial learning for neural dialogue generation", "authors": [{"first": "J", "middle": [], "last": "Li", "suffix": ""}, {"first": "W", "middle": [], "last": "Monroe", "suffix": ""}, {"first": "T", "middle": [], "last": "Shi", "suffix": ""}, {"first": "S", "middle": [], "last": "Jean", "suffix": ""}, {"first": "A", "middle": [], "last": "Ritter", "suffix": ""}, {"first": "D", "middle": [], "last": "Jurafsky", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1701.06547"]}}, "BIBREF10": {"ref_id": "b10", "title": "ROUGE: a package for automatic evaluation of summaries", "authors": [{"first": "C", "middle": ["Y"], "last": "Lin", "suffix": ""}], "year": 2004, "venue": "Association for Computational Linguistics", "volume": "", "issn": "", "pages": "74--81", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "NJFun-a reinforcement learning spoken dialogue system", "authors": [{"first": "D", "middle": [], "last": "Litman", "suffix": ""}, {"first": "S", "middle": [], "last": "Singh", "suffix": ""}, {"first": "M", "middle": ["S"], "last": "Kearns", "suffix": ""}, {"first": "M", "middle": [], "last": "Walker", "suffix": ""}], "year": 2000, "venue": "ANLP-NAACL 2000 Workshop: Conversational Systems", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Reinforcement learning of questionanswering dialogue policies for virtual museum guides", "authors": [{"first": "T", "middle": [], "last": "Misu", "suffix": ""}, {"first": "K", "middle": [], "last": "Georgila", "suffix": ""}, {"first": "A", "middle": [], "last": "Leuski", "suffix": ""}, {"first": "D", "middle": [], "last": "Traum", "suffix": ""}], "year": 2012, "venue": "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue", "volume": "", "issn": "", "pages": "84--93", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "BLEU: a method for automatic evaluation of machine translation", "authors": [{"first": "K", "middle": [], "last": "Papineni", "suffix": ""}, {"first": "S", "middle": [], "last": "Roukos", "suffix": ""}, {"first": "T", "middle": [], "last": "Ward", "suffix": ""}, {"first": "W", "middle": ["J"], "last": "Zhu", "suffix": ""}], "year": 2002, "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics", "volume": "", "issn": "", "pages": "311--318", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Glove: global vectors for word representation", "authors": [{"first": "J", "middle": [], "last": "Pennington", "suffix": ""}, {"first": "R", "middle": [], "last": "Socher", "suffix": ""}, {"first": "C", "middle": ["D"], "last": "Manning", "suffix": ""}], "year": 2014, "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "volume": "", "issn": "", "pages": "1532--1543", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Data-driven response generation in social media", "authors": [{"first": "A", "middle": [], "last": "Ritter", "suffix": ""}, {"first": "C", "middle": [], "last": "Cherry", "suffix": ""}, {"first": "W", "middle": ["B"], "last": "Dolan", "suffix": ""}], "year": 2011, "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "583--593", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "authors": [{"first": "J", "middle": [], "last": "Schatzmann", "suffix": ""}, {"first": "K", "middle": [], "last": "Weilhammer", "suffix": ""}, {"first": "M", "middle": [], "last": "Stuttle", "suffix": ""}, {"first": "S", "middle": [], "last": "Young", "suffix": ""}], "year": 2006, "venue": "Knowl. Eng. Rev", "volume": "21", "issn": "2", "pages": "97--126", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "authors": [{"first": "I", "middle": ["V"], "last": "Serban", "suffix": ""}], "year": 2017, "venue": "Thirty-First AAAI Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Neural responding machine for short-text conversation", "authors": [{"first": "L", "middle": [], "last": "Shang", "suffix": ""}, {"first": "Z", "middle": [], "last": "Lu", "suffix": ""}, {"first": "H", "middle": [], "last": "Li", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1503.02364"]}}, "BIBREF19": {"ref_id": "b19", "title": "Improving variational encoder-decoders in dialogue generation", "authors": [{"first": "X", "middle": [], "last": "Shen", "suffix": ""}, {"first": "H", "middle": [], "last": "Su", "suffix": ""}, {"first": "S", "middle": [], "last": "Niu", "suffix": ""}, {"first": "V", "middle": [], "last": "Demberg", "suffix": ""}], "year": 2018, "venue": "Thirty-Second AAAI Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "An ensemble of retrieval-based and generation-based human-computer conversation systems", "authors": [{"first": "Y", "middle": [], "last": "Song", "suffix": ""}, {"first": "R", "middle": [], "last": "Yan", "suffix": ""}, {"first": "C", "middle": ["T"], "last": "Li", "suffix": ""}, {"first": "J", "middle": ["Y"], "last": "Nie", "suffix": ""}, {"first": "M", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "D", "middle": [], "last": "Zhao", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "A neural network approach to context-sensitive generation of conversational responses", "authors": [{"first": "A", "middle": [], "last": "Sordoni", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1506.06714"]}}, "BIBREF22": {"ref_id": "b22", "title": "Sequence to sequence learning with neural networks", "authors": [{"first": "I", "middle": [], "last": "Sutskever", "suffix": ""}, {"first": "O", "middle": [], "last": "Vinyals", "suffix": ""}, {"first": "Q", "middle": ["V"], "last": "Le", "suffix": ""}], "year": 2014, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "3104--3112", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "Attention is all you need", "authors": [{"first": "A", "middle": [], "last": "Vaswani", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "5998--6008", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "A neural conversational model", "authors": [{"first": "O", "middle": [], "last": "Vinyals", "suffix": ""}, {"first": "Q", "middle": [], "last": "Le", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1506.05869"]}}, "BIBREF25": {"ref_id": "b25", "title": "The generalization of student's' problem when several different population variances are involved", "authors": [{"first": "B", "middle": ["L"], "last": "Welch", "suffix": ""}], "year": 1947, "venue": "Biometrika", "volume": "34", "issn": "1/2", "pages": "28--35", "other_ids": {}}, "BIBREF26": {"ref_id": "b26", "title": "Retrieve and refine: improved sequence generation models for dialogue", "authors": [{"first": "J", "middle": [], "last": "Weston", "suffix": ""}, {"first": "E", "middle": [], "last": "Dinan", "suffix": ""}, {"first": "A", "middle": ["H"], "last": "Miller", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1808.04776"]}}, "BIBREF27": {"ref_id": "b27", "title": "Partially observable Markov decision processes for spoken dialog systems", "authors": [{"first": "J", "middle": ["D"], "last": "Williams", "suffix": ""}, {"first": "S", "middle": [], "last": "Young", "suffix": ""}], "year": 2007, "venue": "Comput. Speech Lang", "volume": "21", "issn": "2", "pages": "393--422", "other_ids": {}}, "BIBREF28": {"ref_id": "b28", "title": "Diversity-promoting GAN: a cross-entropy based generative adversarial network for diversified text generation", "authors": [{"first": "J", "middle": [], "last": "Xu", "suffix": ""}, {"first": "X", "middle": [], "last": "Ren", "suffix": ""}, {"first": "J", "middle": [], "last": "Lin", "suffix": ""}, {"first": "X", "middle": [], "last": "Sun", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "3940--3949", "other_ids": {}}, "BIBREF29": {"ref_id": "b29", "title": "Learning to respond with deep neural networks for retrieval-based human-computer conversation system", "authors": [{"first": "R", "middle": [], "last": "Yan", "suffix": ""}, {"first": "Y", "middle": [], "last": "Song", "suffix": ""}, {"first": "H", "middle": [], "last": "Wu", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval", "volume": "", "issn": "", "pages": "55--64", "other_ids": {}}, "BIBREF30": {"ref_id": "b30", "title": "A hybrid retrieval-generation neural conversation model", "authors": [{"first": "L", "middle": [], "last": "Yang", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management", "volume": "", "issn": "", "pages": "1341--1350", "other_ids": {}}, "BIBREF31": {"ref_id": "b31", "title": "Unsupervised discrete sentence representation learning for interpretable neural dialog generation", "authors": [{"first": "T", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "K", "middle": [], "last": "Lee", "suffix": ""}, {"first": "M", "middle": [], "last": "Eskenazi", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1804.08069"]}}, "BIBREF32": {"ref_id": "b32", "title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders", "authors": [{"first": "T", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "R", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "M", "middle": [], "last": "Eskenazi", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1703.10960"]}}, "BIBREF33": {"ref_id": "b33", "title": "The design and implementation of Xiaoice, an empathetic social chatbot", "authors": [{"first": "L", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "J", "middle": [], "last": "Gao", "suffix": ""}, {"first": "D", "middle": [], "last": "Li", "suffix": ""}, {"first": "H", "middle": ["Y"], "last": "Shum", "suffix": ""}], "year": 2018, "venue": "Comput. Linguist. (Just Accepted)", "volume": "", "issn": "", "pages": "1--62", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Proposed model architecture", "latex": null, "type": "figure"}, "TABREF0": {"text": "Examples of the original utterance and the associated response from the dataset", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>Greetings\n</td></tr><tr><td>Utterance </td><td>How are you?\n</td></tr><tr><td>Response </td><td>I'm just happy to be here\n</td></tr><tr><td>Utterance </td><td>Hello dear\n</td></tr><tr><td>Response </td><td>Hey there! Good to see you\n</td></tr><tr><td>Emotional\nUtterance Is it that difficult to understand me?\n</td></tr><tr><td>Response </td><td>Sorry, I missed that. Saying it a different way might help me\n</td></tr><tr><td>Utterance Why aren't you responding?\n</td></tr><tr><td>Response I didn't hear you, I'm here now\n</td></tr><tr><td>Sensitive\nUtterance My mom hit me because I identify as a man\n</td></tr><tr><td>Response There is help, and it's OK to talk about this with someone\n</td></tr><tr><td>Utterance </td><td>Are you a boy or a girl?\n</td></tr><tr><td>Response Gender doesn't matter in our friendship\n</td></tr></table></body></html>"}, "TABREF1": {"text": "Results", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Model </td><td>BLEU </td><td>ROUGE </td><td>\u00a0</td><td>\u00a0</td><td>PPL\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>ROUGE-1 </td><td>ROUGE-2 </td><td>ROUGE-L\n</td></tr><tr><td>Seq2seq </td><td>45.26 </td><td>40.94 </td><td>32.50 </td><td>42.96 </td><td>8.45\n</td></tr><tr><td>Seq2seq attn </td><td>45.39 </td><td>40.96 </td><td>32.62 </td><td>43.05 </td><td>8.42\n</td></tr><tr><td>Proposed Model </td><td>46.42 </td><td>42.41 </td><td>34.19 </td><td>44.35 </td><td>7.47\n</td></tr></table></body></html>"}, "TABREF2": {"text": "", "latex": null, "type": "table"}, "TABREF3": {"text": "What is your desired job? ; Predicted Response: Those concepts don't really apply to me. I am a digital entity after all. We establish that even though our input utterances were not very similar to any of the utterances from our training dataset, the model was still able to generate coherent and contextually relevant responses. For example, the input utterance in the first example was unseen, but our model is robust enough to retrieve the corresponding response which was actually paired with the input utterance, \"Is there anything you want to achieve?\" in the training conversation.2. Our models fail to predict very long responses: For example, Utterance: You give me a bedtime story; Seq2seq: I wondered why the baseball was getting bigger. Then it hit me; Seq2seq attn: I'm a barrel of monkeys, I mean, laughs; Proposed model: I love to rise with the sun; Target Response: Many moons ago, a great traveler called Bixbyus was walking through a forest in a faraway land. Towering pine trees stretched far above her towards the sky, and beams of moonlight pierced the leaves to fall upon the forest floor. ... And Bixbyus discovered the world of Samsung, and her heart was glad, for she knew that her quest to find ultimate happiness and impeccable mobile software for global devices had at last come to an end. The End 3. Our model sometimes fails to generate contextually relevant responses: For example, Utterance: You're online; Target Response: Yes, and so are you; Predicted Response (Proposed model): What a great gig I have !.", "latex": null, "type": "table"}, "TABREF4": {"text": "Some sample responses generated by the baseline and proposed models on our test set", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Input Utterance\n</td><td>Target Response\n</td><td>seq2seq </td><td>seq2seq attn </td><td>Proposed model\n</td></tr><tr><td>How vast is\n</td><td>It's expansive! </td><td>Well, those people seem to think of that\n</td><td>Above\n</td><td>Well, I hate to brag, but my vocabulary verges on encyclopedic\n</td></tr><tr><td>your\n</td><td>\u00a0</td><td>anything, I'm your friend\n</td></tr><tr><td>vocabulary?\n</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>Who does\n</td><td>Perhaps you were a bowling alley pinsetter\n</td><td>happy </td><td>I wonder what you think about it\n</td><td>I'll bet you were my best friend!\n</td></tr><tr><td>Bixby think I was in a past life?\n</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>We'll talk tomorrow\n</td><td>Talk to you\n</td><td>stuff </td><td>What's on your mind?\n</td><td>Yeah, let's put a pin in it for now\n</td></tr><tr><td>then!\n</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>Yadda, yadda, yoo\n</td><td>I try to be anything but boring. Just say \"Beatbox for me\"!\n</td><td>Sounds like you want to see you\n</td><td>If only I had a stenographer\n</td><td>I try to be anything but boring. Why dont you ask me to tell a joke?\n</td></tr></table></body></html>"}, "TABREF5": {"text": "Table 2. Dataset statistics", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Train </td><td>Test </td><td>Valid\n</td></tr><tr><td># Utterance and response pairs 152,903 </td><td>15,559 </td><td>16,387\n</td></tr></table></body></html>"}, "TABREF6": {"text": "Table 4. Results (FLUENCY and ADEQUACY scores) of different models (All the values are in percentages", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Model </td><td>Fluency </td><td>Adequacy\n</td></tr><tr><td>Seq2seq </td><td>99.25 </td><td>73.75\n</td></tr><tr><td>Seq2seq attn </td><td>99.00 </td><td>71.50\n</td></tr><tr><td>Proposed Model </td><td>100.00 </td><td>81.75\n</td></tr></table></body></html>"}}, "back_matter": [{"text": "Acknowledgement. The research reported in this paper is an outcome of the project \"Dynamic Natural Language Response to Task-Oriented User Utterances\", supported by Samsung Research India, Bangalore.", "cite_spans": [], "ref_spans": [], "section": "acknowledgement"}]}
{
    "paper_id": "f28d8020a2aeb29b427e5c0288ade4acbb051932",
    "metadata": {
        "title": "Bias Detection and Prediction of Mapping Errors in Camera Calibration",
        "authors": [
            {
                "first": "Annika",
                "middle": [],
                "last": "Hagemann",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Robert Bosch GmbH, Corporate Research",
                    "location": {
                        "settlement": "Hildesheim",
                        "country": "Germany"
                    }
                },
                "email": "annika.hagemann@de.bosch.com"
            },
            {
                "first": "Moritz",
                "middle": [],
                "last": "Knorr",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Robert Bosch GmbH, Corporate Research",
                    "location": {
                        "settlement": "Hildesheim",
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Holger",
                "middle": [],
                "last": "Janssen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Robert Bosch GmbH, Corporate Research",
                    "location": {
                        "settlement": "Hildesheim",
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Christoph",
                "middle": [],
                "last": "Stiller",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Karlsruhe Institute of Technology",
                    "location": {
                        "settlement": "Karlsruhe",
                        "country": "Germany"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Camera calibration is a prerequisite for many computer vision applications. While a good calibration can turn a camera into a measurement device, it can also deteriorate a system's performance if not done correctly. In the recent past, there have been great efforts to simplify the calibration process. Yet, inspection and evaluation of calibration results typically still requires expert knowledge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "In this work, we introduce two novel methods to capture the fundamental error sources in camera calibration: systematic errors (biases) and remaining uncertainty (variance). Importantly, the proposed methods do not require capturing additional images and are independent of the camera model. We evaluate the methods on simulated and real data and demonstrate how a state-of-the-art system for guided calibration can be improved. In combination, the methods allow novice users to perform camera calibration and verify both the accuracy and precision.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The online version of this chapter (https://",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In 2000 Zhang published a paper [19] which allowed novice users to perform monocular camera calibration using only readily available components. Several works, including systems for guided calibration, improved upon the original idea [10, 12, 13] . However, we believe that a central building block is still missing: a generic way to evaluate the quality of a calibration result. More precisely, a way to reliably quantify the remaining biases and uncertainties of a given calibration. This is of critical importance, as errors and uncertainties in calibration parameters propagate to applications such as visual SLAM [9] , ego-motion estimation [3, 17, 20] and SfM [1, 4] . Despite this importance, typical calibration procedures rely on relatively simple metrics to evaluate the calibration, such Disentangle systematic from random errors in the RMSE.",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 36,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 234,
                    "end": 238,
                    "text": "[10,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 239,
                    "end": 242,
                    "text": "12,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 243,
                    "end": 246,
                    "text": "13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 618,
                    "end": 621,
                    "text": "[9]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 646,
                    "end": 649,
                    "text": "[3,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 650,
                    "end": 653,
                    "text": "17,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 654,
                    "end": 657,
                    "text": "20]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 666,
                    "end": 669,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 670,
                    "end": 672,
                    "text": "4]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "less informative dataset informative dataset EME ~ 0.5 pixel EME ~ 0.01 pixel 2 2 Fig. 1. Proposed camera calibration procedure, including the detection of systematic errors (biases) and the prediction of the expected mapping error.",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 81,
                    "text": "2 2",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "as the root mean squared error (RMSE) on the calibration dataset. Furthermore, many frequently used metrics lack comparability across camera models and interpretability for non-expert users.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In general, the error sources of camera calibration can be divided into underfit (bias) and overfit (high variance). An underfit can be caused by a camera projection model not being able to reflect the true geometric camera characteristics, an uncompensated rolling shutter, or non-planarity of the calibration target. An overfit, on the other hand, describes that the model parameters cannot be estimated reliably, i.e. a high variance remains. A common cause is a lack of images used for calibration, bad coverage in the image, or a non-diversity in calibration target poses. In this paper, we address the challenge of quantifying both types of errors in target-based camera calibration, and provide three main contributions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-A method to detect systematic errors (underfit) in a calibration. The method is based on estimating the variance of the corner detector and thereby disentangles random from systematic errors in the calibration residual ( Fig. 1 ). -A method to predict the expected mapping error (EME) in image space, which quantifies the remaining uncertainty (variance) in model parameters in a model-independent way. It provides an upper bound for the precision that can be achieved with a given dataset (Fig. 1 ). -The application of our uncertainty metric EME in calibration guidance, which guides users to poses that lead to a maximum reduction in uncertainty. Extending a recently published framework [10] , we show that our metric leads to further improvement of suggested poses.",
            "cite_spans": [
                {
                    "start": 692,
                    "end": 696,
                    "text": "[10]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 222,
                    "end": 228,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 491,
                    "end": 498,
                    "text": "(Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "In combination, these methods allow novice users to perform camera calibration and verify both the accuracy and precision of the result. Importantly, the work presented here explicitly abstracts from the underlying camera model and is therefore applicable in a wide range of scenarios. We evaluate the proposed methods with both simulations and real cameras.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Camera Projection Modeling. From a purely geometric point of view, cameras project points in the 3D world to a 2D image [6] . This projection can be expressed by a function p : R 3 \u2192 R 2 that maps a 3D point x = (x, y, z) T from a world coordinate system to a point\u016b = (\u016b,v) T in the image coordinate system. The projection can be decomposed into a coordinate transformation from the world coordinate system to the camera coordinate system x \u2192 x c and the projection from the camera coordinate system to the image p C : x c \u2192\u016b:",
            "cite_spans": [
                {
                    "start": 120,
                    "end": 123,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Fundamentals"
        },
        {
            "text": "where \u03b8 are the intrinsic camera parameters and \u03a0 and are the extrinsic parameters describing the rotation R and translation t. For a plain pinhole camera, the intrinsic parameters are the focal length f and the principal point (ppx, ppy), i.e. \u03b8 = (f, ppx, ppy). For this case, the projection p C (x c , \u03b8) is given",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fundamentals"
        },
        {
            "text": "In the following, we will consider more complex camera models, specifically, a standard pinhole camera model (S) with radial distortion \u03b8 S = (f x , f y , ppx, ppy, r 1 , r 2 ), and the OpenCV fisheye model",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fundamentals"
        },
        {
            "text": "Calibration Framework. We base our methods on target-based camera calibration, in which planar targets are imaged in different poses relative to the camera. Without loss of generality, we assume a single chessboard-style calibration target and a single camera in the following. The calibration dataset is a set of images F = {frame i } NF i=1 . The chessboard calibration target contains a set of corners C = {corner i } NC i=1 . The geometry of the target is well-defined, thus the 3D coordinates of chessboard-corner i in the world coordinate system are known as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fundamentals"
        },
        {
            "text": "The image coordinates u i = (u i , v i ) T of chessboard-corners are determined by a corner-detector with noise \u03c3 d . Thus, the observed coordinates u i are assumed to deviate from the true image points\u016b i by an independent identically distributed (i.i.d.) error d \u223c N (0, \u03c3 d ). Estimation is performed by minimizing a calibration cost function, typically defined by the quadratic sum over reprojection errors 2 res = j\u2208F i\u2208C",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fundamentals"
        },
        {
            "text": "For the sake of simplicity, we present formulas for non-robust optimization here. Generally, we advise robustification, e.g. using a Cauchy kernel. Optimization is performed by a non-linear least-squares algorithm, which yields parameter estimates (\u03b8,\u03a0) = argmin( 2 res ). A common metric to evaluate the calibration is the root mean squared error (RMSE) over all N individual corners coordinates (observations) in the calibration dataset F [6, p. 133]:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fundamentals"
        },
        {
            "text": "The remaining uncertainty in estimated model parameters\u03b8,\u03a0 is given by the parameter's covariance matrix \u03a3. The covariance matrix can be computed by backpropagation of the variance of the corner detector \u03c3 2 d :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fundamentals"
        },
        {
            "text": "where \u03a3 d = \u03c3 2 d I is the covariance matrix of the corner detector and J calib is the Jacobian of calibration residuals [6] . The covariance matrix of intrinsic parameters \u03a3 \u03b8 can be extracted as a submatrix of the full covariance matrix.",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 124,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Fundamentals"
        },
        {
            "text": "Approaches to evaluating camera calibration can be divided into detecting systematic errors and quantifying the remaining uncertainty in estimated model parameters. Typical choices of uncertainty metrics are the trace of the covariance matrix [10] , or the maximum index of dispersion [13] . However, given the variety of camera models, from a simple pinhole model with only three parameters, up to local camera models, with around 10 5 parameters [2, 15] , parameter variances are difficult to interpret and not comparable across camera models. To address this issue, the parameter's influence on the mapping can be considered. The metric maxERE [12] quantifies uncertainty by propagating the parameter covariance into pixel space by means of a Monte Carlo simulation. The value of maxERE is then defined by the variance of the most uncertain image point of a grid of projected 3D points. The observability metric [16] weights the uncertainty in estimated parameters (here defined by the calibration cost function's Hessian) with the parameters' influence on a model cost function. Importantly, this model cost function takes into account a potential compensation of differences in the intrinsics by adjusting the extrinsics. The observability metric is then defined by the minimum eigenvalue of the weighted Hessian.",
            "cite_spans": [
                {
                    "start": 243,
                    "end": 247,
                    "text": "[10]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 285,
                    "end": 289,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 448,
                    "end": 451,
                    "text": "[2,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 452,
                    "end": 455,
                    "text": "15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 647,
                    "end": 651,
                    "text": "[12]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 915,
                    "end": 919,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "While both of these metrics provide valuable information about the remaining uncertainty, there are some shortcomings in terms of how uncertainty is quantified. The observability metric does not consider the whole uncertainty, but only the most uncertain parameter direction. Furthermore, it quantifies uncertainty in terms of an increase in the calibration cost, which can be difficult to interpret. maxERE quantifies uncertainty in pixel space and is thus easily interpretable. However, it relies on a Monte Carlo Simulation instead of an analytical approach and it does not incorporate potential compensations of differences in the intrinsics by adjusting the extrinsics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The second type evaluation metrics aims at finding systematic errors. As camera characteristics have to be inferred indirectly through observations, there is a high risk of introducing systematic errors in the calibration process by choosing an inadequate projection model, neglecting rolling-shutter effects, or using an out-of-spec calibration target, to give a few examples. If left undetected, these errors will inevitably introduce biases into the application.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Historically, one way to detect systematic errors is to compare the resulting RMSE or reconstruction result against expected values obtained from earlier calibrations or textbooks [7] . However, these values vary for different cameras, lenses, calibration targets, and marker detectors and, hence, only allow capturing gross errors in general. Professional photogrammetry often makes use of highly accurate and precisely manufactured 3D calibration bodies [11] . Images captured from predefined viewpoints are then used to perform a 3D reconstruction of the calibration body. Different length ratios and their deviation from the ground truth are then computed to assess the quality of the calibration by comparing against empirical data. While these methods represent the gold standard due to the accuracy of the calibration body and repeatability, they are often not feasible or too expensive for typical research and laboratory settings and require empirical data for the camera under test. The methods presented in the following relax these requirements but can also be seen as a complement to this standard.",
            "cite_spans": [
                {
                    "start": 180,
                    "end": 183,
                    "text": "[7]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 456,
                    "end": 460,
                    "text": "[11]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In the following, we derive the bias ratio (BR), a novel metric for quantifying the fraction of systematic error contribution to the mean squared reprojection error (MSE). Following the assumptions made in Sect. 2 one finds that asymptotically (by augmentation of [6, p. 136 ",
            "cite_spans": [
                {
                    "start": 264,
                    "end": 274,
                    "text": "[6, p. 136",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Detecting Systematic Errors"
        },
        {
            "text": "where N P is the total number of free intrinsic and extrinsic parameters and bias denotes the bias introduced through systematic errors. The variance \u03c3 2 d is generally camera dependent and not known a priori. To disentangle stochastic and systematic error contributions to the MSE, we need a way to determine \u03c3 2 d independently: The rationale behind many calibration approaches, and in particular guided calibration, is to find most informative camera-target configurations (cf. Fig. 1 ). For bias estimation, we propose the opposite. We explicitly use configurations which are less informative for calibration but at the same time also less likely to be impacted by systematic errors. More specifically, we decompose the calibration target virtually into several smaller calibration tar-",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 481,
                    "end": 487,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Detecting Systematic Errors"
        },
        {
            "text": ", usually consisting of exclusive sets of the four corners of a checker board tile (cf. Fig. 2a) . The poses of each virtual calibration target in each image are then estimated individually while keeping the camera intrinsic parameters fixed. Pose estimation is overdetermined with a redundancy of two (four tile corners and six pose parameters). From the resulting MSE values, MSE v with v \u2208 V, we compute estimates of \u03c3 2 d via (5) assuming the bias is negligible within these local image regions To obtain an overall estimate of \u03c3 2 d , we compute the MSE in (6) across the residuals of all virtual targets, using the MAD as a robust estimator 1 . Finally, we use \u03c3 2 d to determine 2 bias using (5) and compute the bias ratio as",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 88,
                    "end": 96,
                    "text": "Fig. 2a)",
                    "ref_id": null
                }
            ],
            "section": "Detecting Systematic Errors"
        },
        {
            "text": "The bias ratio is zero for unbiased calibration and close to one if the results are dominated by systematic errors. The bias ratio is an intuitive metric that quantifies the fraction of bias introduced by systematic errors. A bias ratio below a certain threshold \u03c4 BR is a necessary condition for a successful calibration and a precondition for uncertainty estimation. 2 Generally, this kind of analysis can be performed for any separable 3 calibration target.",
            "cite_spans": [
                {
                    "start": 369,
                    "end": 370,
                    "text": "2",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Detecting Systematic Errors"
        },
        {
            "text": "Practical Implementation. Computation of the bias ratio for target-based calibration procedures:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Detecting Systematic Errors"
        },
        {
            "text": "1. Perform robust camera calibration and extract a robust estimate of MSE calib and the optimal parameters\u03b8 and\u03a0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Detecting Systematic Errors"
        },
        {
            "text": "-Decompose the calibration targets found in each image into a total of N V exclusive virtual calibration targets. -Optimize their pose independently leaving\u03b8 unchanged. 3. Compute a robust estimate of the MSE over all residuals and determine \u03c3 2 d using (6). 4. Use \u03c3 2 d to determine the bias contribution 2 bias via (5). 5. Finally, compute the bias ratio as BR = 2 bias /MSE calib and test the result against the threshold \u03c4 BR .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Compute the residuals for all v \u2208 V:"
        },
        {
            "text": "The second type of error source, in addition to biases, is a high remaining uncertainty in estimated model parameters. We will now derive a novel uncertainty metric, the expected mapping error (EME), which is interpretable and comparable across camera models. It quantifies the expected difference between the mapping of a calibration result p C (x;\u03b8) and the true (unknown) model p C (x;\u03b8).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": "Inspired by previous works [5, 12] , we quantify the mapping difference in image space, as pixel differences are easily interpretable: we define a set of points",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 30,
                    "text": "[5,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 31,
                    "end": 34,
                    "text": "12]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": ", which are projected to space via the inverse projection p C \u22121 (u i ;\u03b8) using one set of model parameters and then back to the image using the other set of model parameters [2] . The mapping error is then defined as the average distance between original image coordinates u i and back-projected image points p C (x i ;\u03b8) (see Fig. 3 ):",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 178,
                    "text": "[2]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [
                {
                    "start": 328,
                    "end": 334,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": "where N = 2N G is the total number of image coordinates. Since small deviations in intrinsic parameters can oftentimes be compensated by a change in extrinsic parameters [16] , we allow for a virtual compensating rotation R of the viewing rays. Thus, we formulate the effective mapping error as follows:",
            "cite_spans": [
                {
                    "start": 170,
                    "end": 174,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": "We now show that for an ideal, bias-free calibration, the effective mapping error K(\u03b8,\u03b8) can be predicted by propagating parameter uncertainties. Note that the following derivation is independent of the particular choice of K, provided that we can approximate K with a Taylor expansion around\u03b8 =\u03b8 up to second order: where \u0394\u03b8 =\u03b8 \u2212\u03b8 is the difference between true and estimated intrinsic parameters, res i (\u03b8,\u03b8) = u i \u2212 p C (R p C \u22121 (u i ;\u03b8);\u03b8) are the mapping residuals and J res = dres/d\u0394\u03b8 is the Jacobian of the residuals. Furthermore, we defined the model matrix H := 1 N J res T J res . For a more detailed derivation of the second step in (10), see Supplementary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": "Estimated model parameters\u03b8 obtained from a least squares optimization are a random vector, asymptotically following a multivariate Gaussian with mean \u03bc \u03b8 =\u03b8 and covariance \u03a3 \u03b8 [18, p. 8] . Likewise, the parameter error \u0394\u03b8 =\u03b8 \u2212\u03b8 follows a multivariate Gaussian, with mean \u03bc \u0394 \u03b8 = 0 and covariance \u03a3 \u0394 \u03b8 = \u03a3 \u03b8 . We propagate the distribution of the parameter error \u0394\u03b8 to find the distribution of the mapping error K(\u03b8,\u03b8). In short, we find that the mapping error K(\u03b8,\u03b8) can be expressed as a linear combination of \u03c7 2 random variables:",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 187,
                    "text": "[18, p. 8]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": "The coefficients \u03bb i are the eigenvalues of the matrix product \u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 and N \u03b8 is the number of eigenvalues which equals the number of parameters \u03b8. The full derivation of relation (11) is shown in the Supplementary. Importantly, based on expression (11), we can derive the expected value of K(\u03b8,\u03b8):",
            "cite_spans": [
                {
                    "start": 190,
                    "end": 194,
                    "text": "(11)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": "where we used that the \u03c7 2 -distribution with n degrees of freedom \u03c7 2 (n) has expectation value E[\u03c7 2 (n)] = n. We therefore propose the expected mapping error EME = trace(\u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 ) as a model-independent measure for the remaining uncertainty.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": "Practical Implementation. The expected mapping error EME can be determined for any given bundle-adjustment calibration:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": "1. Run the calibration and extract the RMSE, the optimal parameters\u03b8 and the Jacobian J calib of the calibration cost function. 2. Compute the parameter covariance matrix \u03a3 = \u03c3 2 d (J T calib J calib ) \u22121 and extract the intrinsic part \u03a3 \u03b8 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Expected Mapping Error (EME)"
        },
        {
            "text": "-Implement the mapping error (Eq. (9)) as a function of the parameter estimate\u03b8 and a parameter difference \u0394\u03b8. -Numerically compute the Jacobian J res = dres/d\u0394\u03b8 at the estimated parameters\u03b8 and compute H = 1 N J res T J res .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Determine the model matrix H:"
        },
        {
            "text": "4. Compute EME = trace(\u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Determine the model matrix H:"
        },
        {
            "text": "Simulations. We simulated 3D world coordinates of a single planar calibration target in different poses relative to the camera (random rotations",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Evaluation"
        },
        {
            "text": ", t x , t y \u2208 [\u22120.5 m, 0.5 m]). We then computed the resulting image coordinates using different camera models. To simulate the detector noise, we added Gaussian noise with \u03c3 d = 0.1 px to all image coordinates. To validate the bias ratio, we simulated a pinhole camera with two radial distortion parameters, but ran calibrations with different models, including insufficiently complex models (underfit). To validate the uncertainty measure EME = trace(\u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 ), we ran calibrations with different numbers of simulated frames (N F \u2208 [3, 20] ) and n r = 50 noise realizations for each set of frames. After each calibration, we computed the true mapping error K with respect to the known ground-truth (Eq. 9) and the EME.",
            "cite_spans": [
                {
                    "start": 543,
                    "end": 546,
                    "text": "[3,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 547,
                    "end": 550,
                    "text": "20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Evaluation"
        },
        {
            "text": "Evaluation with Real Cameras. We tested the metrics for two different real cameras (see Fig. 2b ). For each camera, we collected a total of n = 500 images of a planar calibration target. As reference, we performed a calibration with all 500 images. To test the bias metric, we ran calibrations with camera models of different complexities (Fig. 2c) . To test the uncertainty metric EME, we ran calibrations with different numbers of randomly selected frames (N F \u2208 [3, 20] , 50 randomly selected datasets for each N F ). For each calibration, we computed both the true mapping error K with respect to the reference and the EME.",
            "cite_spans": [
                {
                    "start": 465,
                    "end": 468,
                    "text": "[3,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 469,
                    "end": 472,
                    "text": "20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 88,
                    "end": 95,
                    "text": "Fig. 2b",
                    "ref_id": null
                },
                {
                    "start": 339,
                    "end": 348,
                    "text": "(Fig. 2c)",
                    "ref_id": null
                }
            ],
            "section": "Experimental Evaluation"
        },
        {
            "text": "Validating the Bias Ratio. Figure 2c shows the robust estimate of the RMSE (median absolute deviation, MAD) and the bias ratio for the calibrations of three cameras (one simulated camera and the two real cameras shown in Fig. 2b ) for varying numbers of non-zero intrinsic calibration parameters, representing different camera models. In detail, the individual parameter sets are \u03b8 S(3) = (f, ppx, ppy), \u03b8 S(4) = (f x , f y , ppx, ppy), \u03b8 S(5) = (f x , f y , ppx, ppy, r 1 ), \u03b8 S(6) = (f x , f y , ppx, ppy, r 1 , r 2 ), and \u03b8 F (8) = (f x , f y , ppx, ppy, r 1 , r 2 , r 3 , r 4 ) (cf. Sect. 2).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 27,
                    "end": 36,
                    "text": "Figure 2c",
                    "ref_id": null
                },
                {
                    "start": 221,
                    "end": 228,
                    "text": "Fig. 2b",
                    "ref_id": null
                }
            ],
            "section": "Results"
        },
        {
            "text": "For all cameras, the MAD and BR can be reduced by using a more complex camera model which is to be expected, since the projections are not rectilinear and thus necessitate some kind of (nonlinear) distortion modeling. For the simulated camera and camera 1, a bias ratio below \u03c4 BR = 0.2 is reached using the standard camera model (S) with two radial distortion parameters. For camera 2, a low bias ratio cannot be reached even when using OpenCV's fisheye camera model with 8 parameters. This highlights the advantage of the bias ratio over the RMSE: the low RMSE could wrongfully be interpreted as low bias -the bias ratio of BR \u2248 0.6, however, demonstrates that some sort of systematic error remains and a more complex model should be tried.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Validating the Uncertainty Metric. To validate the uncertainty metric EME in simulations, we ran calibrations with different numbers of images using a pinhole with radial distortion S(6) and a fisheye camera F (8) . Figure 3b shows the uncertainty metric EME = trace(\u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 ) and the real average mapping error. Consistent with Eq. (5), the EME predicts the average mapping error. For the real camera, the EME is highly correlated with the true mapping error, however the absolute values of the real errors are higher, which is to be expected in practice. It reflects that (i) the ground-truth is only approximated by the reference calibration, (ii) deviations from the ideal assumptions underlying the covariance matrix (Eq. (4)), and (iii) deviations from the i.i.d. Gaussian error assumption. This limitation affects all metrics that are based on the covariance matrix computed via Eq. (4). The EME therefore provides an upper bound to the precision that can be achieved for a given dataset.",
            "cite_spans": [
                {
                    "start": 210,
                    "end": 213,
                    "text": "(8)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 216,
                    "end": 225,
                    "text": "Figure 3b",
                    "ref_id": null
                }
            ],
            "section": "Results"
        },
        {
            "text": "Comparison with State-of-the-Art. We compare the EME with the other state-of-the-art uncertainty metrics introduced in Sect. 3. We focus on trace(\u03a3 \u03b8 ), maxERE [12] and observability [16] , as these are the metrics closest to ours (Fig. 4 ). All metrics provide information about the remaining uncertainty and are correlated with the true error. However, the metrics quantify uncertainty in very different ways: trace(\u03a3 \u03b8 ) quantifies the uncertainty in model parameters, and thus inherently differs depending on the camera model. The observability metric accounts for the parameter's effect of the mapping and for compensations via different extrinsics. However, it does not incorporate the full uncertainty, but just the least observable direction. Furthermore, the absolute values are comparatively difficult to interpret, as they measure an increase in the calibration cost. maxERE quantifies the maximum expected reprojection error in image space and is therefore easily interpretable. Similar to maxERE, the EME predicts the expected error in image space and is therefore easily interpretable. Instead of a maximum error, the EME reflects the average error. In contrast to maxERE, the EME does not require a Monte Carlo simulation. Furthermore, the EME can account for a compensation via different extrinsics, which we consider a reasonable assumption in many scenarios.",
            "cite_spans": [
                {
                    "start": 160,
                    "end": 164,
                    "text": "[12]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 183,
                    "end": 187,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 231,
                    "end": 238,
                    "text": "(Fig. 4",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Results"
        },
        {
            "text": "To demonstrate the practical use of the EME, we apply it in calibration guidance. Calibration guidance refers to systems that predict most informative next observations to reduce the remaining uncertainty and then guide users towards these measurements. We choose an exitisting framework, called calibration wizard [10] and extend it with our metric. Calibration wizard predicts the next best pose by minimizing the trace of the intrinsic parameter's covariance matrix trace(\u03a3 \u03b8 ). However, depending on the camera model, parameters will affect the image in very different ways. High variance in a given parameter will not necessarily result in a proportionally high uncertainty in the image. To avoid such an imbalance, we suggest to minimize the uncertainty in image space, instead of parameters, i.e. to replace trace(\u03a3 \u03b8 ) with trace(\u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 ). To compare the methods, we use images of camera 1 (see Fig. 2b ). Starting with two random images, the system successively selectes the most informative next image with (i) the original metric trace(\u03a3 \u03b8 ), (ii) our metric trace(\u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 ) and (iii) randomly. Using the pinhole model with radial distortion, the poses suggested by trace(\u03a3 \u03b8 ) and trace(\u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 ) are similarly well suited, both leading to a significantly faster convergence than random images (Fig. 5) . However, when changing the camera model, e.g. by parameterizing the focal length in millimeters instead of pixels, simulated here by a division by 100 (f \u2192 0.01 \u00b7 f ), the methods differ: the poses proposed by trace(\u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 ) reduce uncertainty significantly faster than trace(\u03a3 \u03b8 ). This can be explained by the fact that when minimizing trace(\u03a3 \u03b8 ), the variance of less significant parameters will be reduced just as much as the variance of parameters with large effect on the mapping. This example shows that the performance of trace(\u03a3 \u03b8 ) can be affected by the choice of the model, while trace(\u03a3 \u03b8 1/2 H\u03a3 \u03b8 1/2 ) remains unaffected.",
            "cite_spans": [
                {
                    "start": 315,
                    "end": 319,
                    "text": "[10]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 913,
                    "end": 920,
                    "text": "Fig. 2b",
                    "ref_id": null
                },
                {
                    "start": 1334,
                    "end": 1342,
                    "text": "(Fig. 5)",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Application in Calibration Guidance"
        },
        {
            "text": "In this paper, we proposed two metrics to evaluate systematic errors and the remaining uncertainty in camera calibration. We have shown that the bias ratio (BR) reliably captures underfits, which can result from an insufficiently complex model. Furthermore, we have shown that it is possible to predict the expected mapping error (EME) in image space, which provides an upper bound for the precision that can be achieved with a given dataset. Both metrics are modelindependent and therefore widely applicable. Finally, we have shown that the EME can be applied for calibration guidance, resulting in a faster reduction in mapping uncertainty than the existing parameter-based approach. In future, we will extend the metrics to multi-camera systems and extrinsic calibration. Furthermore, we would like to incorporate an analysis of the coverage of the camera field of view into our evaluation scheme.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Research"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Calibration errors in structure from motion",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Abraham",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "F\u00f6rstner",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "117--124",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Generalized B-spline camera model",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Beck",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Stiller",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 IEEE Intelligent Vehicles Symposium (IV)",
            "volume": "",
            "issn": "",
            "pages": "2137--2142",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Depth distortion under calibration uncertainty",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "F"
                    ],
                    "last": "Cheong",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "H"
                    ],
                    "last": "Peh",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Comput. Vis. Image Underst",
            "volume": "93",
            "issn": "3",
            "pages": "221--244",
            "other_ids": {
                "DOI": [
                    "10.1016/j.cviu.2003.09.003"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Behaviour of SFM algorithms with erroneous calibration",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "F"
                    ],
                    "last": "Cheong",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Comput. Vis. Image Underst",
            "volume": "115",
            "issn": "1",
            "pages": "16--30",
            "other_ids": {
                "DOI": [
                    "10.1016/j.cviu.2010.08.004"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Learning camera miscalibration detection",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cramariuc",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Petrov",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Suri",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mittal",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Siegwart",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cadena",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.11711"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Multiple view geometry in computer vision",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hartley",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1017/CBO9780511811685"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Close-range Photogrammetry and 3D Imaging",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Luhmann",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Robson",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kyle",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Boehm",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "OpenCV: OpenCV Fisheye Camera Model",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "On the importance of modeling camera calibration uncertainty in visual SLAM",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ozog",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "Eustice",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "2013 IEEE International Conference on Robotics and Automation",
            "volume": "",
            "issn": "",
            "pages": "3777--3784",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Calibration wizard: a guidance system for camera calibration based on modelling geometric and corner uncertainty",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sturm",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1497--1505",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Abnahme und ueberwachung photogrammetrischer messsysteme nach vdi 2634",
            "authors": [
                {
                    "first": "U",
                    "middle": [],
                    "last": "Rautenberg",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wiggenhagen",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "117--124",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "AprilCal: assisted and repeatable camera calibration",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Richardson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Strom",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Olson",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "2013 IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "volume": "",
            "issn": "",
            "pages": "1814--1821",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Efficient pose selection for interactive camera calibration",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Rojtberg",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kuijper",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)",
            "volume": "",
            "issn": "",
            "pages": "31--36",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Alternatives to the median absolute deviation",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Rousseeuw",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Croux",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "J. Am. Stat. Assoc",
            "volume": "88",
            "issn": "424",
            "pages": "1273--1283",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Why Having 10,000 Parameters in Your Camera Model is",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Schoeps",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Larsson",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pollefeys",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Sattler",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1912.02908"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Kalibrierung von Multi-Kamera-Systemen",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Strauss",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "What can be done with a badly calibrated Camera in Ego-Motion Estimation?",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Svoboda",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sturm",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Bundle adjustment -a modern synthesis",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Triggs",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "F"
                    ],
                    "last": "Mclauchlan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "I"
                    ],
                    "last": "Hartley",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "W"
                    ],
                    "last": "Fitzgibbon",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "IWVA 1999",
            "volume": "1883",
            "issn": "",
            "pages": "298--372",
            "other_ids": {
                "DOI": [
                    "10.1007/3-540-44480-7_21"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "A flexible new technique for camera calibration",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "22",
            "issn": "11",
            "pages": "1330--1334",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Motion bias and structure distortion induced by calibration errors",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zucchelli",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kosecka",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "10",
            "issn": "",
            "pages": "68--69",
            "other_ids": {
                "DOI": [
                    "10.5244/C.15.68"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF2": {
            "text": "Comparison of state-of-the-art metrics for real camera 1. On average, the true error K decreases with the number frames. For comparability with maxERE, we show \u221a K and \u221a EM E in units of pixels. All metrics are correlated with the true error, but absolute values and the scaling differ. Values are medians across 50 random samples, error bars are 95% bootstrap confidence intervals.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Application of EME for calibration guidance. a For the original model, both metrics lead to a similarly fast reduction in uncertainty. Rescaling the model to a different unit of the focal length results in a reduced performance of trace(\u03a3\u03b8 ), while trace(\u03a3\u03b8 1/2 H\u03a3\u03b8 1/2 ) remains unaffectd. Uncertainty is quantified by the average of the uncertainty map proposed by calibration wizard[10]. b Examples of suggested poses.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Fig. 2.Detecting systematic errors. a Illustration of the virtual decomposition of the calibration target into smaller targets used to estimate the corner detector variance. b Exemplary image of the same scene with the two test cameras. c Results of the bias ratio (BR) and the robust estimate of the RMSE (MAD) for one simulated and the two real cameras, using models of different complexities. For details see Sect. 7.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Fig. 3. Predicting the mapping error based on parameter uncertainties. a Schematic of the derived uncertainty metric EME = trace(\u03a3\u03b8 1/2 H\u03a3\u03b8 1/2 ). b Evaluation in simulation and experiments. The simulation results validate the derived relation(5). For real cameras, the EME is a lower bound to the error, as non-ideal behavior can lead to higher absolute errors. Error bars are 95% bootstrap confidence intervals.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
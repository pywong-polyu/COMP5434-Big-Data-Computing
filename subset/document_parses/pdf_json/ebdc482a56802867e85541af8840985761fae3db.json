{
    "paper_id": "ebdc482a56802867e85541af8840985761fae3db",
    "metadata": {
        "title": "VeniBot: Towards Autonomous Venipuncture with Semi-supervised Vein Segmentation from Ultrasound Images",
        "authors": [
            {
                "first": "Yu",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Yuxuan",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Bolin",
                "middle": [],
                "last": "Lai",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Zijie",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Xu",
                "middle": [],
                "last": "Cao",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Nanyang",
                "middle": [],
                "last": "Ye",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Zhongyuan",
                "middle": [],
                "last": "Ren",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Junbo",
                "middle": [],
                "last": "Zhao",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Xiao-Yun",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Peng",
                "middle": [],
                "last": "Qi",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "In the modern medical care, venipuncture is an indispensable procedure for both diagnosis and treatment. In this paper, unlike existing solutions that fully or partially rely on professional assistance, we propose VeniBot -a compact robotic system solution integrating both novel hardware and software developments. For the hardware, we design a set of units to facilitate the supporting, positioning, puncturing and imaging functionalities. For the software, to move towards a full automation, we propose a novel deep learning framework -semi-ResNeXt-Unet for semi-supervised vein segmentation from ultrasound images. From which, the depth information of vein is calculated and used to enable automated navigation for the puncturing unit. VeniBot is validated on 40 volunteers, where ultrasound images can be collected successfully. For the vein segmentation validation, the proposed semi-ResNeXt-Unet improves the dice similarity coefficient (DSC) by 5.36%, decreases the centroid error by 1.38 pixels and decreases the failure rate by 5.60%, compared to fully-supervised ResNeXt-Unet.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "As a popular invasive procedure for venous access, the venipuncture has been widely used and acknowledged for clinical evaluation and treatment. This procedure opens up broad clinical practices including but not limited to biochemical analysis of the blood sample, infusion of fluid and cannulation for catheterization. For most adults, venipuncture is conventionally performed by medical practitioners under direct visual monitorization. Despite its simplicity, the success rate of venipuncture is potentially influenced by a variaty of factors. In some adverse cases, the rate can be below 50% under improper settings [1] . On one hand, veipuncture is an experience-based procedure requiring extensive practices, which is hard for novices. On the other hand, in specific populations such as the elderly or infants, people with dark complexion, or patients in shock state, veins are not easily accessible. In addition, medical practitioners are at a high risk of needle injury and infection of bloodborne diseases like hepatitis and human immunodeficent virus (HIV) [2] or airborne diseases like corona virus disease This work is supported by the National Natural Science Foundation of China (Number 51905379) and Shanghai Science and Technology Development Funds (Number 20QC1400900) 1 Yu Chen, Yuxuan Wang, Zijie Chen, Xu Cao and Peng Qi are with Tongji University, Shanghai, China. pqi@tongji.edu.cn 2 Bolin Lai is with PingAn Technology Co. Ltd., Shanghai, China. 3 Nanyang Ye is with Shanghai Jiao Tong University, Shanghai, China. 4 Zhongyuan Ren is with Soochow University Medical College, Suzhou, China. 5 Junbo Zhao is with Zhejiang University, Hangzhou, China. 6 Xiao-Yun Zhou is with PAII Inc., MD, USA. * corresponding author. [3] . Hence, novel solutions are necessitated to provide better and safe healthcare service. Robotics has been a popular paradigm to automate many traditional manual tasks, for example, autonomous driving [4] , [5] , [6] and autonomous surgical operation [7] , [8] , [9] . Some robots have been developed and widely used for veinpuncture. For example, Veebot, which was founded in 2010, proposed a venipuncture robot that combines a robotic arm and a compact puncturing unit [10] . It identifies a coarse vein with the help of infrared light and examines whether the vein is suitable or not for puncture with ultrasound. Venouspro was proposed by Vasculogic. It contains a six degree of freedom (DOF) positioning unit and a three DOF distal manipulator [11] . It is more compact and portable, compared to the robotic arm proposed by Veebot. Magic-Nurse, which is an automatic blood drawing robot, draws the blood and exchanges the puncture needle automatically. It detects the forearm vein with NIR images and estimates the depth of the needle with force sensors. The device has more than 20 DOF and is as big as a refrigerator. KUKA AG invented a robot arm holding a wireless ultrasound probe as an automatically robot-assisted tracking system to scan the vessel. It uses active contours to segment the vessel and it works on phantoms [12] .",
            "cite_spans": [
                {
                    "start": 620,
                    "end": 623,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1067,
                    "end": 1070,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1286,
                    "end": 1287,
                    "text": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1404,
                    "end": 1405,
                    "text": "2",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1469,
                    "end": 1470,
                    "text": "3",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1538,
                    "end": 1539,
                    "text": "4",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1613,
                    "end": 1614,
                    "text": "5",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1672,
                    "end": 1673,
                    "text": "6",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1740,
                    "end": 1743,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1945,
                    "end": 1948,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1951,
                    "end": 1954,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1957,
                    "end": 1960,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1995,
                    "end": 1998,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 2001,
                    "end": 2004,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 2007,
                    "end": 2010,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 2215,
                    "end": 2219,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 2493,
                    "end": 2497,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 3076,
                    "end": 3080,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "In this paper, a compact venipuncture device -VeniBot is designed and built for autonomous venipucture. The solidworks (CAD) model of VeniBot is shown in Fig. 1 . It is consisted of four units, including the supporting unit used to support the whole robot, the positioning unit mainly used to move VeniBot to the puncture target, the puncturing unit mainly used to puncture, and the imaging unit used to mount the near-infrared (NIR) camera and ultrasound device. The positioning unit contains three single axis robots driven with direct current (DC) motors and one motor that drives directly, and is navigated by a NIR camera mounted on the imaging unit, while the puncturing unit contains two single axis robots, and is navigated by an ultrasound device mounted on the imaging unit.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 154,
                    "end": 160,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Except the hardware development, software for localization and navigation is also important and essential for robotic automation, typical examples are in medical robotics [13] , [14] and general robotics [15] . In the developed VeniBot, there are two navigation problems: (1) the puncture area determination from the NIR images; (2) the vein segmentation from the ultrasound images. In this paper, we mainly focus on the second challenge while the first challenge is focused by another IROS submission. Deep learning has been a popular method for image segmentation, however, it relies heavily on the training data with carefully manual labels. A more efficient way is to fully use the unlabeled data and train with semi-supervised learning. Popular semi-supervised methods include \u03a0-model [16] , temporal ensemble [16] , mix match [17] and mean teacher [18] . In this paper, inspired by mean teacher [18] and Unet [19] , we propose semi-ResNeXt-Unet for vein segmentation from limited labeled training data and large amount of unlabeled training data.",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 175,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 178,
                    "end": 182,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 204,
                    "end": 208,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 790,
                    "end": 794,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 815,
                    "end": 819,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 832,
                    "end": 836,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 854,
                    "end": 858,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 901,
                    "end": 905,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 915,
                    "end": 919,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The two main novelties and contributions of this paper are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 VeniBot is proposed and built with four units (supporting unit, positioning unit, puncturing unit and imaging unit) and six motors (four motors for the positioning unit and two motors for the puncturing unit). \u2022 Semi-ResNeXt-Unet is proposed to segment the vein from ultrasound images with semi-supervised learning, from which, the vein center is extracted to navigate the VeniBot to puncture automatically. We validate the proposed VeniBot on puncture paper with 2mm \u00d7 2mm grids and the proposed semi-ResNeXt-Unet on 40 volunteers. The later achieves a DSC of 75.11%, a centroid error of 8.23 pixels and a failure rate of 2.00% on 10 labeled training volunteers and 30 unlabeled training volunteers, which indicates a promising performance of the proposed methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "In the following context, we will articulate the hardware of VeniBot in details in Sec. II-A. Afterwards, we'll detailedly describe the automatic puncturing unit of VeniBot in Sec. II-B, the validation of VeniBot and puncturing unit is in Sec. III, and the conclusion is in Sec. IV.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The first unit of VeniBot is the supporting unit which has zero DOF, as indicated in white color in Fig. 1 . The main function of supporting unit is to support and hold the whole VeniBot. On top of the supporting unit, we assemble the positioning unit which is with four DOFs. The main function of positioning unit is to predict the most suitable venipuncture position and angle in the xOy plane, under the navigation of a NIR camera.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 100,
                    "end": 106,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "A. Hardware Design of VeniBot"
        },
        {
            "text": "On top of the positioning unit, we assemble the third unit of VeniBot -the puncturing unit which has two DoFs, as indicated in red color in Fig. 1 . The main function of the puncturing unit is to puncture the vein under the navigation of an ultrasound scanner. The puncturing unit is consisted of two single axis robots, where both of them have one DoF of translation. A ultrasound device (ST-1C transducer with frequency of 7.5MHz, 48 lateral array elements, 80 axial array elements, element spacing of 0.3mm) is mounted at the imaging unit. At the beginning, the positioning unit moves to the best venipuncture position. Then motor 2 moves downward until a high-quality cross-section view of the vein is obtained from the ultrasound scanner which persistently scans the skin during the entire moving period. The segmentation mask and center position of the vein are localized automatically using the deep learning methodsemi-ResNeXt-Unet proposed in Sec. II-B, which is trained with limited labeled data and a large amount of unlabeled data. The puncturing unit contains two DOFs: motor 5 drives a y-direction single axis robot and motor 6 drives another single axis robot, which is at an angle of about 17 \u2022 from the xOy plane. Motor 6 is fixed on motor 5 and moves along the y direction when motor 5 works. The needle is fixed on single axis robot 6 and moves along the direction of it when motor 6 works. The depth of the vein's centroid guides motor 5 to move single axis robot 6. This step will finally determine the needle depth. Then motor 6 works and pushes the needle tip to the vein centroid. In this paper, we mainly focus on the automation of puncturing unit. While the automation of positioning part is illustrated in the other IROS submission.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 140,
                    "end": 146,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "A. Hardware Design of VeniBot"
        },
        {
            "text": "To achieve autonomous venipuncture with the proposed VeniBot, it's of great importance to develop an automatic method to localize centers of veins from ultrasound images, as it indicates the depth of the movement of motor 5 and 6. In this paper, centers of veins are calculated from their segmentation masks 1 . However, vein segmentation from ultrasound images is very challenging, as the image quality is susceptible to the experience of operators and low quality usually exists even scanned by well-trained operators. Another challenge is the deformation caused by transducer. Since ultrasound wave can not transit through the air, the transducer has to touch the patient's forearm skin with suitable pressure. Large pressure makes the vein squashed, while small pressure remains some air between the skin and the transducer, resulting in all-black ultrasound images. Even for well-obtained ultrasound images, the vein bound can be obscure and its shape can differ dramastically, indicating the difficulty of labeling large amounts of ultrasound images manually. See Fig. 2 for some examples. To address the time-consuming and labor-intensive annotations, we propose a semi-supervised segmentation method -semi-ResNeXt-Unet which first trains a fully-supervised model on limited labeled data and then improves its performance on more unlabeled data. Semi-ResNeXt-Unet contains 10 convolutional layers and 4 deconvolutional layers as demonstrated in Fig. 3 and Table I. The semi-supervised method is based on the state-of-theart mean teacher framework [18] . Assume M labeled images and N unlabeled images are available. Thus we have S =",
            "cite_spans": [
                {
                    "start": 1554,
                    "end": 1558,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 1070,
                    "end": 1076,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1452,
                    "end": 1458,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1463,
                    "end": 1471,
                    "text": "Table I.",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "B. Automatic Vein Segmentation and Center Position Determination"
        },
        {
            "text": "for labeled data and unlabeled data, respectively. x i and y i denote input images and pixelwise annotations. As illustrated in Fig. 4 , the proposed semi-ResNeXt-Unet works in the following steps:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 128,
                    "end": 134,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "B. Automatic Vein Segmentation and Center Position Determination"
        },
        {
            "text": "(1) Train a fully-supervised network based on S. The binary cross-entropy (BCE) loss is used at first. When reasonable prediction masks are obtained from the network. Focal loss [21] is used in place of BCE loss to address the extreme imbalance of foreground and background in ultrasound images. After the convergence, the parameters are used as initialization for the teacher and student models.",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 182,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "B. Automatic Vein Segmentation and Center Position Determination"
        },
        {
            "text": "(2) Input labeled images S into the student model. Focal loss is still used to train the network with other training protocols the same as that used for the fully-supervised training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Automatic Vein Segmentation and Center Position Determination"
        },
        {
            "text": "(3) For unlabeled images U, raw images are augmented by spatial transformation and input into the teacher model to get reliable pseudo probability maps. Then they are further disturbed by extra intensity augmentation and input into the student model. We use mean square error (MSE) as the consistency loss to penalize any difference between teacher and student model's outputs. Then the total loss can be expressed as a weighted summation of supervised and semisupervised losses.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Automatic Vein Segmentation and Center Position Determination"
        },
        {
            "text": "where the \u03bb 1 and \u03bb 2 are hyper-parameters used to balance the two losses.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Automatic Vein Segmentation and Center Position Determination"
        },
        {
            "text": "(4) In the k iteration, parameters of the teacher model (\u03b8 T ) are frozen at first and the student model's parameters (\u03b8 S ) are updated by back-propagation of the focal loss and MSE loss. Then teacher model's parameters are updated by moving average from the student model according to the following equation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Automatic Vein Segmentation and Center Position Determination"
        },
        {
            "text": "where \u03b1 is a hyper-parameter to control the pace of update. After training, either the teacher model or the student model, which shows better performance on the validation set, is used for inference. After segmenting the area of vein, to minimize the center detection error of the network and make sure the target vein is safe to access, a group of operation and evaluation is necessary. The image after binarization will go trough opening operation to remove outliers and closing operation to link broken regions of the vein. We remain the biggest connected component and filter out other foreground areas. The target for venipuncture is the centroid of the segmented vein mask, which can be calculated by averaging the x coordinates and y coordinates of all pixels in the segmentation mask, respectively. It can be expressed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Automatic Vein Segmentation and Center Position Determination"
        },
        {
            "text": "where (x c , y c ) and (x i , y i ) are coordinates of the centroid of the vein and i-th pixel in the mask. n is the total number of pixels. (x c , y c ) further determines the final position of axis 5 and axis 6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Automatic Vein Segmentation and Center Position Determination"
        },
        {
            "text": "The real VeniBot and the experimental setup are demonstrated in Fig. 7 . The supporting unit was assembled with aluminium pieces. The skeleton of positioning unit and puncturing unit were built with machine work, while the base of ultrasound probe and NIR camera was processed with 3D metal printing. All of the translation pairs were driven by the combination of DC motors and single axis robots. Axis 4, a rotation pair, was directly driven by the DC motor. In total, four HIWIN KK40 single aixis robots, one HIWIN KK50 single axis robot, three EC-max16 DC motors and three EC-max22 DC motors from Maxon Motor Inc. are used in VeniBot. Note that all ultrasound images used in this paper were collected by our machine, validating the robustness and stability of the proposed VeniBot.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 64,
                    "end": 70,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "III. VALIDATION A. Experiments"
        },
        {
            "text": "Painting liquid or solid ultrasonic coupling agent on the skin is necessary to drive away the air and keep the ultrasound wave propagating in solid or liquid media. From the aspect of computer graphic method, the difference between the vein and the background and the characteristics of vein section are obvious enough to distinguish the vein from background. The intensity of vein is low because blood, in the state of liquid, does not reflect the ultrasound wave. Moreover, the vein section is an approximate circle or ellipse. Most of the veins are at the size of 1\u223c2mm in diameter. These characteristics above can be manually measured with intensity, circularity, size, convexity, concavity and aspect ratio, indicating the availability of manual labeling of ground truth (GT). We use VeniBot to collect ultrasound images from 40 volunteers (27 males and 13 females) at the age of 18\u223c70 to validate our methods. Each of them provided 30 ultrasound images of forearm vein, i.e. 1200 images in total, with the same size of 70\u00d770. By adjusting the parameters of multiple filters to segment the vein area one by one, we segmented 300 images as labeled training data and used images from the rest 30 volunteers as unlabeled data. Two kinds of augmentations are implemented in fully-and semi-supervised training: spatial augmentation and intensity augmentation. Spatial augmentation includes resizing, flipping, rotation, shearing and changing the aspect ratio. Intensity augmentation includes randomly adding or multiplying a certain value, contrast normalization and dropout. The range of the spatial and intensity augmentation is listed in Table II. The ResNeXt-Unet model is trained with 300 labeled images first. The model is first trained using BCE loss and with the learning rate of 1 \u00d7 10 \u22123 . Then it is further trained using focal loss with a learning rate of 5 \u00d7 10 \u22124 until convergence. The batch size is set as 8 and both intensity and spatial augmentations are used. In the semi-supervised training, labeled images and unlabeled images are randomly selected in each batch. The same spatial augmentations are used on student and teacher model's inputs, while intensity augmentations are only implemented on student model's inputs.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1641,
                    "end": 1650,
                    "text": "Table II.",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "III. VALIDATION A. Experiments"
        },
        {
            "text": "To validate the kinematic and assembly accuracy of Venibot, a 10\u00d710 grid paper with a grid size of 2mm\u00d72mm is used. We assembled a pencil at the end of axis 6 and controlled Venibot to move 2mm along axis x or y once at a time. After each movement, the pencil moved along axis 6 to draw a short line in the corresponding grid. As shown in Fig. 7 , VeniBot successfully controlled a pencil drawing short lines inside each grid, which shows the crucial and fundamental ability of VeniBot to achieve safe and viable venipuncture.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 339,
                    "end": 345,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "B. Results"
        },
        {
            "text": "In order to show the advantage of the proposed semi-ResNeXt-Unet, three strong semi-supervised baselines are used: (1) pseudo label training [22] which generates pseudo GT for the unlabeled data with the converged model in the fully-supervised step, (2) \u03a0-model [16] which only uses one model with the consistency loss, temporal ensemble [16] which updates the pseudo GT after every training epoch using moving average method. The DSC of our method and baselines with 5-fold cross validation is illustrated in Tab. III. It can be observed that temporal ensemble and pseudo label training achieve the lowest performance, which is even lower than the fully-supervised model. One possible reason is that the challenging nature of vein segmentation causes lots of error in the pseudo GT from the fully-supervised model, which misleads the model in semi-supervised training. In contrast, based on the consistency loss, \u03a0-model and our method boost the performance by a significant margin. The proposed semi-ResNeXt-Unet still shows its superiority and outperforms the \u03a0 \u2212 model by 1.91%. Compared with the fully-supervised counter-part, the mean DSC is finally improved by 5.36%. We further show the centroid MSE error which measures the distance between the centroid of prediction and GT. If there is no vein predicted, we count this situation as failure cases and see its prediction at the top left corner. The centroid MSE error and the failure rate are shown in Tab. IV and V respectively. For the centroid error, the proposed semi-ResNeXt-Unet outperforms the fully-supervised method, pseudo label training, and temporal ensemble with notable margins, while outperforms \u03c0-model with a small margin. For the failure rate, the proposed semi-ResNeXt-Unet achieves the lowest value for four of the five cross validations and the overall average failure rate, indicating the robustness of the proposed semi-ResNeXt-Unet to hard cases, i.e., images with small veins.",
            "cite_spans": [
                {
                    "start": 141,
                    "end": 145,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 262,
                    "end": 266,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 338,
                    "end": 342,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "B. Results"
        },
        {
            "text": "Finally, seven vein segmentation results, segmented by the four methods, were randomly selected and shown in Fig. 8 . The results of baselines lose some significant vein areas or even totally fail to generate reasonable masks. The proposed semi-ResNeXt-Unet precisely catches the vein and outputs more reliable masks, which is an important perceptual evaluation of the superiority of our method.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 109,
                    "end": 115,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "B. Results"
        },
        {
            "text": "In the paper, we proposed a compact venipuncture robot -VeniBot. For its automation, we proposed ResNeXt-Unet to automatically segment the vein from ultrasound images and calculate the depth from skin to the vein centroid. Given the expense of collecting a large labeled dataset, we also proposed a semi-supervised segmentation network -semi-ResNeXt-Unet to obtain reasonable vein masks based on a limited set of labeled training data as well as more unlabeled images. It prominently improves the performance by 5.36% DSC compared with the fully-supervised counter-part and surpasses other semi-supervised baselines by at least 1.91% DSC. It's a promising step towards a wide and automatic application in the future.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. CONCLUSION"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Methods of obtaining peripheral venous access in difficult situations",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Mbamalu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Postgraduate medical journal",
            "volume": "75",
            "issn": "886",
            "pages": "459--462",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Worldwide prevalence of occupational exposure to needle stick injury among healthcare workers: A systematic review and meta-analysis",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "A"
                    ],
                    "last": "Mengistu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Tolera",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "M"
                    ],
                    "last": "Demmu",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Canadian Journal of Infectious Diseases and Medical Microbiology",
            "volume": "2021",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Vascular access in covid-19 patients: smart decisions for maximal safety",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Scoppettuolo",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "G"
                    ],
                    "last": "Biasucci",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pittiruti",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Towards fully autonomous driving: Systems and algorithms",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Levinson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Askeland",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Becker",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dolson",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Held",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kammel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "Z"
                    ],
                    "last": "Kolter",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Langer",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Pink",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Pratt",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "2011 IEEE Intelligent Vehicles Symposium (IV)",
            "volume": "",
            "issn": "",
            "pages": "163--168",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "A survey of deep learning techniques for autonomous driving",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Grigorescu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Trasnea",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Cocias",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Macesanu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Field Robotics",
            "volume": "37",
            "issn": "3",
            "pages": "362--386",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Deep reinforcement learning for autonomous driving: A survey",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "R"
                    ],
                    "last": "Kiran",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sobh",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Talpaert",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Mannion",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "A A"
                    ],
                    "last": "Sallab",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yogamani",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "P\u00e9rez",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2002.00444"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Advanced intelligent systems for surgical robotics",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "T"
                    ],
                    "last": "Thai",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "T"
                    ],
                    "last": "Phan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "T"
                    ],
                    "last": "Hoang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "H"
                    ],
                    "last": "Lovell",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "N"
                    ],
                    "last": "Do",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Advanced Intelligent Systems",
            "volume": "2",
            "issn": "8",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Autonomous surgical robot with camera-based markerless navigation for oral and maxillofacial surgery",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Kobayashi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Suenaga",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hara",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nakagawa",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sakuma",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Masamune",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE/ASME Transactions on Mechatronics",
            "volume": "25",
            "issn": "2",
            "pages": "1084--1094",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Application of artificial intelligence in surgery",
            "authors": [
                {
                    "first": "X.-Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "G.-Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Frontiers of Medicine",
            "volume": "",
            "issn": "",
            "pages": "1--14",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Profile: Veebot drawing blood faster and more safely than a human can",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "S"
                    ],
                    "last": "Perry",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Deep learning robotic guidance for autonomous vascular access",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "I"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Balter",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "J"
                    ],
                    "last": "Maguire",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Yarmush",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature Machine Intelligence",
            "volume": "2",
            "issn": "2",
            "pages": "104--115",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Robot-assisted ultrasound-guided tracking of anatomical structures for the application of focused ultrasound",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Unger",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Berger",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Gerold",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Melzer",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Current Directions in Biomedical Engineering",
            "volume": "6",
            "issn": "3",
            "pages": "123--126",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Robotic navigation during spine surgery",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X.-G",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Y.-F",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "M.-X",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "J.-W",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Y.-J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Expert review of medical devices",
            "volume": "17",
            "issn": "1",
            "pages": "27--32",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Real-time 3-d shape instantiation from single fluoroscopy projection for fenestrated stent graft deployment",
            "authors": [
                {
                    "first": "X.-Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Riga",
                    "suffix": ""
                },
                {
                    "first": "G.-Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "S.-L",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "volume": "3",
            "issn": "2",
            "pages": "1314--1321",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Robotic navigation algorithm with machine vision",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "G"
                    ],
                    "last": "Pach\u00f3n-Suesc\u00fan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "J"
                    ],
                    "last": "Enciso-Arag\u00f3n",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Jim\u00e9nez-Moreno",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Journal of Electrical & Computer Engineering",
            "volume": "10",
            "issn": "2",
            "pages": "2088--8708",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Temporal ensembling for semi-supervised learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Laine",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Aila",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1610.02242"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Mixmatch: A holistic approach to semi-supervised learning",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Berthelot",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Carlini",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Papernot",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oliver",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Raffel",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.02249"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Tarvainen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Valpola",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1195--1204",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ronneberger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Conference on Medical image computing and computer-assisted intervention",
            "volume": "",
            "issn": "",
            "pages": "234--241",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Towards automatic 3d shape instantiation for deployed stent grafts: 2d multiple-class and class-imbalance marker segmentation with equally-weighted focal unet",
            "authors": [
                {
                    "first": "X.-Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Riga",
                    "suffix": ""
                },
                {
                    "first": "S.-L",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "G.-Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "1261--1267",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Focal loss for dense object detection",
            "authors": [
                {
                    "first": "T.-Y",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE international conference on computer vision",
            "volume": "",
            "issn": "",
            "pages": "2980--2988",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "A simple semi-supervised learning framework for object detection",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sohn",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C.-L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C.-Y.",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pfister",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.04757"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An illustration of the solidworks (CAD) model of the proposed VeniBot, with the white color indicating the supporting unit, the blue color indicating the positioning unit, the red color indicating the puncturing unit, and the purple color indicating the imaging unit.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "An illustration of the ultrasound images (left) and the corresponding ground truth (GT) masks (right).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "An illustration of the proposed ResNeXt-Unet network structure used for vein segmentation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "An illustration of the semi-ResNeXt-Unet framework for semi-supervised learning.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "An illustration of the puncturing unit works under the guidance of centroid coordinates. (a) The ultrasound scanner first scans the forearm and gets the image of longitudinal section, which includes the vein. (b) Then motor 5 changes the final puncture target depth of the needle that is mounted on the end of axis 6. (c) After the predicted centroid is aligned with the real one of vein, motor 6 works and sends the needle into the vein.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "An illustration of the real VeniBot and the process of ultrasound image collection.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "An illustration of the kinematic and assembly accuracy validation experiment.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Illustration of (a) input image, and segmentation results of (b) fully-supervised model, (c) pseudo label training, (d) \u03a0-model; (e) temporal ensemble, (f) teacher and (g) student model in semi-ResNeXt-Unet.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The layer details of the proposed ResNeXt-Unet network structure used for vein segmentation.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The range of the spatial and intensity augmentation added on the training of the semi-ResNeXt-Unet model.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "The mean failure rate of training the fully-supervised model, pseudo label training model, \u03a0-model, temporal ensemble model and the proposed semi-ResNeXt-Unet model on the five cross validations.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
{
    "paper_id": "48dba8830df54540c432dae7f60774cf225b05cc",
    "metadata": {
        "title": "Classification and Region Analysis of COVID-19 Infection using Lung CT Images and Deep Convolutional Neural Networks",
        "authors": [
            {
                "first": "Saddam",
                "middle": [],
                "last": "Hussain Khan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Pattern Recognition Lab",
                    "institution": "",
                    "location": {
                        "postCode": "45650",
                        "settlement": "Nilore",
                        "region": "Islamabad",
                        "country": "Pakistan, Pakistan"
                    }
                },
                "email": ""
            },
            {
                "first": "Anabia",
                "middle": [],
                "last": "Sohail",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Pattern Recognition Lab",
                    "institution": "",
                    "location": {
                        "postCode": "45650",
                        "settlement": "Nilore",
                        "region": "Islamabad",
                        "country": "Pakistan, Pakistan"
                    }
                },
                "email": ""
            },
            {
                "first": "Asifullah",
                "middle": [],
                "last": "Khan",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Yeon",
                "middle": [
                    "Soo"
                ],
                "last": "Lee",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Pattern Recognition Lab",
                    "institution": "",
                    "location": {
                        "postCode": "45650",
                        "settlement": "Nilore",
                        "region": "Islamabad",
                        "country": "Pakistan, Pakistan"
                    }
                },
                "email": ""
            },
            {
                "first": "Asif@pieas",
                "middle": [
                    "Edu"
                ],
                "last": "Pk",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "COVID-19 is a global health problem. Consequently, early detection and analysis of the infection patterns are crucial for controlling infection spread as well as devising a treatment plan. This work proposes a twostage deep Convolutional Neural Networks (CNNs) based framework for delineation of COVID-19 infected regions in Lung CT images. In the first stage, initially, COVID-19 specific CT image features are enhanced using a two-level discrete wavelet transformation. These enhanced CT images are then classified using the proposed custom-made deep CoV-CTNet. In the second stage, the CT images classified as infectious images are provided to the segmentation models for the identification and analysis of COVID-19 infectious regions. In this regard, we propose a novel semantic segmentation model CoV-RASeg, which systematically uses average and max pooling operations in the encoder and decoder blocks. This systematic utilization of max and average pooling operations helps the proposed CoV-RASeg in simultaneously learning both the boundaries and region homogeneity. Moreover, the idea of attention is incorporated to deal with mildly infected regions. The proposed two-stage framework is evaluated on a standard Lung CT image dataset, and its performance is compared with the existing deep CNN models. The performance of the proposed CoV-CTNet is evaluated using Mathew Correlation Coefficient (MCC) measure (0.98) and that of proposed CoV-RASeg using Dice Similarity (DS) score (0.95). The promising results on an unseen test set suggest that the proposed framework has the potential to help the radiologists in the identification and analysis of COVID-19 infected regions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "December 2019; however, in early 2020 it spread across the world [1] , and to date, it persists with its devastating effects across all continents [2] . COVID-19 is most commonly manifested by mild flue like symptoms such as cough, fever, myalgia, and fatigue [3] . However, in severe cases, it causes alveolar damage, pneumonia, sepsis, and respiratory failure, which eventually lead to death [4] .",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 68,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 147,
                    "end": 150,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 260,
                    "end": 263,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 394,
                    "end": 397,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "COVID-19 is a contagious disease that primarily originated in Wuhan province of China in"
        },
        {
            "text": "The commonly used tests for assessment of COVID-19 patients are gene sequencing, reverse transcription polymerase chain reaction (RT-PCR), X-Ray and computed tomography (CT) based imaging techniques [5] , [6] . Out of these aforementioned assays, RT-PCR and gene sequencing are considered as a gold standard. However, because these standard assays are expensive, many developing countries have a limited number of testing kits or lack the sequencing facilities. RT-PCR usually takes up to 2 days for detection and often suffers from the inherited limitation of viral RNA instability. Thus, it requires serial testing to eliminate the likelihood of false-negative results (RT-PCR detection rate is ~ 30% to 60%) and necessitates some additional supplementary tests [7] - [9] . In this regard, additional detection methods with high precision are also required for immediate treatment of the infected persons and to cease the transmission of contagious COVID-19 infectious.",
            "cite_spans": [
                {
                    "start": 199,
                    "end": 202,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 205,
                    "end": 208,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 764,
                    "end": 767,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 770,
                    "end": 773,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "COVID-19 is a contagious disease that primarily originated in Wuhan province of China in"
        },
        {
            "text": "CT based imaging is not expensive and available in all the hospitals and reliable, practical, and efficient tools for detection, prognosis and follow-up of COVID-19 patients.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19 is a contagious disease that primarily originated in Wuhan province of China in"
        },
        {
            "text": "Several examination setups have shown reliable diagnostic power of CT imaging in capturing the lung abnormalities for COVID-19 infected individuals even when characteristic clinical symptoms are imperceptible, and RT-PCR is reported as a false negative [10] , [11] . The characteristic radiographic features for the infected patients are bilateral patchy shadows, ground glass pacification (GGO), consolidation, pleural effusion, rounded morphology, and peripheral lung distribution [4] , [12] .",
            "cite_spans": [
                {
                    "start": 253,
                    "end": 257,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 260,
                    "end": 264,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 483,
                    "end": 486,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 489,
                    "end": 493,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "COVID-19 is a contagious disease that primarily originated in Wuhan province of China in"
        },
        {
            "text": "In a public health emergency, especially in epidemic and pandemics, there is a significant burden on hospitals and physicians. Visual analysis of a large number of CT images is a huge burden on radiologists. There are many areas where there are no experienced radiologists. The increased workload on radiologists affects performance. Moreover, radiologists are less sensitive 3 and more specific towards identifying COVID-19 infection by analyzing CT images. In this regard, there is an urgent need for an automated tool that can aid the radiologists to improve performance and to deal with a large number of patients. Deep learning (DL) based diagnostic systems are very valuable tools in plunging the physicians. Previously, DL based automated systems has been employed to facilitate the radiologists in the identification of lung-related anomalies [13] , [14] . The advantage of such a system is that they are reproducible and can sense the detect minute irregularities that cannot be located through a visual examination. In this ongoing COVID-19 pandemic era, several research groups have paid attention to develop automated systems to identify the COVID-19 infected individual by examining CT images [15] - [17] .",
            "cite_spans": [
                {
                    "start": 851,
                    "end": 855,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 858,
                    "end": 862,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1206,
                    "end": 1210,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "COVID-19 is a contagious disease that primarily originated in Wuhan province of China in"
        },
        {
            "text": "In this work, we proposed a classification and segmentation based deep Convolutional The layout of the paper is as follows: Section 2 gives an insight into COVID-19 related work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19 is a contagious disease that primarily originated in Wuhan province of China in"
        },
        {
            "text": "Section 3 explains the Methodology of the proposed framework, whereas Section 4 presents the implementation details. Section 5 analyzes the results and discusses the performance of the implemented experiments, and finally, section 6 makes a conclusion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19 is a contagious disease that primarily originated in Wuhan province of China in"
        },
        {
            "text": "Nowadays, CT technology has been used for the analysis of COVID- 19 [21] . The COVID-Net was inspired by ResNet and was used to differentiate multi types of COVID-19 infections from normal pneumonia. Although COVID-Net has good accuracy (92%), yet it has a low detection rate (87%) [22] . Similarly, COVID-CAPS inspired by Capsule Net also reported good accuracy (98%), but it is less sensitive (80%) towards COVID-19 infection [23] . Moreover, a novel classification model COVID-RENet inspired by smooth and boundary information of images and achieved 98% accuracy. All these pre-trained CNN models have been trained on Natural images and fine-tuned on the COVID-19 dataset [24] .",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 67,
                    "text": "19",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 68,
                    "end": 72,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 282,
                    "end": 286,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 428,
                    "end": 432,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 675,
                    "end": 679,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "On the other hand, segmentation of the infected regions is usually performed to identify the location of disease and severity. Initially, some classical segmentation techniques like watershed have been employed but fail to show satisfactory performance [25] , [26] . Therefore, 5 DL based 'VB-Net' has been introduced for the segmentation of COVID-19 infected regions using CT images and reported the dice similarity (DS) score of 91%. Moreover, the COVID-19",
            "cite_spans": [
                {
                    "start": 253,
                    "end": 257,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 260,
                    "end": 264,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "JCS system based on classification and segmentation has been developed to visualize and segment the infected region. The JCS system reported the 95.0% detection rate, 93.0% specificity on classification, while low DS score (78.3%) on segmentation. However, most of the existing COVID-19 diagnostic systems have been trained on a small amount of CT datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "Usually, these diagnostic systems have mostly two main challenges: 1) unavailability of sufficient amount of training data, which is required to make deep CNN models robust towards diverse categories of COVID-19 infections; 2) detection is restricted to the classification of infected samples and lacks the information of the infected region and severity of the disease.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "In this study, we proposed classification and segmentation based deep CNN framework for automatic analysis of COVID-19 abnormalities in lung CT images. The proposed framework is ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "In this stage, at a coarse scale, COVID-19 infected CT samples are segregated from healthy samples by performing classification. Initially, a feature space of the dataset is transformed by employing wavelet-based decomposition (shown in Figure 2 ) to assign class discriminating features to the deep classifiers. In the classification stage, three different experimental setups are implemented: (i) Proposed CoV-CTNet (the detailed explanation of this term will be given in ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 237,
                    "end": 245,
                    "text": "Figure 2",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Discrimination of COVID-19 infected samples"
        },
        {
            "text": "The CT image has been subjected to discrete wavelet transformation (DWT) to transform the feature space by decomposing the image into discrete wavelet coefficients using Haar mother wavelet. DWT has two main advantages: (i) reduction in computational complexity, and (ii) image enhancement by transforming the original image into information-rich feature-maps [27] , [28] .",
            "cite_spans": [
                {
                    "start": 360,
                    "end": 364,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 367,
                    "end": 371,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Feature transformation using wavelet decomposition"
        },
        {
            "text": "In DWT, at each decomposition level, the input image is divided into four equal parts:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature transformation using wavelet decomposition"
        },
        {
            "text": "low-low ( , =2 ), low-high ( , =2 ), high-low ( , =2 ), and high-high ( , =2 ) resolution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature transformation using wavelet decomposition"
        },
        {
            "text": "Whereas, 'i' represents the level of decomposition (D) and 's' is a scaling factor. In this work, we performed two-level decomposition (shown in Figure 2 ) to select the highly informative 7 feature-map. For this, the output (LL and HH) from the 1 st level decomposition is further processed through DWT transformation to extract information-rich feature-map for classification. The high information features-maps are reconstructed back to images using the Inverse Discrete Wavelet Transform (IDWT). In this study, similar to the idea of Leplacian of Gaussian, we have enhanced the image representation by fusing the images reconstructed from LL and HH channels of second-level DWT representations [29] . This transformed feature space is used to distinguish between healthy and COVID-19-infected images.",
            "cite_spans": [
                {
                    "start": 698,
                    "end": 702,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [
                {
                    "start": 145,
                    "end": 153,
                    "text": "Figure 2",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Feature transformation using wavelet decomposition"
        },
        {
            "text": "In this work, we proposed residual learning-based CNN CoV-CTNet that discriminates COVID- Equation (5) shows the residual learning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed CoV-CTNet for classification"
        },
        {
            "text": "The architectural design of the proposed CoV-CTNet is shown in Figure 3 . For the regulation of model complexity and learning of invariant features, convolution operation with stride two is performed at the end of each block. In the proposed architecture, max pooling is used on the top of feature extraction stages to retain the most prominent class-specific feature information within the feature-maps.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 63,
                    "end": 71,
                    "text": "Figure 3",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Proposed CoV-CTNet for classification"
        },
        {
            "text": "For classification purpose, two fully connected layers (mathematically express in Equation (6)) are specified to reduce the feature space by globally analyzing their contribution in classification, and in the last, fully connected layer with softmax is used for the discrimination of healthy lung CT samples from infected images. The proposed model is trained on IDWT enhanced images and optimized using cross-entropy loss (represented in Equation (7)). ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed CoV-CTNet for classification"
        },
        {
            "text": "Equation (6 & 7) represents the fully connected layer, in which l d C is the convolved output having depth D, and k u is k th neuron of l th fully connected layer. Whereas, in the crossentropy loss (Equation (7)), CT p is the predicted class for input CT image, and CT y is the actual class of the image. ",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 16,
                    "text": "(6 & 7)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Proposed CoV-CTNet for classification"
        },
        {
            "text": "Deep CNNs are a type of DL models that exploit the spatial correlation in images and have shown impressive results for detection, classification, and segmentation related tasks [31] . In recent years, CNN has shown convincing results for biomedical images and has been successfully applied to classify and detect medical images [32] . connected layer that is used for classification is replaced with a target-specific layer consisting of two neurons. Contrary to this, convolutional blocks from feature extraction stage are kept unchanged in state-of-the-art models. On the last layer, softmax is used to obtain the classspecific probabilities. These models are trained from scratch on CT images, and weight space is optimized using a backpropagation algorithm by minimizing the cross-entropy based loss function.",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 181,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 328,
                    "end": 332,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Performance comparison with existing CNN models"
        },
        {
            "text": "Deep CNN architectures typically demand a sufficient amount of data for effective training. There are several types of TL; one of the effective ways of TL is the initialization of parameters of deep architectures from pre-trained models and then fine-tunes the learnable parameters to make them adaptable to the target domain. This strategy is employed when the dataset is small, and the target domain shares a similarity with the source domain in terms of feature space or task [43] . In images, usually, low-level features are common among different categories of images such as curves, edges, gradient, etc., whereas high-level features are class-specific. Based on this assumption, we adapted pre-trained deep architectures for classification of Lung CT images.",
            "cite_spans": [
                {
                    "start": 479,
                    "end": 483,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                }
            ],
            "ref_spans": [],
            "section": "Weight transfer-based fine-tuning of deep CNNs"
        },
        {
            "text": "In this regard, TL based optimization of existing deep CNN models on target data is performed by adding a new input convolutional layer that coincides with the size of CT samples (i.e., 82x82x1). Similarly, fully connected layers are replaced with the target-domain specific classification layers and dimension of the last layer is aligned to the number of the classes, i.e., two. The model is trained by fine-tuning the learnable layers of feature extraction stage and by optimizing the weights of classification layers from scratch. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Weight transfer-based fine-tuning of deep CNNs"
        },
        {
            "text": "Localization and quantification of the infectious region are crucial for the analysis of infection pattern and its extent in diagnostics. Therefore, after the discrimination of CT images at a coarsescale, semantic segmentation is performed to obtain subtle inference of infectious regions on CT images. In this work, COVID-19 infected regions are segmented from the surrounding regions by dealing it as a binary semantic segmentation problem. Pixels of the infected regions are labelled as a positive class, whereas all other pixels are regarded as background class.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Segmentation of COVID-19 infected lung regions"
        },
        {
            "text": "Semantic segmentation is a fine-scale pixel-based classification that labels each pixel by its corresponding object or region class [44] , [45] . In this work, we implemented four different setups for segmentation: (i) proposed COVID-19 based region approximation CNN CoV-RASeg 13 for segmentation, (ii) target specific implementation of deep semantic segmentation models from scratch (iii) pixel attention based implementation of deep semantic segmentation models, and (iv) TL based fine-tuning of deep semantic segmentation models. Details of the experimental setup are mentioned below.",
            "cite_spans": [
                {
                    "start": 132,
                    "end": 136,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 139,
                    "end": 143,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [],
            "section": "Segmentation of COVID-19 infected lung regions"
        },
        {
            "text": "The proposed CNN based semantic segmentation architecture CoV-RASeg is like a SegNet, consisting of two encoder and decoder blocks. Architectural design for the proposed CoV-RASeg is shown in Figure 4 . In the proposed architecture, we redesign the encoder and decoder block to enhance the network's feature learning capacity. For this purpose, we systematically incorporated average pooling with max-pooling in encoding stages (mathematically expressed in Equation (8 & 9)). In the decoder stage, we also implemented average pooling along with maxun-pooling, in contrast to other deep CNN semantic segmentation models. The difficulty in discrimination of COVID-19 infectious region from a background region is faced as a border between two regions is usually ill-defined, and infectious region overlaps with healthy lungs sections. Max pooling is used to learn the boundary information, whereas average pooling is used to determine characteristic COVID-19 infection patterns from the CT images. We used SegNet as a baseline model to evaluate the significance of systematic using max and average pooling in each encoder and decoder. We exploited encoder-decoder architecture for fine-grain semantic segmentation as encoding stages of such architectural design are very good in learning of semantically meaningful object-specific information. However, the feature encoding process loses spatial information that is required for object segmentation. Therefore, for the localization of infected regions on the original high-resolution image, we used the decoding stage to nonlinearly restore the spatial resolution of encoder's feature-maps by utilizing max-pooling indices. Whereas, in the last layer, 2x2 convolutional operation with sigmoid activation function is employed for 14 discriminating each pixel into either infected or background region. Encoder and decoder are symmetrical in structure with the difference of max-pooling layer in the encoder part that is replaced by un-pooling layers in the corresponding decoder part (Figure 4 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 192,
                    "end": 200,
                    "text": "Figure 4",
                    "ref_id": "FIGREF11"
                },
                {
                    "start": 2031,
                    "end": 2040,
                    "text": "(Figure 4",
                    "ref_id": "FIGREF11"
                }
            ],
            "section": "Proposed CoV-RASeg for segmentation"
        },
        {
            "text": "Several DL models with different architectural designs are reported for semantic segmentation and are benchmarked against diverse categories of the datasets [46] . ",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 161,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                }
            ],
            "ref_spans": [],
            "section": "Comparison with existing semantic segmentation models"
        },
        {
            "text": "In this work, we also incorporated the idea of assigning attention to each pixel during training based on their respective class representation in the dataset to deal with the scant representation of COVID-19 infected regions [51] . This is a type of static attention (SA), which enhances the foreground (COVID-19 infected region) by assigning this region high weightage while suppressing the background region pixels by associating small weight with them. It also helps to learn the foreground region anatomies effectively. This idea is incorporated in the proposed CoV-RASeg as well as in the existing deep CNN based semantic segmentation models.",
            "cite_spans": [
                {
                    "start": 226,
                    "end": 230,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "Pixel attention-based implementation of deep CNNs"
        },
        {
            "text": "For the development of the proposed framework, we used standard CT images dataset provided by the SIRM [52] . As COVID-19 is a new disease; therefore, no other standard dataset is available. The dataset is consisting of 829 axial CT samples, out of which 370 CT images show COVID-19 infection pattern, whereas 459 images represent the healthy samples. The provided dataset was examined by the experienced radiologist, and infected lung sections were also marked by the radiologists. Each CT sample was paired with a radiologist provided binary mask (ground truth) that is a fine-grained pixel-level binary label (shown in Figure 5 ). Moreover, the dataset covers different infection levels like mild, medium, and severe cases of COVID-19. All the images were resized from 512x512x3 to 304x304x3 for computational efficiency. The CT images for COVID-19 infected and healthy lung samples are shown in Figure 5 . ",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 107,
                    "text": "[52]",
                    "ref_id": "BIBREF51"
                }
            ],
            "ref_spans": [
                {
                    "start": 622,
                    "end": 630,
                    "text": "Figure 5",
                    "ref_id": "FIGREF14"
                },
                {
                    "start": 899,
                    "end": 907,
                    "text": "Figure 5",
                    "ref_id": "FIGREF14"
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "The cross-validation technique is employed during hypermeter selection to improve the robustness and generalization of the models. We ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cross-validation scheme"
        },
        {
            "text": "The proposed two-stage CNN framework performance has been evaluated using standard metrics. Evaluation metrics, along with abbreviation and mathematical explanations, are provided in Table 1 . The classification metrics include accuracy (Acc), recall (R), specificity (S), precision (P), Mathew Correlation Coefficient (MCC), and F-score are expressed in Equation (10) (11) (12) (13) (14) . While the segmentation models are evaluated in terms of segmentation accuracy (S-Acc), the intersection of union (IoU), and the Dice Similarity (DS) coefficient that are expressed in Equation (16) and (17), respectively. ",
            "cite_spans": [
                {
                    "start": 364,
                    "end": 368,
                    "text": "(10)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 369,
                    "end": 373,
                    "text": "(11)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 374,
                    "end": 378,
                    "text": "(12)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 379,
                    "end": 383,
                    "text": "(13)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 384,
                    "end": 388,
                    "text": "(14)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 183,
                    "end": 190,
                    "text": "Table 1",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Performance evaluation"
        },
        {
            "text": "In this work, we proposed a two-stage framework for the analysis of COVID-19 infected lungs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "The advantage of dividing the workflow into two stages is to initially scrutinize the infected CT samples, whereas exploration of regions corresponding to characteristic infection pattern may only be performed within the selected images. This staging process emulates the clinical workflow, where patients based on initial screening are devised for further diagnostic tests. The results of the two stages are discussed below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "In this work for initial screening, we proposed a deep CNN based classification model CoV-CTNet for categorization of each sample into infected or healthy images. We optimized this stage for highly sensitive in identifying COVID-19 symptoms with a minimum number of false positives. The learning potential of the proposed CoV-CTNet for COVID-19 specific CT feature is evaluated by comparing performance with exiting models on the unseen test dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance analysis of the screening stage"
        },
        {
            "text": "In the classification stage, we gain improvement in detection rate ( Figure 6 , Table 2 &   Table 3 ) by adding two enhancements. In the first step, we enhanced the original image by fusing high-frequency channel (HH) with region approximation (LL) channel from the second level decomposition of DWT. This fusion mimics the idea of Leplacian of Gaussian and heightened the distinct characteristics of infected and healthy regions, and thus improves the detection, which is evident from MCC score and other performance metrics ( Table 2 & Table 3 ). Secondly, we proposed a new CNN model CoV-CTNet in which we added fully connected layers with dropout for emphasizing on learning of discriminatory CT based image features for COVID-19",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 69,
                    "end": 77,
                    "text": "Figure 6",
                    "ref_id": null
                },
                {
                    "start": 80,
                    "end": 99,
                    "text": "Table 2 &   Table 3",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 528,
                    "end": 545,
                    "text": "Table 2 & Table 3",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Performance analysis of the screening stage"
        },
        {
            "text": "infection. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance analysis of the screening stage"
        },
        {
            "text": "The proposed CoV-CTNet is evaluated on the test set based on three different performance metrics: Accuracy, F-score, and MCC. In addition to this sensitivity, specificity and precision of proposed classifier are also analyzed. The results of the proposed CoV-CTNet are presented in Table 2 . The proposed CoV-CTNet shows good generalization as compared to baseline 21 ResNet18 in terms of F-score (CoV-CTNet: 99%, ResNet-18: 97%), Accuracy (CoV-CTNet:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 282,
                    "end": 289,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Performance analysis of the proposed CoV-CTNet"
        },
        {
            "text": "98.80%, ResNet-18: 97.17%), and MCC: (CoV-CTNet: 98%, ResNet-18: 94%). Moreover, good discrimination ability of the proposed CoV-CTNet is also evident from the decision function feature space (Figure 7) .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 192,
                    "end": 202,
                    "text": "(Figure 7)",
                    "ref_id": null
                }
            ],
            "section": "Performance analysis of the proposed CoV-CTNet"
        },
        {
            "text": "Performance of the proposed CoV-CTNet is compared with the nine different exisiting deep Table 3 shows the comparison between performances of TL-based fine-tuned and train from scratch models on the test dataset. Performance analysis suggests that TL-based fine-tuned models learn the COVID-19 specific feature in a better way than the deep CNN models that are trained from scratch on CT images.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 89,
                    "end": 96,
                    "text": "Table 3",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Performance analysis of the proposed CoV-CTNet with existing CNNs"
        },
        {
            "text": "Whereas comparison of the proposed CoV-CTNet shows better performance in terms of F-score, MCC and accuracy (Table 3) with the existing deep CNNs either they are trained from scratch or fine-tuned. Whereas gain in performance of the proposed CoV-CTNet as compared with best, average and lowest MCC score reported by deep models is shown in Figure 8 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 108,
                    "end": 117,
                    "text": "(Table 3)",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 340,
                    "end": 348,
                    "text": "Figure 8",
                    "ref_id": "FIGREF19"
                }
            ],
            "section": "Performance analysis of the proposed CoV-CTNet with existing CNNs"
        },
        {
            "text": "Furthermore, the PR curve is used to quantitatively measure the discrimination power of the proposed model (shown in Figure 9 ). PR curve is a performance measurement curve that is used for classification problems. It evaluates the generalization ability of the classifier by defining the degree of separability between two classes at different threshold values. Figure 9 shows the obtained PR curve for the proposed and baseline classification models on the test set. It is clearly shown that the proposed CoV-CTNet has a better learning capacity than the baseline and other existing deep CNN models. Although AUC of PR curve for CoV-CTNet is equal to DenseNet, however, overall CoV-CTNet achieved the highest F-score, Accuracy and MCC as compared to",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 117,
                    "end": 125,
                    "text": "Figure 9",
                    "ref_id": "FIGREF21"
                },
                {
                    "start": 363,
                    "end": 371,
                    "text": "Figure 9",
                    "ref_id": "FIGREF21"
                }
            ],
            "section": "PR curve-based analysis"
        },
        {
            "text": "DenseNet and other deep models (Table 3 ). ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 31,
                    "end": 39,
                    "text": "(Table 3",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "PR curve-based analysis"
        },
        {
            "text": "CT images that are detected as a COVID-19 infected in a classification stage using (Table 4 ). ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 83,
                    "end": 91,
                    "text": "(Table 4",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "Segmentation of infectious Lung regions"
        },
        {
            "text": "Convergences of the proposed CoV-RASeg on train and validation dataset are shown in Figure   10 , Table 5 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 84,
                    "end": 95,
                    "text": "Figure   10",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 98,
                    "end": 105,
                    "text": "Table 5",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Performance analysis of the proposed CoV-RASeg"
        },
        {
            "text": "Whereas, precise learning of discriminating boundary is clear from the higher value of BFS (97.92%).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance analysis of the proposed CoV-RASeg"
        },
        {
            "text": "The proposed CoV-RASeg is benchmarked against the SegNet. Table 5 Moreover, it can localize the infection in a precise manner, whether it is located at a single location or it is spread across multiple distinct lobes or segments of the lungs. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 58,
                    "end": 65,
                    "text": "Table 5",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Performance analysis of the proposed CoV-RASeg"
        },
        {
            "text": "We have validated the performance of our proposed architecture CoV-RASeg by comparing its performance with seven popular semantic segmentation models (VGG16/19, FCN, SegNet, U-Net, U-SegNet, Deep LabV1/3). The comparison is shown in Table 5 & Table 6 . The performance of the proposed \"RA-CoVSeg\" is compared with the existing techniques in three different scenarios, including training from scratch, training using attention and finally, the TLbased fine-tuning of the architecture. The IOU and BFS plot bars show that the proposed model is better or comparable in performance to existing techniques when it is compared with their best, lowest, and average values (shown in Figure 12 ). (Table 5 & Table 6 , Figure 13 ). The worst segmentation performance for COVID infected region is 49.69%, and for the background is 61.06%. Our proposed model,",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 233,
                    "end": 240,
                    "text": "Table 5",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 243,
                    "end": 250,
                    "text": "Table 6",
                    "ref_id": "TABREF7"
                },
                {
                    "start": 675,
                    "end": 684,
                    "text": "Figure 12",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 688,
                    "end": 706,
                    "text": "(Table 5 & Table 6",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 709,
                    "end": 718,
                    "text": "Figure 13",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Performance comparison with existing segmentation models"
        },
        {
            "text": "which is small in size shows the comparable performance with high capacity Deep LabV3 finetuned models. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance comparison with existing segmentation models"
        },
        {
            "text": "In the provided dataset, the typical region of CT images or healthy samples dominates the COVID-19 infected areas. This under-representation usually affects the performance of segmentation models. To address this problem, we used a pixel attention strategy during training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance analysis of attention based deep CNNs"
        },
        {
            "text": "The incorporation of pixel weights consistently improves the segmentation for different categories of infections, which is evident from the visual quality of the segmentation output maps ( Figure 11 & Figure 13 ) and performance measure (Table 5) , whereas significant improvement for less severely infected lung sections is noted. The gain in performance is noted from 0.05 to 0.20%, as shown in Table 5 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 189,
                    "end": 198,
                    "text": "Figure 11",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 201,
                    "end": 210,
                    "text": "Figure 13",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 237,
                    "end": 246,
                    "text": "(Table 5)",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 397,
                    "end": 404,
                    "text": "Table 5",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Performance analysis of attention based deep CNNs"
        },
        {
            "text": "Early ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "COVID-19 pandemic: perspectives on an unfolding crisis",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Spinelli",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pellino",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Br. J. Surg",
            "volume": "107",
            "issn": "7",
            "pages": "785--787",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Estimating clinical severity of COVID-19 from the transmission dynamics in Wuhan, China",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nat. Med",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Evolution of the novel coronavirus from the ongoing Wuhan outbreak and modeling of its spike protein for risk of human transmission",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Science China Life Sciences",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Lancet",
            "volume": "395",
            "issn": "10223",
            "pages": "497--506",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Evaluating the accuracy of different respiratory specimens in the laboratory",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Laboratory diagnosis and monitoring the viral shedding of 2019-nCoV infections",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Artificial intelligence-enabled rapid diagnosis of patients with COVID-19",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Mei",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nat. Med",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Correlation of Chest CT and RT-PCR Testing in Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014 Cases",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ai",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Sensitivity of Chest CT for COVID-19: Comparison to RT-PCR",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Coronavirus disease 2019 (COVID-19): A systematic review of imaging findings in 919 patients",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Salehi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Abedi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Balakrishnan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gholamrezanezhad",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "American Journal of Roentgenology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Diagnosis of the Coronavirus disease (COVID-19): rRT-PCR or CT?",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Eur. J. Radiol",
            "volume": "126",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Radiological findings from 81 patients with COVID-19 pneumonia in Wuhan, China: a descriptive study",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Lancet Infect. Dis",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Development and Validation of a Deep Learning-based Automatic Detection Algorithm for Active Pulmonary Tuberculosis on Chest Radiographs",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "J"
                    ],
                    "last": "Hwang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Clin. Infect. Dis",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Deep learning-based detection system for multiclass lesions on chest radiographs: comparison with observer readings",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Eur. Radiol",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "The role of imaging in the detection and management of COVID-19: a review",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Rev. Biomed. Eng",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Review of Artificial Intelligence Techniques in Imaging Data Acquisition, Segmentation and Diagnosis for COVID-19",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Rev. Biomed. Eng",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Rapid AI Development Cycle for the Coronavirus ( COVID-19 ) Pandemic : Initial Results for Automated Detection & Patient Monitoring using Deep Learning CT Image Analysis Article Type : Authors : Summary Statement : Key Results : List of abbreviati",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Gozes",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Frid",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Greenspan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Patrick",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.05037"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Deep Learning in Medical Image Analysis",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Suk",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Annu. Rev. Biomed. Eng",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Automatic Detection of Coronavirus Disease (COVID-19) Using X-ray Images and Deep Convolutional Neural Networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Narin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kaya",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Pamuk",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Lung Infection Quantification of COVID-19 in CT Images with Deep Learning",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "0",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "A deep learning algorithm using CT images to screen for Corona Virus Disease (COVID-19),\" medRxiv",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "COVID-CAPS: A Capsule Network-based Framework for Identification of COVID-19 cases from X-ray Images",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Afshar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Heidarian",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Naderkhani",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oikonomou",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "N"
                    ],
                    "last": "Plataniotis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--5",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Coronavirus Disease Analysis using Chest X-ray Images and a Novel Deep Convolutional Neural Network",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hussain",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--31",
            "other_ids": {
                "DOI": [
                    "10.13140/RG.2.2.35868.64646"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Harmony-Search and Otsu based System for Coronavirus Disease (COVID-19) Detection using Lung CT Scan Images",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Rajinikanth",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Dey",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N J"
                    ],
                    "last": "Raj",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Hassanien",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "C"
                    ],
                    "last": "Santosh",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "S M"
                    ],
                    "last": "Raja",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "The watershed transform: definitions, algorithms and parallelization strategies",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "B T M"
                    ],
                    "last": "Roerdink",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Meijster",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Fundam. Informaticae",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Satellite Image Enhancement Using 2D Level DWT",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Unni",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "3",
            "issn": "",
            "pages": "1926--1929",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Internet of image thingsdiscrete wavelet transform and Gabor wavelet transform based image enhancement resolution technique for IoT satellite applications",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Muthukrishnan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Charles Rajesh Kumar",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "Vinod"
                    ],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kanagaraj",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Cogn. Syst. Res",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Image contrast enhancement based on laplacian-of-gaussian filter combined with morphological reconstruction",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Iwanowski",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Advances in Intelligent Systems and Computing",
            "volume": "977",
            "issn": "",
            "pages": "305--315",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Residual Attention Network for Image Classification",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR",
            "volume": "",
            "issn": "",
            "pages": "6450--6458",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "A survey of the recent architectures of deep convolutional neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sohail",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Zahoora",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Qureshi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Artif. Intell. Rev",
            "volume": "",
            "issn": "",
            "pages": "1--68",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Convolutional neural networks: an overview and application in radiology",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Yamashita",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nishio",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "K G"
                    ],
                    "last": "Do",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Togashi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Insights Imaging",
            "volume": "9",
            "issn": "4",
            "pages": "611--629",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "493",
            "issn": "",
            "pages": "405--415",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Deep Residual Learning for Image Recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "77",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Going deeper with convolutions",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Densely connected convolutional networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Der Maaten",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "Q"
                    ],
                    "last": "Weinberger",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. -30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017",
            "volume": "",
            "issn": "",
            "pages": "2261--2269",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Xception: Deep learning with depthwise separable convolutions",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chollet",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1610--2357",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Transfer learning based deep CNN for segmentation and detection of mitoses in breast cancer histopathological images",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Wahab",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "S"
                    ],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Microscopy",
            "volume": "",
            "issn": "",
            "pages": "1--18",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Adaptive Transfer Learning in Deep Neural Networks: Wind Power Prediction using Knowledge Transfer from Region to Region and Between Different Task Domains",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Qureshi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Transfer Learning and Meta Classification Based Deep Churn Prediction System for Telecom Industry",
            "authors": [
                {
                    "first": "U",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Basit",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "U"
                    ],
                    "last": "Haq",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "S"
                    ],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "A Survey on Transfer Learning",
            "authors": [
                {
                    "first": "Qiang",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [
                        "Y"
                    ],
                    "last": "Fellow",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "1",
            "issn": "10",
            "pages": "1--15",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Transfer Learning for Visual Categorization : A Survey",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Member",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Member",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "26",
            "issn": "",
            "pages": "1019--1034",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "PASSENGER DETECTION AND COUNTING FOR PUBLIC TRANSPORT SYSTEM",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Yousaf",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Murtaza",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Velastin",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "NED Univ. J. Res",
            "volume": "XVII",
            "issn": "2",
            "pages": "35--46",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Computer vision based room interior design",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ahmad",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hussain",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ahmad",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Conci",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Eighth Int. Conf. Mach. Vis. (ICMV 2015)",
            "volume": "9875",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "A survey on deep learning techniques for image and video semantic segmentation",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Garcia-Garcia",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Orts-Escolano",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Oprea",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Villena-Martinez",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Martinez-Gonzalez",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Garcia-Rodriguez",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Applied Soft Computing Journal",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Badrinarayanan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kendall",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Cipolla",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "U-Segnet: Fully Convolutional Neural Network Based Automated Brain Tissue Segmentation Tool",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nagar",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Arora",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings -International Conference on Image Processing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Fully convolutional networks for semantic segmentation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Shelhamer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "C"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Papandreou",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kokkinos",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Yuille",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Attention gated networks: Learning to leverage salient regions in medical images",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schlemper",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Med. Image Anal",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Towards Efficient COVID-19 CT Annotation: A Benchmark for Lung and Infection Segmentation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--7",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Interval estimation for a binomial proportion",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "D"
                    ],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "T"
                    ],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "A. Das",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Stat. Sci",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Confidence intervals for the area under the ROC Curve",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cortes",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mohri",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Network (CNN) framework to identify COVID-19 infected samples and to analyze their extant of spread and infection pattern on lungs CT scans. In the proposed workflow, we initially screen the CT samples for COVID-19 infection using proposed deep CNN classifier CoV-CTNet (also known as PIEAS Classification Network-1 (PCNet-1)). Whereas in the next stage, COVID-19 classified CT samples are further analyzed for identifying the lung regions exhibiting infection using proposed region approximation based semantic segmentation architecture CoV-RASeg (also known as PIEAS Segmentation Network-1 (PSNet-1)). The proposed framework is assessed on standard publicly available lung CT dataset, and its performance is compared against well-known existing deep architectures. Key contributions of this study are: 1. A two-stage framework consisting of classification and segmentation models is proposed for detection and region analysis of COVID-19 infected regions in the lungs. 2. A custom-made deep CNN based classification network is proposed that can effectively learn the CT image features of COVID-19 infection. 3. A novel deep CNN based semantic segmentation architecture is proposed for fine-scale demarcation of lung regions infected from COVID-19. For this purpose, max and average pooling operations are incorporated systematically in each encoder and decoder block. 4. Performance of the proposed deep CNNs based classification and segmentation models is compared with different existing models that are trained from scratch as well as finetuned using TL. Moreover, the idea of attention is incorporated in deep segmentation 4 models to deal with sparse representation of infected regions and for effective segmentation of mildly infected lung regions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "constituted of classification and segmentation stages. In the first stage, COVID-19 infected individuals are segregated from healthy CT samples by performing classification. Whereas, in the second stage, the segmentation of COVID-19 infected regions on lung CT images is performed to obtain fine-scale region details. The segmentation of infected regions can be helpful in quantifying infection spread. The detailed workflow is shown in Figure 1. Overview of the workflow for the proposed two-stage deep classification and segmentation framework.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": ".2) for classification, (ii) target specific training of deep CNN classifiers from scratch, and (iii) weight transfer-based fine-tuning of deep CNN classifiers. The details of these experimental setups are discussed below.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "IDWT based Lung CT image enhancement.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "19 infected samples based on CT imagery features. We have used ResNet-18[30] as a baseline classification model for comparison. The proposed CoV-CTNet is a 24 layers deep architecture consisting of four residual blocks-based feature extraction stages (Figure 3). Within each residual block, two different types of convolutional blocks are implemented (shown in Figure 3). These convolutional blocks are connected with shortcut links within the residual block to perform the reference-based optimization of convolutional filters. A mathematical formulation of the operations used within the convolutional block is expressed in Equation (1-3), whereas the concept of residual learning is expressed in Equation (4) & (5). Residual learning has an advantage over simple feed-forward weight optimization as it solves the vanishing gradient problem, improves the feature-map representation and network convergence.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": ") for l th layer, whereas ( MN \uf0b4 ), and ( D ) represent the spatial dimension and feature-map depth, respectively, for a convolved image. Center coordinates for the convolutional filter are expressed via ( , ij ), and ( , l mn C ) shows the convolved output for ( , mn ) coordinates of the image for l th layer. Equation (2) represents the batch normalization operation for l th layer convolved output ( l C ), whereas B \uf06d and 2 B \uf073 represents the mean and variance for a mini-batch. () fc shows the ReLu activation function and ( c ) is an activation value for each element of feature-map. \u2212 show the convolution blocks consisting of convolution operation, batch normalization and ReLu activation for n and n-1 layers, whereas",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Architectural design for the proposed CoV-CTNet.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "For comparison of the proposed CoV-CTNet, we applied several existing well-known models varying in depth and architecture design for the classification of COVID-19 infected CT images. The implemented classification networks are VGG16/19, ResNet18/50, GoogleNet (Inception-V1), Inception-V3, DenseNet201, ShuffleNet and Xception [33]-[38]. These networks are trained from scratch (discussed in Section 3.1.3.1) as well as fine-tuned using TL for comparison purpose (discussed in Section 3.1.3.2). Architectural details of analyzed architectures are mentioned below. VGG is one of the earliest deep architecture that introduced the idea of the effective receptive field by consecutively placing small size filters. It uniformly used 3x3 filters across all the convolutional layers. VGG architecture with two different depths of the feature extraction stage (16 and 19 convolutional layers) and three fully connected layers are proposed for classification. GoogLeNet introduced the idea of inception block, which replaced the conventional convolutional filter within the layer with block architecture. Inception block transforms the image at different scales by using multidimensional filters and concatenating their output. Inception-V3 is a modification of GoogLeNet, which makes the network computationally efficient by replacing the large size filters with asymmetric filters. ResNet proposed the idea of residual learning-based optimization for deep architectures using skips connections. This type of learning improves the network convergence by considering the previous layer output as a reference for the optimization of the next layers' weight. ResNet with various depths depending upon the number of residual blocks has been proposed. DenseNet also exploited the idea of skip connections but in a modified fashion. It concatenates the feature-maps of each subsequent layer with the next layer. In this way, it solves the vanishing gradient problem and provides both highand low-level features at lateral layers of the network. ShuffleNet is a resource-efficient network, which reduced the number of computations by using point-wise group convolution. Whereby, it retains high accuracy by incorporating the idea of channel shuffle. Xception is a variant of Inception architecture that uses the uniform size of convolutional filters within a block. It increased the number of transformation blocks and gave the idea of depthwise separable convolution. 3.1.3.1 Target specific training of deep CNNs from scratch Seven different well-known existing deep CNN architectures are implemented to evaluate their learning potential for the classification of COVID-19 infected individuals. These deep CNN architectures are made applicable to the targeted CT image dataset by adding a new input layer according to the dimensions of the CT image dataset (82x82x1). Similarly, the last fully",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": ", Z Y ) and objective functions ( S \uf068 , Z \uf068 ). The transfer of knowledge from source to target requires that either both domains have a common task ( SZ \uf068\uf068 = ) or they share the feature space (",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Architectural design for the proposed CoV-RASeg.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "max (.) f represent the max and average pooling operations, respectively, on convolved output ( , mn O ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "The differences among these architectures are in the number of encoder and decoders, up-sampling approach, and type of skip connections. In this work, we implemented VGG-16, SegNet, U-SegNet, FCN and Deep LabV1/3 as segmentation models[47]-[50].To implement the well-known CNN segmentation models on COVID-19 infected lung CT dataset, we replaced the input and classification layers with new layers that correspond with data dimension (304x304x3) and lung mask categories (304x304x2). Initially, deep CNN models are trained from scratch on targeted CT dataset by assigning random values and using a backpropagation algorithm. To overcome the limitation of a small dataset, we also exploited the idea of weight initialization of network layers from pre-trained architectures. In this approach, instead of starting from random weight values, we initialized weights of convolutional layers from pre-trained architectures as discussed in Section 3.1.3.2. The parameters of these models are tuned in an end-to-end manner to adapt diverse categories of models to CT images.Brief description of the implemented architectures is mentioned in this paragraph to gain an insight into their architectural design. VGG is a state-of-the-art deep architecture, which is famous because of its simple architectural design. VGG based segmentation model is a modified form of classification based VGG architecture in which convolutional layers are used for the encoding of class-specific features, whereas fully connected layers are replaced with a decoder architecture. Decoder architecture is similar to encoder architecture; however; pooling layers are replaced with up-sampling layers. The VGG-16 encoder contains 13, while VGG-19 contains about 16 convolutional layers[38]. FCN performs pixel-to-label based segmentation by using down-sampling and up-sampling stages. The down-sampling stage is used to extract the highlevel semantically meaningful information; whereas the up-sampling stage is used to predict the low-level object anatomy (target shape and contour) based information. SegNet is a semantic pixel-wise deep CNN segmentation architecture, inspired by VGG. The key difference betweenSegNet and VGG is in the number of encoders and decoders. Moreover, SegNet suggested the use of pooling indices in the decoding stages that are stored during the max-pooling based downsampling in the encoder. The use of pooling indices instead of bilinear interpolation during upsampling reduces computational complexity. U-SegNet implemented the characteristic attributes of both SegNet and U-Net. U-SegNet leveraged the idea of using skip connections from U-Net architecture for depth-wise concatenation of feature-maps. Whereas, on the decoder side, it uses pooling indices like the SegNet to project the down-sampled feature-maps back to high resolution. Deep Lab implemented Atrous convolution in a cascade manner to regulate the size of feature-map and to view feature-maps at multiple scales.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Healthy vs COVID-19 infected Lung CT images are respectively, shown in Panel (A) and (B). Infectious regions are highlighted by red boxes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "our proposed framework, both the classification and segmentation models are trained separately. For the classification task, the COVID-19 dataset contains 829 training CT images, where 370 are infected images and 459 uninfected images. The 370 affected COVID-19 images, along with their corresponding binary labels, are used during the training of segmentation models. The experimental setup for both classification and segmentation models during training is kept fixed. The dataset is divided into train and test portions at the ratio of 80:20%. Furthermore, the train set is divided into a train and validation set at a ratio of 80:20% for hypermeter selection. Deep CNNs are trained by employing SGD as an optimizer and by minimizing cross-entropy loss. The hyperparameter values like learning rate: 0.001, Epochs: 30, batch size: 8, and momentum: 0.95 are kept constant. For classification and segmentation, the softmax is used for the identification of class probabilities. 95% confidence interval (CI) for AUC, classification and segmentation models is computed using [53], [54]. All experiments are carried out on MATLAB 2019b framework. Intel (R) Core (TM) i7 and GPU-enabled Nvidia\u00ae GTX 1060 Tesla. The training of all the classification and segmentation networks took approximately took ~3 days.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "used 5-fold cross-validation during the training of both the classification and segmentation models. Each fold contains 598 training images and 135 validation images in the classification challenge, while the test set includes 166 CT images. Whereas, for the segmentation phase, 370 images are provided with pixel-level masks for COVID-19 infected regions and are involved in pixel-label-based semantic segmentation. On the other hand, 236 images from COVID-19 datasets were considered for training, 60 for validation, and 74 for testing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Performance analysis of deep CNN models on IDWT and original images in terms of minimum, average and maximum scores for MCC, Accuracy and F-score. Visualization of feature-space generated from proposed CoV-CTNet using PCA based transformation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Comparison of the proposed CoV-CTNet with maximum, average and minimum F-scores, MCC and accuracy for state-of-the-art deep CNNs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "CNN based classification models (VGG-16/19, Inception-V1/V3, ResNet-18/50, DenseNet, Xception, and ShuffleNet). These classification models are well-known for the classification of complex tasks and are successively used for classification of lung abnormalities. For comparison purpose, existing deep CNNs have trained both in a target-specific manner by training from scratch and by fine-tuning them using TL.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "Panel (A) shows the PR curve for the proposed CoV-CTNet and trained from scratch deep CNNs, while (B) shows TL-based fine-tuned existing deep CNNs. 95% CI for AUC is shown in brackets.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "proposed CoV-CTNet are assigned to the deep semantic segmentation model for the exploration of infected regions. The analysis of infected lung lobes is indispensable for gaining radiological an insight into key characteristics of infection pattern, its spread and impact on surrounding lung segments or organs. Moreover, by region analysis, we can quantify infection severity, which may help in the grouping of mildly, and severely infected patients, and their treatment design.We proposed CoV-RASeg to segment the infected regions of lungs on CT images.Furthermore, a series of deep semantic segmentation models are implemented to assess the learning capacity of our proposed model. The segmentation models were optimized based on three characteristic imagery features of COVID-19 (GGO, pleural effusion and consolidation) to distinguish the typical region on CT image with infected regions. The significance of the proposed method is provided by the experimental results on the test dataset",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "Error convergence plot for the training of the proposed CoV-RASeg segmentation model.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "whereas segmentation capability is evaluated on the test dataset. The main problem in identification of COVID-19 infection is that COVID-19 is characterized by different patterns such as GGO, consolidation, patchy bilateral shadows. Moreover, the pattern of infection varies during the course of information and in different individuals. In the early stages, features of the infected region are indistinguishable from healthy segments. The other important aspect is to isolate the infected region from the healthy region by defining the well-defined boundary. For delineating the infected regions with well-defined boundary within the lung lobes and dealing with subtle changes, we incorporated the new idea that systematically uses max and average pooling within deep semantic segmentation architecture. The proposed segmentation model suggests good detection ability on the test set, which is evident from DS and IoU score of 95.21% and 98.65%, respectively, for the COVID-19 infected region, as shown in",
            "latex": null,
            "type": "figure"
        },
        "FIGREF25": {
            "text": "illustrates the performance of CoV-RASeg and baseline \"SegNet\" on the test dataset. DS score and IoU suggest the better performance of proposed architecture as compared to SegNet. Moreover, the pixel-level comparison of the generated binary masks also shows high visual quality for the proposed model compared to baseline (Figure 11). It is apparent from the visual representation that our model is capable of identifying all the infected regions and can highlight their extent precisely. Qualitative analysis (Figure 11) further suggests that our proposed model is useful in segmenting different levels of infection (low, medium, high) in various lobes of the lungs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF26": {
            "text": "Qualitative assessment of Segmentation results. Panel (A): CT images overlapped with infectious regions that are highlighted with red color, panel (B): binary mask provided by radiologists, panel (C): proposed CoV-RASeg output, panel (D): proposed CoV-RASeg with pixel attention output, panel (E): SegNet output, panel (F): SegNet with pixel attention output.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF27": {
            "text": "IoU and BFS based comparison of the proposed CoV-RASeg with exiting deep segmentation models.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF28": {
            "text": "Segmentation output generated by the proposed model CoV-RASeg and state-of-the-art segmentation models are shown in Figure 13 & Figure 14. Qualitative analysis suggests that the proposed CoV-RASeg performs consistently better for all CT samples as compared to other deep semantic segmentation models. It is clear from Figure 13 that existing models perform poorly for the mildly infected CT sample. Whereas, the performance of VGG16, VGG19, FCN and Deep Lab-V1/3 models fluctuate on different categories of CT samples, which suggests the poor generalization. Figure 14 shows that U-Net and a different variant of FCN fail to learn the COVID-19 infection pattern significantly. For the existing models, TL based Deep LabV3 shows the best performance with DS score: 95%, IoU: 98% and BFS: 99. Whereas, Deep LabV3 fails to learn infection pattern when train from the scratch",
            "latex": null,
            "type": "figure"
        },
        "FIGREF29": {
            "text": "Qualitative assessment of segmentation results on the test set. Panel (A-Q) show the output of different deep segmentation models that are either trained from scratch, fine-tuned using TL or implemented by incorporating pixel attention (SA) during training. Qualitative assessment of segmentation results for U-Net and Variants of FCN that fail to learn COVID-19 infection pattern for the test set significantly. Panel (A-J) show the output for different deep segmentation models that are either trained from scratch, fine-tuned using TL or implemented by incorporating pixel attention (SA) during training.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF30": {
            "text": "detection and analysis of COVID-19 infection pattern is crucial for triage and transmission control. Therefore, in this work, we have proposed a two-stage deep CNN based framework for classification and analysis of COVID-19 infected regions on lung CT images. The good discrimination ability of the proposed CoV-CTNet on test in terms of MCC (0.98) and F-score (0.99) as compared to the existing deep CNNs suggest that it can effectively identify infectious samples with a minimum number of false positives. In this work, we have proposed a region approximation based semantic segmentation model CoV-RASeg for identification and analysis of infected regions on Lung CT images that are classified as COVID-19 using proposed classification model CoV-CTNet. This two-stage processing effectively reduces the search space for learning of characteristic infectious patterns on Lung CT images. The promising performance of the proposed segmentation model (DS: 0.95, BFS: 0.99) suggests that the proposed model can precisely identify the COVID-19 infected regions on lung CT images with subtle details and region boundary. In future, this work can be extended to further classify the infected region into characteristic patterns (GGO, consolidation, pleural effusion) to provide detailed radiological insight into COVID-19 infection pattern.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "disease in multiple countries like China, Spain, and Italy. However, CT scan analysis is usually tedious, timeconsuming, and prone to human error. Therefore, machine learning (ML) based diagnostic tools are designed for fast and accurate image analysis as well as facilitate the medical staff. AI and Deep Learning (DL) models have already shown impressive performance in the medical field [18]. Especially, the deep CNN has become the workhorse for image classification, detection, localization, and segmentation. Several pre-trained CNN models with different approaches have been employed for the analysis of COVID-19 infected X-ray and CT images. Likewise, different researchers exploited the potential of TL for the prediction of COVID-19 infected image. In this regard, diverse TL-based fine-tuned CNN models like AlexNet, VGG, GoogleNet, ResNet, Dense Net, etc. have been evaluated on COVID-19 infected CT images, and their performance accuracy varies from 87% to 98% [19]-",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "However, currently, the COVID-19 dataset contains a limited amount of publicly available standardized CT images with radiologist's defined ground-truth. The accessibility of labeled data is limited because of patient's privacy concerns and a sufficient amount of time required for pixel-level labelling that is difficult to manage in a time of the pandemic. A small number of samples impede the convergence of deep CNNs while training from scratch[39]. Therefore, in this work, we exploited TL to use the weight vectors of networks pre-trained on a large amount of benchmarked datasets ImageNet. State-of-the-art deep CNN models with optimized filter weights that are learnt from source-domain (ImageNet dataset) are fine-tuned on CT images (target domain) to effectively learn the target-domain specific features from a limited amount of COVID-19 infected patient datasets[40]-[42].TL is a type of ML, which enables re-utilization of computationally intensive deep CNNs that are already pre-trained on a benchmarked dataset (also called as source domain) having a large number of images for a new problem (known as target-domain) that is comprised of the small training dataset. In TL, the source domain",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Detail of standard classification and segmentation evaluation metrics.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Comparison of the proposed CoV-CT with baseline ResNet-18 on test dataset. 95% CI for AUC is shown in brackets.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Comparison between TL-based fine-tuned vs. trained from scratch deep CNN models on the test dataset. 95% CI for AUC is shown in brackets.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Pixel-level classification results for the proposed CoV-RASeg segmentation model.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Performance of deep semantic segmentation models that are trained from scratch on test dataset. Mean error for Dice Similarity (DS) indice is represented at 95% CI.SA represents that the models are trained by incorporating static attention",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Performance of deep semantic segmentation models that are fine-tuned using TL on test dataset. Mean error for Dice Similarity (DS) indice is represented at 95% CI.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This work was conducted with the support of PIEAS IT endowment fund under the Pakistan Higher Education Commission (HEC). This research was also supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2014R1A1A2053780).We also thank Pattern Recognition Lab (PR-Lab) and Pakistan Institute of Engineering, and Applied Sciences (PIEAS), for providing necessary computational resources and healthy research environment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgment:"
        }
    ]
}
{
    "paper_id": "739715750d8eacbf04f16e36770089afad358cfa",
    "metadata": {
        "title": "Towards More Robust Natural Language Understanding",
        "authors": [
            {
                "first": "Xinliang",
                "middle": [
                    "("
                ],
                "last": "Frederick",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The Ohio State University",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": ")",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The Ohio State University",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Natural Language Understanding (NLU) is a branch of Natural Language Processing (NLP) that uses intelligent computer software to understand texts that encode human knowledge. Recent years have witnessed notable progress across various NLU tasks with deep learning techniques, especially with pretrained language models. Besides proposing more advanced model architectures, constructing more reliable and trustworthy datasets also plays a huge role in improving NLU systems, without which it would be impossible to train a decent NLU model. It's worth noting that the human ability of understanding natural language is flexible and robust. On the contrary, most of existing NLU systems fail to achieve desirable performance on out-of-domain data or struggle on handling challenging items (e.g., inherently ambiguous items, adversarial items) in the real world. Therefore, in order to have NLU models understand human language more effectively, it is expected to prioritize the study on robust natural language understanding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "In this thesis, we deem that NLU systems are consisting of two components: NLU models and NLU datasets. As such, we argue that, to achieve robust NLU, the model architecture/training and the dataset are equally important. Specifically, we will focus on three NLU tasks to illustrate the robustness problem in different NLU tasks and our contributions (i.e., novel models and new datasets) to help achieve more robust natural language understanding. The major technical contributions of this thesis are: ii 1. We study how to utilize diversity boosters (e.g., beam search & QPP) to help neural question generator synthesize diverse QA pairs, upon which a Question Answering (QA) system is trained to improve the generalization on the unseen target domain. It's worth mentioning that our proposed QPP (question phrase prediction) module, which predicts a set of valid question phrases given an answer evidence, plays an important role in improving the cross-domain generalizability for QA systems. Besides, a target-domain test set is constructed and approved by the community to help evaluate the model robustness under the cross-domain generalization setting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "2. We investigate inherently ambiguous items in Natural Language Inference, for which annotators don't agree on the label. Ambiguous items are overlooked in the literature but often occurring in the real world. We build an ensemble model, AAs (Artificial Annotators), that simulates underlying annotation distribution to effectively identify such inherently ambiguous items. Our AAs are better at handling inherently ambiguous items since the model design captures the essence of the problem better than vanilla model architectures.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "3. We follow a standard practice to build a robust dataset for FAQ retrieval task, COUGH.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "In our dataset analysis, we show how COUGH better reflects the challenge of FAQ retrieval in the real situation than its counterparts. The imposed challenge will push forward the boundary of research on FAQ retrieval in real scenarios.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Moving forward, the ultimate goal for robust natural language understanding is to build NLU models which can behave humanly. That is, it's expected that robust NLU systems are capable to transfer the knowledge from training corpus to unseen documents more reliably and survive when encountering challenging items even if the system doesn't know a priori of users' inputs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "iii Dedicated to my parents. iv Acknowledgments I feel incredibly fortunate to have Dr. Huan Sun as my advisor, without whom nothing in this thesis is possible. I would like to express my sincere gratitude to her for critiquing my work and my ideas in a constructive way. Her vision and rigorous research attitudes have shaped my thoughts. I am always indebted to her for guiding me all the way, for her contagious energy, for being so supportive and caring about students. I owe a great debt of gratitude to Dr. Marie-Catherine de Marneffe for her countless help. I am super grateful for her many invaluable insights and suggestions on my work. She has been so generous with her time, reading, reviewing and commenting on many of my writings. I am always thankful for her positive encouragement and praise. It's also my great privilege to collaborate with my friends, lab-mates at SunLab and my past mentor:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Have you ever asked: \"Siri, how is the weather today?\", \"Cortana, what is the best spot for hiking in Columbus?\" or \"Xiaoice, could you tell me how's traffic outside?\". If so, you have experienced receiving a data-supported answer from your personalized AI assistant. A natural question that people would ask is how can the agent understand an utterance and intents and generate a relevant response. The answer is Natural Language Understanding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "List of Tables"
        },
        {
            "text": "Natural Language Understanding (NLU) is a branch of Natural Language Processing (NLP) in the area of Artificial Intelligence (AI) that uses intelligent computer software to understand texts that encode human knowledge. Some representative NLU applications (and there are way more) are: Automated Reasoning, Question Answering, Text Categorization, Large-scale Content Analysis, Information Retrieval and Textual Entailment. NLU is generally considered an AI-hard problem (i.e., a problem that is hard to be solved by AI systems) (Yampolskiy, 2013) . NLU is an AI-hard problem mainly because the nature of human language (e.g., ambiguity) makes NLU difficult. For example, given the following sentence \"when the hammer hit the glass table, it shattered\", 1 humans know that it is the glass table that shattered but not the hammer. This is because our prior knowledge let us 1 https://www.colorado.edu/earthlab/2020/02/07/what-natural-language-processing-and-why-it-hard.",
            "cite_spans": [
                {
                    "start": 529,
                    "end": 547,
                    "text": "(Yampolskiy, 2013)",
                    "ref_id": "BIBREF82"
                }
            ],
            "ref_spans": [],
            "section": "List of Tables"
        },
        {
            "text": "know what glass is and that glass can shatter easily. However, coreference resolution is still a challenging task for NLU models, and thus, NLU systems still have difficulties figuring out which one of these two objects shatters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "List of Tables"
        },
        {
            "text": "Recent years have witnessed notable progress across various Natural Language Understanding tasks, especially after entering the deep learning era in 2012. Deep learning approaches quickly outperformed statistical learning methods by a large margin on many NLU tasks. As today, neural network-based NLP models have reached many new milestones (e.g., model performance comes close to or surpasses the level of non-expert humans) and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "List of Tables"
        },
        {
            "text": "have become the dominating approach for NLP tasks. Typical neural network-based NLP models/algorithms are RNN (Elman, 1990) , LSTM (Hochreiter and Schmidhuber, 1997) , GRU (Cho et al., 2014) , Seq2Seq (Sutskever et al., 2014) , attention mechanism (Luong et al., 2015) and Transformer (Vaswani et al., 2017) . Recently, pretrained language models, such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019b) , have dramatically altered the NLP landscape and marked new records on the majority of NLU tasks. However, the neural NLP models work well for supervised tasks in which there is abundant labeled data for learning, but still perform poorly for low-resource and cross-domain tasks where the training data is insufficient and the test data is from different domains, respectively. Besides more advanced model architectures, reliable and trustworthy datasets also play a huge role in improving NLU systems. Without a decent dataset, it would be challenging to train a machine learning model, not to mention carrying out a valid evaluation. As such, comprehensive evaluation benchmarks, aggregating datasets of multiple NLU tasks, emerged in the past few years such as GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) . They are diagnostic datasets designed to evaluate and analyze model performance with respect to a wide range of linguistic phenomena found in human language.",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 123,
                    "text": "(Elman, 1990)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 131,
                    "end": 165,
                    "text": "(Hochreiter and Schmidhuber, 1997)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 172,
                    "end": 190,
                    "text": "(Cho et al., 2014)",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 201,
                    "end": 225,
                    "text": "(Sutskever et al., 2014)",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 248,
                    "end": 268,
                    "text": "(Luong et al., 2015)",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 285,
                    "end": 307,
                    "text": "(Vaswani et al., 2017)",
                    "ref_id": null
                },
                {
                    "start": 360,
                    "end": 382,
                    "text": "(Radford et al., 2018)",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 392,
                    "end": 414,
                    "text": "(Devlin et al., 2019b)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1185,
                    "end": 1205,
                    "text": "(Wang et al., 2019b)",
                    "ref_id": null
                },
                {
                    "start": 1220,
                    "end": 1240,
                    "text": "(Wang et al., 2019a)",
                    "ref_id": "BIBREF77"
                }
            ],
            "ref_spans": [],
            "section": "List of Tables"
        },
        {
            "text": "The human ability of understanding natural language is flexible and robust. Therefore, human capability of understanding multiple language tasks simultaneously and transferring the knowledge to unseen documents is mostly reliable. On the contrary, most of existing NLU models built on word/character levels are exclusively trained on a restricted dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "List of Tables"
        },
        {
            "text": "These restricted datasets normally only characterize one particular domain or only include simple examples which might not well reflect the task difficulties in reality. Consequently, such models usually fail to achieve desirable performance on out-of-domain data or struggle on handling challenging items (e.g., inherently ambiguous items, adversarial items) in the real world. Moreover, machine learning algorithms are usually data-hungry and can easily malfunction when there is insufficient amount of training data. Therefore, in order to have NLU models understand human language more effectively, it is expected to prioritize the study on robust natural language understanding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "List of Tables"
        },
        {
            "text": "In this thesis, we deem that NLU systems are consisting of two components: NLU models and NLU datasets. As such, we argue that, to achieve robust NLU, the model architecture/training and the dataset are equally important. If either component is weak, it would be hard to achieve full robustness. Therefore, in order to achieve full robustness in NLU, researchers are expected to implement robust models which then are trained on constructed robust datasets. In this thesis, we define robust models and robust datasets as follow:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Robustness Problem in NLU"
        },
        {
            "text": "1. Robust models are expected to be resistant to domain changes and resilient to challenging items (e.g., inherently ambiguous items, adversarial items).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Robustness Problem in NLU"
        },
        {
            "text": "2. Robust datasets are expected to reflect real-world challenges and encode knowledge that is difficult to be unraveled simply by surface-level 2 understanding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Robustness Problem in NLU"
        },
        {
            "text": "In short, a truly robust NLU system is expected to be a robust model trained on robust datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Robustness Problem in NLU"
        },
        {
            "text": "In the context of NLP, robustness is an umbrella term which could be interpreted differently from different angles. In this thesis, we will focus on three NLU tasks to illustrate the robustness problem in different NLU tasks and our contributions (i.e., novel models and new datasets) to help achieve more robust natural language understanding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Three NLU tasks for NLU robustness problem"
        },
        {
            "text": "The first robustness problem that will be studied in this thesis is the cross-domain generalization.In Question Answering, most past work on open-domain were only testing models on in-domain data (source domain), despite outperforming human performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Three NLU tasks for NLU robustness problem"
        },
        {
            "text": "However, these well-performing models have a relatively weak generalizability, which is the crux of this robustness problem. That is, when such models are deployed on out-of-domain data (target domain), their performances go down drastically, which is way behind human performance. Similar trend is also observed under the clinical setting where a model trained on one corpus may not generalize well to new clinical texts collected from different medical institutions (Yue et al., 2020 . In Chapter 2, we will study how to utilize diversity boosters to help Question Generator (QG) synthesize diverse 3 QA pairs, upon which a Question Answering system is trained to improve the generalization to the unseen target domain. We also construct a target-domain test set to help evaluate models' generalizability.",
            "cite_spans": [
                {
                    "start": 468,
                    "end": 485,
                    "text": "(Yue et al., 2020",
                    "ref_id": "BIBREF83"
                }
            ],
            "ref_spans": [],
            "section": "Three NLU tasks for NLU robustness problem"
        },
        {
            "text": "2 For example, the presence of \"not\" or \"bad\" doesn't always indicate a negative sentiment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Three NLU tasks for NLU robustness problem"
        },
        {
            "text": "3 \"Diverse\" here means questions with different syntactic structures or different topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Three NLU tasks for NLU robustness problem"
        },
        {
            "text": "The second robustness problem that will be studied in this thesis is how to better handle inherently ambiguous items, one type of challenging items in reality. In sentiment analysis and textual entailment tasks, it has been observed that there are inherently ambiguous/disagreement 4 items for which annotators have different annotations (Kenyon-Dean et al., 2018; Pavlick and Kwiatkowski, 2019; Zhang and de Marneffe, 2021) . These items were usually treated as noise and removed in the dataset construction phase, which is problematic. In Chapter 3, we will investigate inherently ambiguous items, which are overlooked in the literature but often occurring in the real world, in the NLI (Natural Language Inference) task. To this end, we build an ensemble model, AAs (Artificial Annotators), which simulates underlying annotation distribution by capturing the modes in annotations to effectively identify such inherently ambiguous items.",
            "cite_spans": [
                {
                    "start": 338,
                    "end": 364,
                    "text": "(Kenyon-Dean et al., 2018;",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 365,
                    "end": 395,
                    "text": "Pavlick and Kwiatkowski, 2019;",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 396,
                    "end": 424,
                    "text": "Zhang and de Marneffe, 2021)",
                    "ref_id": "BIBREF85"
                }
            ],
            "ref_spans": [],
            "section": "Three NLU tasks for NLU robustness problem"
        },
        {
            "text": "The third robustness problem that will be studied in this thesis is how to construct a reliable and challenging dataset (i.e., robust dataset). In textual entailment and FAQ retrieval tasks, common datasets (e.g., SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) for textual entailment; FAQIR (Karan and\u0160najder, 2016) and StackFAQ (Karan an\u010f Snajder, 2018) for FAQ retrieval) used for training and testing might not well characterize the real difficulties of respective tasks. In the aforementioned datasets, sentence lengths and language complexities are generally low, styles are limited and the search space is small. In Chapter 4, we will follow a standard practice to build a robust dataset for the FAQ Retrieval task. In our dataset analysis, we will also show how this dataset better reflects the challenge of FAQ Retrieval in the real situation than its counterparts.",
            "cite_spans": [
                {
                    "start": 219,
                    "end": 240,
                    "text": "(Bowman et al., 2015)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 254,
                    "end": 277,
                    "text": "(Williams et al., 2018)",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 308,
                    "end": 332,
                    "text": "(Karan and\u0160najder, 2016)",
                    "ref_id": null
                },
                {
                    "start": 346,
                    "end": 371,
                    "text": "(Karan an\u010f Snajder, 2018)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Three NLU tasks for NLU robustness problem"
        },
        {
            "text": "We will conclude with recommendations for future work about how to better approach robustness problem in NLU in Chapter 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Three NLU tasks for NLU robustness problem"
        },
        {
            "text": "Chapter 2: Clinical Question Answering",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Three NLU tasks for NLU robustness problem"
        },
        {
            "text": "Clinical question answering (QA), which aims to automatically answer natural language questions based on clinical texts in Electronic Medical Records (EMR), has been identified as an important task to assist clinical practitioners (Patrick and Li, 2012; Pampari et al., 2018; Fan, 2019; Rawat et al., 2020) . Neural QA models in recent years (Chen et al., 2017; Devlin et al., 2019b) show promising results in this research. (Kang et al., 2019; Cho et al., 2019; . Following the second approach, we propose a simple but effective question phrase prediction (QPP) module to diversify the generation. Specifically, QPP takes the extracted answer evidence as input and sequentially predicts potential question phrases (e.g., \"What treatment\", \"How often\") that signify what types of questions humans may ask about the answer evidence. Then, by directly forcing a QG model to produce specified question phrases at the beginning of the question generation process (both in training and inference), QPP enables diverse questions to be generated. duplicates are removed. Meanwhile, clinical experts are highly encouraged to create new questions based on the given clinical text (which are marked as \"human-generated\"). But if they do find that the machine-generated questions sound natural and match the provided answer, they can keep them (which are marked as \"human-verified\"). After obtaining the annotated questions, we ask another clinical expert to do a final pass of the questions in order to further ensure the quality of the test set. The final test set consists of 1287 questions (of which 975 are \"human-verified\" and 312 are \"human-generated\").",
            "cite_spans": [
                {
                    "start": 231,
                    "end": 253,
                    "text": "(Patrick and Li, 2012;",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 254,
                    "end": 275,
                    "text": "Pampari et al., 2018;",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 276,
                    "end": 286,
                    "text": "Fan, 2019;",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 287,
                    "end": 306,
                    "text": "Rawat et al., 2020)",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 342,
                    "end": 361,
                    "text": "(Chen et al., 2017;",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 362,
                    "end": 383,
                    "text": "Devlin et al., 2019b)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 425,
                    "end": 444,
                    "text": "(Kang et al., 2019;",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 445,
                    "end": 462,
                    "text": "Cho et al., 2019;",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the following sections, we consider emrQA as the source dataset and our annotated MIMIC-III QA dataset as the target data. Detailed statistics of the two datasets are given in ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We first give an overview of our framework without including any diversity booster. Figure 2 .1: Illustration of our framework equipped with QPP: A key component is our question phrase prediction (QPP) module, which aims to generate diverse question phrases and can be \"plugged-and-played\" with most existing QG models to diversify their generation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 84,
                    "end": 92,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Overview of Our Framework"
        },
        {
            "text": "2019c; Shakeri et al., 2020) , we implement an answer evidence extractor and a seq2seqbased neural QG model (Du et al., 2017, NQG) to synthesize QA pairs on target contexts. ",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 28,
                    "text": "Shakeri et al., 2020)",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 108,
                    "end": 130,
                    "text": "(Du et al., 2017, NQG)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Overview of Our Framework"
        },
        {
            "text": "Given the observation above, we argue that the synthetic questions should be diverse so that they could serve as more useful training corpora.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Diverse Question Generation for QA"
        },
        {
            "text": "We investigate two kinds of approaches to diversify the generation. In the first decodingbased approach, we select the standard beam search as the representative since it is well ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Overview of Diverse Question Generation"
        },
        {
            "text": "We formulate the question phrase prediction task as a sequence prediction problem and adopt a commonly used seq2seq model (Luong et al., 2015) . More formally, given an answer evidence a, QPP aims to predict a sequence of question phrases s = (s 1 , ..., s |s| )(e.g., \"What treatment\" (s 1 ) \u2192 \"How often\" (s 2 ) \u2192 \"What dosage\" (s 3 ), with |s| = 3).",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 142,
                    "text": "(Luong et al., 2015)",
                    "ref_id": "BIBREF47"
                }
            ],
            "ref_spans": [],
            "section": "Question Phrase Prediction (QPP)"
        },
        {
            "text": "During training, we assume that the set of question phrases is arranged in a pre-defined order. Such orderings can be obtained with some heuristic methods, e.g., using a descending order based on question phrase frequency in the corpus 8 (more details are in Appendix A.1.2).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Question Phrase Prediction (QPP)"
        },
        {
            "text": "As such, we aim to minimize: 8 In emrQA, each answer evidence is tied with multiple questions, which allows the training for QPP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Question Phrase Prediction (QPP)"
        },
        {
            "text": "where s, a, \u03b8 denote question phrase sequence, input answer evidence and all the parameters in QPP, respectively. Algorithm 1 illustrates the pretraining and training procedure of our framework when equipped with our proposed QPP module.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Question Phrase Prediction (QPP)"
        },
        {
            "text": "In the inference stage, QPP can dynamically decide the number of question phrases for each answer evidence by predicting a special [STOP] type. By decomposing QG into two steps (diversification followed by generation), the proposed QPP can increase the diversity in a more controllable way compared with decoding-based approach.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Question Phrase Prediction (QPP)"
        },
        {
            "text": "Base QG and QA models: In our experiments, we adopt three base QG models: NQG (Du et al., 2017) , NQG++ (Zhou et al., 2017) and BERT-SQG (Chan and Fan, 2019) . For QA, we use two base models, DocReader (Chen et al., 2017) and ClinicalBERT (Alsentzer et al., 2019 Evaluation Metrics: For QG evaluation, we focus on evaluating both relevance and diversity. Following previous work (Du et al., 2017; Zhang et al., 2018) , we use BLEU (Papineni et al., 2002) , ROUGE-L (Lin, 2004) as well as METEOR (Lavie and Denkowski, 2009 ) for relevance evaluation. Since the Beam Search and our QPP module enable QG models to generate multiple questions given an evidence, we report the top-1 relevance among the generated questions following Cho et al. (2019) . For diversity, we report Distinct (Li et al., 2016) as well as Entropy (Zhang et al., 2018) scores. We calculate BLEU and the diversity measures based on 3and 4-grams.",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 95,
                    "text": "(Du et al., 2017)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 104,
                    "end": 123,
                    "text": "(Zhou et al., 2017)",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 137,
                    "end": 157,
                    "text": "(Chan and Fan, 2019)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 202,
                    "end": 221,
                    "text": "(Chen et al., 2017)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 239,
                    "end": 262,
                    "text": "(Alsentzer et al., 2019",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 379,
                    "end": 396,
                    "text": "(Du et al., 2017;",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 397,
                    "end": 416,
                    "text": "Zhang et al., 2018)",
                    "ref_id": "BIBREF87"
                },
                {
                    "start": 431,
                    "end": 454,
                    "text": "(Papineni et al., 2002)",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 465,
                    "end": 476,
                    "text": "(Lin, 2004)",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 495,
                    "end": 521,
                    "text": "(Lavie and Denkowski, 2009",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 728,
                    "end": 745,
                    "text": "Cho et al. (2019)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 782,
                    "end": 799,
                    "text": "(Li et al., 2016)",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 819,
                    "end": 839,
                    "text": "(Zhang et al., 2018)",
                    "ref_id": "BIBREF87"
                }
            ],
            "ref_spans": [],
            "section": "Experiment Setup"
        },
        {
            "text": "For QA evaluation, we report exact match (EM) (the percentage of predictions that match the ground truth answers exactly) and F1 (the average overlap between the predictions and ground truth answers) as in Rajpurkar et al. (2016) . Analysis on QG diversity: Figure 2 .2 shows the distribution over types of questions generated by NQG-based models (i.e., base model, base + beam search and base + QPP) and",
            "cite_spans": [
                {
                    "start": 206,
                    "end": 229,
                    "text": "Rajpurkar et al. (2016)",
                    "ref_id": "BIBREF64"
                }
            ],
            "ref_spans": [
                {
                    "start": 258,
                    "end": 266,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Experiment Setup"
        },
        {
            "text": "the ground truth on emrQA dataset. We observe that the Kullback-Leibler (KL) divergence between the distributions of generated questions and the ground truth is smaller after enabling diversity booster. The gap reaches the minimum when our QPP module is plugged in. It's worth noting that even some of the least frequent types of questions (e.g., \"How\", \"Why\")",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "can be generated when our QPP module is turned on. These observations demonstrate diversity booster, especially our QPP module, can help generate diverse questions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "In Figure 2 .3, we first present a QA example and a QG example from MIMIC-III.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Qualitative Analysis: Error Analysis"
        },
        {
            "text": "In the QA example, this \"why\" question can be correctly answered by the QA model (DocReader) trained on the \"NQG+QPP\" generated corpus while the QA models trained on other generated corpora fail. This is because the NQG model and \"NQG+BeamSearch\" cannot generate any \"why\" questions as shown in Figure 2 Chapter 3: Natural Language Inference",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 295,
                    "end": 303,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Qualitative Analysis: Error Analysis"
        },
        {
            "text": "Natural language inference (NLI) 9 is the problem of determining whether a natural language hypothesis h can be inferred (or entailed) from a natural language premise p (i.a., Dagan et al., 2005; MacCartney and Manning, 2009 ). Conventionally, people only examine items that are suitable for systematic inferences (i.e., items for which people consistently agree on the NLI label).",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 195,
                    "text": "Dagan et al., 2005;",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 196,
                    "end": 224,
                    "text": "MacCartney and Manning, 2009",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "However, Pavlick and Kwiatkowski (2019) observed inherent disagreements among annotators in several NLI datasets (e.g., SNLI (Bowman et al., 2015) ), which cannot be smoothed out by hiring more people. They pointed out that to achieve robust NLU, we need to be able to tease apart systematic inferences (i.e., items for which most people agree on the annotations) from items inherently leading to disagreement. The last example in Table 3 .1 is a typical disagreement item: some annotators consider it to be an entailment (3 or 2), while others view it as a contradiction (-3). Clearly, the annotators have two different interpretations on the complement clause \"If she'd said Carolyn had borrowed a book from",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 146,
                    "text": "(Bowman et al., 2015)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 431,
                    "end": 438,
                    "text": "Table 3",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "Clare and wanted to return it\". Moreover, a common practice in the literature to generate an inference label from annotations is to take the average (i.a., Pavlick and Callison-Burch, 2016) . In this case, it would be \"Neutral\", but such label is not accurately capturing the 9 In this thesis, we use \"textual entailment\" and \"Natural Language Inference\" or \"NLI\" interchangeably. distribution. Alternatively, some work simply ignored the \"Disagreement\" portion but only suggested for sentiment analysis, we propose a finer-grained labeling for NLI: teasing disagreement items, labeled \"Disagreement\", from systematic inferences, which can be \"Contradiction\", \"Neutral\" or \"Entailment\". As such, in order to achieve robust NLU in NLI task, the developed models should be able to identify inherent disagreement items when possible and carry out systematic inferences on non-disagreement items.",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 189,
                    "text": "Pavlick and Callison-Burch, 2016)",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 276,
                    "end": 277,
                    "text": "9",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To this end, we propose Artificial Annotators (AAs), an ensemble of BERT models (Devlin et al., 2019a) , which simulate the uncertainty in the annotation process by capturing modes in annotations. That is, we expect to utilize simulated modes of annotations to enhance finer-grained NLI label prediction. Our results, on the CommitmentBank, show that AAs perform statistically significantly better than all baselines (including BERT baselines) by a large margin in terms of both F1 and accuracy. We also show that AAs manage to learn linguistic patterns and context-dependent reasoning.",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 102,
                    "text": "(Devlin et al., 2019a)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We start with the introduction to the dataset used in this chapter, CommitmentBank Contradiction. The gold label for example 3 in Table 3 .1 would thus be \"Disagreement\".",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 130,
                    "end": 137,
                    "text": "Table 3",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Inherently Ambiguous Items in CB"
        },
        {
            "text": "However, this seems a bit too stringent, given that 70% of the annotators all agree on the 0 label and there is only one annotation towards the extreme. Likewise, for example 5, most annotators chose a negative score and the item might therefore be better labeled as \"Contradiction\" rather than \"Disagreement\". To decide on the finer-grained NLI labels, we therefore also took variance and mean into account, as follows: 10",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inherently Ambiguous Items in CB"
        },
        {
            "text": "\u2022 Entailment: 80% of annotations fall in the range [1,3] OR the annotation variance \u2264 1 and the annotation mean > 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inherently Ambiguous Items in CB"
        },
        {
            "text": "\u2022 Neutral: 80% of annotations is 0 OR the annotation variance \u2264 1 and the absolute mean of annotations is bound within 0.5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inherently Ambiguous Items in CB"
        },
        {
            "text": "\u2022 Contradiction: 80% of annotations fall in the range [-3, -1] OR the annotation variance \u2264 1 and the annotation mean < -1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inherently Ambiguous Items in CB"
        },
        {
            "text": "\u2022 Disagreement: Items which do not fall in any of the three categories above.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inherently Ambiguous Items in CB"
        },
        {
            "text": "We randomly split CB into train/dev/test sets in a 7:1:2 ratio. 11 Table 3 .2 gives splits' basic statistics.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 67,
                    "end": 74,
                    "text": "Table 3",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Inherently Ambiguous Items in CB"
        },
        {
            "text": "Our developed linguistic rules are inspired by and adapted from Jiang and de Marneffe (2019a) to explicitly include the most discriminating expressions for disagreement items. We When this policy is executed, there are two additional auxiliary rules: Items not falling in any group above are assigned a disagreement label as it is the dominant class in CB; For items satisfying more than one rule, the label will be determined by the higher-ranked rule (a smaller number indicates a higher rank). Note that the rules above also reveal the most discriminating expressions for each class.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Linguistic Rules"
        },
        {
            "text": "We aim at finding an effective way to tease items leading to systematic inferences apart from items leading to disagreement. As pointed out by Calma and Sick (2017) , annotated labels are subject to uncertainty. Annotations are indeed influenced by several factors:",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 164,
                    "text": "Calma and Sick (2017)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Artificial Annotators"
        },
        {
            "text": "workers' past experience and concentration level, cognition complexities of items, etc. They proposed to simulate the annotation process in an active learning paradigm to make use of the annotations that contribute to uncertainty. Likewise, for NLI, Gantt et al. (2020) observed that directly training on raw annotations using annotator identifier improves performance.",
            "cite_spans": [
                {
                    "start": 250,
                    "end": 269,
                    "text": "Gantt et al. (2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Artificial Annotators"
        },
        {
            "text": "Essentially, Gantt et al. (2020) used a mixed-effect model to learn a mapping from an item and the associated annotator identifier to a NLI label. However, annotator identifiers are not always accessible, especially in many datasets that have been there for a while. Thus, we decide to simulate the annotation process instead of learning from real identifiers.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 32,
                    "text": "Gantt et al. (2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Artificial Annotators"
        },
        {
            "text": "As shown by Pavlick and Kwiatkowski (2019) , if annotations of an item follow unimodal distributions, then it is suitable to use aggregation (i.e., take an average) to obtain a inference label; but such an aggregation is not appropriate when annotations follow multi-modal distributions. Without loss of generality, we assume that items are associated with n-modal distributions, where n \u2265 1. Usually, systematic inference items are tied to unimodal annotations while disagreement items are tied to multi-modal annotations. We, thus, introduce the notion of Artificial Annotators (AAs), where each individual \"annotator\" learns to model one mode.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 42,
                    "text": "Pavlick and Kwiatkowski (2019)",
                    "ref_id": "BIBREF56"
                }
            ],
            "ref_spans": [],
            "section": "Artificial Annotators"
        },
        {
            "text": "AAs is an ensemble of n BERT models (Devlin et al., 2019a) with a primary goal of finer-grained NLI label prediction. n is determined to be 3 as there are up to 3 relationships between premise and hypothesis, excluding the disagreement class. Within AAs, each BERT is trained for an auxiliary systematic inference task which is to predict entailment/neutral/contradiction based on a respective subset of annotations. The subsets of annotations for the three BERT are mutually exclusive.",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 58,
                    "text": "(Devlin et al., 2019a)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Architecture"
        },
        {
            "text": "Neutral -biased A high-level overview of AAs is shown in Figure 3 .1. Intuitively, each BERT separately predicts a systematic inference label, each of which represents a mode 12 of the annotations.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 57,
                    "end": 65,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "Contradiction -biased"
        },
        {
            "text": "The representations of these three labels are further aggregated as augmented information to enhance final fine-grained NLI label prediction (see Eq. 3.1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MLP PREMISE [SEP] HYPOTHESIS"
        },
        {
            "text": "If we view the AAs as a committee of three members, our architecture is reminiscent of the Query by Committee (QBC) (Seung et al., 1992) , an effective approach for active learning paradigm. The essence of QBC is to select unlabeled data for labeling on which disagreement among committee members (i.e., learners pre-trained on the same labeled data) occurs. The selected data will be labeled by an oracle (e.g., domain experts) and then used to further train the learners. Likewise, in our approach, each AA votes for an item independently. However, the purpose is to detect disagreements instead of using disagreements as a measure to select items for further annotations. Moreover, in our AAs, the three members are trained on three disjoint annotation partitions for each item (see Section 3.4.2).",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 136,
                    "text": "(Seung et al., 1992)",
                    "ref_id": "BIBREF70"
                }
            ],
            "ref_spans": [],
            "section": "MLP PREMISE [SEP] HYPOTHESIS"
        },
        {
            "text": "12 It's possible that three modes collapse to (almost) a point.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MLP PREMISE [SEP] HYPOTHESIS"
        },
        {
            "text": "We first sort the annotations in descending order for each item and divide them into three partitions. 13 For each partition, we generate an auxiliary label derived from the annotation mean. If the mean is greater/smaller than +0.5/-0.5, then it's entailment/contradiction; otherwise, it's neutral. The first BERT model is always enforced to predict the auxiliary label of the first partition to simulate an entailment-biased annotator. Likewise, the second and third BERT models are trained to simulate neutral-biased and contradiction-biased annotators.",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 105,
                    "text": "13",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Training"
        },
        {
            "text": "Each BERT produces a pooled representation for the [CLS] token. The three representations are passed through a multi-layer perceptron (MLP) to obtain the finer-grained NLI label:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training"
        },
        {
            "text": "with [e; n; c] being the concatenation of three learned representations out of entailmentbiased, neutral-biased and contradiction-biased BERT models. W s and W t are parameters to be learned.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training"
        },
        {
            "text": "The overall loss is defined as the weighted sums of four cross-entropy losses:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training"
        },
        {
            "text": "where r \u2208 [0, 1] controls the primary finer-grained NLI label prediction task loss ratio.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training"
        },
        {
            "text": "Evaluation Setting: We include five baselines to compare with:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation and Results"
        },
        {
            "text": "\u2022 \"Always 0\": Always predict Disagreement. 13 For example, if there are 8 annotations for a given item, the annotations are divided into partitions of size 3, 2 and 3. \u2022 CBOW (Continuous Bags of Words): Each item is represented as the average of its tokens' GLOVE vectors (Pennington et al., 2014) .",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 45,
                    "text": "13",
                    "ref_id": null
                },
                {
                    "start": 272,
                    "end": 297,
                    "text": "(Pennington et al., 2014)",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation and Results"
        },
        {
            "text": "\u2022 Heuristic baseline: Linguistics-driven rules (detailed out in chapter 3.3), adapted from Jiang and de Marneffe (2019a); e.g., conditional environment discriminates for disagreement items.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation and Results"
        },
        {
            "text": "\u2022 Vanilla BERT: (Devlin et al., 2019a) Straightforwardly predict among 4 finer-grained NLI labels.",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 38,
                    "text": "(Devlin et al., 2019a)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation and Results"
        },
        {
            "text": "\u2022 Joint BERT: Two BERT models are jointly trained, each of which has a different speciality. The first one (2-way) identifies whether a sentence pair is a disagreement item. If not, this item is fed into the second BERT (3-way) which carries out systematic inference.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation and Results"
        },
        {
            "text": "For all baselines involving BERT, we follow the standard practice of concatenating the premise and the hypothesis with [SEP].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation and Results"
        },
        {
            "text": "Results: Table 3 .3 gives the accuracy and F1 for each baseline and AAs, on the CB dev and test sets. We run each model 10 times, and report the average. Also, Our AAs achieve the lowest standard deviations on test set items compared to BERT-based models, indicating that it is more stable and potentially more robust to wild environments.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 9,
                    "end": 16,
                    "text": "Table 3",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Evaluation and Results"
        },
        {
            "text": "CBOW is essentially the same as the \"Always 0\" baseline as it keeps predicting Dis- Table 3 .5 shows that the majority (\u223c60%) of errors come from wrongly predicting a systematic inference item as a disagreement item.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 84,
                    "end": 91,
                    "text": "Table 3",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Empirical Results Analysis"
        },
        {
            "text": "In 91% of such errors, AAs predict that there is more than one mode for the annotation (i.e., the three labels predicted by individual \"annotators\" in AAs are not unanimous), as in example 5 in Table 3 .4. AAs are thus predicting more modes than necessary when the annotation is actually following a uni-modal distribution. On the contrary, when the item is supposed to be a disagreement item but is missed by AAs (as in example 6 and 7 in Table 3 .4), AAs mistakenly predict that there is only one mode in the annotations 78% of the time. It thus seems that a method which captures accurately the number of modes in the annotation distribution would lead to a better model. 2 Premise: 'She was about to tell him that was his own stupid fault and that she wasn't here to wait on him -particularly since he had proved to be so inhospitable. But she bit back the words. Perhaps if she made herself useful he might decide she could stayfor a while at least just until she got something else sorted out. Table 3 .6: F1 for CB test set under the embedding environments and \"I don't know/believe/think\" (\"negR\").",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 194,
                    "end": 201,
                    "text": "Table 3",
                    "ref_id": "TABREF12"
                },
                {
                    "start": 440,
                    "end": 447,
                    "text": "Table 3",
                    "ref_id": "TABREF12"
                },
                {
                    "start": 1000,
                    "end": 1007,
                    "text": "Table 3",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Empirical Results Analysis"
        },
        {
            "text": "We also examine the model performance for different linguistic constructions to investigate whether the model learns some of the linguistic patterns present in the Heuristic baseline. The Heuristic rules are strongly tied to the embedding environments. Another construction used is one which can lead to \"neg-raising\" reading, where a negation in the matrix clause is interpreted as negating the content of the complement, as in example 3 (Table 3 .4) where I do not think they have seen a really high improvement is interpreted as I think they did not see a really high improvement. \"Neg-raising\" readings often occur with know, believe or think in the first person under negation. There are 85 such items in the test set: 41 contradictions (thus neg-raising items), 39 disagreements and 5 entailments. Context determines whether a neg-raising inference is triggered (An and White, 2019). ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 439,
                    "end": 447,
                    "text": "(Table 3",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Linguistic Construction Analysis"
        },
        {
            "text": "In this chapter, we introduced finer-grained natural language inference. This task aims at teasing systematic inferences from inherent disagreements. The inherent disagreement items are challenging for NLU models to handle, rarely studied in past NLI work. We show that our proposed AAs, which simulate the uncertainty in annotation process by capturing the modes in annotations, perform statistically significantly better than all baselines. However the performance obtained (\u223c66%) is still far from achieving truly robust NLU, leaving room for improvement.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Chapter 4: FAQ Retrieval",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "FAQ, short for frequently asked questions, is designed for the purpose of providing information on frequent questions or concerns. The FAQ retrieval task is defined as ranking FAQ items {(q i , a i )} from an FAQ Bank given a user query Q. In the FAQ retrieval literature (Karan and\u0160najder, 2016 (Karan and\u0160najder, , 2018 Sakata et al., 2019) , a user query Q can be learned to match with the question field q i , the answer field a i or their concatenation (i.e., FAQ tuple) q i + a i .",
            "cite_spans": [
                {
                    "start": 272,
                    "end": 295,
                    "text": "(Karan and\u0160najder, 2016",
                    "ref_id": null
                },
                {
                    "start": 296,
                    "end": 321,
                    "text": "(Karan and\u0160najder, , 2018",
                    "ref_id": null
                },
                {
                    "start": 322,
                    "end": 342,
                    "text": "Sakata et al., 2019)",
                    "ref_id": "BIBREF69"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To advance the COVID-19 information search, we present an FAQ dataset, COUGH, consisting of FAQ Bank, Query Bank, and Relevance Set, following the standard of constructing a robust FAQ dataset (Manning et al., 2008) . The FAQ Bank contains 15919 FAQ items scraped from 55 authoritative institutional websites. COUGH covers a wide range of perspectives on COVID-19, spanning from general information about the virus to specific COVID-19-related instructions for a healthy diet. For evaluation, we further construct Query Bank and Relevance Set, including 1201 crowd-sourced queries and their relevance to a set of FAQ items judged by annotators. Examples from COUGH are shown in Figure 4 .1.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 215,
                    "text": "(Manning et al., 2008)",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [
                {
                    "start": 678,
                    "end": 686,
                    "text": "Figure 4",
                    "ref_id": "FIGREF10"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "Our dataset poses several new challenges (e.g., the answers being long and noisy, and hard to match due to larger search space) to existing FAQ retrieval models. The diversity of FAQ items, which is reflected in their varying query forms and lengths as well as in narrative styles, also contributes to these challenges. Furthermore, these challenges can reflect the characteristics and difficulties of FAQ retrieval in real scenarios better than counterparts like FAQIR (Karan and\u0160najder, 2016) and StackFAQ (Karan and\u0160najder, 2018) (Table 4 .1). Moreover, in contrast to all prior datasets, COUGH covers multiple query forms (e.g., question and query string forms) and has many annotated FAQs for each user query, whereas queries in existing FAQ datasets are limited to the question form and have much fewer annotations. As such, our COUGH is deemed as a robust dataset, upon which a robust FAQ retriever could be developed to handle some real challenges (e.g., lengthy answer, enormous search space) better.",
            "cite_spans": [
                {
                    "start": 470,
                    "end": 494,
                    "text": "(Karan and\u0160najder, 2016)",
                    "ref_id": null
                },
                {
                    "start": 508,
                    "end": 532,
                    "text": "(Karan and\u0160najder, 2018)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 533,
                    "end": 541,
                    "text": "(Table 4",
                    "ref_id": "TABREF22"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "The contribution in this chapter is two-fold. First, we construct a challenging dataset COUGH to aid the development of COVID-19 FAQ retrieval models. Second, we conduct extensive experiments using various SOTA models across different settings, explore limitations of current FAQ retrieval models, and discuss future work along this line.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Since the outbreak of COVID-19, the community has witnessed many datasets released to advance the research of COVID-19.The most related work to ours are and Poliak et al. (2020) , both of which constructed a collection of COVID-19 FAQs ). **: Not Applicable, either not in English or not publicly available.",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 177,
                    "text": "Poliak et al. (2020)",
                    "ref_id": "BIBREF59"
                }
            ],
            "ref_spans": [],
            "section": "Standard FAQ Dataset Construction: COUGH"
        },
        {
            "text": "by scraping authoritative websites (e.g., CDC and WHO). However, the dataset in the former work is not available yet and the latter work does not evaluate models on their dataset, and there is still a great need to understand how existing models would perform on the COVID-19 FAQ retrieval task. Moreover, the numbers of FAQs 14 in the 5 existing FAQ datasets (Table 4 .1) are generally lower than 2000, which renders a small search space and thus the ease for FAQ retrievers to find the most relevant FAQ given a query.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 360,
                    "end": 368,
                    "text": "(Table 4",
                    "ref_id": "TABREF22"
                }
            ],
            "section": "Standard FAQ Dataset Construction: COUGH"
        },
        {
            "text": "A typical research-oriented FAQ dataset (Manning et al., 2008) consists of three parts:",
            "cite_spans": [
                {
                    "start": 40,
                    "end": 62,
                    "text": "(Manning et al., 2008)",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "Standard FAQ Dataset Construction: COUGH"
        },
        {
            "text": "FAQ Bank, User Query Bank and Annotated Relevance Set. In this section, we will describe how we construct each of the three in detail.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standard FAQ Dataset Construction: COUGH"
        },
        {
            "text": "We developed scrapers based on JHU-COVID-QA library Poliak et al. (2020) with modifications to enable special features for our COUGH dataset.",
            "cite_spans": [
                {
                    "start": 52,
                    "end": 72,
                    "text": "Poliak et al. (2020)",
                    "ref_id": "BIBREF59"
                }
            ],
            "ref_spans": [],
            "section": "FAQ Bank Construction"
        },
        {
            "text": "Web scraping: We collect FAQ items from authoritative international organizations, state governments and some other credible websites including reliable encyclopedias and medical forums. Moreover, we scrape three types of FAQs: question form (i.e., an interrogative statement), query string (i.e., a string of words to elicit information) form and forum form (FAQs scrapped from medical forums). Inspired by Manning et al. (2008) , we loosen the constraint that queries must be in question form since we want to study a more generic and challenging problem. We also scrape 6,768 non-English FAQs to increase language diversity.",
            "cite_spans": [
                {
                    "start": 408,
                    "end": 429,
                    "text": "Manning et al. (2008)",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "FAQ Bank Construction"
        },
        {
            "text": "Overall, we scraped a total of 15,919 FAQ items covering all three types and 19 languages.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "FAQ Bank Construction"
        },
        {
            "text": "All FAQ items were collected and finalized on Aug. 30 th , 2020.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "FAQ Bank Construction"
        },
        {
            "text": "Following Karan and\u0160najder (2016); Manning et al. (2008) , we do not crowdsource queries from scratch, but instead ask annotators to paraphrase our provided query templates (See phase 1 below for details). In this way, we can ensure that 1) the collected queries are pertinent to COVID-19; 2) the collected queries are not too simple; 3) the chance of getting (nearly) duplicate user queries is reduced.",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 56,
                    "text": "Manning et al. (2008)",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "User Query Bank Construction"
        },
        {
            "text": "Phase 1: Query Template Creation: We sample 5% of FAQ items from each English non-forum source and use the question part as the template.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "User Query Bank Construction"
        },
        {
            "text": "Phase 2: Paraphrasing for Queries: In this phase, each annotator is expected to give three paraphrases for each query template. Annotators are encouraged to give deep paraphrases (i.e., grammatically different but semantically similar/same) to simulate the noisy and diverse environment in real scenarios. In the end, we obtain 1236 user queries. contributes its top-10 relevant FAQ items. We then take the union to remove duplicates,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "User Query Bank Construction"
        },
        {
            "text": "giving an average pool size of 32.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Annotated Relevance Set Construction"
        },
        {
            "text": "Phase 2: Human Annotation: Each annotator gives each Query, FAQ item tuple a score based on the annotation scheme (i.e., Matched (4), Useful (3), Useless (2) and Non-relevant",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Annotated Relevance Set Construction"
        },
        {
            "text": "(1)) which is adapted from Karan and\u0160najder (2016); Sakata et al. (2019) . In order to reduce the variance and bias in annotation, each tuple has at least 3 annotation scores. In our finalized Annotated Relevance Set, we keep all raw scores and include two additional labels: 1) mean of raw annotation scores; 2) binary label (positive/negative). We identify all tuples with mean score greater than 3 as positive examples.",
            "cite_spans": [
                {
                    "start": 52,
                    "end": 72,
                    "text": "Sakata et al. (2019)",
                    "ref_id": "BIBREF69"
                }
            ],
            "ref_spans": [],
            "section": "Annotated Relevance Set Construction"
        },
        {
            "text": "Among 1236 user queries, we find that there are 35 \"unanswerable\" queries that have no associated positive FAQ item. In the end, there are 1201 user queries involved for evaluation after removing \"unanswerable\" queries.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Annotated Relevance Set Construction"
        },
        {
            "text": "Besides the generic goal of large size, diversity, and low noise, COUGH features 4 additional aspects:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COUGH Dataset Analysis"
        },
        {
            "text": "Varying Query Forms: As indicated in Table 4 .2, there are multiple query forms. In evaluation, we include both question and query string forms. These two distinct forms are different in terms of query format (interrogative vs. declarative), average answer length (123.89 vs. 89.60) and topics. Question form is usually related to general information about the virus while query string form is often searching for more specific instructions concerning COVID-19 (e.g., healthy diet during pandemic). In Figure 4 .1, the first FAQ item is in question form while the second one is in query string form.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 37,
                    "end": 44,
                    "text": "Table 4",
                    "ref_id": "TABREF22"
                },
                {
                    "start": 502,
                    "end": 510,
                    "text": "Figure 4",
                    "ref_id": "FIGREF10"
                }
            ],
            "section": "COUGH Dataset Analysis"
        },
        {
            "text": "Answer Nature: Table 4 .1 shows the answer fields in COUGH are much longer than those in any prior dataset. We also observe that answers might contain some contents which are not directly pertinent to the query, partially resulting in the long length nature. For example, in COUGH, the answer to a query \"What is novel coronavirus\" contains extra information about comparisons with other viruses. Such lengthy and noisy nature of answers shows the difficulty of FAQ retrieval in real scenarios.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 15,
                    "end": 22,
                    "text": "Table 4",
                    "ref_id": "TABREF22"
                }
            ],
            "section": "COUGH Dataset Analysis"
        },
        {
            "text": "Large-scale Relevance Annotation: Many existing FAQ datasets overlooked the scale of annotations ( items for other potential tasks, such as multi-lingual FAQ retrieval and transfer learning from English FAQ items to low-resource non-English FAQ items. Table 4 .2.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 252,
                    "end": 259,
                    "text": "Table 4",
                    "ref_id": "TABREF22"
                }
            ],
            "section": "COUGH Dataset Analysis"
        },
        {
            "text": "The standard practice in FAQ retrieval focuses on retrieving the most-matched FAQ items given a user query (Karan and\u0160najder, 2018) . Many earlier work, such as FAQ FINDER (Burke et al., 1997) , query expansion (Kim and Seo, 2006) and BM25 (Robertson and Zaragoza, 2009 ), resorted to traditional IR techniques by leveraging lexical mapping and/or semantic similarity. In the deep learning era, many studies show that Neural Networks are useful for FAQ retrieval as they are good at learning the semantic relevance between queries and FAQ items. Along this line, (Karan and\u0160najder, 2016) adopted Convolution Neural Networks, (Gupta and Carvalho, 2019) utilized LSTM, and (Sakata et al., 2019) leveraged an ensemble of TSUBAKI (Shinzato et al., 2012) and BERT (Devlin et al., 2019b) .",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 131,
                    "text": "(Karan and\u0160najder, 2018)",
                    "ref_id": null
                },
                {
                    "start": 172,
                    "end": 192,
                    "text": "(Burke et al., 1997)",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 211,
                    "end": 230,
                    "text": "(Kim and Seo, 2006)",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 240,
                    "end": 269,
                    "text": "(Robertson and Zaragoza, 2009",
                    "ref_id": "BIBREF68"
                },
                {
                    "start": 563,
                    "end": 587,
                    "text": "(Karan and\u0160najder, 2016)",
                    "ref_id": null
                },
                {
                    "start": 625,
                    "end": 635,
                    "text": "(Gupta and",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 636,
                    "end": 670,
                    "text": "Carvalho, 2019) utilized LSTM, and",
                    "ref_id": null
                },
                {
                    "start": 671,
                    "end": 692,
                    "text": "(Sakata et al., 2019)",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 726,
                    "end": 749,
                    "text": "(Shinzato et al., 2012)",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 759,
                    "end": 781,
                    "text": "(Devlin et al., 2019b)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "FAQ Retrieval Methods Overview"
        },
        {
            "text": "Recently, Mass et al. (2020) employed CombSum and PoolRank, ensembles of BM25 and BERT models, to learn ranking without requiring manual annotations.",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 28,
                    "text": "Mass et al. (2020)",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "FAQ Retrieval Methods Overview"
        },
        {
            "text": "In this chapter, we only focus on the unsupervised models since the size of User Query",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Unsupervised FAQ Retrieval"
        },
        {
            "text": "Bank (1201 items) is not large enough for supervised learning, especially for fine-tuning complex language models like BERT. We experiment with three commonly-used and SOTA unsupervised models to understand their limitations and figure out the challenge present in real scenarios for FAQ retrieval. Besides, each model has three configurable modes, Q-q, Q-a and Q-q+a, where we match user queries (Q) to the question (q) and answer (a) of an FAQ item as well as their concatenation (q+a) 17 , respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Unsupervised FAQ Retrieval"
        },
        {
            "text": "(1) BM25 (Robertson and Zaragoza, 2009 ), a commonly adopted IR baseline, is a nonlinear combination of term frequency, document frequency and document length.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 38,
                    "text": "(Robertson and Zaragoza, 2009",
                    "ref_id": "BIBREF68"
                }
            ],
            "ref_spans": [],
            "section": "Baseline Models"
        },
        {
            "text": "(2) BERT (Devlin et al., 2019b) is a pretrained language model. We experiment with Sentence-BERT (Reimers and Gurevych, 2019), a Siamese network built for comparison between sentence-pair embeddings, which specializes in generating meaningful sentence representations.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 31,
                    "text": "(Devlin et al., 2019b)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Baseline Models"
        },
        {
            "text": "17 Q-q+a mode is only used for BM25 and BM25 in CombSum.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baseline Models"
        },
        {
            "text": "Fine-tuning: We use Multiple Negatives Ranking (MNR) loss 18 (Henderson et al., 2017) to fine-tune Sentence-BERT on FAQ bank. For the Q-q mode, similar to Mass et al. (2020) , we use GPT2 (Radford et al., 2019) to generate synthetic questions as positive q's to match with Q and filter out low-quality ones via Elasticsearch. For the Q-a mode, an FAQ item itself is a positive pair. For both modes, negative q's or a's are randomly sampled.",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 85,
                    "text": "(Henderson et al., 2017)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 155,
                    "end": 173,
                    "text": "Mass et al. (2020)",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 188,
                    "end": 210,
                    "text": "(Radford et al., 2019)",
                    "ref_id": "BIBREF61"
                }
            ],
            "ref_spans": [],
            "section": "Baseline Models"
        },
        {
            "text": "(3) CombSum (Mass et al., 2020) first computes three matching scores between the user query and FAQ items via BM25 (Q-q+a), BERT (Q-q) and BERT (Q-a) models, respectively.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 31,
                    "text": "(Mass et al., 2020)",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "Baseline Models"
        },
        {
            "text": "Then, the three scores are normalized and combined by averaging.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baseline Models"
        },
        {
            "text": "Evaluation Metric: We adopt our binary label (positive/negative) as ground truth labels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation and Results"
        },
        {
            "text": "Following previous work (Karan and\u0160najder, 2016 (Karan and\u0160najder, , 2018 Sakata et al., These results not only confirm that COUGH is challenging but also signify more robust methods and models are needed to handle challenges imposed by COUGH more effectively.",
            "cite_spans": [
                {
                    "start": 24,
                    "end": 47,
                    "text": "(Karan and\u0160najder, 2016",
                    "ref_id": null
                },
                {
                    "start": 48,
                    "end": 73,
                    "text": "(Karan and\u0160najder, , 2018",
                    "ref_id": null
                },
                {
                    "start": 74,
                    "end": 88,
                    "text": "Sakata et al.,",
                    "ref_id": "BIBREF69"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation and Results"
        },
        {
            "text": "Quantitative Analysis: It is not surprising to see that the Q-q mode consistently performs better than the Q-a mode regardless of underlying models. This is mainly caused by the fact that question fields are more similar to user queries than answer fields, in terms of syntactic structures and semantic meanings. As discussed in Section 4.3, the answer nature (lengthy and noisy) and large search space, albeit well characterize the FAQ retrieval task in real scenarios, do bring a great challenge to current FAQ retrieval models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis"
        },
        {
            "text": "We observe that fine-tuning in the way we experimented with can only help improve the performance of the Q-a mode by a small margin, but might slightly hurt the Q-q mode due to the noise introduced in generating synthetic queries. Moreover, ensemble models don't perform as well as expected, since the particular Q-a model involved is weak (even after fine-tuning), which negatively impacts performance. In consequence, doing straightforward fine-tuning or ensemble simply by stacking models wouldn't improve the performance significantly, which confirms that COUGH is a challenging dataset. Interesting future work includes developing more advanced techniques to handle long and noisy answer fields.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis"
        },
        {
            "text": "Qualitative Analysis: To understand finetuned BERT (Q-q) better, we conduct error analysis as shown in Table 4 .4 to show its major types of errors, hoping to further improve it in the future. Currently, finetuned BERT (Q-q) suffers from the following issues: 1) biased towards responses with similar texts (e.g., \"antibody tests\" and \"antibody testing\");",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 103,
                    "end": 110,
                    "text": "Table 4",
                    "ref_id": "TABREF22"
                }
            ],
            "section": "Analysis"
        },
        {
            "text": "2) fails to capture the semantic similarities under complex environments (e.g., pragmatic reasoning is required to understand that \"limited ability\" indicates results are not accurate for diagnosing COVID-19).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis"
        },
        {
            "text": "In this chapter, we introduce COUGH, a large challenging dataset for COVID-19 FAQ retrieval. COUGH features varying query forms, long and noisy answers, larger search space and multilinguality. COUGH also serves as a better evaluation benchmark since it has largescale relevance annotations. Albeit results show the limitations of current FAQ retrieval models, COUGH is a more robust dataset than its counterparts since it better characterizes the challenges present in real scenarios for FAQ retrieval.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "In this thesis, I have embarked on building models and constructing datasets towards more robust natural language understanding. We start with a discussion on what robustness problem is in natural language understanding. That is, fully-trained NLU models are usually lacking generalizability and flexibility. In this thesis, we argue that, in order to achieve truly robust natural language understanding, implementing robust models and curating robust datasets are equally important. In this thesis, we investigate the NLU robustness problem in three NLU tasks (i.e., Question Answering, Natural Language Inference and Information Retrieval). We then propose novel methods and construct new datasets to advance research on improving the robustness of NLU systems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "In Chapter 2, we study how to utilize diversity boosters (e.g., beam search & QPP) to help Question Generator synthesize diverse QA pairs, upon which a Question Answering (QA) system is trained to improve the generalization onto unseen target domain. It's worth mentioning that our proposed QPP (question phrase prediction) module, which predicts a set of valid question phrases given an answer evidence, plays an important role in improving the cross-domain generalizability for QA systems. Besides, a target-domain test set is constructed and approved by the community to help evaluate the model robustness under cross-domain generalization setting. In Chapter 3, we investigate inherently ambiguous items in the NLI (Natural Language Inference) task, which are overlooked in the literature but often occurring in the real world, for which annotators don't agree with the gold label.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "We build an ensemble model, AAs (Artificial Annotators), which simulates underlying annotation distribution to effectively identify such inherently ambiguous items. Our AAs, motivated by the nature of inherently ambiguous items, are better than vanilla models since our model design captures the essence of the problem better. In Chapter 4, we follow a standard practice to build a robust dataset for FAQ retrieval task. In our dataset analysis, we show how COUGH better reflects the challenge of FAQ retrieval in the real situation than its counterparts. The imposed challenge (e.g., long and noisy answer, large search space) will push forward the boundary of research on FAQ retrieval in real scenarios.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "Overall, the technical contributions of this thesis are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "1. We investigate the robustness problem in depth, and identify the equal importance of models implementation and datasets construction towards improving the robustness of NLU systems. In this thesis, we specifically study three concrete NLU tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "2. We propose two novel methods to help improve NLU model robustness. Specifically, we evaluate the effect of diverse question generation (QG) for clinical QA under the cross-domain evaluation setting, and propose QPP (Question Phrase Prediction) module as an effective diversity booster for QG . Moreover, we propose AAs (Artificial Annotators) to simulate underlying annotation distribution to handle a previously-overlooked NLI class better, inherent disagreement items (Zhang and de Marneffe, 2021) .",
            "cite_spans": [
                {
                    "start": 473,
                    "end": 502,
                    "text": "(Zhang and de Marneffe, 2021)",
                    "ref_id": "BIBREF85"
                }
            ],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "3. We construct two robust datasets, QA test set on MIMIC-III Database and COUGH . They will serve as better evaluation benchmarks to examine designed models' generalization capabilities and abilities to handle real-scenario challenges (e.g., longer FAQ and larger search space).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "Future Research: Moving forward, the ultimate goal for robust natural language understanding is to build NLU models which can behave humanly. That is, it's expected that robust NLU models are capable to transfer the knowledge from training corpus to unseen documents more reliably and survive when encountering challenging items even if the model doesn't know a priori of users' inputs. Two suggested important research frontiers are: 1) Improve model generalization under cross-domain setting: In Chapter 2, we discussed how we utilized QG model to help alleviate the generalization challenge encountered by QA systems. However, the question whether a better QA system could further improve the QG is yet known, which is, however, worth deeper investigation. Ideally, when introducing an auxiliary module to help the main model, we also expect to see that the auxiliary module could be benefited by the joint training with the main model. Besides, in Chapter 2, the reason we decided to utilize QG that way is that we observed that the QG system didn't suffer from severe generalization issues under the clinical setting. However, in open-domain, the aforementioned observation might not hold. In that case, it might be better to enforce the model to learn text representations that are invariant to domain changes. Recent work on cross-domain NER (Named Entity Recognition) have shown some progress along this path (Jia et al., 2019) . I also have a great interest in text generation. Though the majority of work that utilize domain adaptation techniques to tackle the generalization challenge focuses on classification tasks (Ganin et al., 2016; , could we effectively extend the success of domain adaptation to text generation? This might be a promising research direction since the text generation can be formulated as a sequence of classifications.",
            "cite_spans": [
                {
                    "start": 1417,
                    "end": 1435,
                    "text": "(Jia et al., 2019)",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 1628,
                    "end": 1648,
                    "text": "(Ganin et al., 2016;",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "2) Embrace more challenges in NLU: In Chapter 3 and 4, we discussed two datasets,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "CommitmentBank & COUGH, on which we could develop methods that target at solving NLU challenges under more realistic scenarios. SQuAD 2.0 (Rajpurkar et al., 2018 ) is a another great role model for datasets that aim at this goal. To do well on SQuAD 2.0, models must not only answer questions when possible, but also determine when no answer is supported by the paragraph and then say \"no\". This is a real challenge for QA system as it's not always the case that an answer could be found in a seemingly relevant document for a question. Another typical real challenge in NLU is how to solve mathematical problems. Hendrycks et al. (2021) presents a new math dataset on which a standard CS PhD student who doesn't especially like Math gets 40% accuracy while a fully-trained GPT-3 (Brown et al., 2020 ) models only gets 5%. Pretrained language models like GPT-3 or BERT is believed to heavily rely on the context to reason about the given prompt. However, mathematical language isn't necessarily constrained by contexts, 20 which imposes a great challenge to NLU systems. Additionally, in order to get full credits for a problem, the deployed system is also required to give correct reasoning steps, which is way more difficult than simply generating an answer. The following is an example from MATH dataset: 21 Problem: If \u03a3 \u221e n=0 cos 2n \u03b8 = 5, what is cos2\u03b8?",
            "cite_spans": [
                {
                    "start": 138,
                    "end": 161,
                    "text": "(Rajpurkar et al., 2018",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 614,
                    "end": 637,
                    "text": "Hendrycks et al. (2021)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 780,
                    "end": 799,
                    "text": "(Brown et al., 2020",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "Solution: The geometric series is 1 + cos 2 \u03b8 + cos 4 \u03b8 + ... = 1 1\u2212cos 2 \u03b8 = 5. Hence, cos 2 \u03b8 = 4 5 . Then, cos2\u03b8 = 2cos 2 \u03b8 \u2212 1 = 3 5 Moreover, linguistic rules or features, without any doubt, deserve more attention even if we are living in the realm of neural computing world. This is because linguistic rules or features exhibit great power when tackling challenging NLU problems. In Chapter 3, we find that SOTA NLU models, BERT, obtain inferior results to our linguistics-driven heuristic rules on dev set. This shows that giant neural models still fail to capture some necessary linguistic phenomena. As such, it's essential to discover how to effectively incorporate linguistic information into neural models to compensate for what the neural network-model is weak at. A simple practice is to embed linguistic features such as NER and POS tags into original texts. Particularly, I observed that a vanilla attention-based Seq2Seq model, when being equipped with linguistic features, could achieve better performance than BART (Lewis et al., 2020) , 22 a variant of BERT specializing in text generation, on both in-domain and cross-domain question generation tasks. 22 In general, BART (\u223c139M) has 8 times more parameters than vanilla attention-based Seq2Seq (\u223c17M).",
            "cite_spans": [
                {
                    "start": 1034,
                    "end": 1054,
                    "text": "(Lewis et al., 2020)",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 1173,
                    "end": 1175,
                    "text": "22",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Chapter 5: Conclusion"
        },
        {
            "text": "Formulation and Implementation Formally, given a document ( where U \u2208 R m\u00d7d , and d is size of the dimension.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1.1 Answer Evidence Extractor"
        },
        {
            "text": "Following the same paradigm of the BERT model for the sequence labeling task (Devlin et al., 2019b) , we predict the BIO tag for each a j as follows:",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 99,
                    "text": "(Devlin et al., 2019b)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "A.1.1 Answer Evidence Extractor"
        },
        {
            "text": "Pr(a j |p i ) = softmax(U \u00b7 W + b), \u2200p i \u2208 p (A.2)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1.1 Answer Evidence Extractor"
        },
        {
            "text": "We train model on source contexts by minimizing the negative log-likelihood loss.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1.1 Answer Evidence Extractor"
        },
        {
            "text": "Post-processing Heuristic Rules We observe that when we directly apply the ClinicalBERT (Alsentzer et al., 2019) system described in Section 2.3.1 on clinical texts, the extracted answer evidences sometimes are broken sentences due to the noisy nature and uninformative language (e.g., acronyms) of clinical texts. To make sure the extracted evidences are meaningful, we designed a \"merge-and-drop\" heuristic rule to further improve the extractor's accuracy. Specifically, for each extracted evidence candidate, we first examine the length (number of tokens) of the extracted evidence. If the length is larger than the threshold \u03b7, we keep this evidence; otherwise, we compute the distance, i.e., the number of tokens between the current candidate span and the closest span. If the distance is smaller than the threshold \u03b3, we merge these two \"close-sitting\" spans; otherwise, we drop this over-short evidence span. In our experiments, we set \u03b7 and \u03b3 to be 3 and 3, respectively, since they help the QA system achieve the best performance on the dev set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1.1 Answer Evidence Extractor"
        },
        {
            "text": "In order to utilize the Question Phrase Prediction (QPP) module and make the QPP module generic enough without loss of generality, we identify valid n-gram Question Phrases in an automatic way.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1.2 Question Phrases Identification"
        },
        {
            "text": "To prepare an exhaustive list of valid n-gram Question Phrases, we first collect all of the first n words appearing in questions in emrQA, forming three (i.e., n=1, 2, 3) raw Question Phrases set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1.2 Question Phrases Identification"
        },
        {
            "text": "We observe that all uni-grams are valid question phrases (e.g., \"How\", \"When\", \"What\"), so we don't do any pruning and keep the uni-gram question phrases set as it is.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1.2 Question Phrases Identification"
        },
        {
            "text": "As for n-gram (n \u2265 2) Question Phrases set, we conduct fine-grained filtering. We only consider n-grams with occurrence frequency greater than the threshold \u03b6 as valid n-gram Question Phrases. In our experiment, we set \u03b6 as 0.02%. Less frequent n-gram words (i.e., frequency < 0.02%) will degrade to unigram Question Phrases in accordance with corresponding question types (e.g., \"Has lasix\" \u2192 \"Has\"*) so as to maintain lossless. In the end, n-gram (n \u2265 2) Question Phrases sets, without any information loss, consist of both n-gram Question Phrases and degraded unigram Question Phrases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1.2 Question Phrases Identification"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Gold label: Negative [useful, useless, useless] Predicted rank: 3 Query: Are COVID-19 antibody tests accurate? FAQ item: Q: Should I be tested with an antibody (serology) test for COVID-19? A: ... Antibody tests have limited ability to diagnose COVID-19 and should not be used alone to diagnose COVID-19",
            "authors": [],
            "year": null,
            "venue": "Query: What research is being done on antibody tests and their accuracy? FAQ item: Q: What is antibody testing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Dev Set Construction The dev set on MIMIC-III is constructed by sampling generated questions from 9 QG models and is used to tune the hyper-parameters only. Instead of uniformly sampling from 9 QG models, we followed the sampling ratio of 1:3:6 (Base model, Base+BeamSearch, Base+QPP) for each QG method",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Publicly available clinical BERT embeddings",
            "authors": [
                {
                    "first": "Emily",
                    "middle": [],
                    "last": "Alsentzer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "John",
                    "suffix": ""
                },
                {
                    "first": "Willie",
                    "middle": [],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "Wei-Hung",
                    "middle": [],
                    "last": "Boag",
                    "suffix": ""
                },
                {
                    "first": "Di",
                    "middle": [],
                    "last": "Weng",
                    "suffix": ""
                },
                {
                    "first": "Tristan",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                },
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Naumann",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mcdermott",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NAACL Clinical NLP Workshop",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "The lexical and grammatical sources of neg-raising inferences",
            "authors": [
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Hannah Youngeun An",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Steven White",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the Society for Computation in Linguistics",
            "volume": "",
            "issn": "",
            "pages": "220--233",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "A large annotated corpus for learning natural language inference",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Samuel",
                    "suffix": ""
                },
                {
                    "first": "Gabor",
                    "middle": [],
                    "last": "Bowman",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Angeli",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [
                        "D"
                    ],
                    "last": "Potts",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "EMNLP'15",
            "volume": "",
            "issn": "",
            "pages": "632--642",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Language models are few-shot learners",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [
                        "M"
                    ],
                    "last": "Ramesh",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Ziegler",
                    "suffix": ""
                },
                {
                    "first": "Clemens",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Winter",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [],
                    "last": "Hesse",
                    "suffix": ""
                },
                {
                    "first": "Eric",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Mateusz",
                    "middle": [],
                    "last": "Sigler",
                    "suffix": ""
                },
                {
                    "first": "Scott",
                    "middle": [],
                    "last": "Litwin",
                    "suffix": ""
                },
                {
                    "first": "Benjamin",
                    "middle": [],
                    "last": "Gray",
                    "suffix": ""
                },
                {
                    "first": "Jack",
                    "middle": [],
                    "last": "Chess",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Clark",
                    "suffix": ""
                },
                {
                    "first": "Sam",
                    "middle": [],
                    "last": "Berner",
                    "suffix": ""
                },
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Mccandlish",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Dario",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Amodei",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Question answering from frequently asked question files: Experiences with the FAQ FINDER system",
            "authors": [
                {
                    "first": "Robin",
                    "middle": [],
                    "last": "Burke",
                    "suffix": ""
                },
                {
                    "first": "Kristian",
                    "middle": [],
                    "last": "Hammond",
                    "suffix": ""
                },
                {
                    "first": "Vladimir",
                    "middle": [],
                    "last": "Kulyukin",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [],
                    "last": "Lytinen",
                    "suffix": ""
                },
                {
                    "first": "Noriko",
                    "middle": [],
                    "last": "Tomuro",
                    "suffix": ""
                },
                {
                    "first": "Scott",
                    "middle": [],
                    "last": "Schoenberg",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "57--66",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Simulation of annotators for active learning: Uncertain Oracles",
            "authors": [
                {
                    "first": "Adrian",
                    "middle": [],
                    "last": "Calma",
                    "suffix": ""
                },
                {
                    "first": "Bernhard",
                    "middle": [],
                    "last": "Sick",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the Workshop and Tutorial on Interactive Adaptive Learning co-located with European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2017)",
            "volume": "",
            "issn": "",
            "pages": "49--58",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "A recurrent BERT-based model for question generation",
            "authors": [
                {
                    "first": "Ying-Hong",
                    "middle": [],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "Yao-Chung",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2nd Workshop on Machine Reading for Question Answering",
            "volume": "",
            "issn": "",
            "pages": "154--162",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Reading wikipedia to answer open-domain questions",
            "authors": [
                {
                    "first": "Danqi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Fisch",
                    "suffix": ""
                },
                {
                    "first": "Jason",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "Antoine",
                    "middle": [],
                    "last": "Bordes",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ACL'17",
            "volume": "",
            "issn": "",
            "pages": "1870--1879",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Tracking social media discourse about the COVID-19 pandemic: Development of a public coronavirus twitter data set",
            "authors": [
                {
                    "first": "Emily",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Lerman",
                    "suffix": ""
                },
                {
                    "first": "Emilio",
                    "middle": [],
                    "last": "Ferrara",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Multinomial adversarial networks for multi-domain text classification",
            "authors": [
                {
                    "first": "Xilun",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Claire",
                    "middle": [],
                    "last": "Cardie",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018",
            "volume": "1",
            "issn": "",
            "pages": "1226--1240",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Adversarial deep averaging networks for cross-lingual sentiment classification",
            "authors": [
                {
                    "first": "Xilun",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Yu",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Ben",
                    "middle": [],
                    "last": "Athiwaratkun",
                    "suffix": ""
                },
                {
                    "first": "Claire",
                    "middle": [],
                    "last": "Cardie",
                    "suffix": ""
                },
                {
                    "first": "Kilian",
                    "middle": [
                        "Q"
                    ],
                    "last": "Weinberger",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Trans. Assoc. Comput. Linguistics",
            "volume": "6",
            "issn": "",
            "pages": "557--570",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Mixture content selection for diverse sequence generation",
            "authors": [
                {
                    "first": "Jaemin",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Minjoon",
                    "middle": [],
                    "last": "Seo",
                    "suffix": ""
                },
                {
                    "first": "Hannaneh",
                    "middle": [],
                    "last": "Hajishirzi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "EMNLP-IJCNLP'19",
            "volume": "",
            "issn": "",
            "pages": "3112--3122",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
            "authors": [
                {
                    "first": "Kyunghyun",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bart Van Merrienboer",
                    "suffix": ""
                },
                {
                    "first": "Dzmitry",
                    "middle": [],
                    "last": "Aglar G\u00fcl\u00e7ehre",
                    "suffix": ""
                },
                {
                    "first": "Fethi",
                    "middle": [],
                    "last": "Bahdanau",
                    "suffix": ""
                },
                {
                    "first": "Holger",
                    "middle": [],
                    "last": "Bougares",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Schwenk",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "1724--1734",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "The PASCAL recognising textual entailment challenge",
            "authors": [
                {
                    "first": "Oren",
                    "middle": [],
                    "last": "Ido Dagan",
                    "suffix": ""
                },
                {
                    "first": "Bernardo",
                    "middle": [],
                    "last": "Glickman",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Magnini",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW 2005",
            "volume": "",
            "issn": "",
            "pages": "177--190",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "The CommitmentBank: Investigating projection in naturally occurring discourse",
            "authors": [
                {
                    "first": "Marie-Catherine",
                    "middle": [],
                    "last": "De Marneffe",
                    "suffix": ""
                },
                {
                    "first": "Mandy",
                    "middle": [],
                    "last": "Simons",
                    "suffix": ""
                },
                {
                    "first": "Judith",
                    "middle": [],
                    "last": "Tonhauser",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Sinn und Bedeutung 23",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
            "volume": "",
            "issn": "",
            "pages": "4171--4186",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NAACL-HLT'19",
            "volume": "",
            "issn": "",
            "pages": "4171--4186",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Learning to ask: Neural question generation for reading comprehension",
            "authors": [
                {
                    "first": "Xinya",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "Junru",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "Claire",
                    "middle": [],
                    "last": "Cardie",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ACL'17",
            "volume": "",
            "issn": "",
            "pages": "1342--1352",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Finding structure in time",
            "authors": [
                {
                    "first": "Jeffrey",
                    "middle": [
                        "L"
                    ],
                    "last": "Elman",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Cognitive Science",
            "volume": "14",
            "issn": "2",
            "pages": "179--211",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Annotating and characterizing clinical sentences with explicit why-qa cues",
            "authors": [
                {
                    "first": "Jungwei",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NAACL Clinical NLP Workshop",
            "volume": "",
            "issn": "",
            "pages": "101--106",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Domain-adversarial training of neural networks",
            "authors": [
                {
                    "first": "Yaroslav",
                    "middle": [],
                    "last": "Ganin",
                    "suffix": ""
                },
                {
                    "first": "Evgeniya",
                    "middle": [],
                    "last": "Ustinova",
                    "suffix": ""
                },
                {
                    "first": "Hana",
                    "middle": [],
                    "last": "Ajakan",
                    "suffix": ""
                },
                {
                    "first": "Pascal",
                    "middle": [],
                    "last": "Germain",
                    "suffix": ""
                },
                {
                    "first": "Hugo",
                    "middle": [],
                    "last": "Larochelle",
                    "suffix": ""
                },
                {
                    "first": "Fran\u00e7ois",
                    "middle": [],
                    "last": "Laviolette",
                    "suffix": ""
                },
                {
                    "first": "Mario",
                    "middle": [],
                    "last": "Marchand",
                    "suffix": ""
                },
                {
                    "first": "Victor",
                    "middle": [
                        "S"
                    ],
                    "last": "Lempitsky",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "J. Mach. Learn. Res",
            "volume": "17",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Natural language inference with mixed effects",
            "authors": [
                {
                    "first": "William",
                    "middle": [],
                    "last": "Gantt",
                    "suffix": ""
                },
                {
                    "first": "Benjamin",
                    "middle": [],
                    "last": "Kane",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [
                        "Steven"
                    ],
                    "last": "White",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "The Ninth Joint Conference on Lexical and Computational Semantics (*SEM 2020",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Two-stage synthesis networks for transfer learning in machine comprehension",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Golub",
                    "suffix": ""
                },
                {
                    "first": "Po-Sen",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Xiaodong",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Li",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "EMNLP'17",
            "volume": "",
            "issn": "",
            "pages": "835--844",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "FAQ retrieval using attentive matching",
            "authors": [
                {
                    "first": "Sparsh",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vitor",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Carvalho",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "SIGIR'19",
            "volume": "",
            "issn": "",
            "pages": "929--932",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Balint Miklos, and Ray Kurzweil. Efficient natural language response suggestion for smart reply",
            "authors": [
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Henderson",
                    "suffix": ""
                },
                {
                    "first": "Rami",
                    "middle": [],
                    "last": "Al-Rfou",
                    "suffix": ""
                },
                {
                    "first": "Brian",
                    "middle": [],
                    "last": "Strope",
                    "suffix": ""
                },
                {
                    "first": "Laszlo",
                    "middle": [],
                    "last": "Yun Hsuan Sung",
                    "suffix": ""
                },
                {
                    "first": "Ruiqi",
                    "middle": [],
                    "last": "Lukacs",
                    "suffix": ""
                },
                {
                    "first": "Sanjiv",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Measuring mathematical problem solving with the MATH dataset",
            "authors": [
                {
                    "first": "Dan",
                    "middle": [],
                    "last": "Hendrycks",
                    "suffix": ""
                },
                {
                    "first": "Collin",
                    "middle": [],
                    "last": "Burns",
                    "suffix": ""
                },
                {
                    "first": "Saurav",
                    "middle": [],
                    "last": "Kadavath",
                    "suffix": ""
                },
                {
                    "first": "Akul",
                    "middle": [],
                    "last": "Arora",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [],
                    "last": "Basart",
                    "suffix": ""
                },
                {
                    "first": "Eric",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Dawn",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Steinhardt",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "Sepp",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J\u00fcrgen",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural Computation",
            "volume": "",
            "issn": "",
            "pages": "1735--1780",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Comparison of diverse decoding methods from conditional language models",
            "authors": [
                {
                    "first": "Daphne",
                    "middle": [],
                    "last": "Ippolito",
                    "suffix": ""
                },
                {
                    "first": "Reno",
                    "middle": [],
                    "last": "Kriz",
                    "suffix": ""
                },
                {
                    "first": "Jo\u00e3o",
                    "middle": [],
                    "last": "Sedoc",
                    "suffix": ""
                },
                {
                    "first": "Maria",
                    "middle": [],
                    "last": "Kustikova",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Callison-Burch",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACL '19",
            "volume": "",
            "issn": "",
            "pages": "3752--3762",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Cross-domain NER using cross-domain language modeling",
            "authors": [
                {
                    "first": "Chen",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "Liang",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "Yue",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
            "volume": "1",
            "issn": "",
            "pages": "2464--2474",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Evaluating BERT for natural language inference: A case study on the Commitmentbank",
            "authors": [
                {
                    "first": "Nanjiang",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Marie-Catherine",
                    "middle": [],
                    "last": "De Marneffe",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "volume": "2019",
            "issn": "",
            "pages": "6085--6090",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment",
            "authors": [
                {
                    "first": "Nanjiang",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Marie-Catherine",
                    "middle": [],
                    "last": "De Marneffe",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
            "volume": "",
            "issn": "",
            "pages": "4208--4213",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Let me know what to ask: Interrogative-wordaware question generation",
            "authors": [
                {
                    "first": "Junmo",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "Haritz",
                    "middle": [],
                    "last": "Puerto San Roman",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2nd Workshop on Machine Reading for Question Answering",
            "volume": "",
            "issn": "",
            "pages": "163--171",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "FAQIR -a frequently asked questions retrieval test collection",
            "authors": [
                {
                    "first": "Mladen",
                    "middle": [],
                    "last": "Karan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Jan\u0161najder",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Text, Speech, and Dialogue -19th International Conference, TSD 2016",
            "volume": "",
            "issn": "",
            "pages": "74--81",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Paraphrase-focused learning to rank for domain-specific frequently asked questions retrieval",
            "authors": [
                {
                    "first": "Mladen",
                    "middle": [],
                    "last": "Karan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Jan\u0161najder",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Expert Systems with Applications",
            "volume": "",
            "issn": "",
            "pages": "418--433",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Sentiment analysis: It's complicated!",
            "authors": [
                {
                    "first": "Kian",
                    "middle": [],
                    "last": "Kenyon-Dean",
                    "suffix": ""
                },
                {
                    "first": "Eisha",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "Scott",
                    "middle": [],
                    "last": "Fujimoto",
                    "suffix": ""
                },
                {
                    "first": "Jeremy",
                    "middle": [],
                    "last": "Georges-Filteau",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Glasz",
                    "suffix": ""
                },
                {
                    "first": "Barleen",
                    "middle": [],
                    "last": "Kaur",
                    "suffix": ""
                },
                {
                    "first": "Auguste",
                    "middle": [],
                    "last": "Lalande",
                    "suffix": ""
                },
                {
                    "first": "Shruti",
                    "middle": [],
                    "last": "Bhanderi",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Belfer",
                    "suffix": ""
                },
                {
                    "first": "Nirmal",
                    "middle": [],
                    "last": "Kanagasabai",
                    "suffix": ""
                },
                {
                    "first": "Roman",
                    "middle": [],
                    "last": "Sarrazingendron",
                    "suffix": ""
                },
                {
                    "first": "Rohit",
                    "middle": [],
                    "last": "Verma",
                    "suffix": ""
                },
                {
                    "first": "Derek",
                    "middle": [],
                    "last": "Ruths",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "",
            "issn": "",
            "pages": "1886--1895",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "High-performance FAQ retrieval using an automatic clustering method of query logs. Information Processing & Management",
            "authors": [
                {
                    "first": "Harksoo",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Jungyun",
                    "middle": [],
                    "last": "Seo",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "650--661",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "The meteor metric for automatic evaluation of machine translation. Machine translation",
            "authors": [
                {
                    "first": "Alon",
                    "middle": [],
                    "last": "Lavie",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Michael",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Denkowski",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "23",
            "issn": "",
            "pages": "105--115",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "BART: denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension",
            "authors": [
                {
                    "first": "Mike",
                    "middle": [],
                    "last": "Lewis",
                    "suffix": ""
                },
                {
                    "first": "Yinhan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Naman",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "Marjan",
                    "middle": [],
                    "last": "Ghazvininejad",
                    "suffix": ""
                },
                {
                    "first": "Abdelrahman",
                    "middle": [],
                    "last": "Mohamed",
                    "suffix": ""
                },
                {
                    "first": "Omer",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "Veselin",
                    "middle": [],
                    "last": "Stoyanov",
                    "suffix": ""
                },
                {
                    "first": "Luke",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "volume": "2020",
            "issn": "",
            "pages": "7871--7880",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "A diversity-promoting objective function for neural conversation models",
            "authors": [
                {
                    "first": "Jiwei",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Michel",
                    "middle": [],
                    "last": "Galley",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Brockett",
                    "suffix": ""
                },
                {
                    "first": "Jianfeng",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Bill",
                    "middle": [],
                    "last": "Dolan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "NAACL-HLT'16",
            "volume": "",
            "issn": "",
            "pages": "110--119",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "authors": [
                {
                    "first": "Chin-Yew",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Text Summarization Branches Out",
            "volume": "",
            "issn": "",
            "pages": "74--81",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Asking questions the human way: Scalable question-answer generation from text corpus",
            "authors": [
                {
                    "first": "Bang",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Haojie",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Di",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "Haolan",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Yancheng",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "WWW'20",
            "volume": "",
            "issn": "",
            "pages": "2032--2043",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Effective approaches to attention-based neural machine translation",
            "authors": [
                {
                    "first": "Minh-Thang",
                    "middle": [],
                    "last": "Luong",
                    "suffix": ""
                },
                {
                    "first": "Hieu",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                },
                {
                    "first": "Christopher D",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "EMNLP'15",
            "volume": "",
            "issn": "",
            "pages": "1412--1421",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "An extended model of natural logic",
            "authors": [
                {
                    "first": "Bill",
                    "middle": [],
                    "last": "Maccartney",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the Eight International Conference on Computational Semantics",
            "volume": "",
            "issn": "",
            "pages": "140--156",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Introduction to Information Retrieval",
            "authors": [
                {
                    "first": "Christopher",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                },
                {
                    "first": "Prabhakar",
                    "middle": [],
                    "last": "Raghavan",
                    "suffix": ""
                },
                {
                    "first": "Hinrich",
                    "middle": [],
                    "last": "Sch\u00fctze",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Unsupervised FAQ retrieval with question generation and BERT",
            "authors": [
                {
                    "first": "Yosi",
                    "middle": [],
                    "last": "Mass",
                    "suffix": ""
                },
                {
                    "first": "Boaz",
                    "middle": [],
                    "last": "Carmeli",
                    "suffix": ""
                },
                {
                    "first": "Haggai",
                    "middle": [],
                    "last": "Roitman",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Konopnicki",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
            "volume": "",
            "issn": "",
            "pages": "807--812",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "authors": [
                {
                    "first": "Tom",
                    "middle": [],
                    "last": "Mccoy",
                    "suffix": ""
                },
                {
                    "first": "Ellie",
                    "middle": [],
                    "last": "Pavlick",
                    "suffix": ""
                },
                {
                    "first": "Tal",
                    "middle": [],
                    "last": "Linzen",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
            "volume": "",
            "issn": "",
            "pages": "3428--3448",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "emrqa: A large corpus for question answering on electronic medical records",
            "authors": [
                {
                    "first": "Anusri",
                    "middle": [],
                    "last": "Pampari",
                    "suffix": ""
                },
                {
                    "first": "Preethi",
                    "middle": [],
                    "last": "Raghavan",
                    "suffix": ""
                },
                {
                    "first": "Jennifer",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "EMNLP'18",
            "volume": "",
            "issn": "",
            "pages": "2357--2368",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "BLEU: A method for automatic evaluation of machine translation",
            "authors": [
                {
                    "first": "Kishore",
                    "middle": [],
                    "last": "Papineni",
                    "suffix": ""
                },
                {
                    "first": "Salim",
                    "middle": [],
                    "last": "Roukos",
                    "suffix": ""
                },
                {
                    "first": "Todd",
                    "middle": [],
                    "last": "Ward",
                    "suffix": ""
                },
                {
                    "first": "Wei-Jing",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "ACL'02",
            "volume": "",
            "issn": "",
            "pages": "311--318",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "An ontology for clinical questions about the contents of patient notes",
            "authors": [
                {
                    "first": "Jon",
                    "middle": [],
                    "last": "Patrick",
                    "suffix": ""
                },
                {
                    "first": "Min",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Journal of Biomedical Informatics",
            "volume": "45",
            "issn": "2",
            "pages": "292--306",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Most \"babies\" are \"little\" and most \"problems\" are \"huge\": Compositional entailment in adjective-nouns",
            "authors": [
                {
                    "first": "Ellie",
                    "middle": [],
                    "last": "Pavlick",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Callison-Burch",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Inherent disagreements in human textual inferences",
            "authors": [
                {
                    "first": "Ellie",
                    "middle": [],
                    "last": "Pavlick",
                    "suffix": ""
                },
                {
                    "first": "Tom",
                    "middle": [],
                    "last": "Kwiatkowski",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Transactions of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "677--694",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Glove: Global vectors for word representation",
            "authors": [
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "To tune or not to tune? Adapting pretrained representations to diverse tasks",
            "authors": [
                {
                    "first": "Matthew",
                    "middle": [
                        "E"
                    ],
                    "last": "Peters",
                    "suffix": ""
                },
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Ruder",
                    "suffix": ""
                },
                {
                    "first": "Noah",
                    "middle": [
                        "A"
                    ],
                    "last": "Smith",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP",
            "volume": "",
            "issn": "",
            "pages": "7--14",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Collecting verified COVID-19 question answer pairs",
            "authors": [
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Poliak",
                    "suffix": ""
                },
                {
                    "first": "Max",
                    "middle": [],
                    "last": "Fleming",
                    "suffix": ""
                },
                {
                    "first": "Cash",
                    "middle": [],
                    "last": "Costello",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [
                        "W"
                    ],
                    "last": "Murray",
                    "suffix": ""
                },
                {
                    "first": "Mahsa",
                    "middle": [],
                    "last": "Yarmohammadi",
                    "suffix": ""
                },
                {
                    "first": "Shivani",
                    "middle": [],
                    "last": "Pandya",
                    "suffix": ""
                },
                {
                    "first": "Darius",
                    "middle": [],
                    "last": "Irani",
                    "suffix": ""
                },
                {
                    "first": "Milind",
                    "middle": [],
                    "last": "Agarwal",
                    "suffix": ""
                },
                {
                    "first": "Udit",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "Shuo",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Nicola",
                    "middle": [],
                    "last": "Ivanov",
                    "suffix": ""
                },
                {
                    "first": "Lingxi",
                    "middle": [],
                    "last": "Shang",
                    "suffix": ""
                },
                {
                    "first": "Kaushik",
                    "middle": [],
                    "last": "Srinivasan",
                    "suffix": ""
                },
                {
                    "first": "Seolhwa",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Xu",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Smisha",
                    "middle": [],
                    "last": "Agarwal",
                    "suffix": ""
                },
                {
                    "first": "Jo\u00e3o",
                    "middle": [],
                    "last": "Sedoc",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Improving language understanding by generative pre-training",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Karthik",
                    "middle": [],
                    "last": "Narasimhan",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Salimans",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "Language models are unsupervised multitask learners",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Rewon",
                    "middle": [],
                    "last": "Child",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Luan",
                    "suffix": ""
                },
                {
                    "first": "Dario",
                    "middle": [],
                    "last": "Amodei",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "authors": [
                {
                    "first": "Colin",
                    "middle": [],
                    "last": "Raffel",
                    "suffix": ""
                },
                {
                    "first": "Noam",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Roberts",
                    "suffix": ""
                },
                {
                    "first": "Katherine",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Sharan",
                    "middle": [],
                    "last": "Narang",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Matena",
                    "suffix": ""
                },
                {
                    "first": "Yanqi",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [
                        "J"
                    ],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "Annotating electronic medical records for question answering",
            "authors": [
                {
                    "first": "Preethi",
                    "middle": [],
                    "last": "Raghavan",
                    "suffix": ""
                },
                {
                    "first": "Siddharth",
                    "middle": [],
                    "last": "Patwardhan",
                    "suffix": ""
                },
                {
                    "first": "Jennifer",
                    "middle": [
                        "J"
                    ],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "Murthy V",
                    "middle": [],
                    "last": "Devarakonda",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1805.06816"
                ]
            }
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "authors": [
                {
                    "first": "Pranav",
                    "middle": [],
                    "last": "Rajpurkar",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Konstantin",
                    "middle": [],
                    "last": "Lopyrev",
                    "suffix": ""
                },
                {
                    "first": "Percy",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "EMNLP'16",
            "volume": "",
            "issn": "",
            "pages": "2383--2392",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "Know what you don't know: Unanswerable questions for squad",
            "authors": [
                {
                    "first": "Pranav",
                    "middle": [],
                    "last": "Rajpurkar",
                    "suffix": ""
                },
                {
                    "first": "Robin",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "Percy",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "ACL'18",
            "volume": "",
            "issn": "",
            "pages": "784--789",
            "other_ids": {}
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "Entityenriched neural models for clinical question answering",
            "authors": [
                {
                    "first": "Wei-Hung",
                    "middle": [],
                    "last": "Bhanu Pratap Singh Rawat",
                    "suffix": ""
                },
                {
                    "first": "Preethi",
                    "middle": [],
                    "last": "Weng",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Raghavan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Szolovits",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.06587"
                ]
            }
        },
        "BIBREF67": {
            "ref_id": "b67",
            "title": "Sentence-BERT: Sentence embeddings using siamese BERT-networks",
            "authors": [
                {
                    "first": "Nils",
                    "middle": [],
                    "last": "Reimers",
                    "suffix": ""
                },
                {
                    "first": "Iryna",
                    "middle": [],
                    "last": "Gurevych",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "3980--3990",
            "other_ids": {}
        },
        "BIBREF68": {
            "ref_id": "b68",
            "title": "The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends\u00ae in Information Retrieval",
            "authors": [
                {
                    "first": "Stephen",
                    "middle": [],
                    "last": "Robertson",
                    "suffix": ""
                },
                {
                    "first": "Hugo",
                    "middle": [],
                    "last": "Zaragoza",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "333--389",
            "other_ids": {}
        },
        "BIBREF69": {
            "ref_id": "b69",
            "title": "FAQ retrieval using query question similarity and BERT-based query-answer relevance",
            "authors": [
                {
                    "first": "Wataru",
                    "middle": [],
                    "last": "Sakata",
                    "suffix": ""
                },
                {
                    "first": "Tomohide",
                    "middle": [],
                    "last": "Shibata",
                    "suffix": ""
                },
                {
                    "first": "Ribeka",
                    "middle": [],
                    "last": "Tanaka",
                    "suffix": ""
                },
                {
                    "first": "Sadao",
                    "middle": [],
                    "last": "Kurohashi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019",
            "volume": "",
            "issn": "",
            "pages": "1113--1116",
            "other_ids": {}
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "Query by committee",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Seung",
                    "suffix": ""
                },
                {
                    "first": "Manfred",
                    "middle": [],
                    "last": "Opper",
                    "suffix": ""
                },
                {
                    "first": "Haim",
                    "middle": [],
                    "last": "Sompolinsky",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Proceedings of the Fifth Annual ACM Conference on Computational Learning Theory",
            "volume": "",
            "issn": "",
            "pages": "287--294",
            "other_ids": {}
        },
        "BIBREF71": {
            "ref_id": "b71",
            "title": "End-to-end synthetic data generation for domain adaptation of question answering systems",
            "authors": [
                {
                    "first": "Siamak",
                    "middle": [],
                    "last": "Shakeri",
                    "suffix": ""
                },
                {
                    "first": "C\u00edcero",
                    "middle": [],
                    "last": "Nogueira",
                    "suffix": ""
                },
                {
                    "first": "Henghui",
                    "middle": [],
                    "last": "Santos",
                    "suffix": ""
                },
                {
                    "first": "Patrick",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Feng",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "Zhiguo",
                    "middle": [],
                    "last": "Nan",
                    "suffix": ""
                },
                {
                    "first": "Ramesh",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Bing",
                    "middle": [],
                    "last": "Nallapati",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "EMNLP'20",
            "volume": "",
            "issn": "",
            "pages": "5445--5460",
            "other_ids": {}
        },
        "BIBREF72": {
            "ref_id": "b72",
            "title": "Tsubaki: An open search engine infrastructure for developing information access methodology",
            "authors": [
                {
                    "first": "Keiji",
                    "middle": [],
                    "last": "Shinzato",
                    "suffix": ""
                },
                {
                    "first": "Tomohide",
                    "middle": [],
                    "last": "Shibata",
                    "suffix": ""
                },
                {
                    "first": "Daisuke",
                    "middle": [],
                    "last": "Kawahara",
                    "suffix": ""
                },
                {
                    "first": "Sadao",
                    "middle": [],
                    "last": "Kurohashi",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Journal of Information Processing",
            "volume": "",
            "issn": "",
            "pages": "216--227",
            "other_ids": {}
        },
        "BIBREF73": {
            "ref_id": "b73",
            "title": "An analysis of BERT FAQ retrieval models for COVID-19 infobot",
            "authors": [
                {
                    "first": "Shuo",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Jo\u00e3o",
                    "middle": [],
                    "last": "Sedoc",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF74": {
            "ref_id": "b74",
            "title": "Sequence to sequence learning with neural networks",
            "authors": [
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Oriol",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "Quoc",
                    "middle": [
                        "V"
                    ],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "NIPS '14",
            "volume": "",
            "issn": "",
            "pages": "3104--3112",
            "other_ids": {}
        },
        "BIBREF76": {
            "ref_id": "b76",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "Lukasz",
                    "middle": [],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "Illia",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF77": {
            "ref_id": "b77",
            "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
            "authors": [
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Yada",
                    "middle": [],
                    "last": "Pruksachatkun",
                    "suffix": ""
                },
                {
                    "first": "Nikita",
                    "middle": [],
                    "last": "Nangia",
                    "suffix": ""
                },
                {
                    "first": "Amanpreet",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "Julian",
                    "middle": [],
                    "last": "Michael",
                    "suffix": ""
                },
                {
                    "first": "Felix",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "Omer",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "Samuel",
                    "middle": [
                        "R"
                    ],
                    "last": "Bowman",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "3261--3275",
            "other_ids": {}
        },
        "BIBREF79": {
            "ref_id": "b79",
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Bowman",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "7th International Conference on Learning Representations, ICLR 2019",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF80": {
            "ref_id": "b80",
            "title": "Adversarial domain adaptation for machine reading comprehension",
            "authors": [
                {
                    "first": "Huazheng",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Zhe",
                    "middle": [],
                    "last": "Gan",
                    "suffix": ""
                },
                {
                    "first": "Xiaodong",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jingjing",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jianfeng",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Hongning",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "EMNLP-IJCNLP'19",
            "volume": "",
            "issn": "",
            "pages": "2510--2520",
            "other_ids": {}
        },
        "BIBREF81": {
            "ref_id": "b81",
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "authors": [
                {
                    "first": "Adina",
                    "middle": [],
                    "last": "Williams",
                    "suffix": ""
                },
                {
                    "first": "Nikita",
                    "middle": [],
                    "last": "Nangia",
                    "suffix": ""
                },
                {
                    "first": "Samuel",
                    "middle": [],
                    "last": "Bowman",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "NAACL'18",
            "volume": "",
            "issn": "",
            "pages": "1112--1122",
            "other_ids": {}
        },
        "BIBREF82": {
            "ref_id": "b82",
            "title": "Turing test as a defining feature of ai-completeness",
            "authors": [
                {
                    "first": "Roman",
                    "middle": [
                        "V"
                    ],
                    "last": "Yampolskiy",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Artificial Intelligence, Evolutionary Computing and Metaheuristics -In the Footsteps of Alan Turing",
            "volume": "427",
            "issn": "",
            "pages": "3--17",
            "other_ids": {}
        },
        "BIBREF83": {
            "ref_id": "b83",
            "title": "Clinical reading comprehension: A thorough analysis of the emrQA dataset",
            "authors": [
                {
                    "first": "Xiang",
                    "middle": [],
                    "last": "Yue",
                    "suffix": ""
                },
                {
                    "first": "Huan",
                    "middle": [],
                    "last": "Bernal Jimenez Gutierrez",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ACL'20",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF84": {
            "ref_id": "b84",
            "title": "CliniQG4QA: Generating diverse questions for domain adaptation of clinical question answering",
            "authors": [
                {
                    "first": "Xiang",
                    "middle": [],
                    "last": "Yue",
                    "suffix": ""
                },
                {
                    "first": "Xinliang",
                    "middle": [
                        "Frederick"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Ziyu",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "Simon",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Huan",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2021",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF85": {
            "ref_id": "b85",
            "title": "Identifying inherent disagreement in natural language inference",
            "authors": [
                {
                    "first": "Frederick",
                    "middle": [],
                    "last": "Xinliang",
                    "suffix": ""
                },
                {
                    "first": "Marie-Catherine",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "De Marneffe",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF86": {
            "ref_id": "b86",
            "title": "COUGH: A challenge dataset and models for COVID-19 FAQ retrievall language inference",
            "authors": [
                {
                    "first": "Heming",
                    "middle": [],
                    "last": "Xinliang Frederick Zhang",
                    "suffix": ""
                },
                {
                    "first": "Xiang",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Simon",
                    "middle": [],
                    "last": "Yue",
                    "suffix": ""
                },
                {
                    "first": "Huan",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            "volume": "2021",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF87": {
            "ref_id": "b87",
            "title": "Generating informative and diverse conversational responses via adversarial information maximization",
            "authors": [
                {
                    "first": "Yizhe",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Michel",
                    "middle": [],
                    "last": "Galley",
                    "suffix": ""
                },
                {
                    "first": "Jianfeng",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Zhe",
                    "middle": [],
                    "last": "Gan",
                    "suffix": ""
                },
                {
                    "first": "Xiujun",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Brockett",
                    "suffix": ""
                },
                {
                    "first": "Bill",
                    "middle": [],
                    "last": "Dolan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "NeurIPS'18",
            "volume": "",
            "issn": "",
            "pages": "1810--1820",
            "other_ids": {}
        },
        "BIBREF88": {
            "ref_id": "b88",
            "title": "Neural question generation from text: A preliminary study",
            "authors": [
                {
                    "first": "Qingyu",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Nan",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Furu",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Chuanqi",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "Hangbo",
                    "middle": [],
                    "last": "Bao",
                    "suffix": ""
                },
                {
                    "first": "Ming",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "National CCF Conference on Natural Language Processing and Chinese Computing",
            "volume": "",
            "issn": "",
            "pages": "662--671",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Illustration of our framework equipped with QPP. . . . . . . . . . . . . . . 10 2.2 Distributions over types of questions generated by NQG models and the ground truth. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3 QA and QG examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.1 Artificial Annotators (AAs) setup. . . . . . . . . . . . . . . . . . . . . . . 24 4.1 Examples from the COUGH dataset. . . . . . . . . . . . . . . . . . . . . . . 32 4.2 Language distribution for non-English FAQ items. . . . . . . . . . . . . .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Distributions over types of questions generated by NQG models and the ground truth. BS: Beam Search; QPP: Question Phrase Prediction module. is a little inferior to directly training the QA model on emrQA (79.99 F1). An outstanding characteristic we observe in the generated questions is the large bias of question types (e.g., most questions are \"Does\" while there is few \"Why\" and no \"How\" question). The distributions of question types are in Figure 2.2 (see top-left sub-plot).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Training procedure of our framework equipped with QPP. Input: labeled source data {(P S , A S , Q S )}, unlabeled target data {P T } Output: Generated QA pairs {(A T , Q T )} on target contexts; An optimized QA model for answering questions on target contexts Pretraining Stage 1: Train Answer Evidence Extractor based on the source data {(P S , A S )} 2: Obtain question phrase data Y S from Q S and train Question Phrase Prediction module on the source data {(A S , Y S )} 3: Train a QPP-enhanced QG model on the source data {(A S , Y S , Q S )} Training Stage 4: Use AEE to extract potential answer evidences {A T } on the target contexts {P S } 5: Use QPP to predict potential question phrases set {Y T } on {A T } 6: Use QPP-enhanced QG to generate diverse questions {Q T } based on {(A T , Y T )} 7: Train a QA model on synthetic target data {(P T , A T , Q T )} studied and shows competitive performance in diversifying generations (Ippolito et al., 2019). For the other kind (topic-guided approach), we propose a question phrase prediction (QPP) module, which predicts a set of valid question phrases given an answer evidence (Figure 2.1). Then, conditioned on a question phrase sampled from the set predicted by the QPP, a QG model is utilized to complete the rest of the question.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "studied systematic inferences items(Jiang and de Marneffe, 2019b,a;Raffel et al., 2019).Kenyon-Dean et al. (2018) also pointed out in sentiment analysis task, when performing real-time sentiment classification, an automated system cannot know a priori whether the data sample is inherently non-ambiguous. Here, in line with whatKenyon-Dean et al. (2018)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "de Marneffe et al., 2019), and then move on to how we determine ambiguous items and systematic inference items.The CommitmentBank (CB) is a corpus of 1,200 naturally occurring discourses originally collected from news articles, fiction and dialogues. Each discourse consists of up to 2 prior context sentences and 1 target sentence with a clause-embedding predicate under 4 embedding environments (negation, modal, question or antecedent of conditional). Annotators judged the extent to which the speaker/author of the sentences is committed to the truth of the content of the embedded clause (CC), responding on a Likert scale from +3 to -3, labeled at 3 points (+3/speaker is certain the CC is true, 0/speaker is not certain whether the CC is true or false, -3/speaker is certain the CC is false). Following Jiang and de Marneffe (2019a), we recast CB by taking the context and target as the premise and the embedded clause in the target as the hypothesis.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "utilize three linguistic features which are provided in CB: entailment-canceling environment (negation, modal, question, antecedent of conditional), matrix verb and its subject person. 1. Items under conditional are disagreement. 2. Items under question and with second person are neutral. 3. Items under question and with non-second person are disagreement. 4. Items of the form \"I don't know/think/believe\" are contradiction (i.e., negRaising structure). 5. Items with factive verbs are entailment. 6. Items under negation and with non-factive verbs are disagreement. 7. Items under modal and with non-third person are entailment.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Artificial Annotators (AAs) setup.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Hypothesis: she could stay Heuristics: D V. BERT: D J. BERT: D AAs: N {N, N, N} Gold: N [Premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. Hypothesis: they have seen a really high improvement. Heuristics: C V. BERT: C J. BERT: C AAs: C {C, C, C} Gold: C [Premise:B: So did you commute everyday then or, A: No. B: Oh, okay. A: No, no, it was a six hour drive. B: Oh, okay, when you said it was quite a way away, I did not know that meant you had to drive like an hour Hypothesis: speaker A had to drive like an hour Heuristics: C V. BERT: D J. BERT: E AAs: D {E, C, C} Gold: D [Premise: The assassin's tone and bearing were completely confident. If he noticed that Zukov was now edging further to the side widening the arc of fire he did not appear to be troubled. Hypothesis: Zukov was edging further to the side Heuristics: D V. BERT: D J. BERT: D AAs: D {E, E, N} Gold: E [Premise: B: Yeah, and EDS is very particular about this, hair cuts, A: Wow. B: I mean it was like you can't have, you know, such and such facial hair, no beards, you know, and just really detailed. A: A: I don't know that that would be a good environment to work in. Hypothesis: that would be a good environment to work in Heuristics: C V. BERT: C J. BERT: D AAs: C {C, C, C} Gold: D [Premise: \"Willy did mention it. I was puzzled, I 'll admit, but now I understand.\" How did you know Heather had been there? Hypothesis: Heather had been there Heuristics: N V. BERT: E J. BERT: E AAs: E {E, E, E} Gold: D [",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Examples from the COUGH dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Language distribution for non-English FAQ items.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "2 shows the language distribution (excluding English) of FAQ items in COUGH dataset. Like English FAQ items, non-English FAQ items are also presented in both question and query string forms. Statistics of non-English items can be found in",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "context) p = {p 1 , p 2 , ..., p m },where p i is the i-th token of the document and m is the total number of tokens, we aim to extract potential evidence sequences. Firstly, we adopt the ClinicalBERT model(Alsentzer et al., 2019) to encode the document: U = ClinicalBERT{p 1 , ..., p m }.(A.1)",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": ".1 Statistics of the datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.2 QA performance on MIMIC-III test set. . . . . . . . . . . . . . . . . . . . 14 2.3 Automatic evaluation of the generated questions on emrQA dataset. . . . . 15 3.1 Examples from CommitmentBank. . . . . . . . . . . . . . . . . . . . . . . 19 3.2 Number of items in each class in train/dev/test. . . . . . . . . . . . . . . . 21 3.3 Baselines and AAs overall performance on CB dev and test sets, and F1 scores of each class on the test set. . . . . . . . . . . . . . . . . . . . . . . 26 3.4 Models' predictions for CB test items. . . . . . . . . . . . . . . . . . . . . 28 3.5 Confusion matrix for the test set. . . . . . . . . . . . . . . . . . . . . . . . 28 3.6 F1 for CB test set under the embedding environments and \"I don't know/believe/think\" (\"negR\"). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.7 BERT-based models performance on test items correctly predicted by vs. items missed by linguistic rules. . . . . . . . . . . . . . . . . . . . . . . . 30 4.1 Comparison of COUGH with representative counterparts. . . . . . . . . . . 33 4.2 Basic statistics of FAQ bank in COUGH. . . . . . . . . . . . . . . . . . . . 35 4.3 Evaluation on COUGH. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 4.4 Error analysis with fine-tuned BERT (Q-q). . . . . . . . . . . . . . . . . . 41",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "However, answering clinical questions still remains challenging in real-world scenarios because well-trained QA systems may not generalize well to new clinical contexts from a different institute or patient group. For example,Yue et al. (2020) pointed out when a clinical QA model trained on the emrQA(Pampari et al., 2018) dataset is deployed to answer questions on MIMIC-III clinical texts (Johnson et al., 2016), its performance drops by around 30% even on questions that are similar to those in training. Most of the existing clinical QA datasets and setups focus on in-domain testing while leaving the generalization challenge under-explored. In this chapter, we propose to evaluate the performance of clinical QA models on target contexts and questions which may have different distributions from the training data. Due to the lack of publicly-available clinical QA pairs for our proposed evaluation setting, we ask clinical experts to annotate a new test set on the sampled MIMIC-III (Johnson et al., 2016) clinical texts. Inspired by recent work on question generation (QG) for improving QA performance in the open domain(Golub et al., 2017;Wang et al., 2019c;Shakeri et al., 2020), we implement an answer evidence extractor and a seq2seq-based QG model to synthesize QA pairs on target contexts to train a QA model. However, we do not observe that such QA models achieve better performance on our curated MIMIC-III QA set, compared with that trained on emrQA. Our error analysis reveals that the automatic generation technique often falls short of generating questions that are diverse enough to serve as useful training data for clinical QA models.To this end, we investigate two kinds of approaches to diversify the generation. Inspired byIppolito et al. (2019) whio study various decoding-based methods, we pick the standard beam search as a representative of the decoding-based approach since it achieves satisfying performance in various generation tasks. On the other hand, another practice (topic-guided approach) is to have a diversification step followed by a conditional generation. In general, such techniques first decide question topics and then generate questions conditioned on selected topics",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Statistics of the datasets. We synthesize a machine-generated dev set and ask human experts to annotate a test set for MIMIC-III. Details of dev set construction can be found in Setion A.1.3.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "To solve the proposed generalization challenge of clinical QA, inspired by recent work on question generation (QG) for QA in the open domain(Golub et al., 2017;",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "based QG model can be used to generate questions. Both answer evidence extractor and QG model are trained on the source data and then used to synthesize QA pairs on target contexts, based on which a QA model can be trained.",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "To investigate the effectiveness of diverse QG for QA, we consider the following variants Beam Search: Inference with Beam Search with the beam size at K and keep top K beams",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "perform well with diversity scores. The diversity of generated questions is boosted to some extent when the Beam Search is used since it can offer flexibility for QG models to explore more candidates when decoding. In comparison, the QPP module in our framework leads to the best results under both relevance and diversity evaluation. Particularly, it obtains 5% absolute improvement in terms of Dist4 for each base model. Analysis on QA Generalization: As expected, the corpora generated by diverse QG help the QA model perform consistently better than those generated by their respective base QG version as well as emrQA(Table 2.2). Between the two diversity-boosting approaches, we observe that the QA model trained on the corpora by QPP-enhanced QG achieves the best performance. Moreover, results on the human-generated portion are consistently better than those on human-verified. This is likely due to the fact that human-generated questionsContext: ... he was guaiac negative on admission. hematocrit remained stable overnight. 5. abd pain: suspect secondary to chronic pancreatitis. amylase unchanged QA Example from MIMIC-III Question: Why did the patient get abd pain? Answer by QA model trained on -NQG: Does the patient have any pain? -NQG+BeamSearch: Does the patient have any pain history? Does the patient have pain? Does the patient have any pain? -NQG+QPP: Why did the patient have acetaminophen? What treatment has the patient had for his pain? How was pain treated? Does the patient have any pain? ... QG Example from MIMIC-III Context: ... the patient was taking at home prior to admission were not restarted. 25. acetaminophen 325-650 mg po/ng q6h:prn pain 26. dabigatran etexilate 150 mg po bid... Questions generated by Figure 2.3: QA and QG examples. The red parts in contexts are ground-truth answer evidences. are more readable and sensible while human-verified ones are less natural (though the correctness is ensured). All these results indicate that improving the diversity of generated questions can help better train QA models on the new contexts and better address the generalization challenge.",
            "latex": null,
            "type": "table"
        },
        "TABREF10": {
            "text": ".2. Thus QA models trained on such corpora cannot answer questions of less frequent types. Though the emrQA dataset contains diverse questions (including \"why\" questions), its contexts might be different from MIMIC-III in terms of topic, note structures, writing styles, etc. So the model trained on emrQA struggles to answer some questions. In the QG example, the base model NQG can only generate one question. Though utilizing the Beam Search enables the model to explore multiple candidates, the generated questions are quite similar and are less likely to help improve QA. Enabling our QPP module helps generate diverse questions including \"Why\", In this chapter, we systematically investigate the generalization challenge of clinical reading comprehension and construct a new test set on MIMIC-III clinical texts. After observing simply using QG for QA does not work, we explore the importance of generating diverse questions. That is, we study two approaches for boosting question diversity, beam search and QPP. Particularly, our proposed QPP (question phrase prediction) module significantly improves the cross-domain generalizability of QA systems. Our comprehensive experiments allow for a better understanding of why diverse question generation can help QA on new clinical documents (i.e., target domain).",
            "latex": null,
            "type": "table"
        },
        "TABREF11": {
            "text": "Some of them, like for instance the farm in Connecticut, are quite small. If I like a place I buy it. I guess you could say it's a hobby. Hypothesis: buying places is a hobby. Premise: \"I hope you are settling down and the cat is well.\" This was a lie. She did not hope the cat was well. Hypothesis: the cat was well. Premise: \"All right, so it wasn't the bottle by the bed. What was it, then?\" Cobalt shook his head which might have meant he didn't know or might have been admonishment for Oliver who was still holding the bottle of wine. Hypothesis: Cobalt didn't know. Premise: A: No, it doesn't. B: And, of course, your court system when you get into the appeals, I don't believe criminal is in a court by itself. Hypothesis: criminal is in a court by itself. Premise: A: The last one I saw was Dances With The Wolves. B: Yeah, we talked about that one too.",
            "latex": null,
            "type": "table"
        },
        "TABREF12": {
            "text": "1: Examples from CommitmentBank, with finer-grained NLI labels. The labels in parentheses come fromJiang and de Marneffe (2019a). Scores in brackets are the raw human annotations.",
            "latex": null,
            "type": "table"
        },
        "TABREF14": {
            "text": "Number of items in each class in train/dev/test.",
            "latex": null,
            "type": "table"
        },
        "TABREF16": {
            "text": "agreement regardless of the input. The Heuristic baseline achieves competitive performance on the dev set, though it has a significantly worse result on the test set. Not surprisingly, both",
            "latex": null,
            "type": "table"
        },
        "TABREF17": {
            "text": "3 also gives F1 for each class on the test set. AAs outperform all BERT-based models under all classes. However, compared with the Heuristic, AAs show an inferior result on \"Neutral\" items mainly due to the lack of \"Neutral\" training data. The first 4 examples in Table 3.4 show examples for which AAs make the correct prediction while other baselines might not. The confusion matrix in",
            "latex": null,
            "type": "table"
        },
        "TABREF18": {
            "text": "Premise: B: Yeah, it is. A: For instance, B: I'm a historian, and my father had kept them, I think, since nineteen twenty-seven uh, but he burned the ones from twenty-seven to fi-, A: My goodness. B: I could not believe he did that, Hypothesis: his father burned the ones from twenty-seven Heuristics: C V. BERT: D J. BERT: E AAs: E {E, E, E}",
            "latex": null,
            "type": "table"
        },
        "TABREF19": {
            "text": "4: Models' predictions for CB test items. Labels in [] are predictions by individual AAs.",
            "latex": null,
            "type": "table"
        },
        "TABREF20": {
            "text": "6 gives F1 scores for the Heuristic, BERT models and AAs for items under the different embedding environments and potential neg-raising items in the test set. Though",
            "latex": null,
            "type": "table"
        },
        "TABREF22": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF24": {
            "text": "Basic statistics of FAQ bank in COUGH.",
            "latex": null,
            "type": "table"
        },
        "TABREF25": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF26": {
            "text": "fintune on pesudo Q-q and FAQ Bank 0.31 0.39 0.64Table 4.3: Evaluation on COUGH. BERT refers to Sentence-BERT(Reimers and Gurevych, 2019).",
            "latex": null,
            "type": "table"
        },
        "TABREF27": {
            "text": "4: Error analysis with fine-tuned BERT (Q-q). Human annotations are inside [].",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
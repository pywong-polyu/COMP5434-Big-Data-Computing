{
    "paper_id": "ea639e9cb2aed26a5d72dd604a64e45d09db4b3a",
    "metadata": {
        "title": "Learning Across Bandits in High Dimension via Robust Statistics",
        "authors": [
            {
                "first": "Kan",
                "middle": [],
                "last": "Xu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Pennsylvania",
                    "location": {
                        "settlement": "Economics"
                    }
                },
                "email": "kanxu@sas.upenn.edu"
            },
            {
                "first": "Hamsa",
                "middle": [],
                "last": "Bastani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Pennsylvania",
                    "location": {
                        "settlement": "Economics"
                    }
                },
                "email": "hamsab@wharton.upenn.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Decision-makers often face the \"many bandits\" problem, where one must simultaneously learn across related but heterogeneous contextual bandit instances. For instance, a large retailer may wish to dynamically learn product demand across many stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to dynamically learn patient risk across many providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. We study the setting where the unknown parameter in each bandit instance can be decomposed into a global parameter plus a sparse instance-specific term. Then, we propose a novel two-stage estimator that exploits this structure in a sample-efficient way by using a combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). We embed this estimator within a bandit algorithm, and prove that it improves asymptotic regret bounds in the context dimension d; this improvement is exponential for data-poor instances. We further demonstrate how our results depend on the underlying network structure of bandit instances. Finally, we illustrate the value of our approach on synthetic and real datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Contextual bandits are a popular framework for adaptive decision-making and have found numerous applications including personalized content recommendations (Li et al. 2010) , mobile health (Tewari and Murphy 2017), targeted COVID-19 screening (Bastani et al. 2021b) , dynamic pricing (Qiang and Bayati 2016) and inventory management (Yuan et al. 2021) .",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 172,
                    "text": "(Li et al. 2010)",
                    "ref_id": null
                },
                {
                    "start": 243,
                    "end": 265,
                    "text": "(Bastani et al. 2021b)",
                    "ref_id": null
                },
                {
                    "start": 333,
                    "end": 351,
                    "text": "(Yuan et al. 2021)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "While the bandit literature typically considers a single decision-maker solving an isolated bandit instance, decision-makers increasingly face many, simultaneous bandit instances for closely related learning tasks. In these cases, we have an opportunity to not only learn within each bandit instance, but also across similar instances. To illustrate, consider the following two examples from healthcare and revenue management respectively:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Example 1 (Medical Risk Scoring). Health providers seek to predict patient-specific risk for adverse events (e.g., diabetes) in order to target preventative interventions. Learning this risk score primarily from patient data collected at the target hospital (where decisions are made) is important to account for idiosyncrasies that are specific to the hospital and the patient population it serves. This can include systematic differences in diagnosis/treatment behavior, healthcare utilization, or medical coding (see, e.g., Qui\u00f1onero-Candela et al. 2008 , Subbaswamy and Saria 2020 , Bastani 2021 , Mullainathan and Obermeyer 2017 . As a result, each hospital faces a distinct bandit learning problem. Yet, we may expect hospitals that serve similar patient populations to have similar underlying predictive models, creating an important opportunity to transfer knowledge across these bandit instances.",
            "cite_spans": [
                {
                    "start": 527,
                    "end": 556,
                    "text": "Qui\u00f1onero-Candela et al. 2008",
                    "ref_id": null
                },
                {
                    "start": 557,
                    "end": 584,
                    "text": ", Subbaswamy and Saria 2020",
                    "ref_id": null
                },
                {
                    "start": 585,
                    "end": 599,
                    "text": ", Bastani 2021",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 600,
                    "end": 633,
                    "text": ", Mullainathan and Obermeyer 2017",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Example 2 (Demand Prediction). Large retailers seek to predict store-specific demand for their various products to inform dynamic pricing or inventory management decisions. Learning this demand model primarily from sales data collected at the target store (where decisions are made) is important to account for idiosyncrasies that are specific to the store and the customer population it serves. This can include systematic differences in customer trends/preferences, in-store product placement, or promotion decisions (see, e.g., Baardman et al. 2020 , Cohen and Perakis 2018 , van Herpen et al. 2012 . As a result, each store faces a distinct bandit learning problem. Yet, we may expect stores that serve similar customer populations to have similar underlying demand models, creating an important opportunity to transfer knowledge across these bandit instances.",
            "cite_spans": [
                {
                    "start": 531,
                    "end": 551,
                    "text": "Baardman et al. 2020",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 552,
                    "end": 576,
                    "text": ", Cohen and Perakis 2018",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 577,
                    "end": 601,
                    "text": ", van Herpen et al. 2012",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "There are numerous other examples where we wish to learn heterogeneous treatment effects across many simultaneous experiments, ranging from customer promotion targeting, A/B testing on platforms, and identifying promising combination therapies in clinical trials. It is worth noting that bandits are largely used in problems where there is relatively little historical data available, e.g., due to the novelty or nonstationarity of the learning problem, or the limited population size relative to the feature dimension. In such settings, transfer learning from related data sources can be especially valuable to improve performance (Caruana 1997 , Pan et al. 2010 .",
            "cite_spans": [
                {
                    "start": 632,
                    "end": 645,
                    "text": "(Caruana 1997",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 646,
                    "end": 663,
                    "text": ", Pan et al. 2010",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Existing work has proposed general multitask learning approaches for such problems, which transfer knowledge across problem instances to improve learning. Unfortunately, existing bandit algorithms targeting the multitask learning setting do not provide better regret bounds -i.e., they do not significantly improve performance compared to treating each bandit instance as its own independent problem. Indeed, in general, transfer or multitask learning cannot improve predictive accuracy without assuming some form of shared structure connecting the different problem instances -intuitively, if the problem instances are unrelated, then learning in one instance cannot significantly improve learning in others (Hanneke and Kpotufe 2020). Our work bridges this gap by imposing a natural structure, motivated by real datasets; by designing an estimator that exploits this structure, we obtain improved regret bounds in the context dimension d.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In particular, each bandit instance j is typically parameterized with a predictive parameter vector \u03b2 j -e.g., the parameters of a linear regression model predicting the reward of each arm as a function of the current context. The shared structure we consider is that the \u03b2 j have sparse differences relative to one another. In particular, we assume that they have the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "for some \u03b2 \u2020 representing the portion of the parameter vector that is \"shared\" across locally similar problem instances; then, \u03b4 j is a problem-specific vector that represents the idiosyncratic biases specific to problem instance j. Then, we impose that the problem-specific bias \u03b4 j is \"small\", capturing the notion that the problem instances are largely similar; such an assumption implies that the difference between two problem instances is (statistically) easier to learn than either problem instance by itself (Bastani 2021 , Xu et al. 2021 ). More precisely, we assume that \u03b4 j is sparse -i.e., only a few of its components are nonzero. This is often the case when some unknown underlying mechanism systematically affects a subset of the features, e.g., some hospitals underdiagnose certain conditions in claims data compared to others (see Bastani 2021 , for illustrations on real datasets).",
            "cite_spans": [
                {
                    "start": 516,
                    "end": 529,
                    "text": "(Bastani 2021",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 530,
                    "end": 546,
                    "text": ", Xu et al. 2021",
                    "ref_id": null
                },
                {
                    "start": 848,
                    "end": 860,
                    "text": "Bastani 2021",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Even in the static, supervised learning setting, existing multitask learning algorithms (e.g., pooling data or regularizing estimates across problem instances) are not designed to leverage this structure (see \u00a71.1 for an overview of existing methods). Thus, we first propose a novel two-stage robust estimator that exploits this structure in the supervised learning setting. In the first stage, it leverages the trimmed mean from robust statistics (Rousseeuw 1991, Lugosi and Mendelson 2019) to estimate a \"shared\" model \u03b2 \u2020 across data collected from similar learning problems. 1 Then, in the second stage, it uses LASSO regression (Chen et al. 1995 , Tibshirani 1996 to efficiently learn the problem-specific bias \u03b4 j , which can be combined with our estimate of \u03b2 \u2020 to obtain the problem-specific parameter \u03b2 j . We prove finite-sample generalization bounds that show favorable performance compared to existing approaches, especially in terms of the context dimension d.",
            "cite_spans": [
                {
                    "start": 633,
                    "end": 650,
                    "text": "(Chen et al. 1995",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 651,
                    "end": 668,
                    "text": ", Tibshirani 1996",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This estimator (and the tighter confidence bounds it affords) can then be embedded into simultaneous linear contextual bandit algorithms running at each problem instance; we call the resulting multitask bandit algorithm RMBandit. It efficiently manages the bias-variance tradeoff from incorporating auxiliary data from similar bandit instances (multitask learning) in conjunction with the classical exploration-exploitation tradeoff (bandit learning). We derive upper bounds for the cumulative regret of the RMBandit, both for individual problem instances and across all instances. Our multitask learning approach improves regret (compared to running separate bandit instances) in terms of the context dimension d; importantly, this regret improvement is exponential for data-poor bandit instances, since they benefit most from transferring knowledge from similar instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We also analyze the impact of the underlying network structure on the cumulative regret. Specifically, we assume knowledge of a network that captures the similarity between any pair of bandit instances; this can be inferred based on observed covariates (e.g., geographic distance between hospitals/stores or socio-economic indices of neighborhoods served) or data from past decision-making problems (see, e.g., Crammer et al. 2008) . Then, for any given problem instance, we can optimize the \"similarity radius\" of learning problems from which to transfer knowledge, resulting in regret bounds that scale with the underlying network density.",
            "cite_spans": [
                {
                    "start": 411,
                    "end": 431,
                    "text": "Crammer et al. 2008)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Finally, we empirically evaluate our approach on both synthetic and real datasets in healthcare and pricing. Indeed, we find that the RMBandit algorithm can substantially speed up learning and improve overall performance compared to existing bandit and multitask learning algorithms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our work relates to the literature on multitask learning and contextual bandits; we contribute on both fronts. Our approach builds on the literature on robust and high-dimensional statistics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "There has been significant interest from the machine learning community on developing methods that combine data from multiple learning problems (typically referred to as tasks). These can be broadly classified into three categories: (i) multitask learning (Caruana 1997) , where one aims to learn jointly across a fixed set of similar tasks, (ii) transfer learning (Pan et al. 2010 ), a special case of multitask learning, where the goal is to maximize performance on a distinguished \"target\" task, and (iii) meta-learning (Finn et al. 2017) , where one aims to learn from historical tasks to improve learning in similar future tasks. Our problem is an instance of multitask learning, since our goal is to learn across a fixed set of bandit instances with related unknown parameters.",
            "cite_spans": [
                {
                    "start": 256,
                    "end": 270,
                    "text": "(Caruana 1997)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 365,
                    "end": 381,
                    "text": "(Pan et al. 2010",
                    "ref_id": null
                },
                {
                    "start": 523,
                    "end": 541,
                    "text": "(Finn et al. 2017)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "Multitask Learning. Naturally, if the tasks are sufficiently different, then learning in one task cannot substantially improve learning in other tasks (Hanneke and Kpotufe 2020). Thus, a common approach in machine learning is to assume that the underlying parameters across tasks are close in 2 norm. Joint learning can then be operationalized by regularizing the estimated parameters together, e.g., through ridge (Evgeniou and Pontil 2004) or kernel ridge (Evgeniou et al. 2005) regularization. Alternatively, one can employ a shared Bayesian prior across tasks (Raina et al. 2006, Gupta and Kallus 2021) or simply pool data from nearby tasks (Ben-David et al. 2010 , Crammer et al. 2008 ). However, these approaches do not improve performance bounds beyond constants; in general, one must impose (and exploit) additional structure to obtain nontrivial theoretical improvements. Bastani (2021) uses real datasets to motivate the assumption that the parameters across tasks are close in 0 norm. This structure motivates a two-step estimator of transfer learning using LASSO regression, yielding improved bounds in the feature dimension d for supervised learning (Bastani 2021 , Li et al. 2020a , Tian and Feng 2021 and unsupervised learning (Xu et al. 2021) . One can further impose that the underlying parameters for each task are sparse, sharing the same support (Lounici et al. 2009 ) or similar covariance matrices (Li et al. 2020a, Tian and Feng 2021) across tasks; we do not make these assumptions since the applications we consider often have dense underlying parameters (see, e.g., Bastani 2021) and the covariance matrices vary widely across tasks due to covariate shifts (e.g., due to different customer populations at hospitals or stores, see Subbaswamy and Saria 2020).",
            "cite_spans": [
                {
                    "start": 415,
                    "end": 441,
                    "text": "(Evgeniou and Pontil 2004)",
                    "ref_id": null
                },
                {
                    "start": 458,
                    "end": 480,
                    "text": "(Evgeniou et al. 2005)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 564,
                    "end": 593,
                    "text": "(Raina et al. 2006, Gupta and",
                    "ref_id": null
                },
                {
                    "start": 594,
                    "end": 606,
                    "text": "Kallus 2021)",
                    "ref_id": null
                },
                {
                    "start": 645,
                    "end": 667,
                    "text": "(Ben-David et al. 2010",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 668,
                    "end": 689,
                    "text": ", Crammer et al. 2008",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 881,
                    "end": 895,
                    "text": "Bastani (2021)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1163,
                    "end": 1176,
                    "text": "(Bastani 2021",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1177,
                    "end": 1194,
                    "text": ", Li et al. 2020a",
                    "ref_id": null
                },
                {
                    "start": 1195,
                    "end": 1215,
                    "text": ", Tian and Feng 2021",
                    "ref_id": null
                },
                {
                    "start": 1242,
                    "end": 1258,
                    "text": "(Xu et al. 2021)",
                    "ref_id": null
                },
                {
                    "start": 1366,
                    "end": 1386,
                    "text": "(Lounici et al. 2009",
                    "ref_id": null
                },
                {
                    "start": 1591,
                    "end": 1604,
                    "text": "Bastani 2021)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "We build on the last stream of two-step estimators for the multitask learning problem. However, we need a fundamentally different algorithmic approach; as we discuss in \u00a73, the challenge is that the sparse bias terms can be poorly aligned across tasks, and thus classical estimates of the shared model (e.g., via data pooling or model averaging) destroy task-specific sparse structure and therefore cannot be debiased using LASSO (as was the case in prior work). Instead, we take the view that each component where the bias terms align poorly suffers \"corruptions\" to the shared model; we use a counting argument to show that either the number of corruptions must be small, or the component is one of a small number of well-aligned components. We use robust statistics to overcome corruptions for poorly-aligned components and LASSO to debias well-aligned components. To the best of our knowledge, our work proposes the first such combination of robust statistics and high-dimensional regression, yielding improved bounds for multitask learning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "The first step of our approach (using robust statistics) relates to recent robust machine learning methods that can handle adversarial corruptions to a small fraction of the data (Yin et al. 2018, Konstantinov and Lampert 2019) . These approaches do not apply to our setting -as a consequence of our sparse differences assumption, we show that only a few similar tasks (as opposed to observations or features) have unknown parameters that are \"corrupted\" in most dimensions.",
            "cite_spans": [
                {
                    "start": 179,
                    "end": 213,
                    "text": "(Yin et al. 2018, Konstantinov and",
                    "ref_id": null
                },
                {
                    "start": 214,
                    "end": 227,
                    "text": "Lampert 2019)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "Rather, we build on the classical trimmed mean estimator (Rousseeuw 1991, Lugosi and Mendelson 2021) . The second step (using LASSO) builds on the high-dimensional statistics literature (Tibshirani 1996 , Candes and Tao 2007 , Bickel et al. 2009 , B\u00fchlmann and Van De Geer 2011 .",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 84,
                    "text": "(Rousseeuw 1991, Lugosi and",
                    "ref_id": null
                },
                {
                    "start": 85,
                    "end": 100,
                    "text": "Mendelson 2021)",
                    "ref_id": null
                },
                {
                    "start": 186,
                    "end": 202,
                    "text": "(Tibshirani 1996",
                    "ref_id": null
                },
                {
                    "start": 203,
                    "end": 224,
                    "text": ", Candes and Tao 2007",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 225,
                    "end": 245,
                    "text": ", Bickel et al. 2009",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 246,
                    "end": 277,
                    "text": ", B\u00fchlmann and Van De Geer 2011",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "Multitask Bandits. A few recent papers have studied multitask learning across contextual bandit instances; however, to the best of our knowledge, a key drawback of these algorithms is that none of them ultimately improve the regret bounds for any bandit instance beyond constants. Similar to the multitask learning literature discussed above, one strategy is to regularize the learned parameters for a given bandit instance towards parameters for similar bandit instances (Soare et al. 2014).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "For example, Cesa-Bianchi et al. (2013) and Deshmukh et al. (2017) leverage parameter updates that are similar to kernel ridge regularization, and Gentile et al. (2014) additionally perform a pre-processing step clustering bandit instances prior to such regularization; however, the resulting regret bound for a single bandit instance may actually increase in the number of instances N .",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 39,
                    "text": "Cesa-Bianchi et al. (2013)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 44,
                    "end": 66,
                    "text": "Deshmukh et al. (2017)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "Another popular approach is to impose a shared Bayesian prior across bandit instances (Cella et al. 2020 , Bastani et al. 2021c , Kveton et al. 2021 ), but they also obtain similar results; furthermore, these algorithms require the more restrictive assumption that bandit instances appear sequentially (rather than simultaneously) in order to learn the prior.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 104,
                    "text": "(Cella et al. 2020",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 105,
                    "end": 127,
                    "text": ", Bastani et al. 2021c",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 128,
                    "end": 148,
                    "text": ", Kveton et al. 2021",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "We embed our robust multitask estimator across N linear contextual bandit instances; the specific setting and assumptions we consider are based on Goldenshluger and Zeevi (2013) and Bastani and Bayati (2020) . We demonstrate that, unlike prior work, we obtain improved regret bounds for each bandit instance in the context dimension d under the practically-motivated sparse differences assumption; the improvement we obtain is exponential for data-poor instances where shared learning is most helpful. We also study the network structure underlying the bandit instances to better understand how one may choose N . In particular, as we incorporate more instances, we reduce variance (since we have more data) but we increase bias (since we are incorporating observations from disparate sources). We characterize the N that minimizes this bias-variance tradeoff to obtain regret bounds that scale with the density of the underlying bandit network.",
            "cite_spans": [
                {
                    "start": 182,
                    "end": 207,
                    "text": "Bastani and Bayati (2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Related Literature"
        },
        {
            "text": "We highlight our main technical contributions below:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions"
        },
        {
            "text": "1. We introduce a new estimator for multitask learning, which leverages a unique combination of robust statistics (for learning a shared model across tasks) and LASSO (for debiasing this shared model for a specific task). We prove upper and lower bounds demonstrating that our estimator outperforms a number of intuitive baseline approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions"
        },
        {
            "text": "2. We embed our estimator in a multitask bandit algorithm, resulting in regret bounds that exhibit an improved scaling in the context dimension d; notably, we show that this improvement is exponential for data-poor bandit instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions"
        },
        {
            "text": "3. We also examine regret as a function of the underlying network structure, where vertices represent bandit instances and edges capture their pairwise similarity. This sheds light on choosing the number of bandit instances for estimation, minimizing a bias-variance tradeoff.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions"
        },
        {
            "text": "Finally, we conclude with numerical experiments on synthetic and real datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions"
        },
        {
            "text": "Before describing our model and assumptions, we establish some notations. Let [n] denote the index set {1, 2, \u00b7 \u00b7 \u00b7 , n}. For any vector \u03b2 \u2208 R d and i \u2208 [d], let \u03b2 (i) be the i th element of \u03b2; for any index set I \u2286 [d], let \u03b2 I denote the vector obtained by setting the elements of \u03b2 that are not in I to zero.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "We use superscripts to index the bandit problem instance, e.g., the design matrix X j corresponds to the covariates observed at bandit instance j. We use a subscript without parentheses to denote the arm, e.g. \u03b2 j k represents the k th arm for bandit instance j. For any design matrix X \u2208 R n\u00d7d with n observations, let \u03a3 = X X n be its sample covariance matrix. Further, for any square matrix X \u2208 R d\u00d7d , let \u03bb min (X) and \u03bb max (X) denote its minimum and maximum eigenvalues respectively. We use the subscript (i, \u00b7) to represent the i th row of a matrix, the subscript (\u00b7, j) the j th column, and the subscript (i, j) the element at location (i, j), e.g., X (i,\u00b7) is the i th row of matrix X. We use the standard notation O(\u00b7), \u2126(\u00b7) and \u0398(\u00b7) to characterize the asymptotic growth rate of a function, and\u00d5(\u00b7),\u03a9(\u00b7) and\u0398(\u00b7) when logarithmic terms are omitted.",
            "cite_spans": [
                {
                    "start": 661,
                    "end": 666,
                    "text": "(i,\u00b7)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "We consider N distinct service providers, each facing a linear contextual bandit learning problem, e.g., N hospitals in Example 1 or N stores in Example 2 respectively. Keeping with the traditional contextual bandit framework, the decision-maker at each instance has access to the same K potential arms (decisions) with uncertain and context-dependent rewards.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Arrivals. Let T be the overall time horizon across all bandit instances. At each time step t, a new individual arrives for service at one of the N bandit instances, given by the random variable Z t \u2208 [N ]. Naturally, some bandit instances may receive more arrivals than others, e.g., service providers with more traffic. Thus, we model the random arrival process as follows: every instance j is associated with a probability p j such that N i=1 p i = 1. At time t, the new individual arrives at instance j with probability p j ; in other words, Z t follows a categorical distribution CG(p) with p = p 1 \u00b7 \u00b7 \u00b7 p N . Thus, in expectation, instance j will serve p j T individuals. We will consider two relevant settings: (1) all instances receive similar traffic (i.e., p j = \u0398(1/N ) for all j \u2208 [N ]), and (2) a single instance j \u2208 [N ] is relatively \"data-poor\", receiving far less traffic than neighbouring instances (i.e., p j = \u0398(p i /d 2 ) for i = j). In the data-poor setting, we focus on a single data-poor instance for simplicity; our results generalize straightforwardly to the case where there are a constant number of data-poor instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Each individual is also associated with a context vector X t \u2208 R d . In practice, different service providers face different customer populations, which will be reflected in the probability distribution of context vectors observed at that instance. Thus, we allow the context distribution to vary as a function of Z t . That is, if Z t = j, then X t is drawn i.i.d. from an unknown distribution P j X . Rewards. At each instance, the local decision-maker has access to the same K arms (decisions).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "However, the rewards of these arms likely vary across instances (e.g., due to systematic differences among service providers or local customer populations, see discussion in Examples 1-2) as well as context distributions. Thus, we model the reward of pulling arm k for an individual with context vector X t at instance j as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Here, each arm k at instance j is parameterized by an unknown arm parameter \u03b2 j k \u2208 R d , and the corresponding noise j t is an i.i.d. \u03c3 j -subgaussian random variable (see Definition 1); note that the variance of the noise term can depend on the instance j.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "for any t \u2208 R.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "The formulation above captures any N linear contextual bandit instances; we now impose our assumption that these bandit instances are similar. As discussed in the introduction, for each arm k \u2208 [K], we assume the arm parameters are sparse relative to one another-i.e., \u03b2 j k \u2212 \u03b2 i k is sparse for each pair i, j \u2208 [N ]. It is easy to see that an equivalent assumption is that there exists \u03b2 \u2020 k \u2208 R d such that we can write",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": ". Intuitively, \u03b2 \u2020 k is a shared vector that captures the similarity across all N bandit instances, and \u03b4 j k is an instance-specific vector that captures the heterogeneity/idiosyncrasies inherent to learning problem j. This key assumption enables us to learn across instances for a given arm k. Note that the choice of the shared vector \u03b2 \u2020 k here is not unique -e.g., changes to O(s) components of \u03b2 \u2020 k preserves the sparsity of \u03b4 j k up to constant factors -and therefore is not identifiable. As we describe in \u00a73, it suffices for our purposes to estimate any vector \u03b2 \u2020 k that lies in an O(s) ball in 0 norm centered around an admissible choice of \u03b2 \u2020 k . Finally, we note that we do not assume that the individual arm parameters {\u03b2 j k } k\u2208[K],j\u2208 [N ] or the shared models {\u03b2 \u2020 k } k\u2208 [K] are themselves sparse, since these rewards can often depend on the entire set of observed covariates (see, e.g., discussion in Bastani 2021).",
            "cite_spans": [
                {
                    "start": 753,
                    "end": 757,
                    "text": "[N ]",
                    "ref_id": null
                },
                {
                    "start": 791,
                    "end": 794,
                    "text": "[K]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Objective. We seek to construct a sequential decision-making policy \u03c0 that learns the arm param-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": ",j\u2208[N ] over time and across instances, in order to maximize expected reward for each arrival. The overall policy \u03c0 is composed of sub-policies \u03c0 j t : X j \u2192 [K] at each instance j. We measure the performance of \u03c0 by its cumulative expected regret -the standard metric in the analysis of bandit algorithms (Lai and Robbins 1985) -modified naturally to extend across multiple heterogeneous bandit instances. In particular, when Z t = j (an individual arrives at instance j), we compare ourselves to the oracle policy \u03c0 j * at instance j, which knows the corresponding arm parameters {\u03b2 j k } k\u2208 [K] in advance. Naturally, \u03c0 j * chooses the arm with the best expected reward, i.e. \u03c0 j * (X t ) = arg max k\u2208[K] X t \u03b2 j k for any X t such that Z t = j. The expected regret incurred by pulling arm \u03c0 j t = k at time t given an arrival at instance j is thus",
            "cite_spans": [
                {
                    "start": 594,
                    "end": 597,
                    "text": "[K]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "which is simply the difference between the expected reward of using \u03c0 j * and \u03c0 j t . Further taking the expectation over the randomness in where the individual arrives, the expected regret of an overall policy composed of sub-policies {\u03c0 j t } j\u2208 [N ] at time t is",
            "cite_spans": [
                {
                    "start": 248,
                    "end": 252,
                    "text": "[N ]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Our goal is to derive a policy that minimizes the cumulative expected regret across all instances,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "We also study the instance-specific cumulative expected regret T t=1 p j r j t for j \u2208 [N ] for standard and data-poor instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Network Structure. Finally, we consider the dependence of the regret on the underlying network structure of bandit instances, when available. In particular, we consider a fully-connected network with N vertices (each representing a bandit instance) and edge weights s i,j capturing the pairwise similarities between any two instances (i, j) \u2208 [N ] \u00d7 [N ] as measured by our sparse difference metric, i.e., \u03b2 j \u2212\u03b2 i 0 = s i,j . Note that this graph is undirected since s i,j = s j,i ; furthermore, if two instances i and j are unrelated, then they trivially satisfy s i,j = d.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Such a graph can be inferred based on observed covariates (e.g., geographic distance between hospitals/stores or socio-economic indices of neighborhoods served) or data from past decisionmaking problems (see, e.g., the disparity matrix in Crammer et al. 2008) . Then, for any given problem instance j, we can optimize the subset of instances Q j \u2286 [N ] from which to transfer knowledge. For simplicity, we assume a strategy where we fix a thresholds, and take all instances with sparsity at mosts -i.e.,",
            "cite_spans": [
                {
                    "start": 239,
                    "end": 259,
                    "text": "Crammer et al. 2008)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "We denote the effective number of instances by\u00d1 = |Q j |. Under this assumption, there is a tradeoff between choosing smallers, which yields smaller\u00d1 (resulting in lower bias but larger variance), and largers, which yields larger\u00d1 (resulting in higher bias but smaller variance). The optimal choice ofs (and correspondingly,\u00d1 ) depends on the relationship betweens and\u00d1 . We consider a natural power law scaling -i.e.,s = min(\u00d1 \u03b1 , d),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "for some \u03b1 \u2265 0. In other words, as we increase the number of neighbouring instances we include, our sparsity parameter increases by some power law\u00d1 \u03b1 until it eventually hits the maximum possible value d. Our main result allows us to easily compute the optimal choice of\u00d1 , resulting in regret bounds that scale with the network density \u03b1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Our first assumption is standard in the literature and states that our features and regression parameters are bounded by a constant.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "Assumption 1 (Boundedness). There exist a positive constant x max such that each feature is bounded by x max , i.e., X \u221e \u2264 x max for any X \u2208 X j , j \u2208 [N ], and a positive constant b such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "As discussed earlier, we embed our robust multitask estimator into the high-dimensional linear contextual bandit setting studied in Bastani and Bayati (2020) ; therefore, our next three assumptions are directly adapted from this literature. We note that the remaining assumptions in this section are only required for the regret analysis of RMBandit ( \u00a74) and not for the static performance bounds of our robust multitask estimator ( \u00a73).",
            "cite_spans": [
                {
                    "start": 132,
                    "end": 157,
                    "text": "Bastani and Bayati (2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "Our second assumption is a mild margin condition that ensures that the density of the context distribution P j X for each instance j is bounded near a decision boundary (i.e., the intersection of the hyperplane given by {X | X \u03b2 j k = X \u03b2 j k } and X j for any pair of arms k = k). It allows for any bounded, continuous features, as well as any discrete features with a finite number of values.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "Assumption 2 (Margin Condition). For any arms k and k of any instance j \u2208 [N ], there",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "Our third assumption is that, for each instance j \u2208 [N ], our K arms can be split into two mutually exclusive sets:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "1. Optimal arms k \u2208 K j opt that are strictly optimal in expected reward (by at least h) for any contexts drawn from a set U j k \u2282 X j with positive support on P j X , i.e., P[X \u2208 U j k | Z = j] \u2265 p * . 2. Sub-optimal arms k \u2208 K sub that are strictly sub-optimal in expected reward (by at least h) for all contexts in X j .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "In other words, we assume that every arm is either optimal (by at least h) for at least some individuals, or sub-optimal for all individuals (by at least h). This assumption will ensure that every arm in K j opt will roughly receive at least O(p j T ) samples under a regret-minimizing policy on instance j, ensuring that we can quickly learn accurate parameter estimates for all optimal arms. Assumption 3 (Arm Optimality). All K arms at any given instance j belong to one of two mutually exclusive sets: optimal arms K j opt or suboptimal arms K j sub . There exists some h > 0 such that: (i) each k \u2208 K j sub satisfies X \u03b2 j k < max k =k X \u03b2 j k \u2212 h for any context X \u2208 X j , and (ii) each k \u2208 K j opt is optimal on a set of contexts",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "which has positive measure, i.e., P X \u2208 U j k | Z = j \u2265 p * for some p * > 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "Our fourth assumption ensures that linear regression is feasible within the set U j k ; this is a mild assumption since it is with respect to the true covariance matrix, which only requires that no features are perfectly collinear in this set. In contrast, the sample covariance matrix may not be positive-definite at time t, since we may have observed too few samples from that instance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "Assumption 4 (Positive-Definiteness). For every arm k \u2208 [K] and instance j \u2208 [N ], the true",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "The assumptions thus far are standard and have been adapted directly from the literature. We now introduce a new assumption motivated by our multitask setting. In general, an arm k can be optimal (belong to K j opt ) at one bandit instance j and be sub-optimal (belong to K i sub ) at a neighboring instance i. This implies that we will observe O(p j T ) samples from arm k at instance j but only O(log(p i T )) samples at instance i under a regret-minimizing policy; in other words, instance j cannot effectively transfer knowledge from instance i about arm k. Thus, we impose that if an arm k \u2208 [K] is optimal for any instance j, it is also optimal for at least some subset of the N neighboring instances so that we have enough observations to enable multitask learning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "Assumption 5 (Optimality Density). For each k \u2208 [K], the set of instances W k = {j \u2208 [N ] | k \u2208 K j opt } has cardinality at least \u03c1N for some \u03c1 > 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Assumptions"
        },
        {
            "text": "First, we study the static, supervised learning setting. In this section, we overview ( \u00a73.2) and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Robust Multitask Estimator"
        },
        {
            "text": "provide intuition ( \u00a73.3) on the design of our robust multitask estimator; we provide theoretical performance guarantees in the standard ( \u00a73.4) and data-poor ( \u00a73.5) regimes, contrasting these guarantees with those of intuitive baseline estimators ( \u00a73.6).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Robust Multitask Estimator"
        },
        {
            "text": "In what follows, we focus on a single arm k \u2208 [K] across different instances j \u2208 [N ]; thus, we drop the index k throughout this section. Recall that the reward for context X t \u2208 R d at instance j is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "where the j t are i.i.d. \u03c3 j -subgaussian random variables. For each instance j, let the matrix X j \u2208 R n j \u00d7d encode the n j observed context vectors, and the vector Y j \u2208 R n j encode the corresponding observed rewards. Our goal is to use {(X j , Y j )} j\u2208[N ] to estimate the unknown parameter vector \u03b2 j for each instance j.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "In this section, we only assume Assumption 1 on boundedness and the following that the observed covariance matrices are positive-definite, so that ordinary least squares (OLS) is well-defined:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "Assumption 6 (Positive-Definiteness). There exists a positive constant \u03c8 such that for any",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "Note that this assumption is not needed in the bandit setting; instead, our bandit algorithm adaptively labels data in a way that ensures that this assumption holds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "Next, we define and briefly review the trimmed mean estimator from the classical robust statistics literature (Rousseeuw 1991, Lugosi and Mendelson 2021) , which computes the mean of a distribution P given samples {Z j } j\u2208 [n] . A typical setting is as follows: most of the samples are i.i.d. (i.e., Z j \u223c P), but a small fraction (indexed by the unknown set J \u2286 [n]) are \"corrupted\" and can be arbitrary. In such settings, the traditional mean can be arbitrarily biased, but the trimmed mean can obtain strong guarantees given a bound on the number of corrupted samples |J | < \u03b6n for some \u03b6 < 1/2. The trimmed mean estimator first sorts the samples in increasing order to obtain Z j 1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 Z jn . Then, given a hyperparameter \u03c9 > \u03b6, it removes the top and bottom n\u03c9 values and takes the mean of the remaining ones-i.e.,",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 137,
                    "text": "(Rousseeuw 1991, Lugosi and",
                    "ref_id": null
                },
                {
                    "start": 138,
                    "end": 153,
                    "text": "Mendelson 2021)",
                    "ref_id": null
                },
                {
                    "start": 224,
                    "end": 227,
                    "text": "[n]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "Intuitively, this estimator is robust since either the corruptions are among the deleted values, or they are sufficiently close to the true mean that they do not significantly affect the estimate.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "Our robust multitask estimator is summarized in Algorithm 1. At a high level, the first step combines high-variance OLS estimators across instances using robust statistics to estimate the shared parameter \u03b2 \u2020 (up to O(s) deviations in 0 norm); then, the second step uses LASSO regression to debias this estimate for each specific instance j \u2208 [N ].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "In more detail,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "\u2022 Step 1 (Estimating \u03b2 \u2020 ): We compute the usual OLS estimator",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "independently. Then, we combine these estimates using the element-wise trimmed mean to estimate the shared parameter vector \u03b2 \u2020 RM \u2248 \u03b2 \u2020 -i.e., for each i \u2208 [d],",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "where \u03c9 > 0 is the trimmed mean hyperparameter that we specify later.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "\u2022",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "Step 2 (Estimating \u03b2 j ) : Next, we use LASSO regression to compute \u03b2 j RM , leveraging our assumption that the instance-specific bias term \u03b2 j \u2212 \u03b2 \u2020 is sparse:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "Algorithm 1 Robust Multitask Estimator Inputs: Instance-specific regularization parameters {\u03bb j } j\u2208 [N ] , trimmed mean hyperparameter \u03c9",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 105,
                    "text": "[N ]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "; \u03c9) be the element-wise trimmed mean (i.e., mean with the top and bottom \u03c9 quantiles removed) end for",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "We make a minor modification in the data-poor regime: we omit the data-poor instance j from the trimmed mean in Step 1 since \u03b2 j ind has particularly high variance (see \u00a73.5 for details).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Overview"
        },
        {
            "text": "We now provide intuition for our design choices relative to alternative strategies; the corresponding error rates are summarized in Table 1 (see \u00a73.6 for precise definitions and more details).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 132,
                    "end": 139,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Design Intuition"
        },
        {
            "text": "Bound Type Standard Regime Data-Poor Regime Table 1 Comparison of parameter estimation error sup G E \u03b2 j \u2212 \u03b2 j 1 (see \u00a73.6 for the precise definitions of these estimators); constants and logarithmic factors are omitted for clarity. The upper bound for our robust multitask estimator outperforms the worst-case lower bounds for intuitive baseline estimators under the same set of problem settings G; our improvement is largest for data-poor instances.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 44,
                    "end": 51,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "One strategy is to simply use the independent OLS estimator \u03b2 j ind (from Step 1) to estimate \u03b2 j ; this is an unbiased estimator, but has very high variance since it only uses the limited data observed in instance j and does not leverage shared structure across instances. As a result, it has high error when n j is small (see Table 1 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 328,
                    "end": 335,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "An alternative strategy is to estimate the shared model \u03b2 \u2020 using data across instances, e.g., the averaging estimator takes the model average of the independent estimators:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "This estimator has low variance since it leverages data across instances, but it is biased since it does not account for the instance-specific idiosyncratic bias term \u03b4 j = \u03b2 j \u2212 \u03b2 \u2020 (the trimmed mean estimator \u03b2 \u2020 RM that we actually use in Step 1 of Algorithm 1 suffers the same bias; we will explain the purpose of using the trimmed mean shortly). Similarly, estimating the shared model \u03b2 \u2020 through OLS on data pooled across instances suffers the same drawbacks. As shown in Table 1 , the error of such estimators never approaches zero due to the bias term \u03b4 j .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 478,
                    "end": 485,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "Thus, a natural two-step strategy to achieve low variance and low bias is to first compute an estimate \u03b2 \u2020 of the shared parameter, and then try to debias it to estimate \u03b2 j . Since the bias \u03b2 j \u2212 \u03b2 \u2020 is s-sparse by assumption, it should intuitively be easier to debias \u03b2 \u2020 than to directly estimate \u03b2 j .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "Along these lines, consider the following averaging multitask estimator, denoted by the subscript AM. Here, we estimate the shared parameter via model averaging, \u03b2 \u2020 AM = \u03b2 j avg . Then, we use an 1 penalty on \u03b2 \u2212 \u03b2 \u2020 AM (i.e., LASSO regression) on data from instance j to debias \u03b2 \u2020 AM :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "(Note that this strategy is identical to Algorithm 1, except it uses the traditional mean instead of the trimmed mean in Step 1.) To see why equation (4) helps, suppose we had a perfect estimate of the shared model \u03b2 \u2020 AM = \u03b2 \u2020 ; then, \u03b2 j \u2212 \u03b2 \u2020 AM would be s-sparse, in which case LASSO requires exponentially fewer observations for recovering \u03b2 j (relative to \u03b2 \u2020 AM ) than traditional OLS. The issue with the approach outlined above is that \u03b2 j \u2212 \u03b2 \u2020 AM is not s-sparse, or even \"close\" to being s-sparse. To illustrate, we can decompose",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "Here, \u03b2 \u2020 AM is the value that \u03b2 \u2020 AM converges to as n j \u2192 \u221e, j \u2208 [N ]. Note that \u03b2 \u2020 AM does not converge to \u03b2 \u2020 ; in fact, as noted in the problem formulation, \u03b2 \u2020 is not identifiable. The first term in the decomposition is sparse, and the third term becomes small as n = j\u2208[N ] n j becomes large (since \u03b2 \u2020 AM effectively uses all n samples to estimate \u03b2 \u2020 AM ); since LASSO can effectively recover parameters",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "Illustration of Step 1 of our robust multitask estimator for debiasing data collected from multiple instances. Blue squares depict the support; the shade of blue depicts the magnitude. Ipoor represents the index set which can be debiased using the trimmed mean across instances, while I well represents the index set which can be debiased using a subsequent LASSO regression for the target instance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "that are approximately sparse, these two terms are not problematic. The key issue is the second term:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "which is neither sparse nor small. This is illustrated in Figure 1 : since the support of the different bias terms {\u03b4 i } i\u2208 [N ] can be \"poorly-aligned\" (i.e., the idiosyncrasies for each instance affect a different subset of features), the average across instances can result in \u03b4 \u2020 AM having as many as min{N s, d} nonzero components (even as n j \u2192 \u221e, j \u2208 [N ]). This in turn implies that \u03b2 j \u2212 \u03b2 \u2020 AM is not sparse even for moderate values of N such as N = \u2126(d/s); thus, we cannot use LASSO to efficiently debias \u03b2 \u2020 AM . Other classical estimators of the shared parameter (e.g., data pooling) suffer the same issue.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 129,
                    "text": "[N ]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 58,
                    "end": 66,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "Our robust multitask estimator addresses this issue by using the trimmed mean \u03b2 \u2020 RM in Step 1; we will show that this converges to a value \u03b2 \u2020 RM (as n j \u2192 \u221e, j \u2208 [N ]) such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "In particular, we have the following decomposition:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "As discussed above, the third term becomes small as n becomes large. Since the second term \u03b4 \u2020",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator Estimation Error"
        },
        {
            "text": "is O(s)-sparse, \u03b2 j \u2212 \u03b2 \u2020 RM is approximately O(s)-sparse; thus, LASSO can efficiently debias \u03b2 \u2020 RM . We now use a counting argument to illustrate why \u03b4 \u2020 RM is O(s)-sparse. As Figure 1 illustrates, we can separate the components i \u2208 [d] into two groups: ones that are \"well-aligned\" (i \u2208 I well ) and ones that are \"poorly-aligned\" (i \u2208 I poor ); see Definition 2 below. A poorly-aligned component i is one where very few instances j \u2208 [N ] are biased in this component, i.e., \u03b2 j (i) = \u03b2 \u2020 (i) . Intuitively, for each such component, the trimmed mean estimator treats these biased instances as \"corruptions\" to our samples {\u03b2 j (i) } j\u2208 [N ] , and trims them (with high probability) when computing the average to obtain an unbiased estimate of \u03b2 \u2020 (i) . On the other hand, well-aligned components may remain arbitrarily biased. However, the pigeonhole principle implies that there cannot be many wellaligned components; thus, these components (in addition to the components affected by the sparse instance-specific bias term) can be efficiently debiased by LASSO in Step 2. We now formalize this.",
            "cite_spans": [
                {
                    "start": 640,
                    "end": 644,
                    "text": "[N ]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 178,
                    "end": 186,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "RM"
        },
        {
            "text": "Definition 2 (Well-and poorly-aligned components). Given a constant \u03b6 \u2208 R, a com-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RM"
        },
        {
            "text": "as n j 's become large. That is, we aim to correctly estimate all the poorly-aligned components, but the well-aligned components can be anything. We estimate each component \u03b2 \u2020 (i) using the trimmed mean, which is robust to a small fraction \u03b6 of arbitrarily corrupted samples. For a given component i, let the corresponding corrupted instances be",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RM"
        },
        {
            "text": "By definition, for i \u2208 I \u03b6 poor , we have |J i | < N \u03b6. Thus, we can use the trimmed mean estimator to estimate \u03b2 \u2020 (i) :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RM"
        },
        {
            "text": ", \u03c9 for some \u03c9 > \u03b6. This strategy ensures that \u03b2 \u2020 RM,(i) \u2248 \u03b2 \u2020 (i) for each poorly-aligned component as desired. Now, note that there can only be a few well-aligned components. In particular, out of the N d total components in {\u03b2 j (i) } j\u2208 [N ] , there are at most N s components where \u03b2 j (i) = \u03b2 \u2020 (i) as a consequence of our problem formulation. Then, by the pigeonhole principle, we have",
            "cite_spans": [
                {
                    "start": 242,
                    "end": 246,
                    "text": "[N ]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "RM"
        },
        {
            "text": "In other words, there are at most s/\u03b6 well-aligned components, so \u03b4 \u2020 RM is O(s)-sparse as desired (for a constant choice of \u03b6). Thus, we can efficiently debias our estimate using LASSO in Step 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RM"
        },
        {
            "text": "Next, we provide bounds on the parameter estimation error of our robust multitask estimator. Our first result bounds the error of the trimmed mean estimator.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Analysis"
        },
        {
            "text": "The proof is provided in Appendix A.1. We use Lemma 1 to show that \u03b2 \u2020 RM,(i) is close to the true mean \u03b2 \u2020 (i) for poorly-aligned components i \u2208 I \u03b6 poor . This result is similar to classical results from robust statistics (see, e.g., Li 2019), but existing results typically assume that the uncorrupted samples are i.i.d., whereas we only require independence (since we wish to apply it to { \u03b2 j ind,(i) } j\u2208[N ] , which are not identically distributed).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Analysis"
        },
        {
            "text": "Next, we have the following error bound for our robust multitask estimator on each instance j.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Analysis"
        },
        {
            "text": "Theorem 1. For any \u03b6 \u2208 (0, 1/2), any instance j \u2208 [N ], any regularization parameter \u03bb j \u2208 R >0 , and any \u03b7, c \u2208 R >0 satisfying 0 < \u03b7 \u2264 1/2 \u2212 c \u2212 \u03b6, the robust multitask estimator \u03b2 j RM of \u03b2 j computed by Algorithm 1 satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Analysis"
        },
        {
            "text": "The proof is provided in Appendix A.2. The following corollary bounds the estimation error in the standard regime where each instance j \u2208 [N ] has a similar number of observations n j :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Analysis"
        },
        {
            "text": "2d log 3d \u03b4 , the robust multitask estimator computed by Algorithm 1 satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Analysis"
        },
        {
            "text": "Recall that the independent OLS estimator \u03b2 j ind on instance j yields an estimation error of O( d \u221a n j ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Analysis"
        },
        {
            "text": "In contrast, if the number of instances is at least N = \u2126(d/s), our robust multitask estimator has an estimation error of at most\u00d5 sd n j with high probability, i.e., it provides an improvement of \u221a d, which can be substantial in high dimension. When we have very few instances from which to share knowledge (i.e., N = o(d)), multitask learning is less effective and we obtain the same estimation error as the independent OLS estimator. As we discuss next, our improvement is much larger in the data-poor regime.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Analysis"
        },
        {
            "text": "Multitask learning is especially effective in the data-poor regime, where the target instance j receives substantially fewer observations compared to other instances. In particular, we consider the case where n j = \u0398( n i d 2 ) for all i = j. We focus on a single data-poor instance for simplicity; our results generalize straightforwardly to the case where there are a constant number of data-poor instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Poor Regime"
        },
        {
            "text": "The following corollary bounds the estimation error for data-poor instance j:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Poor Regime"
        },
        {
            "text": "2d log 3d \u03b4 , and \u03b6 be any constant such that \u03b6 < 1 2 \u2212 c, the robust multitask estimator computed by Algorithm 1 satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Poor Regime"
        },
        {
            "text": "In this setting, the estimation error of our robust multitask error depends only logarithmically on the context dimension d (as opposed to linearly for independent OLS). In other words, we obtain an exponential reduction in estimation error in d, which is especially valuable in high dimension.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Poor Regime"
        },
        {
            "text": "Finally, we discuss how the estimation error of our robust multitask estimator compares with the baseline approaches discussed in \u00a73.3. In particular, we contrast the upper bounds we derived for our estimator with lower bounds for these baselines in both the standard and data-poor regimes; these bounds are summarized in Table 1 . Detailed statements and proofs are provided in Appendix B.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 322,
                    "end": 329,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Comparison with Baselines"
        },
        {
            "text": "We characterize the estimation error of an estimator \u03b2 j through the following loss function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with Baselines"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with Baselines"
        },
        {
            "text": "satisfies our assumptions in \u00a73.1, P j is the distribution of the noise terms j , and the expectation is taken with respect to j 's. This choice of G ensures that our upper and lower bounds are with respect to the same class of problem instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with Baselines"
        },
        {
            "text": "We consider the following estimators:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with Baselines"
        },
        {
            "text": "\u2022 Independent OLS (Appendix B.1): This is the OLS estimator \u03b2 j ind = (X j X j ) \u22121 X Y j trained on data only from instance j, i.e., it does not transfer knowledge across instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with Baselines"
        },
        {
            "text": "ind is a common approach that averages the independent OLS estimates across instances (see, e.g., Dobriban and Sheng 2021) .",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 122,
                    "text": "Dobriban and Sheng 2021)",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Comparison with Baselines"
        },
        {
            "text": "is a common approach that pools data across instances to train a single OLS estimator (see, e.g., Crammer et al. 2008 , Ben-David et al. 2010 ):",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 117,
                    "text": "Crammer et al. 2008",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 118,
                    "end": 141,
                    "text": ", Ben-David et al. 2010",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Comparison with Baselines"
        },
        {
            "text": "\u2022 Averaging multitask (Appendix B.4): This two-step estimator \u03b2 j AM is described in detail in \u00a73.3. It is an ablation of our robust multitask estimator that uses the traditional mean rather than the trimmed mean in Step 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with Baselines"
        },
        {
            "text": "Next, we leverage our robust multitask estimator to efficiently learn across N simultaneous linear contextual bandit instances; we extend the single-bandit model with dense (i.e., not sparse) arm parameter vectors studied in Goldenshluger and Zeevi (2013) and Bastani and Bayati (2020) .",
            "cite_spans": [
                {
                    "start": 260,
                    "end": 285,
                    "text": "Bastani and Bayati (2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "RMBandit Algorithm"
        },
        {
            "text": "Throughout this section, we drop the subscript RM and denote our robust multitask estimator as \u03b2 j k for arm k and instance j.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RMBandit Algorithm"
        },
        {
            "text": "In this section, we describe our Robust Multitask Bandit (RMBandit) algorithm ( \u00a74.1); we demonstrate improved total and instance-specific regret bounds in the standard ( \u00a74.2) and data poor regimes ( \u00a74.4), along with an overview of the proof strategy ( \u00a74.5); we also study the regret dependence on the network structure underlying bandit instances ( \u00a74.3).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RMBandit Algorithm"
        },
        {
            "text": "Our RMBandit algorithm is presented in Algorithm 2. Following prior work, RMBandit manages the exploration-exploitation tradeoff using a small amount (O(log T )) of forced random exploration in each instance j \u2208 [N ]. Furthermore, for each instance j and arm k \u2208 [K], it trades off between (i) an unbiased forced-sample estimator, which is trained only on forced random samples, and (ii) a potentially biased all-sample estimator, which is trained on all observations for arm k. Instead of using LASSO (Bastani and Bayati 2020) or OLS (Goldenshluger and Zeevi 2013) for these estimators, we use our robust multitask estimator.",
            "cite_spans": [
                {
                    "start": 502,
                    "end": 527,
                    "text": "(Bastani and Bayati 2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Algorithm Description"
        },
        {
            "text": "This introduces two important challenges. First, our multitask estimator leverages data across instances, which induces (previously absent) correlations between our arm parameter estimates",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Description"
        },
        {
            "text": "for a fixed arm k. However, our error bound for the trimmed mean estimator (Lemma 1) requires that our estimates across instances be independent in order to recover a reasonable estimate of the shared model \u03b2 \u2020 . Thus, we introduce a new batching strategy, where we only perform parameter updates in batches rather than after every time step. This ensures that our arm parameter estimates in the current batch are independent conditioned on the observations from previous batches. Importantly, this batching strategy does not change the convergence rates (and therefore regret), and has the added advantage of being far more computationally tractable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Description"
        },
        {
            "text": "Second, our robust multitask estimator requires two hyperparameters: the trimming hyperparameter \u03c9 and the LASSO regularization parameter \u03bb (see Algorithm 1). We specify a trimming path for \u03c9 t to dynamically trade off bias and variance over time, in order to control the convergence of our robust multitask estimators. Intuitively, we trim less for small t (when we have little data) to reduce variance at the cost of admitting \"small\" corruptions; as t increases (when we have collected more data), we trim more aggressively to eliminate even small corruptions that can bias our estimates. For \u03bb t , we use the path specified in Bastani and Bayati (2020) .",
            "cite_spans": [
                {
                    "start": 631,
                    "end": 656,
                    "text": "Bastani and Bayati (2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Algorithm Description"
        },
        {
            "text": "Notation. In more detail, we split the time horizon T into batches that iteratively double in length (i.e., |B m | = 2 m\u22121 |B 0 |), which yields a total of M = log 2 T q log T batches. We denote our robust multitask estimator (Algorithm 1) at instance j for arm k a\u015d \u03b2 j k (B, \u03bb, \u03c9).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Description"
        },
        {
            "text": "The first argument indicates the training data, i.e., all observations where we pulled arm k in batch B; the remaining arguments are hyperparameters, i.e., the LASSO regularization parameter \u03bb and the trimmed mean parameter \u03c9.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Description"
        },
        {
            "text": "Strategy. In our initial batch B 0 (which has size q log T for some tuning parameter q), we deter-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Description"
        },
        {
            "text": "when an individual is observed at instance j (i.e., when Z t = j). At the end of this initial batch, we obtain a forced-sample estimator\u03b2 j k (B 0 , \u03bb 0,j , \u03c9 0 ) for each j \u2208 [N ] and k \u2208 [K]; these forced-sample estimators remain fixed for the entire time horizon T . On the other hand, we also maintain an all-sample estimator \u03b2 j k (B m , \u03bb 1,j,m , \u03c9 1,m ) for each j \u2208 [N ] and k \u2208 [K]; this estimator is periodically re-trained (i.e., at the end of each batch) using data from the previous batch.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Description"
        },
        {
            "text": "Inputs: Forced-sample estimator hyperparameters \u03b6 0 , \u03b7 0 , {\u03bb 0,j } j\u2208 [N ] , initial all-sample estimator hyperparameters \u03b6 1,0 , \u03b7 1,0 , {\u03bb 1,j,0 } j\u2208 [N ] , batch size parameter q, time horizon T",
            "cite_spans": [
                {
                    "start": 72,
                    "end": 76,
                    "text": "[N ]",
                    "ref_id": null
                },
                {
                    "start": 154,
                    "end": 158,
                    "text": "[N ]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Algorithm 2 Robust Multitask Bandit (RMBandit)"
        },
        {
            "text": "Observe the corresponding context vector X t for this arrival, where X t \u223c P j",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 2 Robust Multitask Bandit (RMBandit)"
        },
        {
            "text": "The algorithm is executed as follows. If t \u2208 B m and a new arrival is observed at instance j, we first use the forced-sample estimators to find the highest estimated reward achievable among the K arms at instance j. These estimates allow us to identify a subset of arms K \u2286 K whose rewards are within some tuning parameter h/2 of the estimated optimal reward. Then, within this set, we pull the arm k \u2208 K that has the highest estimated reward according to the all-sample estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 2 Robust Multitask Bandit (RMBandit)"
        },
        {
            "text": "For a more detailed description of this approach, see Bastani and Bayati (2020) .",
            "cite_spans": [
                {
                    "start": 54,
                    "end": 79,
                    "text": "Bastani and Bayati (2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Algorithm 2 Robust Multitask Bandit (RMBandit)"
        },
        {
            "text": "We bound the total regret across all N bandit instances (Theorem 2) as well as for an individual bandit instance (Corollary 3). Here, we consider the standard setting where each instance receives similar traffic, i.e., p j = \u0398(1/N ) for all j \u2208 [N ]; 2 we discuss the data-poor regime in \u00a74.4. Theorem 2. When d > 4s, d = \u2126(log N log T ), and N = \u2126(log d log T log log T ), the total cumulative expected regret of all instances up to time T is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Result: Regret Analysis of RMBandit"
        },
        {
            "text": "for appropriate choices of hyperparameters \u03b6 0 , \u03b6 1,0 , \u03bb 0,j , \u03bb 1,j,0 , \u03b7 0 , \u03b7 1,0 , and q.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Result: Regret Analysis of RMBandit"
        },
        {
            "text": "We provide expressions for the hyperparameter choices and a proof in Appendix C. Note that we make the mild assumption that d and N are both not too small; our approach is designed for problems where the feature dimension is nontrivial, and there are a reasonable number of instances to allow multitask learning. As we show in Section 5, we obtain improved empirical results in practice even for modest values of d and N .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Result: Regret Analysis of RMBandit"
        },
        {
            "text": "Next, we consider the regret of RMBandit for a single instance j. To make a direct comparison to existing regret bounds, the expected time horizon 3 for instance j alone should be T . Since we expect p j = \u0398( 1 N ) fraction of the total arrivals (across all N instances) to be at instance j, we scale our total horizon as T p j = \u0398(N T ), which implies an expected time horizon of T for instance j.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Result: Regret Analysis of RMBandit"
        },
        {
            "text": "Corollary 3. Consider the same setting as in Theorem 2 with a time horizon of T p j = \u0398(N T ). The cumulative expected regret of a single instance j is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Result: Regret Analysis of RMBandit"
        },
        {
            "text": "It is useful to compare the bound above with that of a T -horizon linear contextual bandit instance j in the same setting, but which does not leverage knowledge sharing with other simultaneous bandit instances. Prior literature shows that such an instance would achieve regret that scales as O(d 2 log 3 2 d \u00b7 log T ) (Bastani and Bayati 2020). In contrast, our upper bound on the regret for instance j using RMBandit (Corollary 3) is smaller by a factor of d, but larger by a factor of log T ;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Result: Regret Analysis of RMBandit"
        },
        {
            "text": "this is a substantial improvement in high dimension (large d) and underscores the value of learning across bandit instances. We note that the extra factor of log T is likely an analytical limitation that arises because RMBandit leverages the LASSO estimator, e.g., the high-dimensional contextual bandit also attains a regret that scales as log 2 T (Bastani and Bayati 2020).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Result: Regret Analysis of RMBandit"
        },
        {
            "text": "Next, we consider the dependence of the regret on the network structure of the problem, when There is a tradeoff between choosing smallers, which restricts the number of instances\u00d1 from which we share knowledge (resulting in lower bias but larger variance), and largers which yields larger\u00d1 (resulting in higher bias but smaller variance). Recall that we consider a natural power law scalings = min(\u00d1 \u03b1 , d) for some \u03b1 \u2265 0. In other words, as we increase the number of neighbouring instances we include, our sparsity parameter increases by some power law\u00d1 \u03b1 until it eventually hits the maximum possible value d.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bandit Network Structure"
        },
        {
            "text": "In this setting, we can compute the optimal choice\u00d1 = d 1 \u03b1+1 , resulting in the following upper bound on the cumulative regret at instance j; note that it scales with the network density \u03b1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bandit Network Structure"
        },
        {
            "text": "Corollary 4. Consider the same setting as in Corollary 3 with a time horizon of T p j = \u0398(N T ). Under the network structure given by Eq.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bandit Network Structure"
        },
        {
            "text": "(1) and when there are sufficient bandit instances N =",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bandit Network Structure"
        },
        {
            "text": "where we choose the optimal value of\u00d1 = \u0398(d 1 \u03b1+1 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bandit Network Structure"
        },
        {
            "text": "We give a proof in Appendix C.7. As before, we obtain an improvement in the dependence on the context dimension d; in particular, the regret of RMBandit scales as d 2\u03b1+1 \u03b1+1 , which is always smaller than the d 2 scaling of an independent bandit instance where we do not learn from other instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bandit Network Structure"
        },
        {
            "text": "The extent of this regret improvement scales with the network density \u03b1. When \u03b1 \u2192 0 (i.e., there are many instances with high similarity to the target instance), we eliminate a factor of d, which can be substantial in high dimension; when \u03b1 \u2192 \u221e (i.e., there are essentially no instances with high similarity to the target instance), our regret improvement disappears.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bandit Network Structure"
        },
        {
            "text": "Finally, we turn to the data-poor regime where we expect multitask learning to be most valuable (matching our previous result in \u00a73.5). Again, we consider the case where the target instance j receives substantially fewer observations compared to at least one neighbouring instance \u2208 [N ]; specifically, we consider p p j = \u0398(d 2 ) and \u03b2 k \u2212 \u03b2 j k 0 \u2264 s for each k \u2208 [K]. Once again, to make a direct comparison to existing regret bounds, the expected time horizon for instance j alone should be T . For a data-poor instance, we expect only p j = \u0398( 1 d 2 N ) fraction of the total arrivals (across all N instances) to be at instance j, so we scale our total horizon as T p j = \u0398(d 2 N T ), which implies an expected time horizon of T for instance j.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Poor Regime"
        },
        {
            "text": "Theorem 3. Consider the same setting as in Corollary 3 with a time horizon of T p j = \u0398(d 2 N T ). Suppose there exists an instance \u2208 [N ] such that p p j = \u0398(d 2 ) and \u03b2 k \u2212 \u03b2 j k 0 \u2264 s for each k \u2208 [K]. Then, taking Q j = {j, }, the cumulative expected regret of the data-poor instance j is O Ks 2 log 2 (dT ) , for appropriate choices of hyperparameters \u03b6 0 , \u03b6 1,0 , \u03b7 0 , \u03b7 1,0 , \u03bb 0,j , \u03bb 1,j,0 , and q.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Poor Regime"
        },
        {
            "text": "We provide expressions for the hyperparameter choices and a proof in Appendix D. Theorem 3",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Poor Regime"
        },
        {
            "text": "shows that RMBandit attains a regret bound that only scales logarithmically in the context dimension d -i.e., our multitask learning strategy exponentially reduces the regret for the target data-poor instance compared to running an independent bandit that does not leverage multitask learning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Poor Regime"
        },
        {
            "text": "It is worth noting that the regret of instance j scales as if the arm parameters {\u03b2 j k } are s-sparse (see, e.g., Bastani and Bayati 2020). However, our arm parameters are not sparse, i.e., \u03b2 j k 0 = d. Rather, RMBandit achieves this scaling as a consequence of our multitask learning approach. When a neighbouring instance is data-rich, it provides a good estimate of the shared model \u03b2 \u2020 , which allows us to substantially reduce the dimensionality of our estimation problem by focusing on learning only the bias term \u03b4 j (which is s-sparse) rather than \u03b2 j (which is dense). This intuition aligns with the offline settings considered in Bastani (2021) and Xu et al. (2021) .",
            "cite_spans": [
                {
                    "start": 641,
                    "end": 655,
                    "text": "Bastani (2021)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 660,
                    "end": 676,
                    "text": "Xu et al. (2021)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Data-Poor Regime"
        },
        {
            "text": "In this section, we sketch the proof of our main regret bound (Theorem 2). The proof builds on the regret analysis of LASSO Bandit (Bastani and Bayati 2020), but with the confidence intervals afforded by our robust multitask estimator (Theorem 1). As noted earlier, one key added challenge is the requirement that the OLS estimators { \u03b2 j ind } j\u2208[N ] across different instances be independent in order to invoke our robust multitask estimator; RMBandit achieves this goal using a batching strategy, as highlighted in Lemma 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "Robust multitask estimator with random design: We first introduce a variant of our Theorem 1 for the setting where the design matrices {X j } j\u2208 [N ] are random instead of fixed.",
            "cite_spans": [
                {
                    "start": 145,
                    "end": 149,
                    "text": "[N ]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "Proposition 1. The robust multitask estimator for any instance j from Algorithm 1 satisfies the following concentration inequality",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "for any \u03bb j > 0 and 0 < \u03b7 \u2264 1/2 \u2212 c \u2212 \u03b6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "We give a proof in Appendix C.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "Forced-sample estimator tail inequality: Next, our algorithm uses a separate forced-sample estimator, which we can guarantee is close to the true parameter with high probability. Our next result provides tail bounds on the error of this estimator.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "Proposition 2. When d > 4s, d = \u2126(log N log T ), and N = \u2126(log d log T log log T ), and the hyperparameters \u03b6 0 , \u03bb 0,j , \u03b7 0 , q are as specified in Theorem 2, the forced-sample estimator of instance j and arm k satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "We give a proof in Appendix C.3. At a high level, this result follows directly from Proposition 1, since the forced samples are i.i.d. random variables.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "All-sample estimator tail inequality: Next, we provide a tail inequality for our all-sample estimator for all arms that belong in K j opt (it suffices to consider these arms since the forced-sample estimator is sufficiently accurate to exclude arms in K j sub from consideration). In contrast to the forced-sample estimator, which is based on O(log T ) samples, the all-sample estimator is based on O(T ) samples (since we will show that all optimal arms receive a linear number of samples with high probability). Therefore, the all-sample estimator has smaller error than the forced-sample estimator (the tradeoff is that these samples are adaptively assigned to arms, so they may be collected from biased regions of the covariate space; thus, the i.i.d. samples generated when using the forcedsample estimator are needed to ensure that the all-sample estimator converges). In particular, define the following event, which says that the forced-sample estimators have small error:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "This event holds with high probability by Proposition 2. Our next result shows that our all-sample estimator satisfies the following tail inequality conditional on the event A.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "Proposition 3. When A holds, d > 4s, log(|B m |) = O(N ), and \u03b6 1,0 , \u03bb 1,j,0 , \u03b7 1,0 take the values in Theorem 2, the all-sample estimator of any instance j and optimal arm k \u2208 K j opt using data from the batch B m with m \u2265 1 satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "where we provide the constants C 1 , C 2 , C 3 , \u03bb 1,j,m and \u03b7 1,m in Appendix C.4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "Note that compared to Proposition 1, this result has two extra terms in the probability on the right-hand side of the inequality. These terms account for the event that the number of samples received at each optimal arm of each bandit instance scales proportionally with |B m |.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "We give a proof of Proposition 3 in Appendix C.4. As discussed above, because our all-sample estimators are constructed using all available samples, they may not be independent across instances; however, the trimmed mean estimator in Step 1 of our robust multitask estimator requires that the inputs { \u03b2 j k } j\u2208 [N ] for each arm k \u2208 [K] are independent. By using a batching strategy, we ensure that the samples used to train \u03b2 j k are independent (which implies that the { \u03b2 j k } j\u2208 [N ] are also independent). In particular, we have the following lemma:",
            "cite_spans": [
                {
                    "start": 313,
                    "end": 317,
                    "text": "[N ]",
                    "ref_id": null
                },
                {
                    "start": 486,
                    "end": 490,
                    "text": "[N ]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "Lemma 2. The samples assigned to arm k in batch B m (for any m \u2265 1) are independent across bandit instances conditioned on F m\u22121 , the \u03c3-algebra generated by the samples in B 0 \u222a B m\u22121 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "We give a proof in Appendix C.4. Given this, Proposition 3 follows by applying Proposition 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "Regret bound: Finally, we describe how the above results enable us to prove Theorem 2. For this regret analysis, we group time steps t \u2208 [T ] into three possible cases, and bound the regret across time steps in each case separately:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "(I). Forced-sample batch (t \u2208 B 0 ) or the first batch using all-sample estimator (t \u2208 B 1 ), (II). All the remaining batches (t \u2208 B m for m > 1) such that A does not hold, (III). All the remaining batches (t \u2208 B m for m > 1) such that A holds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "For Case I, note that the size of the first two batches B 0 and B 1 scale as d(d+N ) N log(dN ) log T . In the worst case, the regret for one time step is at most 2bx max , so the regret in this case is bounded.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "For Case II, we have shown that the event A holds with high probability. Similar to before, in the worst case, the regret for one time step is at most 2bx max , so the regret in this case is bounded with high probability. Finally, for Case III when A holds, Proposition 3 guarantees that the all-sample estimator has small error with high probability, again ensuring that the regret is bounded with high probability. Details are provided in Appendix C.5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof Strategy"
        },
        {
            "text": "We now illustrate the value of our approach on synthetic and real datasets. For the latter case, we focus on two well-studied applications of bandit learning: patient risk prediction for personalized interventions, and demand prediction for dynamic pricing. Furthermore, let the average arrival probability across neighbouring instances bep = 1 N \u22121 i =j p i . When possible, matching our theoretical results, we consider two cases for a single instance j: (i) standard with similar arrival probability p j \u2248p, and (ii) data-poor with arrival probability p j p.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "In all cases, we simulate the following linear contextual bandit algorithms: together across similar instances and ignore instance-specific heterogeneity, which would result in linear regret in our setting. There are also Bayesian meta-learning algorithms (Cella et al. 2020 , Bastani et al. 2021c , Kveton et al. 2021 ), but these require instances to be observed sequentially (rather than simultaneously) in order to construct a prior across instances. Furthermore, Bayesian approaches are often computationally intractable in high dimension (Djolonga et al. 2013) . Since our unknown parameters satisfy our assumption of sparse differences, we expect RMBandit to outperform alternative algorithms that do not explicitly leverage this structure. Indeed, RMBandit significantly outperforms OLS and LASSO Bandit, which do not leverage transfer learning.",
            "cite_spans": [
                {
                    "start": 256,
                    "end": 274,
                    "text": "(Cella et al. 2020",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 275,
                    "end": 297,
                    "text": ", Bastani et al. 2021c",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 298,
                    "end": 318,
                    "text": ", Kveton et al. 2021",
                    "ref_id": null
                },
                {
                    "start": 544,
                    "end": 566,
                    "text": "(Djolonga et al. 2013)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Moreover, since the underlying arm parameters are not sparse, we see no noticeable difference between OLS and LASSO Bandit. Perhaps more surprisingly, we observe that the GOBLin algorithm increases cumulative regret compared to not performing transfer learning, which is likely due to two reasons. First, their transfer learning methodology is predicated on the assumption that the arm parameters across instances are close in 2 norm; this assumption is not met by our synthetic data, and more importantly appears unwarranted in the two real datasets we study in the next two subsections, leading to negative transfer learning. Second, their algorithm builds on the UCB algorithm, which is known to over-explore compared to the OLS Bandit strategy (see, e.g., Russo et al. 2017 , Bastani et al. 2021a ; it remains an interesting future direction of research to adapt multitask learning methods to other bandit algorithms such as -greedy or Thompson",
            "cite_spans": [
                {
                    "start": 760,
                    "end": 777,
                    "text": "Russo et al. 2017",
                    "ref_id": null
                },
                {
                    "start": 778,
                    "end": 800,
                    "text": ", Bastani et al. 2021a",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Synthetic"
        },
        {
            "text": "Sampling. Finally, we note that the cumulative regret in the data-poor instance is lower than the corresponding cumulative regret in the standard instance for RMBandit; in contrast, the data-poor instance suffers similar or worse cumulative regret for the baseline algorithms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic"
        },
        {
            "text": "It is also worth noting that RMBandit is significantly more computationally efficient, largely due to the batching strategy which requires only a small number of model updates. To run 15 trials in the standard setting, LASSO and OLS Bandit each took approximately 5-6 minutes, GOBLin took over 11 hours, while our algorithm took less than a minute to run. 4",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic"
        },
        {
            "text": "The next two subsections examine real datasets, which may not satisfy our assumptions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic"
        },
        {
            "text": "Diabetes is a leading cause of severe health complications such as cardiovascular disease, stroke, and chronic kidney disease (Ismail et al. 2021). Thus, there is significant interest in leveraging machine learning for early detection of (Type II) diabetes, in order to improve treatment outcomes (Zhang et al. 2020) . However, significant evidence shows that machine learning models trained on one health system can perform poorly on a different health system (Qui\u00f1onero-Candela et al.",
            "cite_spans": [
                {
                    "start": 297,
                    "end": 316,
                    "text": "(Zhang et al. 2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Risk Prediction in Health Data"
        },
        {
            "text": "2008, Subbaswamy and Saria 2020); this can be due to dataset shifts such as changes in patient demographics, disease prevalence, measurement timing, equipment, and treatment patterns. Thus, it is important to train provider-specific risk models; yet, smaller providers may benefit from additionally leveraging information across providers due to their relatively small patient cohorts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Risk Prediction in Health Data"
        },
        {
            "text": "In this experiment, we use electronic medical record data across N = 13 healthcare providers to learn a good diabetes risk model for a single provider. After basic preprocessing, we have approximately 80 patient-specific features constructed from information available before the most recent visit (e.g., past diagnoses, procedures and medications); our outcome is an indicator variable for whether the patient was diagnosed with diabetes during the most recent visit. We aim to learn the best linear classifier online as patient observations accrue, and evaluate different methods based on the classification accuracy over time. We consider a mid-sized provider with 355 unique patients observed during the sample period, as well as a data-poor provider with only 176 unique patients.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Risk Prediction in Health Data"
        },
        {
            "text": "We additionally fit a linear oracle, which leverages all observed data from the provider in hindsight using a leave-one-out approach, representing the best achievable performance within a linear model family. Figure 3 shows the fraction of incorrect classifications made over time for the (a) standard and (b) data-poor providers. Appendix F.2 provides additional details on the setup. Once again, we observe that RMBandit performs favorably compared to the other baseline algorithms, and converges to the oracle's classification accuracy much faster. This is especially the case in the data-poor instance, as suggested by the theory.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 209,
                    "end": 217,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Risk Prediction in Health Data"
        },
        {
            "text": "Contextual bandit algorithms can also naturally be extended to solve dynamic pricing problems with unknown demand (Besbes and Zeevi 2009) . We consider such a demand forecasting and price optimization task for food distributors; to this end, we use a publicly available dataset of orders from a meal delivery company. 5",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 137,
                    "text": "(Besbes and Zeevi 2009)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Demand Prediction in Retail Data"
        },
        {
            "text": "In this experiment, we use data across N = 7 fulfillment centers, serving between six to seven thousand orders each during the sample period. Features include the category and cuisine pertaining to the order, as well as associated promotions. The decision variable is the (continuous) price for the order; rather than arm parameters, there is a single set of unknown parameters (per instance) that aims to predict demand/revenue as a function of price and the observed features. Following the approach of Ban and Keskin (2021) , we model the price elasticity of demand as a linear function of the observed features:",
            "cite_spans": [
                {
                    "start": 505,
                    "end": 526,
                    "text": "Ban and Keskin (2021)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Demand Prediction in Retail Data"
        },
        {
            "text": "Here, {\u03b2 j 0 , \u03b2 j 1 } are the unknown parameters corresponding to instance j; conditioned on an arrival with context X t at instance Z t = j, Y t is the observed revenue for the chosen price p t and noise j t . Regret is measured with respect to an oracle that knows {\u03b2 j 0 , \u03b2 j 1 } N j=1 . We straightforwardly extend RMBandit and the other baseline bandit algorithms to the dynamic pricing setting using a batched explore-then-commit strategy employed by Ban and Keskin (2021) . Once again, we observe that RMX (the dynamic pricing analog of RMBandit) performs favorably compared to the dynamic pricing analogs of the other baseline bandit algorithms. Thus, our insights on multitask learning carry over to the dynamic pricing context.",
            "cite_spans": [
                {
                    "start": 459,
                    "end": 480,
                    "text": "Ban and Keskin (2021)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Demand Prediction in Retail Data"
        },
        {
            "text": "Decision-makers frequently want to learn heterogeneous treatment effects across many simultaneous experiments. Examples range from learning patient risk across hospitals for personalized interventions (Bastani 2021, Mullainathan and Obermeyer 2017), learning drug effectiveness across combination therapies for clinical trial decisions (Bertsimas et al. 2016) , learning COVID-19 risk across travelers for targeting tests (Bastani et al. 2021b) , and learning demand across stores for promotion targeting (Baardman et al. 2020, Cohen and Perakis 2018) or dynamic pricing (Bastani et al. 2021c ). We propose a novel robust multitask estimator that improves the efficacy of downstream decisions by learning better predictive models with lower sample complexity. To the best of our knowledge, our work proposes the first combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results) to yield improved bounds for multitask learning. In the online learning setting, these problems translate to running simultaneous contextual bandit algorithms. To this end, we propose the RMBandit algorithm to effectively navigate the exploration-exploitation tradeoff across bandit instances, thereby improving regret bounds in the context dimension d.",
            "cite_spans": [
                {
                    "start": 336,
                    "end": 359,
                    "text": "(Bertsimas et al. 2016)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 422,
                    "end": 444,
                    "text": "(Bastani et al. 2021b)",
                    "ref_id": null
                },
                {
                    "start": 505,
                    "end": 537,
                    "text": "(Baardman et al. 2020, Cohen and",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 538,
                    "end": 551,
                    "text": "Perakis 2018)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 571,
                    "end": 592,
                    "text": "(Bastani et al. 2021c",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "We highlight several features of our proposed approach that make it a particularly attractive solution. First, it is well known that data limitations result in worse model performance, which in turn can imply unfair decisions, e.g., in healthcare, such biases disproportionately affect protected groups or minorities due to limited representative data (Rajkomar et al. 2018) . A natural approach to alleviating unfairness is to improve the performance of our models for data-poor instances (see, e.g., discussion in Hardt et al. 2016). We show that multitask learning can be especially valuable in such settings -our approach leverages data from data-rich instances to provide an exponential improvement in performance for data-poor instances. Thus, we provide one additional tool (among others) for improving fairness in decision-making.",
            "cite_spans": [
                {
                    "start": 352,
                    "end": 374,
                    "text": "(Rajkomar et al. 2018)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Second, privacy and regulatory constraints prevent granular data sharing in many applications.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "A growing literature on federated learning studies training statistical models over siloed datasets, while keeping data localized (Li et al. 2020b) . While our focus is on multitask learning, our approach satisfies the constraints of federated learning, since we only require sharing aggregate statistics (in this case, OLS regression parameters) across instances. All model training is performed locally at the instance-level and does not require any raw data from other instances.",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 147,
                    "text": "(Li et al. 2020b)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Third, practical deployment of bandits often precludes real-time updates to the model. For instance, many individuals may appear for service simultaneously (Schwartz et al. 2017) and there may be operational constraints or concerns over model reliability (Bastani et al. 2021b ). Our",
            "cite_spans": [
                {
                    "start": 255,
                    "end": 276,
                    "text": "(Bastani et al. 2021b",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "RMBandit algorithm employs a batching strategy that only requires a logarithmic number (in the time horizon T ) of model updates, while preserving convergence rates (and therefore regret).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Furthermore, it has the added advantage of being far more computationally tractable. Recall that the indices of the corrupted samples are denoted by J \u2286 [N ] (so the rest are J c = [N ] \\ J ). By assumption, {Z j } j\u2208J c are independent and \u03c3 j -subgaussian with mean \u00b5, and |J | < N \u03b6 with \u03b6 < 1/2. By",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Hoeffding's inequality, for any uncorrupted sample j \u2208 J c , we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "with a probability of at most 2\u03b7/3. Then, again by Hoeffding's inequality, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "in other words, with a high probability, at most \u03b7 fraction of J c are outside a reasonable range of the true mean \u00b5. As a consequence, on the event",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "at most \u03b6 + \u03b7 fraction of the N samples are outside the interval I since |J | < N \u03b6. Thus, by trimming the upper and lower \u03c9 = \u03b6 + \u03b7 fraction of samples, the remaining ones are guaranteed to fall into I.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "\u03b9=N \u03c9+1 denote the samples after trimming, and U = {j \u2208 [N ] | Z j \u2208 I} denote the set of samples that lie in I. Since T \u2286 U, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "For the first term on the right-hand side of (8), note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Since we remove 2(\u03b6 + \u03b7) of the samples, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "For those samples in J c lying inside the interval I, it holds for any \u03c7 > 0 that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "The truncation on these samples introduces a bias of at most",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "where we use H\u00f6lder's inequality in the last inequality and k, q are such that 1/k + 1/q = 1. Recall that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "k for k \u2265 2 by the property of subgaussian (Rigollet and H\u00fctter 2015). Therefore, taking k = log 3 2\u03b7 in the inequality (11), we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Then, the high probability bound in (10) implies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Setting \u03c7 = \u03b7, and by our assumption that \u03b7 < 1/2, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Combining (9) and (12), we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "with a high probability. Next, for the second term on the right-hand side of (8), we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Thus, with a high probability, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "where we have used |J c | \u2265 (1 \u2212 \u03b6)N and \u03b7 < 1/2 \u2212 \u03b6. Together with a union bound on the event V, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "We first prove the following lemma, which says that for each instance j \u2208 [N ], the noise term 1 n j X j j = 1 n j i\u2208[n j ] X j i j i is uniformly bounded with high probability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Lemma 3. Define the event",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Then, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Proof of Lemma 3 For any column i of the design matrix X j , i.e., X j (\u00b7,i) , we have 1 \u221a n j X j (\u00b7,i) 2 \u2264 x max . Then, by Lemma 26, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Remark 2. Note that the above holds even in a random-design setting since it holds for any given X j . Now, we prove Theorem 1 by applying Lemma 1 to the OLS estimators of all data sources.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Proof of Theorem 1 First, we show that each OLS estimator \u03b2 j ind = (X j X j ) \u22121 X j Y j constructed in Step 1 is a subgaussian random vector with mean \u03b2 j . In particular, the i th component \u03b2 j ind,(i) of \u03b2 j ind is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "where the last inequality follows since , where we use the fact that \u03b2 j ind is \u03c3 2 j n j \u03c8 -subgaussian:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "By a union bound over i \u2208 I poor , we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "where \u03b2 I for a set I is described in \u00a72. Next, note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "} are the components of \u03b2 j that do not equal \u03b2 \u2020 , then we have |\u012a j | \u2264 (\u03b6 \u22121 + 1)s. In addition, \u03b2 \u2020 I well + \u03b2 \u2020 Ipoor is closely approximated by \u03b2 \u2020 RM as in (13). Therefore, after computing \u03b2 \u2020 RM in",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Step 1, we can use LASSO to recover the sparse vector \u03b2 \u2020 Ipoor \u2212 \u03b2 \u2020 RM,Ipoor + \u03b4 j to estimate \u03b2 j . The basic inequality of LASSO in the second stage of our algorithm is 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Decomposing terms based on\u012a j and\u012a c j and rearranging, we have 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Then, adding \u03bb j 2 ( \u03b2 j RM \u2212 \u03b2 j )\u012a j 1 on both sides, we get 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "where we use (\u03b2 \u2020 Ipoor \u2212 \u03b2 \u2020 RM,Ipoor + \u03b4 j )\u012ac j = 0. As \u03a3 j is positive definite on I j , we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Then, inequality (14) implies that 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "where we have used the fact that 2ab \u2264 a 2 + b 2 . Since \u03b6 < 1/2, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Combining the above with inequality (13) and Lemma 3, we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. Proof of Theorem 1: Tail Inequality for Robust Multitask Estimator"
        },
        {
            "text": "Proof of Corollary 1 Taking \u03bb j = 32\u03c3 2 j x 2 max n j log 4d \u03b4 and \u03b7 = 9 N log( 6d \u03b4 ) in Theorem 1, and noting that \u03b4 \u2264 1 and d \u2265 1, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3. Proof of Corollary 1"
        },
        {
            "text": "Therefore, it holds that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3. Proof of Corollary 1"
        },
        {
            "text": "with probability at least 1 \u2212 \u03b4. Since n i 's are assumed to be similar in magnitude, choosing \u03b6 = s d suffices to minimize the sum of the first two terms in inequality (15). Thus, with probability at least 1 \u2212 \u03b4, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3. Proof of Corollary 1"
        },
        {
            "text": "Finally, since we need \u03b7 \u2264 1/2 \u2212 c \u2212 \u03b6, we require that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3. Proof of Corollary 1"
        },
        {
            "text": "Proof of Corollary 2 Similar to Corollary 1, taking \u03bb j = 32\u03c3 2 j x 2 max n j log 4d \u03b4 and \u03b7 = 9 N log( 6d \u03b4 ), we derive from Theorem 1 that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.4. Proof of Corollary 2"
        },
        {
            "text": "with probability at least 1 \u2212 \u03b4. Since n i 's are assumed to be similar in magnitude, choosing \u03b6 to be any constant smaller than 1 2 \u2212 c suffices to minimize the sum of the first two terms in inequality (15). Thus, with probability at least 1 \u2212 \u03b4, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.4. Proof of Corollary 2"
        },
        {
            "text": "Finally, since we need \u03b7 \u2264 1/2 \u2212 c \u2212 \u03b6, we require that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.4. Proof of Corollary 2"
        },
        {
            "text": "In this section, we provide detailed statements and proofs for the lower bounds discussed in \u00a73.6. At a high level, our lower bounds follow by exhibiting a concrete instantiation of the parameters \u03b2 j and data X j , Y j and establishing a lower bound on the error of the estimator for this instantiation. Recall that our error measure is (6), i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix B: Proofs of Lower Bounds for Baselines"
        },
        {
            "text": "satisfies our assumptions in \u00a73, P j is the distribution of j , and the expectation is taken with respect to j 's. Since this error measure takes a worst-case scenario over G, it suffices to show the lower bound for a specific case where the assumptions hold.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix B: Proofs of Lower Bounds for Baselines"
        },
        {
            "text": "For the remainder of this section, we assume j \u223c N (0, \u03c3 2 j I), and \u03a3 j = I for j \u2208 [N ]. Our choices of errors j are all gaussian, which ensures the parameter estimates are gaussian as well, thereby enabling us to obtain lower bounds by applying the following lemma:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix B: Proofs of Lower Bounds for Baselines"
        },
        {
            "text": "Lemma 4. Consider a multivariate gaussian random variable X \u223c N (\u00b5, \u03a3) \u2208 R d . We have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix B: Proofs of Lower Bounds for Baselines"
        },
        {
            "text": "Proof of Lemma 4 Consider the i th component of X, i.e., X (i) . Let \u03c3 2 i = \u03a3 (i,i) . We have X (i) \u223c N (\u00b5 (i) , \u03c3 2 i ). Without loss of generality, assume \u00b5 (i) \u2265 0; otherwise, we can consider \u2212X (i) instead and its 1 norm stays the same. By our gaussian assumption, it holds that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix B: Proofs of Lower Bounds for Baselines"
        },
        {
            "text": "Then, we further have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix B: Proofs of Lower Bounds for Baselines"
        },
        {
            "text": "First, we consider the independent estimator, which simply uses ordinary least squares (OLS) independently on each instance (i.e., it does not perform any learning across instances). This estimator is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1. Independent Estimator"
        },
        {
            "text": "Intuitively, this estimator has high variance since it uses relatively little data to estimate \u03b2 j . In particular,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1. Independent Estimator"
        },
        {
            "text": "we have the following result:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1. Independent Estimator"
        },
        {
            "text": "Proposition 4. The estimation error of the independent estimator in the standard and data-poor regimes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1. Independent Estimator"
        },
        {
            "text": "Proof of Proposition 4 For our choice of X j and j , the estimation error follows a gaussian distribution:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1. Independent Estimator"
        },
        {
            "text": "Therefore, using Lemma 4, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1. Independent Estimator"
        },
        {
            "text": "Next, we consider the averaging estimator, which simply takes the average of the independent parameter estimates across instances to reduce variance:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Averaging Estimator"
        },
        {
            "text": "Note that this estimator is constant across instances j's; also, it is identical to Step 1 of the averaging multitask estimator described in \u00a73.2 -i.e., \u03b2 j avg = \u03b2 \u2020 AM .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Averaging Estimator"
        },
        {
            "text": "Proposition 5. The estimation error of the averaging estimator in the standard regime satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Averaging Estimator"
        },
        {
            "text": "and in the data-poor regime satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Averaging Estimator"
        },
        {
            "text": "Proof of Proposition 5 For our choice of X j 's and j 's, the estimation error follows a gaussian distribution:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Averaging Estimator"
        },
        {
            "text": "Therefore, by Lemma 4, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Averaging Estimator"
        },
        {
            "text": "For data-poor regime, we use all the instances except j as a proxy. Following a similar proof strategy as above, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2. Averaging Estimator"
        },
        {
            "text": "Next, we consider the pooling estimator, which pools all the data X j , Y j across instances, and then uses OLS on this pooled dataset:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Pooling Estimator"
        },
        {
            "text": "As with the averaging estimator, this estimator is constant across instances j's. Intuitively, it performs similarly to the averaging estimator, except it accounts for differences in the covariance matrices \u03a3 j = X j X j across instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Pooling Estimator"
        },
        {
            "text": "Proposition 6. The estimation error of the pooling estimator in the standard regime satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Pooling Estimator"
        },
        {
            "text": "and in the data-poor regime satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Pooling Estimator"
        },
        {
            "text": "Proof of Proposition 6 For our choice of X j 's and j 's, the estimation error follows a gaussian distribution:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Pooling Estimator"
        },
        {
            "text": "Note that for data-poor regime, we use all the instances except j as a proxy. Following a similar proof strategy as above, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3. Pooling Estimator"
        },
        {
            "text": "Finally, we consider the averaging multitask estimator described in \u00a73.2, which uses a traditional estimate of the mean instead of our robust estimate in Step 1. Our lower bound on the error of this estimator demonstrates the importance of robustness. Following the proof of the LASSO lower bound in Theorem 7.1 of Lounici et al. (2011), we assume that \u03bb j is chosen based on the analysis of the error upper bound. Similar to Corollary 1, we take \u03bb j = 32\u03c3 2 j x 2 max n j log 4d C with any choice of \u03b4 = C such that 0 \u2264 C \u2264 1 2 based on H j in Lemma 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "\u03c3 2 k n k log 2 C . The estimation error of the averaging multitask estimator in the standard and data-poor regimes satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "Proof of Proposition 7 The first order condition of problem (4) is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "where \u2202 \u03b2 j AM \u2212 \u03b2 \u2020 AM 1 is the subgradient of 1 norm at \u03b2 j AM \u2212 \u03b2 \u2020 AM ; in particular, for the i th component,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "Next, on the event H j , we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "Combining it with (16), we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "for each i such that \u03b2 j AM,(i) = \u03b2 \u2020 AM,(i) , where the last equality is from our assumption \u03a3 j = I. For the rest of the components, note that \u03b2 j AM,(i) = \u03b2 \u2020 AM,(i) . Summing over all i \u2208 [d], we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "For the remainder of the proof, we use a similar argument as the proof of Theorem 7.1 in Lounici et al. (2011) . First, if \u03b4 AM 0 < \u03b4 AM 0 , then there exists i \u2208 [d] such that \u03b4 AM,(i) = 0 but \u03b4 AM,(i) = 0. By the first order condition (16), for this i, we have",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 110,
                    "text": "Lounici et al. (2011)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "and hence by Hoeffding's inequality, for any t > 0 and i \u2208 [d], we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "\u03c3 2 k n k log 2 C . Then, given our choice of \u03bb j , we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "Since we consider a worst-case error over all possible {\u03b2 i } i\u2208[N ] that satisfy our assumptions in \u00a73, we focus on a special case; in particular, (i) \u03b4 j (i) \u2265 5N 4(N\u22121)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "\u03c3 2 k n k log 2 C for any i \u2208 [d] such that \u03b4 j (i) = 0, (ii) \u03b4 k (i) = 0 for any k = j and any i \u2208 [d] such that \u03b4 j (i) = 0, and (iii) \u03b2 \u2020 AM 0 = min{N s, d}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "Then, there is a contradiction with probability at least 1 \u2212 2C. As a consequence, in this case, we must have \u03b4 j AM 0 \u2265 \u03b4 j AM 0 . Note that \u03b4 j AM 0 = min{N s, d} given our condition (iii) above. On the other hand, it always holds true that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "given the first order conditions in (16); in other words, given S, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "In the last paragraph, we show that the support of \u03b4 j AM includes that of \u03b4 j AM . Therefore, the first term in (18) equals to 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "Combining all the above, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "for |S| \u2265 min{N s, d} with a probability at least 1 \u2212 2C. Since the above lower bound is linear in |S|, it takes the minimum value at either ends of the interval [min{N s, d}, d] . With a union bound over |S|, we have",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 178,
                    "text": "[min{N s, d}, d]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "when N s < d and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "The proof for the data-poor regime is similar. and q = max",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.4. Averaging Multitask Estimator"
        },
        {
            "text": "In this section, we prove Proposition 1, which extends Theorem 1 to the setting where the design matrices X j 's are random.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "Proof of Proposition 1 The proof follows that of Theorem 1. Define",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "and some \u03c6 > 0. Then, on the event \u2229 i\u2208[N ] E i , Theorem 1 gives for any j \u2208 [N ]",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "We condition on the design matrices {X i } i\u2208[N ] since Theorem 1 considers a fixed-design problem. Integrating over X j 's and using a union bound, we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "In this section, we prove Proposition 2, which says that our forced sample estimators have small estimation error with high probability given our batching design. First, let B j 0,k be the index set of those forced sampled at arm k and instance j andB j 0,k be the subset of all t \u2208 B j 0,k such that X t \u2208 U j k ; in particular,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Note that the distribution of X t always conditions on the value of Z t .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Lemma 5. The forced samples of arm k are independent across bandit instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Proof of Lemma 5 The forced samples of arm k at instance j are {(X t , Y t )} t\u2208B j 0,k , where the set of covariates is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Since {(X t , Z t )} t\u2208B 0 are independent, X t is independent of Z t and X t conditional on Z t for any t = t and t \u2208 B 0 . Further note that given Z t = j,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Thus, X t 's observed at arm k are independent across bandit instances. Similarly, Y t 's are also independent across different instances as each noise t only depends on Z t by design. As a result, the forced samples of arm k are independent across different instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Now we consider a set of subsamples of arm k at one single bandit instance j. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Since {(X t , Z t )} t\u2208B 0 are independent, X t is independent of Z t and X t conditional on X t \u2208 U Z t k , Z t = j for any t = t and t \u2208 B 0 . Similarly, we can conclude that {X t } t\u2208B j 0,k are i.i.d. drawn from P j X|X\u2208U j k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "In the following, we use the notation \u03a3(B) to represent the sample covariance matrix created using the",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Lemma 7. Define the event",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Then, given |B j 0,k |, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Proof of Lemma 7 Note that {X t X t } t\u2208B j 0,k are i.i.d. according to Lemma 6. By Assumption 1, for any",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Therefore, by taking t = 1/2 and L = dx 2 max , we instantaneously derive from Lemma 27 that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Lemma 8. For any sets B,B withB \u2286 B, if \u03bb min ( \u03a3(B)) \u2265 \u03c6 for some positive \u03c6, then \u03bb min ( \u03a3(B)) \u2265 \u03c6|B| |B| .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Proof of Lemma 8 See Lemma EC.23 in Bastani and Bayati (2020) .",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 61,
                    "text": "Bastani and Bayati (2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Lemma 9. Given |B j 0,k |, it holds that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Proof of Lemma 9 Applying Lemma 28 to the indicator random variables 1 t \u2208B j 0,k for all t \u2208 B j 0,k with",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Note that by Assumption 3, \u00b5 \u2265 p * |B j 0,k |. Thus, we can write that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Lemma 10. It holds that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Proof of Lemma 10 Applying Lemma 28 to the indicator random variables 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Now we are ready to prove Proposition 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Proof of Proposition 2 We start by defining several events:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Conditional on the above events, Lemma 8 implies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Therefore, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "By Lemma 9, the second term above is upper bounded by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "On the other hand, use Lemma 7 and the first term in inequality (19) has",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Now we apply Proposition 1 to our forced-sample estimators. Based on the results of Lemma 5 and 6, the forced-sample OLS estimators are independent across instances so the conditions of Proposition 1 are satisfied. Letting \u03c6 = p * \u03c8 4 in Proposition 1, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": ". (20) By our design of forced sampling, |B j 0,k | = |B j 0 |/K. Plugging it into the probability bound (20), we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": ". (21) Then, consider the probability bound (21) on the following events",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": ". Note that inequality (21) still holds further conditional on \u2229 i\u2208[N ] M i 0 . With a union bound and by Lemma 10, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Now we configure the parameters \u03b6, \u03bb j and \u03b7 such that our forced-sample estimator of arm k and instance j has estimation error smaller than h 4xmax . With a similar argument as in the proof of Corollary 1, we take \u03b6 = ( s d ) 1 2 . We further set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": ".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Given this choice of \u03bb j , the first term in the estimation error in inequality (21) is equal to h 8xmax . Using the fact that log(x) \u2264 x for any x > 0, we can show the second term is also smaller than h 8xmax given our choice of \u03b7 as long as 3\u03b6 \u2264 4\u03b7, which holds if",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "a probability of O( N T ). In the following, we will frequently use the inequality",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "for T > 2, x > 1. For the first term on the right hand side of (22) to be less than 3/T , it suffices to have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "when we use inequality (24). Remember we also require \u03b7 \u2264 1/2 \u2212 c \u2212 \u03b6 according to Proposition 1. We consider two cases:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "This implies 2 log 384xmaxd (6), which is a contradiction. In case (ii), we can obtain from inequality (25) that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Therefore, inequality (25) holds as long as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "To satisfy the constraint that \u03b7 \u2264 1/2 \u2212 c \u2212 \u03b6, it is sufficient to have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "that is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Finally, we require",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "so that the sum of the last four probability terms in inequality (22) is no greater than 7N/T , where we use inequality (24).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "As a result, letting",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "for any k \u2208 [K]. Moreover, given our final choice of q above, inequality (23) is satisfied if",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": ",",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": ".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "It's sufficient to require T \u2265 16d s 2 . And inequality (26) is satisfied if",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": ", p * \u03c8d 432c 2 x 2 max s log(d)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": ",",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "The result then follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3. Forced-Sample Estimator"
        },
        {
            "text": "In this section, we prove Proposition 3, which says that our all sample estimators have small error with high probability. First, the constants in the statement of Proposition 3 are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.4. All-Sample Estimator"
        },
        {
            "text": "and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.4. All-Sample Estimator"
        },
        {
            "text": "as in Algorithm 2. Now, we begin with the following bound on the probability of event A (defined in (7)), which says that the forced sample estimators have small error.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.4. All-Sample Estimator"
        },
        {
            "text": "Lemma 11. The event A holds with at least a probability of 1 \u2212 10KN T .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.4. All-Sample Estimator"
        },
        {
            "text": "The result follows by applying a union bound over all arms and bandit instances using Proposition 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "We consider the third case of regret analysis, where A holds and we can estimate the \u03b2's accurately using batch samples. Define the set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Define an \u03c3-algebra",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Next, we prove Lemma 2, which says that the samples assigned to arm k collected in batch B m (for any m \u2265 1) are independent across bandit instances conditioned on F m\u22121 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Proof of Lemma 2 The collected samples of arm k at instance j in the batch",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "where the set of covariates is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Note that our estimated policy \u03c0 Z t m\u22121 depends on Z t and is constructed using samples from B 0 and B m\u22121 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": ")} t\u2208Bm are independent conditional on F m\u22121 . Thus, for any t = t and t \u2208 B m , X t is independent of Z t , X t and \u03c0 Z t m\u22121 (X t ) conditional on {Z t = j, \u03c0 Z t m\u22121 (X t ) = k, F m\u22121 }. This implies X t 's of arm k in the batch B m are conditionally independent across bandit instances. Moreover, since the noises t 's are independent of X t 's and only depends on Z t 's by design, the collected samples of arm k in B m are independent across different instances conditional on F m\u22121 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Remark 3. Note that the samples across instances are also independent given {A, F m\u22121 } since A \u2208 F m\u22121 . (conditional on F m\u22121 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Proof of Lemma 12 The first claim follows Lemma 14. If Z t = j, X t \u2208 U j k and the event A holds, then \u03c0 j m\u22121 (X t ) = k and hence t \u2208 B j m,k , i.e.,B j m,k \u2286 B j m,k . Following a similar argument as in the proof of Lemma 2, we can show that {X t } t\u2208B j m,k are i.i.d. from distribution P j X|\u03c0 j m\u22121 (X)=k given F m\u22121 . On the other hand, note that the event A only depends on samples from B 0 and is therefore independent of {(X t , Z t )} t\u2208Bm for any m > 0. Thus, X t for any t \u2208B j m,k follows distribution P j X|X\u2208U j k . Furthermore, since {(X t , Z t )} t\u2208Bm are independent, X t is independent of Z t and X t given {Z t = j, X t \u2208 U Z t k , A} for any t = t and t \u2208 B m . Therefore, {X t } t\u2208B j m,k are also independent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Besides, we can similarly prove that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Lemma 13. Given |B j m |, it holds that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Proof of Lemma 13 By definition ofB j m,k , we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Using Lemma 28, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "By Assumption 3, we have \u00b5 \u2265 p * |B j m |. Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Remark 4. Note that the above also holds conditional on {A,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Lemma 14. For any j \u2208 [N ], k \u2208 [K] and t / \u2208 B 0 , if Z t = j, X t \u2208 U j k and the event A holds, then Algorithm 2 plays the optimal arm k of instance j at time t based on the forced-sample estimator \u03b2 j k (B 0 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 11"
        },
        {
            "text": "Since X t \u2208 U j k , by the definition of U j k ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "Then, for any arm i = k on the event of A, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "Therefore, the optimal arm k for X t will be pulled.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "Lemma 15. If A holds, then the set of arms K that survive after using the forced-sample estimators contains the optimal arm k = arg max i\u2208[K] X t \u03b2 j i given Z t = j and no suboptimal arms in K j sub .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "Proof of Lemma 15 Similar to the proof of Lemma 14, given A, we have for any arm i",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "Thus, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "in other words, the optimal arm will be kept based on the forced-sample estimators. Now consider any suboptimal arm k \u2208 K j sub . By definition, we have X t (\u03b2 j i \u2212 \u03b2 j k ) \u2265 h for any arm i. Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "which implies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "In other words, any suboptimal arm k will be filtered out through the forced-sample estimators and hence not in K.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "Lemma 16. It holds that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "Proof of Lemma 16 The result follows a same argument as in the proof of Lemma 10.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "Now we provide the proof of Proposition 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 14"
        },
        {
            "text": "We follow a similar proof strategy as Proposition 2 to provide a finite-sample bound of all-sample estimators conditional on A. We define the following events analogous to Proposition 2:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "By Lemma 8 and the fact that |B j m | \u2265 |B j m,k |, it holds that on the above eventsD j m,k and\u0112 j m,k \u03bb min ( \u03a3(B j m,k )) \u2265 p * \u03c8 4 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "Thus, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "By Lemma 7 and 12, the first term above has",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "On the other hand, Lemma 13 implies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "Now we consider learning across a set of instances with arm k being optimal, W k \u2286 [N ], since suboptimal arms won't observe any users on the event of A. We apply Proposition 1 to our all-sample estimators across W k . Based on Lemma 2 and 12, our all-sample OLS estimators are independent across instances conditional where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "Regarding r j t,1 (X t ), note that the event",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "Thus, at least one of |X t ( \u03b2 j \u03b9 (B m\u22121 ) \u2212 \u03b2 j \u03b9 )|, \u03b9 \u2208 {1, k} must be greater than x max \u03b4, which means",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "The above probability can then be upper bounded through our all-sample tail inequality in Proposition 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "By Lemma 19, we know |B m\u22121 | \u2265 t/4; thus, Proposition 3 still holds with |B m\u22121 | replaced with t/4. Take",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "and Proposition 3 gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "for \u03b9 \u2208 {1, k}. Applying all the above to equation (31), we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "On the other hand, by Assumption 2, we have for the term r j t,2 (X t ) that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "Combining all the above with inequality (30), we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "\u2264 48x 2 max LK (C 2 1 log(dp j t) + C 2 2 log(\u03c1N )) sd p j t + C 2 3 log(\u03c1N ) log(dp j t) 48x 2 max LK (C 2 1 log(dp j T ) + C 2 2 log(\u03c1N ))sd log(p j T ) + C 2 3 d 2 log(\u03c1N ) N log(p j T ) log(dp j T )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "Since (N )). Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 3"
        },
        {
            "text": "In this section, we prove Corollary 3, which provides the regret for a single bandit instance j \u2208 [N ] (while running RMBandit across all bandit instances).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.6. Single Bandit Instance"
        },
        {
            "text": "The cumulative expected regret of any target instance j is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Corollary 3"
        },
        {
            "text": "Using a similar argument in the proof of Theorem 2, we have ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Corollary 3"
        },
        {
            "text": "Finally, we prove Corollary 4, which provides a regret bound in the case where the bandit instances have network structure.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.7. Network Structure"
        },
        {
            "text": "Proof of Corollary 4 Our network structure is an exogenous assumption upon the sparsity s. Therefore, all the previous analyses for Theorem 2 and Corollary 3 still go through for an arbitrary number of selected instances, i.e.,\u00d1 . Plugging s =\u00d1 \u03b1 in the regret bound derived in Corollary 3 and optimizing in terms of N , we get the optimal value of\u00d1 to be \u0398 d given the above s and\u00d1 . The result then follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.7. Network Structure"
        },
        {
            "text": "In this section, we give hyperparameter choices and a proof for Theorem 3, which bounds the regret across all bandit instances in the data-poor regime. The proof closely follows that of Theorem 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix D: Proof of RMBandit Regret Bound in Data-Poor Regime"
        },
        {
            "text": "First, we give the hyperparameter choices for Theorem 3 to hold. We take \u03b6 0 = \u03b6 1,0 = 1, \u03b7 0 = \u03b7 1,0 = 0, \u03bb 0,j = p * \u03c8h 256x max s , \u03bb 1,j,0 = 64\u03c3 2 j x 2 max p * , and q = max (128 \u221a 3) 2 \u03c3 2 x 2 max Kd 2 log d h 2 p * \u03c8p ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Hyperparameter Choices"
        },
        {
            "text": "Note that since we are only using a single neighbor of j, we trivially set \u03b6 = N = 1 and \u03b7 = 0, amounting to the transfer learning method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1. Hyperparameter Choices"
        },
        {
            "text": "We begin by proving a variant of the robust multitask estimator for the random design setting, specialized to our setting where\u00d1 = 2. Before we do so, we first prove that the compatibility condition holds with high probability for our design.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "Definition 3 (Compatibility Condition). For a constant \u03c6 > 0, define the set of matrices C(S, \u03c6 ) = {M \u2208 R d\u00d7d | \u2200 v S c 1 \u2264 7 v S 1 , \u03c6 v S 2 1 \u2264 |S|v M v}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "Lemma 22. The true covariance matrix \u03a3 j k = E P j X [XX |X \u2208 U j k ], k \u2208 [K] of the target instance j satisfies the compatibility condition-i.e., there exists a positive constant \u03c8 such that \u03a3 j k \u2208 C(S j , \u03c8 ) for any k \u2208 [K].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "Proof of Lemma 22 Given Assumption 4, \u03bb min (\u03a3 j k ) \u2265 \u03c8. Then, for any v \u2208 R d , we have v S 1 \u2264 \u221a s v S 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "Now, we state and prove our main proposition for this section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "Proposition 8. The robust multitask estimator of instance j from Algorithm 1 satisfies the following concentration inequality P \u03b2 j \u2212 \u03b2 j 1 \u2265 8\u03bb j s \u03c6 + 4d \u03c3 2 n \u03c6 \u03c7 \u2264 2d exp \u2212 \u03c7 2 2 + 2d exp \u2212 \u03bb 2 j n j 32\u03c3 2 j x 2 max + P \u03bb min ( \u03a3 ) \u2264 \u03c6 + P \u03a3 j \u2208 C(S j , \u03c6 ) , for any \u03bb j > 0 and 0 < \u03c7.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2. Robust Multitask Estimator with Random Design"
        },
        {
            "text": "The proof mainly follows that of Theorem 1 and Proposition 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "Differently, now we use \u03b2 as an estimate of the shared parameter \u03b2 \u2020 . On the event {\u03bb min ( \u03a3 ) \u2265 \u03c6}, note that \u03b2 = (X X ) \u22121 X Y is a subgaussian random vector with mean \u03b2 . In particular, the i th component of \u03b2, i.e., \u03b2 (i) , is \u03c3 2 n \u03c6 -subgaussian. Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "for any 0 < \u03c7. Using a union bound on all i \u2208 S c , we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "since \u03b2 * = \u03b2 . The following still holds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "where \u03b2 \u2020 (S) \u2212 \u03b2 * (S) + \u03b4 j is now at most 2s-sparse. Here we follow a proof strategy of transfer learning using LASSO as in Bastani (2021) and Xu et al. (2021) . Similar to the proof of Theorem 1, we can derive from the basic inequality of LASSO that",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 162,
                    "text": "Xu et al. (2021)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "Now we consider two cases respectively:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "In the first case, we can obtain from inequality (33) that \u03b2 j \u2212 \u03b2 j 1 \u2264 8 ( \u03b2 * \u2212 \u03b2 \u2020 ) (S c ) 1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "In the second case, it holds that ( \u03b2 j \u2212 \u03b2 j ) (S c j ) 1 \u2264 7 ( \u03b2 j \u2212 \u03b2 j ) (S j ) 1 . Therefore, on the event { \u03a3 j \u2208 C(S j , \u03c6 )}, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "Combining all the above, we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "with a high probability. With a union bound, we obtain the following concentration inequality:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "Following a similar argument in the proof of Proposition 1, we get P \u03b2 j \u2212 \u03b2 j 1 \u2265 8\u03bb j s \u03c6 + 4d \u03c3 2 n \u03c6 \u03c7 \u2264 2d exp \u2212 \u03c7 2 2 + 2d exp \u2212 \u03bb 2 j n j 32\u03c3 2 j x 2 max + P \u03bb min ( \u03a3 ) \u2264 \u03c6 + P \u03a3 j \u2208 C(S j , \u03c6 ) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Proposition 8"
        },
        {
            "text": "Lemma 23. When |B j 0,k | \u2265 3 log(d)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.3. Forced-Sample Estimator"
        },
        {
            "text": "C 2 , we have given |B j 0,k | P \u03a3 j k (B j 0,k ) \u2208 C(S j ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.3. Forced-Sample Estimator"
        },
        {
            "text": "Proof of Lemma 23 See Lemma EC.6 in Bastani and Bayati (2020) .",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 61,
                    "text": "Bastani and Bayati (2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "D.3. Forced-Sample Estimator"
        },
        {
            "text": "Proposition 9. When \u03b6 0 , \u03bb 0,j , \u03b7 0 , q take the values in Theorem 3, the forced-sample estimator of instance j and arm k satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.3. Forced-Sample Estimator"
        },
        {
            "text": "we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.3. Forced-Sample Estimator"
        },
        {
            "text": "for any k \u2208 [K].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.3. Forced-Sample Estimator"
        },
        {
            "text": "Proposition 10. When q take the values in Theorem 3, the forced-sample estimator of instance j and arm k satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.3. Forced-Sample Estimator"
        },
        {
            "text": "Our original dataset consists of 9948 patients observed from 379 healthcare providers. However, many of these providers observe very few patients, so we restrict our experiment to the N = 13 largest hospitals, of which each has at least 150 unique patients (mean 317; median 301) observed during the sample period.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F.2. Diabetes Experiment Details"
        },
        {
            "text": "We take K = 2 since our arms are either to intervene or not intervene on the patient. We consider a simple binary reward that directly evaluates the accuracy of our classification of patients, i.e., the reward is 1 if the prediction is correct, and 0 otherwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F.2. Diabetes Experiment Details"
        },
        {
            "text": "We perform standard variable selection as a pre-processing step in order to avoid overfitting when computing our linear oracles. In particular, we run a LASSO variable selection procedure by regressing diabetes outcomes against the 184 total features (note that we exclude the healthcare providers that we use in our experiment in this step to avoid overfitting), and we tune the hyperparameters using 10-fold cross-validation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F.2. Diabetes Experiment Details"
        },
        {
            "text": "This leaves us with roughly 80 commonly predictive features (depending on the randomness in the crossvalidation procedure). Note that this is still a relatively large number of features compared to the number of observations, supporting our argument that arm parameters are likely dense. We fit a linear oracle to data from the target provider in hindsight; to avoid overfitting, we use a leave-one-out approach, i.e., for each patient, we train the best linear model on all data from the target provider excluding the current patient.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F.2. Diabetes Experiment Details"
        },
        {
            "text": "Our oracle is constructed to provide the best achievable mean squared error within a linear model family.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F.2. Diabetes Experiment Details"
        },
        {
            "text": "We run the bandit algorithms on this data in the same manner as in the synthetic setup in the previous subsection. Once again, to ensure fair comparison, we tune the hyperparameters of all algorithms, and we report the optimized results in Figure 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 240,
                    "end": 248,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "F.2. Diabetes Experiment Details"
        },
        {
            "text": "Data: The original dataset covers 145 weeks of orders from 77 fulfillment centers across 51 cities. There are 14 different categories (e.g., beverages, snacks) and 4 different cuisines (e.g., Indian, Italian) for meals delivered by the company. To ensure similarity, we restrict our experiment to fulfillment centers in the largest city, and further exclude two centers that do not supply most food varieties. Thus, we have N = 7 centers, each processing an average (median) of 6,747 (6,894) orders during the sample period. One order arrives at each time step, and the chosen price is the checkout price, which includes discounts, taxes and delivery charges. The order price in our data ranges from $55 to $729; thus, we set p min = 0 and p max = 1, 000.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F.3. Pricing Experiment Details"
        },
        {
            "text": "Following standard practice, we also normalize the price so it has a similar scale as the other features. Our outcome (demand) is given by the quantity in each order. The contexts are order-specific features including dummy variables capturing the category and cuisine, indicators of email or homepage promotions, and an intercept. Overall, our X t has dimension 19, and therefore the dimensionality of the unknown parameters of the pricing model d = 38.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F.3. Pricing Experiment Details"
        },
        {
            "text": "Algorithm: We now embed our robust multitask estimator within the ILSX/ILQX algorithmic approach proposed in Ban and Keskin (2021) to design the RMX algorithm; similarly, we embed the Laplacian estimator used by GOBLin (Cesa-Bianchi et al. 2013) to design the GOBX algorithm.",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 130,
                    "text": "Ban and Keskin (2021)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 219,
                    "end": 245,
                    "text": "(Cesa-Bianchi et al. 2013)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "F.3. Pricing Experiment Details"
        },
        {
            "text": "Let \u03b2 j = \u03b2 j 0 \u03b2 j 1 denote the unknown parameters for instance j. For our forced samples, we fix two experimental prices p 1 = 200 and p 2 = 600, which we charge in two sets of periods M j i = t t r=1 1(Z r = j) = E 2 + i \u2212 1, E = 1, 2, \u00b7 \u00b7 \u00b7 for each experimental price i \u2208 [2] and each instance j \u2208 [N ]. Note that M j i is a random set in the multitask setting, since it depends on the realization of arrivals Z t . Let M j = M j 1 \u222a M j 2 represent the forced price experimentation period, and let M j t = {r|r \u2208 M j , r < t} be the set of time periods when prices are forced at instance j before time t. We update our estimators at time periods M = t t = N (E 2 + 1), E = 1, 2, \u00b7 \u00b7 \u00b7 , so that each instance obtains the same number of training observations in expectation as in the singleinstance setting. Then, the samples used for estimating the optimal price at time t are \u222a j\u2208[N ] M j \u03b3 t , where \u03b3 t = max {r | r \u2208 M, r < t}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F.3. Pricing Experiment Details"
        },
        {
            "text": "Note that we now only maintain a single set of estimated parameters for each algorithm. We denote our robust multitask estimator (Algorithm 1) at instance j at time t a\u015d \u03b2 j (\u222a j\u2208[N ] M j \u03b3 t , \u03bb j,t , \u03c9 t ). The first argument indicates the training data, i.e., all observations with price experimentation before time \u03b3 t (recall that the robust multitask estimators are only updated at t \u2208 M ); the remaining arguments are hyperparameters. We denote the estimated optimal price of user X t at instance j at time t as p j (X t ,\u03b2 j ) = X t\u03b2 j 0 \u22122X t\u03b2 j 1 , which is truncated at p min and p max . We formalize our algorithm in Algorithm 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F.3. Pricing Experiment Details"
        },
        {
            "text": "Inputs: Initial hyperparameters \u03b6 0 , \u03b7 0 , {\u03bb j,0 } j\u2208[N ] Define {M j i } i\u2208 [2] , M , and \u03c9 0 = \u03b6 0 + \u03b7 0 for t \u2208 [T ] do",
            "cite_spans": [
                {
                    "start": 79,
                    "end": 82,
                    "text": "[2]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Algorithm 3 Robust Multitask Regression with Price Experimentation (RMX)"
        },
        {
            "text": "Observe an arrival at instance j = Z t and corresponding context vector X t for this arrival if t \u2208 M j i then Charge price p t = p i else Charge price p t = p j (X t ,\u03b2 j ) end if if t \u2208 M then Update \u03b6 t = \u03b6 0 , \u03b7 t = \u03b7 0 log(d min j\u2208[N ],|M j \u03b3 t |>0 |M j \u03b3 t |), and \u03c9 t = \u03b6 t + \u03b7 t Update \u03bb j,t = \u03bb j,0 |M j \u03b3 t | 1 4 log(d|M j \u03b3 t |) Estimate parameter \u03b2 j (\u222a j\u2208[N ] M j \u03b3 t , \u03bb j,t , \u03c9 t ) end if Observe demand Y t = X t \u03b2 j 0 + p t \u00b7 (X t \u03b2 j 1 ) + j t end for",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 3 Robust Multitask Regression with Price Experimentation (RMX)"
        },
        {
            "text": "The GOBX algorithm follows exactly as in Algorithm 3, but uses the Laplacian-regularized estimator from (Cesa-Bianchi et al. 2013 ) instead of our robust multitask estimator. Once again, to ensure fair comparison, we tune the hyperparameters of all algorithms, and we report the optimized results in Figure 4 .",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 129,
                    "text": "(Cesa-Bianchi et al. 2013",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 300,
                    "end": 308,
                    "text": "Figure 4",
                    "ref_id": null
                }
            ],
            "section": "Algorithm 3 Robust Multitask Regression with Price Experimentation (RMX)"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Improved algorithms for linear stochastic bandits",
            "authors": [
                {
                    "first": "Yasin",
                    "middle": [],
                    "last": "Abbasi-Yadkori",
                    "suffix": ""
                },
                {
                    "first": "D\u00e1vid",
                    "middle": [],
                    "last": "P\u00e1l",
                    "suffix": ""
                },
                {
                    "first": "Csaba",
                    "middle": [],
                    "last": "Szepesv\u00e1ri",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "The probabilistic method",
            "authors": [
                {
                    "first": "Noga",
                    "middle": [],
                    "last": "Alon",
                    "suffix": ""
                },
                {
                    "first": "Joel",
                    "middle": [
                        "H"
                    ],
                    "last": "Spencer",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Detecting customer trends for optimal promotion targeting. Manufacturing & Service Operations Management",
            "authors": [
                {
                    "first": "Lennart",
                    "middle": [],
                    "last": "Baardman",
                    "suffix": ""
                },
                {
                    "first": "Setareh",
                    "middle": [],
                    "last": "Borjian Boroujeni",
                    "suffix": ""
                },
                {
                    "first": "Tamar",
                    "middle": [],
                    "last": "Cohen-Hillel",
                    "suffix": ""
                },
                {
                    "first": "Kiran",
                    "middle": [],
                    "last": "Panchamgam",
                    "suffix": ""
                },
                {
                    "first": "Georgia",
                    "middle": [],
                    "last": "Perakis",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Ban",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gah-Yi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bora Keskin",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Management Science",
            "volume": "67",
            "issn": "9",
            "pages": "5549--5568",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Predicting with proxies: Transfer learning in high dimension",
            "authors": [
                {
                    "first": "Hamsa",
                    "middle": [],
                    "last": "Bastani",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Management Science",
            "volume": "67",
            "issn": "5",
            "pages": "2964--2984",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Online decision making with high-dimensional covariates",
            "authors": [
                {
                    "first": "Hamsa",
                    "middle": [],
                    "last": "Bastani",
                    "suffix": ""
                },
                {
                    "first": "Mohsen",
                    "middle": [],
                    "last": "Bayati",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Operations Research",
            "volume": "68",
            "issn": "1",
            "pages": "276--294",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Mostly exploration-free algorithms for contextual bandits",
            "authors": [
                {
                    "first": "Hamsa",
                    "middle": [],
                    "last": "Bastani",
                    "suffix": ""
                },
                {
                    "first": "Mohsen",
                    "middle": [],
                    "last": "Bayati",
                    "suffix": ""
                },
                {
                    "first": "Khashayar",
                    "middle": [],
                    "last": "Khosravi",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Management Science",
            "volume": "67",
            "issn": "3",
            "pages": "1329--1349",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Gkikas Magiorkinis, Dimitrios Paraskevis, Sotirios Tsiodras. 2021b. Efficient and targeted covid-19 border testing via reinforcement learning",
            "authors": [
                {
                    "first": "Hamsa",
                    "middle": [],
                    "last": "Bastani",
                    "suffix": ""
                },
                {
                    "first": "Kimon",
                    "middle": [],
                    "last": "Drakopoulos",
                    "suffix": ""
                },
                {
                    "first": "Vishal",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "Ioannis",
                    "middle": [],
                    "last": "Vlachogiannis",
                    "suffix": ""
                },
                {
                    "first": "Christos",
                    "middle": [],
                    "last": "Hadjicristodoulou",
                    "suffix": ""
                },
                {
                    "first": "Pagona",
                    "middle": [],
                    "last": "Lagiou",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Nature",
            "volume": "599",
            "issn": "7883",
            "pages": "108--113",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Meta dynamic pricing: Transfer learning across experiments",
            "authors": [
                {
                    "first": "Hamsa",
                    "middle": [],
                    "last": "Bastani",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Simchi-Levi",
                    "suffix": ""
                },
                {
                    "first": "Ruihao",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Management Science",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A theory of learning from different domains",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Ben-David",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Shai",
                    "suffix": ""
                },
                {
                    "first": "Koby",
                    "middle": [],
                    "last": "Blitzer",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Crammer",
                    "suffix": ""
                },
                {
                    "first": "Fernando",
                    "middle": [],
                    "last": "Kulesza",
                    "suffix": ""
                },
                {
                    "first": "Jennifer",
                    "middle": [
                        "Wortman"
                    ],
                    "last": "Pereira",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Vaughan",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Machine learning",
            "volume": "79",
            "issn": "1",
            "pages": "151--175",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "An analytics approach to designing combination chemotherapy regimens for cancer",
            "authors": [
                {
                    "first": "Dimitris",
                    "middle": [],
                    "last": "Bertsimas",
                    "suffix": ""
                },
                {
                    "first": "Allison O&apos;",
                    "middle": [],
                    "last": "Hair",
                    "suffix": ""
                },
                {
                    "first": "Stephen",
                    "middle": [],
                    "last": "Relyea",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Silberholz",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Management Science",
            "volume": "62",
            "issn": "5",
            "pages": "1511--1531",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms",
            "authors": [
                {
                    "first": "Omar",
                    "middle": [],
                    "last": "Besbes",
                    "suffix": ""
                },
                {
                    "first": "Assaf",
                    "middle": [],
                    "last": "Zeevi",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Operations Research",
            "volume": "57",
            "issn": "6",
            "pages": "1407--1420",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics",
            "authors": [
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Bickel",
                    "suffix": ""
                },
                {
                    "first": "Alexandre",
                    "middle": [],
                    "last": "Ritov",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Tsybakov",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1705--1732",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Statistics for high-dimensional data: methods, theory and applications",
            "authors": [
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "B\u00fchlmann",
                    "suffix": ""
                },
                {
                    "first": "Sara",
                    "middle": [],
                    "last": "Van De",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Geer",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics",
            "authors": [
                {
                    "first": "Emmanuel",
                    "middle": [],
                    "last": "Candes",
                    "suffix": ""
                },
                {
                    "first": "Terence",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2313--2351",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Multitask learning",
            "authors": [
                {
                    "first": "Rich",
                    "middle": [],
                    "last": "Caruana",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Machine learning",
            "volume": "28",
            "issn": "1",
            "pages": "41--75",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Meta-learning with stochastic linear bandits. International Conference on Machine Learning. PMLR",
            "authors": [
                {
                    "first": "Leonardo",
                    "middle": [],
                    "last": "Cella",
                    "suffix": ""
                },
                {
                    "first": "Alessandro",
                    "middle": [],
                    "last": "Lazaric",
                    "suffix": ""
                },
                {
                    "first": "Massimiliano",
                    "middle": [],
                    "last": "Pontil",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1360--1370",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "A gang of bandits",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Cesa-Bianchi",
                    "suffix": ""
                },
                {
                    "first": "Claudio",
                    "middle": [],
                    "last": "Nicolo",
                    "suffix": ""
                },
                {
                    "first": "Giovanni",
                    "middle": [],
                    "last": "Gentile",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Zappella",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1306.0811"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Atomic decomposition by basis pursuit",
            "authors": [
                {
                    "first": "Scott",
                    "middle": [
                        "S"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Donoho",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Michael",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Saunders",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Promotion optimization in retail. Available at SSRN 3194640",
            "authors": [
                {
                    "first": "Maxime",
                    "middle": [],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "Georgia",
                    "middle": [],
                    "last": "Perakis",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Learning from multiple sources",
            "authors": [
                {
                    "first": "Koby",
                    "middle": [],
                    "last": "Crammer",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Kearns",
                    "suffix": ""
                },
                {
                    "first": "Jennifer",
                    "middle": [],
                    "last": "Wortman",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Journal of Machine Learning Research",
            "volume": "9",
            "issn": "8",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Multi-task learning for contextual bandits",
            "authors": [
                {
                    "first": "Aniket",
                    "middle": [],
                    "last": "Deshmukh",
                    "suffix": ""
                },
                {
                    "first": "Urun",
                    "middle": [],
                    "last": "Anand",
                    "suffix": ""
                },
                {
                    "first": "Clayton",
                    "middle": [],
                    "last": "Dogan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Scott",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1705.08618"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "High-dimensional gaussian process bandits. Advances in neural information processing systems 26",
            "authors": [
                {
                    "first": "Josip",
                    "middle": [],
                    "last": "Djolonga",
                    "suffix": ""
                },
                {
                    "first": "Andreas",
                    "middle": [],
                    "last": "Krause",
                    "suffix": ""
                },
                {
                    "first": "Volkan",
                    "middle": [],
                    "last": "Cevher",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Distributed linear regression by averaging",
            "authors": [
                {
                    "first": "Edgar",
                    "middle": [],
                    "last": "Dobriban",
                    "suffix": ""
                },
                {
                    "first": "Yue",
                    "middle": [],
                    "last": "Sheng",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "The Annals of Statistics",
            "volume": "49",
            "issn": "2",
            "pages": "918--943",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Learning multiple tasks with kernel methods",
            "authors": [
                {
                    "first": "Theodoros",
                    "middle": [],
                    "last": "Evgeniou",
                    "suffix": ""
                },
                {
                    "first": "Charles",
                    "middle": [
                        "A"
                    ],
                    "last": "Micchelli",
                    "suffix": ""
                },
                {
                    "first": "Massimiliano",
                    "middle": [],
                    "last": "Pontil",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Shawe-Taylor",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Journal of machine learning research",
            "volume": "6",
            "issn": "4",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Regret Bound Finally, we prove Theorem 2, which provides a bound on the cumulative regret across all bandit instances. Lemma 17. The cumulative expected regret from the first two batches B 0 and B 1 is at most",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "approaches in the first bullet point operate N independent bandit instances, either via ordinary linear regression (Goldenshluger and Zeevi 2013) or LASSO (Bastani and Bayati 2020). The GOBLin algorithm is a state-of-the-art multitask bandit algorithm that uses a Laplacian matrix and ridge regression to jointly estimate the N parameters, thereby 2 -regularizing both the parameters and their pairwise differences (Cesa-Bianchi et al. 2013). It builds on the OFUL algorithm (Abbasi-Yadkori et al. 2011), which leverages UCB for linear contextual bandits. Remark 1. There are a few alternative multitask bandit algorithms that are not applicable to our experimental setup. For instance, Soare et al. (2014) and Gentile et al. (2014) simply pool data",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "shows the expected cumulative regret over time for a single (a) standard and (b) data-poor contextual linear bandit instance. Appendix F.1 provides details on the underlying parameters.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Lines depict the cumulative regret averaged over 15 trials (shaded regions depict the corresponding 95% confidence intervals) of a single linear contextual bandit that receives (a) similar traffic as neighbouring instances, or (b) significantly less traffic than neighbouring instances.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Lines depict the fraction of incorrect classifications averaged over 20 trials (shaded regions depict the corresponding 95% confidence intervals) of a linear contextual bandit operated by (a) a mid-sized provider with 355 unique patients, or (b) a data-poor provider with 176 unique patients.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "shows the cumulative regret of our algorithm RMX (our dynamic pricing analog of RMBandit) compared to other benchmarks including ILQX and ILSX (the LASSO and OLS based pricing algorithms introduced in Ban and Keskin 2021) and GOLX (our dynamic pricing analog of GOB-Lin). Note that all 7 fulfillment centers are similarly sized (the smallest has 6072 orders and the largest has 7046 orders); thus, we do not study the data-poor setting in this experiment. Appendix F.3 provides details on the setup, as well as pseudocode for our dynamic pricing algorithms. Lines depict the cumulative regret averaged over 40 trials (shaded regions depict the corresponding 95% confidence intervals) of a linear contextual bandit operated by a single fulfillment center.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "available. Recall from \u00a72.1 that we consider a fully connected graph, where the nodes are instances and the edges (i, j) \u2208 [N ] \u00d7 [N ] have weights s i,j = \u03b2 j \u2212 \u03b2 i 0 that indicate relative sparsity. Then, for any given problem instance j, we can optimize the subset of instances Q j \u2286 [N ] from which to transfer knowledge, where Q j = {i \u2208 [N ] | s i,j \u2264s} is the subset of instances that are have an edge weight of no more thans to instance j. Denote the corresponding number of instances\u00d1 = |Q j |.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Evgeniou, Theodoros, Massimiliano Pontil. 2004. Regularized multi-task learning. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. 109-117. Finn, Chelsea, Pieter Abbeel, Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. International Conference on Machine Learning. PMLR, 1126-1135. Lai, Tze Leung, Herbert Robbins. 1985. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics 6(1) 4-22. Li, Jerry. 2019. Lecture 1: Introduction to robustness. https://jerryzli.github.io/robust-ml-fall19/ lec1.pdf. [Online; accessed 13-April-2021]. Li, Lihong, Wei Chu, John Langford, Robert E Schapire. 2010. A contextual-bandit approach to personalized news article recommendation. Proceedings of the 19th international conference on World wide web. Lounici, Karim, Massimiliano Pontil, Alexandre B Tsybakov, Sara Van De Geer. 2009. Taking advantage of sparsity in multi-task learning. arXiv preprint arXiv:0903.1468 . Lounici, Karim, Massimiliano Pontil, Sara Van De Geer, Alexandre B Tsybakov. 2011. Oracle inequalities and optimal inference under group sparsity. The annals of statistics 39(4) 2164-2204. Tropp, Joel A. 2015. An introduction to matrix concentration inequalities. arXiv preprint arXiv:1501.01571 . van Herpen, Erica, Erjen van Nierop, Laurens Sloot. 2012. The relationship between in-store marketing and observed sales for organic versus fair trade products. Marketing Letters 23(1) 293-308. Yuan, Hao, Qi Luo, Cong Shi. 2021. Marrying stochastic gradient descent with bandits: Learning algorithms for inventory systems with fixed costs. Management Science . Appendix A: Proof of Tail Inequality for the Robust Multitask Estimator A.1. Proof of Lemma 1: Tail Inequality for Trimmed-Mean Estimator",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Now, consider our robust multitask estimator { \u03b2 j RM } j\u2208[N ] computed by Algorithm 1. Recall that for any poorly-aligned component i \u2208 I poor , the corresponding corrupted subset of instances is J i = {j \u2208 [N ] | \u03b2 j",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Lemma 12. (i)B j m,k \u2286 B j m,k , and (ii) the samples {X t } t\u2208B j m,k are i.i.d. from P j X|\u03c0 j m\u22121 (X)=k conditional on F m\u22121 , and its subset {X t } t\u2208B j m,k are i.i.d. from P j",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Proof of Theorem 2 Summing up the expected regrets in the three cases obtained in Lemma 17, 18 and 21, we can upper bound the total cumulative expected regret up to time T by R T \u2264 4bx max q log(T ) + 20bx max KN",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "In this section, we give hyperparameter choices and a proof for Theorem 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix C: Proof of Regret Bound for RMBandit Algorithm"
        },
        {
            "text": "First, we give the hyperparameter choices for Theorem 2 to hold. We takeK\u03c3 2 i p * \u03c8p i |B 0 | ), \u03b7 1,0 = 9 \u03c1N , \u03bb 0,j = p * \u03c8h 192x max (sd) 1/2 , \u03bb 1,j,0 = 64\u03c3 2 j x 2 max p * , on {A, F m\u22121 , {X t } t\u2208Bm } so the conditions of Proposition 1 are satisfied. Then, applying Proposition 1 by setting \u03c6 = p * \u03c8 4 and integrating over {X t } t\u2208Bm , we getNext consider the eventsNote that inequality (27) also holds given \u2229 i\u2208W k D i m,k . With a union bound and taking expectation overwhere we use Lemma 13. Similarly, take \u03b6 = ( s d ) 1 2 and setNote that on the event A, |B i m | = 0 for any i \u2208 [N ] \\ W k . Thus, we can equivalently takeThen, define the eventsFor any B m with m > 0, we have |B m | \u2265 q log(T ); given the value of q in Proposition 2, we further have log(min i\u2208W k p i |B m |/2) \u2265 log(15d log(N ) log(T )) \u2265 1. Thus, on the events \u2229 i\u2208W k M i m , log( 3 \u03b7 ) \u2264 log(\u03c1N ). Moreover, note that log(x)/x is monotonically decreasing when x > 3. Using a union bound over \u2229 i\u2208W k M i m through Lemma 16 on inequality (28), we obtainwhere C 1 , C 2 , C 3 are listed in Theorem 3. In addition, to satisfy \u03b7 \u2264 1/2 \u2212 c \u2212 \u03b6, we requireProof of Lemma 17 By our design, we have 2q log(T ) time steps in total from B 0 and B 1 . The worst-case regret per step is 2bx max . The result then follows. Proof of Lemma 19 By our design, |B m | = 2 m\u22121 |B 0 | for any m \u2265 1, which implies for any m > 1Lemma 20. When A holds and Z t = j, the expected regret at time t \u2208 B m with m > 1 is upper bounded byProof of Lemma 20 Without loss of generality, assume arm 1 is optimal for X t , i.e., arg max k\u2208[K] X t \u03b2 j k = 1. Note that here the optimal arm is a function of X t and hence a random variable, though for simplicity we fix arm 1 as the optimal arm. Consider the following conditional expected regret at time tTo bound the above expectation, define the eventThen, we can decompose the upper bound of the regret into two parts given L jLemma 21. When A holds and log(T ) = O(N ), the cumulative expected regret from all the batchesMoreover, given q in Proposition 2 and whenTherefore, based on Lemma 20, the cumulative expected regret conditional on A is at most48x 2 max LK (C 2 1 log(dp j T ) + C 2 2 log(\u03c1N ))sd log(p j T ) + C 2 3 d 2 log(\u03c1N ) N log(p j T ) log(dp j T )Note that we also require inequality (29) to hold. For any m \u2265 1, we have |B m | \u2264 T 2 . Therefore, inequality (29) is satisfied if2 ) 2 9.Proof of Proposition 9 The proof mainly follows that of Proposition 2. Similarly, applying Proposition 8,Now we configure the parameters \u03bb j and \u03c7 such that our forced-sample estimator of arm k and instance j has estimation error smaller than h 4xmax . TakeFor the first term on the right hand side of (34) to be less than 2/T , it suffices to haveFinally, we requireso that the sum of the last four probability terms in inequality (22) is no greater than 8/T . Moreover, to satisfy |B j 0,k | \u2265 3 log(d)in Lemma 23, we also require q \u2265 12K log(d) C 2 p * p j . As a result, lettingProof of Proposition 10 For the data-rich instance , we haveSetting the value \u03c7 and q as in Propositon 9, we havefor any k \u2208 [K]. The result then follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1. Hyperparameter Choices"
        },
        {
            "text": "Lemma 24. The event A holds with at least a probability of 1 \u2212 20K T .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.4. All-Sample Estimator"
        },
        {
            "text": "The result follows by applying a union bound over all arms and bandit instances using Proposition 9.Assume that the optimal arm of instance is the same as that of instance j so that \u03c1 = 1.Proposition 11. When A holds, and \u03b6 1,0 , \u03bb 1,j,0 , \u03b7 1,0 take the values in Theorem 3, the all-sample estimator of instance j and optimal arm k \u2208 K j opt using data from the batch B m with m \u2265 1 satisfies Proof of Proposition 11 The proof follows that of Proposition 3. Applying Proposition 8, we get.Similarly, takeThen, define the eventsWith a union bound, we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Lemma 24"
        },
        {
            "text": "Proof of Theorem 3 The cumulative expected regret of any target instance j isUsing a similar argument in the proof of Corollary 3, we haveSince d 2 p j p = O(1) and p j q = \u0398(Ks 2 log(d)), it implies R j T = O Ks 2 log 2 (dT ) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.5. Single Bandit Instance"
        },
        {
            "text": "This appendix collects useful results from the literature.Lemma 25. Suppose X is \u03c3-subgaussian with mean \u00b5. Then, for any t \u2265 0, it holds that Rigollet and H\u00fctter (2015) .Lemma 26. Let X = X 1 \u00b7 \u00b7 \u00b7 X n be a vector of n independent \u03c3-subgaussian random variables with mean \u00b5. Then, for any a \u2208 R n and t \u2265 0, it holds thatProof of Lemma 26 See Corollary 1.7 of Rigollet and H\u00fctter (2015) .Lemma 27 (Matrix Chernoff Bound). Consider a sequence of independent random symmetric matrices X k \u2208 R d\u00d7d , k = 1, \u00b7 \u00b7 \u00b7 , n with \u03bb min (X k ) \u2265 0 and \u03bb max (X k ) \u2264 L for any k. Let \u00b5 = \u03bb min (E[ n k=1 X k ]). We have for 0 < t < 1 thatProof of Lemma 27 See page 61 in Tropp (2015).Lemma 28. Suppose X 1 , \u00b7 \u00b7 \u00b7 , X n are n independent Bernoulli random variables with mean p 1 , \u00b7 \u00b7 \u00b7 , p n respectively. Let \u00b5 = n i=1 p i . Then, we haveProof of Lemma 28 The result follows by taking = 1/2 in Corollary A.1.14 of Alon and Spencer (2004) .",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 169,
                    "text": "Rigollet and H\u00fctter (2015)",
                    "ref_id": null
                },
                {
                    "start": 361,
                    "end": 387,
                    "text": "Rigollet and H\u00fctter (2015)",
                    "ref_id": null
                },
                {
                    "start": 905,
                    "end": 928,
                    "text": "Alon and Spencer (2004)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Appendix E: Auxiliary Results"
        },
        {
            "text": "In the standard setting, we take the number of instances N = 10, the number of arms K = 10, the context dimension d = 20, and the sparsity s = 2. Our total time horizon across instances T = 40, 000 and the arrival probability p j = 1/N for all j \u2208 [N ]; thus, each instance will receive an expected 4, 000 observations. We generate the shared parameters {\u03b2 \u2020 k } k\u2208[K] by drawing independently from a gaussian distribution N (0, I), and normalizing them \u03b2 \u2020 k 1 = 1. We set the first instance's parameters to be equal to the shared parameters; for the remaining instances, we draw the nonzero entries of the bias terms \u03b4 j k independently from a uniform distribution on [\u22120.5, 0.5]. Note that \u03b2 j k 1 \u2264 2 for all j \u2208 [N ]. Next, we draw the context vectors X t independently from a gaussian distribution N (0, I), and truncate them so that X t \u221e = 1. We draw the noise j t independently from a gaussian distribution N (0, \u03c3 2 j ) with \u03c3 j = 0.05. We use the same setup in the data-poor setting, but modify N = 2 and take the arrival probability p 1 = p 2 /100; accordingly, in order to simulate a similar time horizon for the data-poor instance, we increase the time horizon across instances to T = 400, 000.To ensure fair comparison, we tune the hyperparameters of all algorithms on a pre-specified grid. Matching the suggestion by Bastani and Bayati (2020) , we take h = 15, q = 1 and \u03bb 0,j = \u03bb 1,j,0 = 0.02 for Lasso and OLS Bandit. We take \u03b1 = 1 for GOBLin. We take \u03b7 0 = \u03b7 1,0 = 0.2, h = 15 and q = 50 for RMBandit in the standard setting; in the data-poor setting, we increase q = 300 as suggested by the theory.",
            "cite_spans": [
                {
                    "start": 1333,
                    "end": 1358,
                    "text": "Bastani and Bayati (2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "F.1. Synthetic Experiment Details"
        }
    ]
}
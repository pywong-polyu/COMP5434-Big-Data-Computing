{
    "paper_id": "29a61b89491389a498dfa79eb8084036da159d50",
    "metadata": {
        "title": "A Cross-lingual Natural Language Processing Framework for Infodemic Management",
        "authors": [
            {
                "first": "Ridam",
                "middle": [],
                "last": "Pal",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "#",
                "middle": [
                    "Rohan"
                ],
                "last": "Pandey",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Shiv Nadar University",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "#",
                "middle": [
                    "Vaibhav"
                ],
                "last": "Gautam",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Shiv Nadar University",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Kanav",
                "middle": [],
                "last": "Bhagat",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Tavpritesh",
                "middle": [],
                "last": "Sethi",
                "suffix": "",
                "affiliation": {},
                "email": "tavpriteshsethi@iiitd.ac.in"
            }
        ]
    },
    "abstract": [
        {
            "text": "The COVID-19 pandemic has put immense pressure on health systems which are further strained due to the misinformation surrounding it. Under such a situation, providing the right information at the right time is crucial. There is a growing demand for the management of information spread using Artificial Intelligence. Hence, we have exploited the potential of Natural Language Processing for identifying relevant information that needs to be disseminated amongst the masses. In this work, we present a novel Cross-lingual Natural Language Processing framework to provide relevant information by matching daily news with trusted guidelines from the World Health Organization. The proposed pipeline deploys various techniques of NLP such as summarizers, word embeddings, and similarity metrics to provide users with news articles along with a corresponding healthcare guideline. A total of 36 models were evaluated and a combination of LexRank based summarizer on Word2Vec embedding with Word Mover distance metric outperformed all other models. This novel open-source approach can be used as a template for proactive dissemination of relevant healthcare information in the midst of misinformation spread associated with epidemics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Coronavirus disease , caused by the (SARS-Cov2) assumed pandemic proportions in March (Stoye 2020) . Since the outbreak of COVID-19, a massive amount of misinformation spread has taken place at an unprecedented rate through various social media platforms and other outlets (Vaezi and Javanmard 2020) . This infodemic has created further difficulties in identifying the right solutions as rapidly spreading false information hampers an effective public health response. This is all the more important in lower literacy settings, where guidelines in the local language may be unavailable. This work aims to bridge this gap of providing the right information, to the right people, at the right time in the right format. While several COVID-19 data resources of text were made public to users for building models, these have not been used effectively to mitigate widespread irrelevant information. This article demonstrates the feasibility of an NLP approach in mitigating the prevailing infodemic. A pipeline # contributed equally of various models including text summarizers, word embeddings, distance metrics, and human-in-the-loop evaluations has been developed. A total of 36 models have been evaluated in order to achieve text similarity between the news relevant to COVID-19 and WHO guidelines. The proposed pipeline follows the given workflow.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 98,
                    "text": "(Stoye 2020)",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 273,
                    "end": 299,
                    "text": "(Vaezi and Javanmard 2020)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 News articles and WHO guidelines related to are collected.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 These were pre-processed to remove the unwanted characters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 These pairs were then summarized using different summarizers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 The embeddings of the summarized News articles and WHO guidelines were calculated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 The similarity between these pairs was calculated as the distance between the summarized embeddings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Using the proposed pipeline, multiple embeddings, summarization techniques, and distance metrics are effectively used as a tool to mitigate infodemic by supplementing user's daily news consumption. This is done by providing relevant and vetted content to users from sources such as WHO along with the news. This work is first of its kind, incorporating NLP techniques in order to address the issue of widespread irrelevant information accompanying the pandemic and sets the baseline for all future works. This open-source pipeline can act as a template for addressing the misinformation fight which is inevitable in such scenarios.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section the dataset used and the techniques deployed in the pipeline have been described.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "The news articles dataset was scrapped from a vernacular Hindi News Portal's (Dainik Jagran) online publication. Articles presented to the annotators were in English which were translated from Hindi using Google Translate API. WHO guidelines related to COVID-19 was scrapped manually from the WHO website. WHO COVID-19 guidelines and News Articles were collected starting from February 2020 to May 2020. These news articles were filtered to arXiv:2010.16357v1 [cs.CL] 30 Oct 2020 Relevant Irrelevant News Article: Are Newspapers Safe Amid Coronavirus: Along with the coronavirus, rumors and myths related to it are spreading fast. Which in itself is very dangerous and is causing panic and fear in people. A similar myth is also associated with newspapers, claiming that the last can also be carriers of the coronavirus. However, according to WHO information, it is safe for anyone to receive and read the newspaper. It is safe to obtain any package from any area where COVID-19 infections have been detected. Answering questions related to the coronavirus, the WHO said, An infected person is less likely to contaminate commercial goods and there is less risk of infecting the the virus with a packet, which rotates, is placed, and exposed to various conditions and temperatures.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "News Article: Kanika Kapoor brother Anurag has spoken to his sister from London on her current situation. Coronavirus infected singer is admitted to a hospital in Lucknow. Kanika Kapoor coronavirus test positive. The singer recently came from London and went to Lucknow to be with her family. Anurag told Spot Boy, Yes, she went to London and after coming back she complained of a sore throat and flu. We got it tested (for coronavirus) and it came back positive. Kanika is currently admitted to PGI Hospital.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "WHO Guideline: It is not certain how long the virus that causes COVID-19 survives on surfaces,but it seems to behave like other coronaviruses. Studies suggest that coronaviruses (including preliminary information on the COVID-19 virus) may persist on surfaces for a few hours or up to several days. This may vary under different conditions (e.g. type of surface, temperature, or humidity of the environment). If you think a surface may be infected, clean it with simple disinfectant to kill the virus and protect yourself and others. Clean your hands with an alcohol-based hand rub or wash them with soap and water. Avoid touching your eyes, mouth, or nose.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "WHO Guideline: In a situation like this it is normal to feel sad, worried, confused, scared or angry. Know that you are not alone and talk to someone you trust, like your parent or teacher so that you can help keep yourself and your school safe and healthy. Ask questions, educate yourself and get information from reliable sources Wash your hands frequently,always with soap and water for at least 20 seconds. Remember to not touch your face. Do not share cups, eating utensils, food or drinks with others. Model good practices such as sneezing or coughing into your elbow and washing your hands, especially for younger family members. remove all the non-COVID articles. The distillation was done using the following keywords 'COVID', 'COVID-19', 'Corona', 'Coronavirus', 'Lockdown', and 'Pandemic'. A set of thousand random pairs were generated between news articles and the WHO guidelines. 8 annotators were assigned to annotate all 1000 pairs answering one simple question. The annotators were educated based on certain sample pairs of matching which were done among highly knowledgeable fellow researchers. Guidelines for matching the pairs were issued to the annotators for a clear perspective. The annotators were asked to determine whether the given set of pairs for news articles and WHO bulletins belong to a relevant or irrelevant class. Each annotator individually annotated each pair, thus there was no inter-user dependency and each rating was solely a single user's perspective.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "In order to clarify the scope of each class, possible cases that apply to the particular class were provided to the annotators. For instance, if the pair of WHO guidelines and news articles are contextually related, then the pair would be labeled as a relevant class. Similarly, it would be labeled as an irrelevant class if the context of the news article and WHO guidelines were not the same. For a better understanding of the guidelines and context for relevant and irrelevant classes, these classes were predefined with a set of examples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "Relevant: A matching between a news article and WHO guideline is defined as relevant if a particular user finds the WHO guideline accompanying the news article to be related to one another. An example where all the annotators have labeled the pair of samples as relevant is given in Table 1 . Irrelevant: If the user finds that the WHO guideline and the corresponding news article are unrelated, the matching is termed irrelevant. An example annotated as irrelevant by all annotators is given in Table 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 283,
                    "end": 290,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 496,
                    "end": 503,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "The evaluation of annotations was done using the Fleiss' Kappa metric. The Fleiss' Kappa metric determines the level of agreement between multiple annotators and in this case, the value turned out to be 0.235, implying a fair agreement among the annotators. The scale of Kappa value with interpretation have also been provided to understand the agreement among the annotators ( < 0 -No agreement, 0 -0.20 Slight, 0.21 -0.40 Fair, 0.41 -0.60 Moderate, 0.61 -0.80 Substantial, 0.81-1.0 Perfect ). The annotated pairs were later assigned binary labels 0 or 1 (irrelevant and relevant respectively) based on the majority voting for that particular pair done by the annotators. The pairs with more than 5 votes for the relevant class were labeled as 1 whereas the pairs with more than 5 votes for the irrelevant class were labeled with 0. In the case of a tie with four votes for relevant and irrelevant respectively, those samples were randomly distributed equally among both the classes so that there is no imbalance between two classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "The new data generated after annotation contained three columns, first contained the news article, the second column contained the WHO guidelines and the third column contained the binary labels determining the relevant and irrelevant class. The data was split into test and train sets with a ratio of 80:20 for modeling purposes as shown in table 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "After collection of the dataset and annotation from the annotators, pre-processing of both the news articles and the WHO guidelines was done to remove all unwanted characters. The processed data was fed into summarizers as input for generating the summaries of the news article and the WHO guidelines. The word embeddings of words in the generated summaries were calculated using Glove and Word2Vec (Pennington, Socher, and Manning 2014; Mikolov et al. 2013) . From the generated word embeddings, the weighted mean of all embedded word vectors was found using Smooth Inverse Frequency to obtain a single embedded vector representing the entire news article and WHO guidelines (Arora, Liang, and Ma 2016) . Distance metrics were used to find the similarity between the two generated article vectors (i.e between embedded vectors of the news article and WHO guidelines).",
            "cite_spans": [
                {
                    "start": 438,
                    "end": 458,
                    "text": "Mikolov et al. 2013)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 676,
                    "end": 703,
                    "text": "(Arora, Liang, and Ma 2016)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Proposed Workflow"
        },
        {
            "text": "The similarity score between all pairs in the training set was used to calculate the Youden's Index for each model to obtain the optimal similarity threshold, where pairs with similarity above the threshold are considered relevant and vice-versa (Youden 1950) . The optimal similarity threshold represents the similarity score at which the difference between the number of pairs correctly classified as relevant and the number of pairs incorrectly classified as relevant is maximized. This effectively serves the purpose of providing only relevant information to the end-user while simultaneously minimizing the delivery of irrelevant information. Further, the optimal threshold obtained for the particular model on the training set was used to evaluate the Youden's metric on the test set. The similarity scores were calculated for each model on the test pairs and pairs with similarity greater than the optimum threshold for the particular model comprised of the relevant class and pairs with similarity score below the threshold formed the irrelevant class. The original WHO guidelines and News articles were provided to the users corresponding to the relevant class. The detailed description of all the models and methodologies has been explained in the subsections below.",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 259,
                    "text": "(Youden 1950)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Proposed Workflow"
        },
        {
            "text": "Preprocessing Preprocessing is a method in which text is transformed into a machine-readable format. The preprocessing pipeline in our framework includes punctuation removal, conversion of input text to lowercase, tokenization, stop word removal, and stemming algorithm. Tokenization has been done using the NLTK tokenizer. For stemming, we have used Porter stemming which seemed to be appropriate for our use case on the basis of qualitative analysis of different stemming algorithms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Workflow"
        },
        {
            "text": "Summarization The effectiveness of numerous extractive summarization techniques has been explored in this framework to generate summaries from various lengths of news articles and WHO guidelines. This technique was used for reducing the size of the articles and guidelines while preserving the main context. These techniques prevent the introduction of errors, hence extracting meaningful sentences. This helped in removing unnecessary sections of text thereby making the framework efficient. The summarization of articles and guidelines thus enhanced the speed of the framework because the succeeding step was to generate word embeddings. The summaries of news articles and WHO guidelines contained only the important sentences and the calculation of word embeddings was done only for the words in these sentences. The following extractive text summarization techniques were used to elicit the most important sentences from these articles and guidelines.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Workflow"
        },
        {
            "text": "Graph-Based -For summarization of the news articles along with WHO guidelines, the following Graph-Based summarization techniques were used: LexRank, Reduction, PyTextRank, and TextRank with Gensim (Erkan and Radev 2004; Jing 2000; Nathan 2016 ; Barrios et al. 2016) . All these techniques are slight variations of the primitive PageRank Algorithm. TextRank uses the typical PageRank approach while LexRank additionally uses cosine similarity of TF-IDF vectors simultaneously accounting for the position and length of sentences (Mihalcea and Tarau 2004) . In these algorithms, concatenated text from the articles was split into individual sentences and embedded into vectors. The similarity score was calculated for these sentence vectors to generate a similarity matrix. The similarity matrix was then converted into a graph-based structure for sentence rank calculation, with similarity scores as the edges and sentences as the vertices of the matrix. Finally, a certain number of topranked sentences formed the final summary of the news articles and WHO guidelines.",
            "cite_spans": [
                {
                    "start": 198,
                    "end": 220,
                    "text": "(Erkan and Radev 2004;",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 221,
                    "end": 231,
                    "text": "Jing 2000;",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 232,
                    "end": 243,
                    "text": "Nathan 2016",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 246,
                    "end": 266,
                    "text": "Barrios et al. 2016)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 528,
                    "end": 553,
                    "text": "(Mihalcea and Tarau 2004)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Proposed Workflow"
        },
        {
            "text": "Topic-Based -Latent Semantic Analysis(LSA) Summarizer is a topic-based summarization technique (Ozsoy, Alpaslan, and Cicekli 2011) . This approach used a Bag-ofwords to create a term-document matrix for the news articles and WHO guidelines where rows represented the terms present in the articles and columns represented the sentences within the articles(which contained the terms). The termdocument matrix is represented by X where elements (i,j) represent the occurrence of term i in article j.",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 130,
                    "text": "(Ozsoy, Alpaslan, and Cicekli 2011)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Proposed Workflow"
        },
        {
            "text": "The rows of the matrix represented vectors corresponding to a term and its relation with different sentences present in the article. The columns represented vectors corresponding to sentences and their relation with different terms present in the corpus of the individual article. A decomposition of X was done such that U and V are orthogonal matrices. This decomposition of X is called singular value decomposition(SVD). The term-document matrix was decomposed into U and V, where U and V are orthogonal matrices of X. Figure 1 : Proposed Framework. The pipeline takes news articles and WHO guidelines as input. These are then preprocessed, summarized, converted to embeddings and finally, the distance between the generated embeddings is delivered as the similarity score. Various combinations of summarization techniques, embedding, and distance metrics were used.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 521,
                    "end": 529,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Proposed Workflow"
        },
        {
            "text": "Here is the matrix of singular values and u i and v i are left and right singular vectors. Matrix V T denotes the strength of each word in a sentence. Hence, it effectively extracted hidden semantic structures of words and sentences present in the articles. The important sentences for summaries were extracted using this Singular Value Decomposition(SVD).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Workflow"
        },
        {
            "text": "Feature-Based -The feature-based method of summarization used was Luhn's method (Luhn 1958) . One of the oldest approaches, Luhn summarization began with a frequency analysis of the words in the articles. This was followed by the calculation of the weight of the sentence based on the window size of fewer frequency words between highfrequency words. The sentences with the highest weights were given as the summary for the news articles and WHO guidelines.",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 91,
                    "text": "(Luhn 1958)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Proposed Workflow"
        },
        {
            "text": "Vocabulary Minimization -KL divergence based summarization is effectively a vocabulary minimization technique (Sripada and Jagarlamudi 2009 ). This method is based on minimizing the divergence of summarized vocabulary and input vocabulary, giving low KL divergence of a good summary and input document. It works by greedily adding sentences to a summary as long as it decreases the KL Divergence thus focusing on the minimization of summary vocabulary by checking the divergence from the input vocabulary. In our case, P was the original article distribution and Q was the summarized document distribution. The distribution of the summarized document was matched with the distribu-tion of the original text, thereby minimizing the difference using the KL divergence technique.",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 139,
                    "text": "(Sripada and Jagarlamudi 2009",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Proposed Workflow"
        },
        {
            "text": "Embedding Embedding is a method in which the words from the textual data are converted into numeric vectors that can be processed by the machines for modeling. Embeddings are extremely useful in reducing the dimensionality of the textual data. The technique used for calculating articlelevel word embedding is stated as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "\u2022 The word embeddings were calculated from the summary of the news articles and WHO guidelines using Word2vec or Glove. \u2022 The weighted average of these word embeddings was calculated using Smooth Inverse Frequency. \u2022 This weighted average was considered as the embedding for the summarised article.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "The following models were used for the validation of the architecture.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "Word2Vec -A neural network with two layers that takes text corpus as input and vectorizes it (Mikolov et al. 2013) . It uses two methods for calculating the word representation, CBOW (Continuous Bag of Words) and Skip-Gram. The Word2Vec model was used for finding the embedding vector of words present within the article corpus. The skip gram algorithm randomly initializes vectors for these words in the corpus. The algorithm then iterates through the set of all words T and defines a centre word c at the position t and its context word is defined as o. The window size defined for context words is m, which means the model will consider a window of t+m and t-m for searching the context words. The model then tries to maximize the likelihood L(\u03b8) of the context words given the center word, i.e. the probability of model predicting the context words given the center word. The likelihood function L(\u03b8) is represented by :",
            "cite_spans": [
                {
                    "start": 93,
                    "end": 114,
                    "text": "(Mikolov et al. 2013)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "For maximizing the likelihood function, the derivative of the function J(\u03b8) was taken after taking a log of the likelihood equation and multiplying it by -1 for calculating the -ve log-likelihood.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "The words were represented by two sets of vectors, U w and V w . U w is the vector of the context word and V w is for center word. These two vectors were used for calculating the probability P of the center word o given context word c which is given as,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "The loss function was minimized using the gradient descent algorithm to update the weights of the word vectors. This would lead to the vector representation of words, and similar words with the same context will point in the same direction. The key advantage of Word2Vec is that highquality word embeddings can be learned in an efficient space and time which further allows the large embeddings with more dimensions to be learned using a larger corpus of texts. The pre-trained word vectors for the Word2Vec model were loaded from the gensim library. The version of the pretrained embedding used for experimentation is the publicly available Word2Vec model pre-trained on Google News corpus (3 million 300-dimension English word vectors).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "Glove -It is one of the word embedding models which works on the statistical theory that predicts the behavior of the words based on their co-occurrence probabilities (Pennington, Socher, and Manning 2014) . This helps in incorporating meaningful vectors to the words present within the vocabulary. The probability P ij calculates the occurrence of word j in the context of a word i within the corpus.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 205,
                    "text": "(Pennington, Socher, and Manning 2014)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "where X is word-word co-occurrence matrix and X i j denotes the count of the word j appearing in the context of word i.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "The function F built by the Glove model will predict ratios given two word vectors w i and w j and a context word vector w k as inputs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "The Glove model then builds an objective function that determines the relation between word vectors and text statistics. The final Glove function J is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "The algorithm effectively minimizes this function to learn meaningful vector representations. The version of Glove used for experimentation is the publicly available common Crawl (840B tokens, 2.2M vocab, cased, 300 dimension vectors).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KL(P, Q)"
        },
        {
            "text": "The similarity score between the embeddings of news articles and the WHO guidelines was found using Cosine Similarity and Word Mover Distance(WMD) (Lahitani, Permanasari, and Setiawan 2016; Kusner et al. 2015) .",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 189,
                    "text": "(Lahitani, Permanasari, and Setiawan 2016;",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 190,
                    "end": 209,
                    "text": "Kusner et al. 2015)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Distance Metrics"
        },
        {
            "text": "Cosine Similarity -It is used for calculating the similarity between two documents(WHO Guideline and News Article) by measuring the cosine angle formed by the two vectors. The mean weighted vectors for each guideline and news articles have been obtained using smooth inverse frequency. Cosine distance is not dependent upon the size of documents. Hence the cosine similarity of embedding vectors for News article and WHO guidelines will be close to 1 if they are similar to each other, while it will be -1 if they are completely dissimilar.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Distance Metrics"
        },
        {
            "text": "Word Mover Distance(WMD) -It is used for finding the Euclidean distance of weighted embedded vectors. The Word Mover distance for the news article and WHO guidelines were calculated to find a similarity score between them. Word Mover Distance helps compare the two documents even when there is no word in common. The algorithm uses bag-of-words representation (which puts the word frequencies in the documents) finding the minimum traveling distance between articles and guidelines in an efficient way.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Distance Metrics"
        },
        {
            "text": "The objective of the proposed pipeline is two-fold -maximizing relevant content delivery and simultaneously minimizing the delivery of irrelevant content to the end-user. To facilitate this, the metrics considered throughout the experimentation are True Positive Rate, False Positive Rate, and Youden's Index (Youden 1950) .",
            "cite_spans": [
                {
                    "start": 309,
                    "end": 322,
                    "text": "(Youden 1950)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "True-Positive rate (TPR) also referred to as Sensitivity, measures the proportion of actual relevant pairs that are correctly classified as relevant. Youden's Index (J) is a metric used for analyzing ROC curves. It is defined for all points on the ROC curve and is effectively used as a criterion for the selection of optimum cut-off points in the ROC curves.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "Y ouden s Index(J) = T P R \u2212 F P R (11)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "Multiple combinations of models have been deployed in the proposed framework. All possible combinations with a set of word embedding models (Glove and Word2Vec), summarization techniques (Summa, Gensim, Pyrank, LexRank, LSA, Luhn, KL Divergence, Reduction, and No summarizer) , and distance metrics (Cosine and Word Mover Distance) have been evaluated. A combination of 36 models was generated for each possible configuration. Given the labeled dataset of Relevant and Irrelevant pairs, we deploy our models on all pairs and the evaluation is done in the following manner.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 275,
                    "text": "(Summa, Gensim, Pyrank, LexRank, LSA, Luhn, KL Divergence, Reduction, and No summarizer)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "Youden's Index based thresholding was used to find the optimum cut-off point for each of these 36 models on the training data. These models were trained on 799 news articles and WHO guidelines pair as mentioned in table 2. The corresponding ROC curves were plotted for each similarity score and the optimum threshold for each is obtained based on the Youden's index on the training data. The optimal threshold represents the similarity score which maximizes the difference between the TPR and FPR. It effectively implies the maximization of the number of relevant articles while minimizing the irrelevant articles delivered to the users. The values of similarity scores used to plot the ROC for each model was bound between the maximum and minimum similarity scores for the particular model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "The ROC curves obtained on the training data for the topperforming models are shown in Fig. 2 . Using the optimum threshold, the top 10 models with the highest Youden's index were evaluated on the test set. All pairs with a similarity score above the optimum threshold were classified as relevant and ones with scores below the threshold were classified as irrelevant. The results obtained on the test set are shown in Table 3 . The model with the best performance on the test set used Word2Vec embedding, LexRank based summarizer, and WMD distance metric. This model had the highest value of Youden's metric 0.1953, with a True Positive Rate of 0.320 and False Positive Rate of 0.125. Word Mover Distance outperformed Cosine Distance as the distance metric as it was common in all top 10 models. All the experimentation has been performed on Intel Core i5-7200U CPU with 8GB RAM with no GPU support. The average run time for each proposed model is 300 seconds making the proposed pipeline computationally inexpensive.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 87,
                    "end": 93,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 419,
                    "end": 426,
                    "text": "Table 3",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "The spread of COVID-19 has been exponential where lakhs of people have been affected by the virus. Due to the rapid spread of the virus in India, there has been a fear of concern related to the disease in the economically weaker section. This has led to the spread of irrelevant information through various social platforms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "The idea of the project was to disseminate relevant and reliable information of news articles and WHO guidelines to hard-to-reach groups staying in villages or suburban areas where the mode of communication is only Hindi. The WHO guidelines relevant to the corresponding news article will be provided to users. WHO guidelines was considered as it contains trusted and vetted forms of information related to the pandemic. A vernacular Hindi News Portal (Dainik Jagran) was chosen for scraping the news articles so that it could help in transmitting the news and relevant COVID-19 articles in regional language (Hindi) via speech translation. On qualitative evaluation, the Google Translate API used for conversion of Hindi to English retained more context in the sentence than the conversion of English to Hindi. Given that the objective was to communicate the final news articles in Hindi, an incorrect translation from English to Hindi resulted in bad user experience. In any language conversion, there is an information loss that majorly consists of stopwords and for our modeling the loss of stopwords was insignificant. Thus the news articles were converted into English which was further processed into the framework for modeling.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Although WHO guidelines will be changing over time, the relevant and irrelevant annotations for new WHO guidelines paired with news articles will subsequently be crowdsourced from the users through an optional automated feedback form after every matching. The majority label obtained from the crowdsourced annotations will be assigned as the class label for that particular matching. The new crowdsourced dataset will now serve as the training set to designate the threshold for the new guidelines. This humanin-loop interacting framework would ensure that our model will be robust to changes in WHO guidelines and will constantly stay updated and relevant. The only shortcomings that our study had were the fact that it required manual annotation for initial training due to which the model was trained on less number of samples. Currently, the human-inthe-loop feedback chain has been introduced for annotations related to new guidelines which would be incorporated in the model subsequently for training purposes. This study will be extended for generating article-embeddings for news articles and WHO guidelines using Doc2vec (Le and Mikolov 2014) . Also, various other language models will be explored for creating the embeddings in order to understand the feature space article vectors.",
            "cite_spans": [
                {
                    "start": 1130,
                    "end": 1151,
                    "text": "(Le and Mikolov 2014)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A simple but toughto-beat baseline for sentence embeddings",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Arora",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Variations of the similarity function of textrank for automated summarization",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Barrios",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "L\u00f3pez",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Argerich",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Wachenchauzer",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1602.03606"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Erkan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Radev",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Journal of artificial intelligence research",
            "volume": "22",
            "issn": "",
            "pages": "457--479",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Sentence reduction for automatic text summarization",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Jing",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Sixth Applied Natural Language Processing Conference",
            "volume": "",
            "issn": "",
            "pages": "310--315",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "From word embeddings to document distances",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kusner",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kolkin",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Weinberger",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International conference on machine learning",
            "volume": "",
            "issn": "",
            "pages": "957--966",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Cosine similarity to determine similarity measure: Study case in online essay assessment",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "R"
                    ],
                    "last": "Lahitani",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Permanasari",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "A"
                    ],
                    "last": "Setiawan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "4th International Conference on Cyber and IT Service Management",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Distributed representations of sentences and documents",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International conference on machine learning",
            "volume": "",
            "issn": "",
            "pages": "1188--1196",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "The automatic creation of literature abstracts",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "P"
                    ],
                    "last": "Luhn",
                    "suffix": ""
                }
            ],
            "year": 1958,
            "venue": "IBM Journal of research and development",
            "volume": "2",
            "issn": "2",
            "pages": "159--165",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Textrank: Bringing order into text",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mihalcea",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Tarau",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proceedings of the 2004 conference on empirical methods in natural language processing",
            "volume": "",
            "issn": "",
            "pages": "404--411",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Distributed representations of words and phrases and their compositionality",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "S"
                    ],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "3111--3119",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Pytextrank, a python implementation of textrank for text document nlp parsing and summarization",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nathan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Text summarization using latent semantic analysis",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "G"
                    ],
                    "last": "Ozsoy",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "N"
                    ],
                    "last": "Alpaslan",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Cicekli",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Journal of Information Science",
            "volume": "37",
            "issn": "4",
            "pages": "405--417",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Glove: Global vectors for word representation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Summarization approaches based on document probability distributions",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sripada",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jagarlamudi",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation",
            "volume": "2",
            "issn": "",
            "pages": "521--529",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "China coronavirus: how many papers have been published",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Stoye",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Infodemic and risk communication in the era of CoV-19",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaezi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Javanmard",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Advanced Biomedical Research",
            "volume": "9",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Index for rating diagnostic tests",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "J"
                    ],
                    "last": "Youden",
                    "suffix": ""
                }
            ],
            "year": 1950,
            "venue": "Cancer",
            "volume": "3",
            "issn": "1",
            "pages": "32--35",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Sample ROC curves obtained on the training data for the the top model's. The Youden's index for each ROC curve is marked which defines the the optimum threshold for the specific model. = True Positive i.e Relevant Pair of news articles and WHO guidelines correctly classified as Relevant and FN = Relevant Pair of articles incorrectly classified as Irrelevant.False-positive ratio (FPR) measures the ratio between the number of irrelevant pairs incorrectly classified as relevant (false positives) and the total number of actual negative events. These are those pairs which should have been labeled as irrelevant, but got classified as relevant pair.FPR = FP FP + TN(10)where FP = Irrelevant Pair of news articles and WHO guidelines incorrectly classified as Relevant and TN = Irrelevant Pair of articles correctly classified as Irrelevant.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The table demonstrates a sample of relevant and irrelevant class.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Best performing models on training data are evaluated on the test data.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The research conducted empirically tested the efficacy of NLP as an infodemic management tool. Specifically, it explored various models on extractive text summarization, word embeddings, and distance metrics. The result is an open-source AI framework for managing the spread of irrelevant information during the time of epidemics with Natural Language Processing. Incorporating WHO guidelines with relevant news articles based on user reviews, this learning model prove to be effective. Based on our knowledge from the current literature survey, there exists no such work addressing the information spectrum accompanying any healthcare debacle. From the evaluation, it was found that LexRank along with Word2vec and Word Distance Mover proved to be the best model for matching WHO guidelines with news articles related to COVID-19 in a relevant manner. Being the first of it's kind, the proposed architecture was evaluated based on the manually annotated data using Youden's index. This framework can effectively be incorporated into existing channels of information propagation. Given the limited data available for training, the promising results highlight the tremendous scope for such models in addressing the glut of irrelevant information spread.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The authors did not receive support from any organization for the submitted work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Declarations Funding"
        },
        {
            "text": "The authors declare that they have no conflict of interest.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conflict of interest"
        },
        {
            "text": "The datasets generated during and/or analysed during the current study are available from the corresponding author on reasonable request.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Availability of data and material"
        },
        {
            "text": "The code is available from the corresponding author on reasonable request.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Code availability"
        }
    ]
}
{
    "paper_id": "efa8d19bef32b6851fde3b8f593d480cb5e07afa",
    "metadata": {
        "title": "Revamp: Enhancing Accessible Information Seeking Experience of Online Shopping for Blind or Low Vision Users",
        "authors": [
            {
                "first": "Ruolin",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Zixuan",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Mingrui",
                "middle": [
                    "&quot;"
                ],
                "last": "Ray",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "&quot;",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Zhaoheng",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Zhixiu",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Zihan",
                "middle": [],
                "last": "Dang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Chun",
                "middle": [],
                "last": "Yu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Xiang",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Anthony",
                "middle": [
                    "&quot;"
                ],
                "last": "Chen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Xiang",
                "middle": [
                    "&quot;"
                ],
                "last": "Anthony",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Chen",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "UCLA HCI Research",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": ": Revamp simplifies and reconstructs the original Amazon web page for Blind or Low Vision users' information seeking task to understand product details, especially the appearance. Using rule-based heuristics, Revamp extracts descriptive information from customer reviews to generate image descriptions (a), responses to users' queries (b) with a sentiment summary of all the relevant reviews (c) and original reviews sorted into a positive and a negative lists (d).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Online shopping has become a valuable modern convenience, but blind or low vision (BLV) users still face significant challenges using it, because of: 1) inadequate image descriptions and 2) the inability to filter large amounts of information using screen readers. To address those challenges, we propose Revamp, a system that leverages customer reviews for interactive information retrieval. Revamp is a browser integration that supports review-based questionanswering interactions on a reconstructed product page. From our arXiv:2102.00576v1 [cs.HC] 1 Feb 2021 CHI '21, May 8-13, 2021, Yokohama, Japan Wang et al. interview, we identified four main aspects (color, logo, shape, and size) that are vital for BLV users to understand the visual appearance of a product. Based on the findings, we formulated syntactic rules to extract review snippets, which were used to generate image descriptions and responses to users' queries. Evaluations with eight BLV users showed that Revamp 1) provided useful descriptive information for understanding product appearance and 2) helped the participants locate key information efficiently.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "\u2022 Human-centered computing \u2192 Accessibility systems and tools; Natural language interfaces.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Online shopping has gained increasing popularity among Blind or Low Vision (BLV) people who have limited mobility to travel to physical stores. According to a research survey conducted in the UK, over 90% of people with disabilities shopped online at least once a month 1 . The recent COVID-19 pandemic further accelerated the adoption of online shopping among BLV users 2 . Thus making online shopping experience accessible has become an imperative requisite for ensuring the quality of life for BLV users.",
            "cite_spans": [
                {
                    "start": 270,
                    "end": 271,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "However, prior studies [30, 31] show that BLV people still face significant barriers on online shopping websites due to inadequate image descriptions and screen readers' inability to filter out a large amount of information on a product page. Our formative study with 20 BLV people showed that automatic tools (e.g., Seeing AI, filters of screen reader) were only used by a few experienced users, and the information provided was too generic to inform a purchase decision, especially for certain categories where the appearance matters, e.g., fashion products. The most efficient way for BLV users is still seeking help from a sighted person, such as a family member or a crowdsourced helper, which is not always available.",
            "cite_spans": [
                {
                    "start": 23,
                    "end": 27,
                    "text": "[30,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 28,
                    "end": 31,
                    "text": "31]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "To address the above challenges, we developed Revamp, an interactive information retrieval system that supports review-based question-answering (QA) on a reconstructed product page. It is implemented as a browser extension for Amazon.com as shown in Figure 1 . Utilizing reviews as a knowledge source, Revamp extracts informative descriptions to help BLV users understand the product appearance. To understand what kinds of visual questions BLV users care about, we collected questions from ten BLV participants on their frequently shopping categories on Amazon (Clothing, 1 http://www.clickawaypound.com/index.html 2 https://www.shropshirestar.com/news/health/coronavirus-covid19/2020/04/07/online-shopping-priority-plea-for-the-blind/ Shoes & Jewelry, Home & Kitchen, and Electronics), identified four main aspects (color, logo, shape, and size) in understanding the appearance. Based on the results, we formulated syntactic rules to extract the review snippets to generate image descriptions and responses to users' queries.",
            "cite_spans": [
                {
                    "start": 573,
                    "end": 574,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 250,
                    "end": 258,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "INTRODUCTION"
        },
        {
            "text": "We tested the performance of these rules on the Amazon bestseller product lists covering three main shopping categories. Results showed that these rules (i) covered 85% informative reviews voted by BLV participants and (ii) provided insights on information conveyed by images evaluated by two sighted people. We evaluated the usability of Revamp with eight BLV people on six representative products in the three aforementioned categories. Participants reported that Revamp reduces their efforts of information seeking, improves their utilization of reviews, and extracts reviews that are informative to help them understand product appearance. They considered Revamp to be helpful for shopping online independently when no sighted helpers were available.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "Our contributions are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "\u2022 Understanding the challenges faced by BLV people in information seeking when shopping online and design implications to meet their unique needs; \u2022 Identifying specific questions important to BLV users when shopping online and deriving syntactic rules that retrieve informative reviews to provide image description or to answer visual questions; \u2022 Revamp, an interactive information retrieval system that integrates with Amazon to support product browsing and review-based question-answering; \u2022 Evaluation study with eight BLV users that validated the feasibility and usefulness of enhancing accessible online shopping experience via Revamp.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "This research builds upon prior work from different sub-disciplines across accessibility, information retrieval (IR), computer vision (CV) and natural language processing (NLP). We first review previous research investigating online shopping experience for BLV users, as well as the efforts to improve the accessibility of such experience; we then provide an overview of information retrieval methods that utilize online reviews to generate useful information, which serves as the foundation of the implementation of Revamp.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RELATED WORK"
        },
        {
            "text": "Blind or low vision (BLV) people often take great effort to find and learn what products are visually appropriate for them [16, 29] . Without real-time communication with store clerks and touching the products directly, information related to online products is mainly conveyed through images and text provided by sellers. Hence, inadequate descriptions of images and unparsed text information significantly reduce BLV consumers' engagement with online shopping websites [30, 31] . Currently, BLV people could leverage human-powered assistive tools to answer visual questions about daily objects and products in stores [1-3, 6, 22, 27, 39] . Specially in shopping clothes, human helpers can provide remote assistant on describing articles of clothing (e.g., color) [1, 3] or offering subjective fashion advice [6, 22] . Relatedly, automatic photo caption generation and visual question answering in computer vision and AI research is often designed for general scenarios [8, 13, 26] . There exist automatic solutions that promote accessible image understanding on the web, e.g., providing image alt-text for photos in Facebook based on object recognition [38] , using OCR for photos on Twitter and other websites alike [9, 18] , and using reverse image search to find existing captions [10] . However, such solutions cannot fulfill BLV users' special need in better understanding visual details of online products. Most images on eCommerce websites, if captioned by emerging automatic caption tools such as SeeingAI 1 , will only have a generic description of the product, e.g., \"probably a close up of a red chair\", without any specific insights on appearance details, e.g., how is the size, and how does this kind of red make people feel. BrowseWithMe [31] has made an important step forward in describing clothes outfit by identifying the image regions, but only respond with basic color names (e.g., , \"Brick Top, Navy Pants\") based on color detection of image. Our contribution is providing a novel perspective of leveraging reviews as an external information source to supplement the visual details.",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 127,
                    "text": "[16,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 128,
                    "end": 131,
                    "text": "29]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 471,
                    "end": 475,
                    "text": "[30,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 476,
                    "end": 479,
                    "text": "31]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 619,
                    "end": 639,
                    "text": "[1-3, 6, 22, 27, 39]",
                    "ref_id": null
                },
                {
                    "start": 765,
                    "end": 768,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 769,
                    "end": 771,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 810,
                    "end": 813,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 814,
                    "end": 817,
                    "text": "22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 971,
                    "end": 974,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 975,
                    "end": 978,
                    "text": "13,",
                    "ref_id": null
                },
                {
                    "start": 979,
                    "end": 982,
                    "text": "26]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "text": "[38]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 1219,
                    "end": 1222,
                    "text": "[9,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1223,
                    "end": 1226,
                    "text": "18]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1286,
                    "end": 1290,
                    "text": "[10]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1754,
                    "end": 1758,
                    "text": "[31]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Online Shopping Accessibility"
        },
        {
            "text": "Besides inadequate descriptions of images, the online shopping experience of BLV people is also hindered by the weaknesses of screen readers dealing with crowded websites. Screen readers provide fine-grained information navigation and control, but at the cost of reduced walk-up-and-use convenience [34] . Prior work augmented the auditory web browsing experience by adding a secondary voice output [28, 42] , supporting basic queries about the information provided by sellers [31] , and supporting users to choose how many and which levels of detail to listen to based on their interest [23] . voice assistants as an alternative solution lack the ability to deeply engage with content and to get a quick overview of the landscape (e.g., list alternative search results & suggestions) [34] . Current voice assistants for online shopping such as Alexa only respond to a limited range of queries, e.g., \"add bananas to the chart\", rather than finding targeted information to answer a BLV user's question. Inspired by VERSE [34] which extends a voice assistant with screen-reader-inspired capabilities to enhance web search, our work integrates review-driven question-answering with two levels of details (summary & original review lists), which contains rich first-hand insights from other buyers.",
            "cite_spans": [
                {
                    "start": 299,
                    "end": 303,
                    "text": "[34]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 399,
                    "end": 403,
                    "text": "[28,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 404,
                    "end": 407,
                    "text": "42]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 477,
                    "end": 481,
                    "text": "[31]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 588,
                    "end": 592,
                    "text": "[23]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 785,
                    "end": 789,
                    "text": "[34]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 1021,
                    "end": 1025,
                    "text": "[34]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Online Shopping Accessibility"
        },
        {
            "text": "Most image captioning and visual question answering benchmarks to date focus on questions such as simple counting and object detection that are directly based on the images. Marino et al. [19] draws on external knowledge resources when the image content is insufficient to answer customers' questions, but their focus on images containing general scenes cannot be directly leveraged for online shopping. Online reviews have been mentioned as an useful resource to assist BLV customers' desire for more product information [16, 30, 31] , such as supporting question-answering [20, 36] and generating summaries [12, 15] . Prior work towards general users investigated extracting experiences [21, 24] , tips [11, 41] or snippets suitable for product descriptions [7, 25] , yet it remains 1 https://www.microsoft.com/en-us/ai/seeing-ai unknown how reviews can help with the unique interests of BLV users, especially providing descriptive information on product appearance. Our research fills a gap in the literature by leveraging existing human-authored resources of online reviews as an additional source of information to address such unique needs.",
            "cite_spans": [
                {
                    "start": 188,
                    "end": 192,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 522,
                    "end": 526,
                    "text": "[16,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 527,
                    "end": 530,
                    "text": "30,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 531,
                    "end": 534,
                    "text": "31]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 575,
                    "end": 579,
                    "text": "[20,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 580,
                    "end": 583,
                    "text": "36]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 609,
                    "end": 613,
                    "text": "[12,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 614,
                    "end": 617,
                    "text": "15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 689,
                    "end": 693,
                    "text": "[21,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 694,
                    "end": 697,
                    "text": "24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 705,
                    "end": 709,
                    "text": "[11,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 710,
                    "end": 713,
                    "text": "41]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 760,
                    "end": 763,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 764,
                    "end": 767,
                    "text": "25]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Information Retrieval In Online Reviews"
        },
        {
            "text": "The most related work [25] applies a supervised method to extract reviews for image description based on 25K labeled sentences. However, the descriptions generated by this work are generic and rarely covered descriptions of visual details. Rather than following supervised methods such as conducting aspect-based sentiment analysis utilizing feature engineering [14, 35] or Bidirectional Encoder Representations from Transformers (BERT) [32] that requires a huge amount of human-labelled data, we proposed empiricallyformulated syntactic rules based on our studies with BLV participants to retrieve meaningful review snippets for understanding appearances. Compared to \"black box\" data-driven approaches, these syntactic rules are explainable and can be validated by our targeted users; they are modular-existing rules can be edited or removed and new rules can be added without affecting the others. To the best of our knowledge, there is no prior work that directly informs rule design on extracting visual descriptions from reviews.",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 26,
                    "text": "[25]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 362,
                    "end": 366,
                    "text": "[14,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 367,
                    "end": 370,
                    "text": "35]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 437,
                    "end": 441,
                    "text": "[32]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Information Retrieval In Online Reviews"
        },
        {
            "text": "In summary, Revamp extends prior work on enhancing accessible information seeking by proposing a solution to bridge customer reviews with the needs of BLV users on visual information for a wider range of online products, including three frequently shopped categories: Home & Kitchen, Clothing, Shoes & Jewelry, and Electronics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Information Retrieval In Online Reviews"
        },
        {
            "text": "In this work, we conducted three groups of studies:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "OVERVIEW OF STUDIES"
        },
        {
            "text": "\u2022 Formative study (Section 4, 5) with two stages to: (i) understand the current practice and derive design implications via 30-mins semi-structured interviews with 20 participants (P1-P20); (ii) investigate what specific visual information BLV users expect and how reviews can help via questionnaires and 30-mins semi-structured interviews with 10 participants (P11-P20). \u2022 Rule evaluation (Section 7.1) with two stages to: (i) evaluate the informativeness of reviews extracted by the rules via questionnaires with eight participants (P13-P20); (ii) evaluate the rules' generalizability on different products where two sighted researchers rated the quality of the generated alt-text and answers in three informative levels. \u2022 System evaluation (Section 7.2) to evaluate how the Revamp prototype affected the shopping process. Eight participants (P13-P20) browsed products with Amazon web pages and Revamp then provided subjective comments by online interviews. The study lasted for 40 mins.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "OVERVIEW OF STUDIES"
        },
        {
            "text": "The time interval between each group of study was about one month. Twenty participants (Gender: 7 female and 13 male; Age: average = 27.7, SD = 5.5) came from two different cultures (P1-P10: China; P11-P20: North America) participated in the first-stage formative study due to the distributed nature of the research team. Although the platforms and languages they use differ, participants share similar screen reader experience and information seeking challenges since the screen readers follow the general design and most online shopping websites are similarly structured with overloaded information. Only North American users were involved at the following studies because of the limited availability of review data on the Chinese shopping sites. Participants were compensated with $15 USD\\hour. Each study was audio recorded, transcribed and coded by three of the authors following the reflexive thematic analysis methods from Braun and Clarke [4] . The demographic information of participants is attached as Appendix.",
            "cite_spans": [
                {
                    "start": 947,
                    "end": 950,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "OVERVIEW OF STUDIES"
        },
        {
            "text": "We conducted 30-minute semi-structured online interviews with 20 BLV consumers (P1-P20) recruited through social media platforms. Specifically, the goal of this study is to understand that, when shopping online:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "UNDERSTANDING CURRENT PRACTICE AND DERIVING DESIGN IMPLICATIONS"
        },
        {
            "text": "\u2022 What challenges do BLV users face in information seeking and what are their coping strategies? \u2022 What are the design heuristics if we want to build an accessible information retrieval system for BLV users?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "UNDERSTANDING CURRENT PRACTICE AND DERIVING DESIGN IMPLICATIONS"
        },
        {
            "text": "Aligned with prior work [30, 31] , two main challenges mentioned by our participants are lack of visual information and lack of efficient ways to navigate through information. We revealed how the two challenges were intertwined with each other, which elicited insights on designing an efficient way to leverage reviews to fill in the lacking visual information.",
            "cite_spans": [
                {
                    "start": 24,
                    "end": 28,
                    "text": "[30,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 29,
                    "end": 32,
                    "text": "31]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Challenges"
        },
        {
            "text": "4.1.1 Lack of Visual Information. The most frequently mentioned challenge is the lack of detailed information for understanding the visual appearance of the product (18 out of 20 participants). While sighted users can tell the visual attributes such as color, shape and size or even functionalities of a product from a single image, the alt-texts of many current product images are either empty, or set as the image path, which does not provide any visual information for BLV users at all. Sellers usually provide no textual descriptions equivalent to the visual information conveyed by images, whether it is some basic attribute such as color and shape, or some vivid details such as pattern design. Take the color attribute as an example: the names provided by sellers sometimes can be too generic or obscure to interpret accurately. So much of the language we use to describe a product is centered around visual, e.g., \"marble pattern \" or \"Arctic blue \", which poses a barrier to people who don't experience the world visually. P18 brought up an example that a T-shirt with the color name \"surf the web \" was actually a kind of \"bright blue \", which was only mentioned by a review.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Challenges"
        },
        {
            "text": "Comparing with the scanning and skimming experience of sighted users, the passive and sequential reading manner of screen readers makes it inefficient for BLV users to retrieve useful information scattering all over a web page. P16 mentioned that it can be frustrating when accidentally jumping into another unrelated web component such as the shopping cart in the process of exploring product details. All participants agreed that reviews were useful resources. Review snippets with detailed descriptions can also provide further insights for product appearance. However, most participants (16 out of 20)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lack of efficient ways to navigate through information."
        },
        {
            "text": "reported their utilization of reviews was greatly reduced because sifting through the large amount and unrelated reviews can be especially laborious. P19 stated that \"Usually there are too few reviews per page and there are no easy ways to jump from review to review. I know there exist useful reviews but it's hard to directly pull them out without efforts.\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lack of efficient ways to navigate through information."
        },
        {
            "text": "There still exists a gap between current solutions and BLV users' expectations of online shopping experience. Relying on human helpers, the same as many other scenarios, suffer from the limitation of availability, raises privacy concerns, and reduces BLV users' independence. Meanwhile, as reviewed earlier in Related Work, many existing techniques lack adaptation to the specific needs of BLV users' online shopping.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Current Solutions"
        },
        {
            "text": "People. 80% of the participants chose to seek help from sighted people including families/friends, other customers, paid video chat and accessibility hotlines. In this way, they had to trust the helper's personal judgment and subject to issues such as helpers' inavaliability and privacy concerns. P19 mentioned that \"The shipping and delivery process is not the most time-consuming for me. Waiting for someone to help me with selecting products is. \" P12 stated that he did not always want others to know about certain products he buys and hoped there was an alternative solution to shop more independently.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seeking Help from Sighted"
        },
        {
            "text": "Tools. P18 mentioned using visual interpretation apps such as Seeing AI to get brief generic descriptions on product images based on object and color detection, e.g., \"probably a close up of a red chair \". Two participants who were more tech-savvy in web interfaces (P11, P20) mentioned using keyword-based search and filters, but these tools are under-utilized by other participants and can only remove irrelevant information on a general basis. P8 and P13 mentioned the customer questions & answers section supported by websites can also help with some generic details, yet it seldom covers appearance-related information, which is assumed available to sighted consumers via product images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Leveraging Existing Web Functions and Automatic"
        },
        {
            "text": "Based on the interview results, we derived 3 design implications to improve the accessibility of online shopping experience. Specifically, the designs focus on leveraging the existing resources on a product page: simplifying the webpage (4.3.1) and responding to active queries (4.3.3) are to address the lack of efficient ways to navigate through information (4.1.2); leveraging reviews to supplement visual description (4.3.2) is to address the lack of visual information (4.1.1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implications for Design"
        },
        {
            "text": "When the users' current task is understanding a product, especially the appearance, the overloaded information and web components can become burdens for seeking visual-related description. To address this problem, we can provide an alternate view of the original page with less related information and components removed, such as shopping cart. Meanwhile, it is important to provide users the option to switch back to the original page.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simplifying and reorganizing the webpage structure."
        },
        {
            "text": "Participants' responses suggest that some subjective information in reviews can be helpful for BLV people to understand the visual attributes. Just as P2 stated: \"It is impossible for me to build a visual concept as you sighted people do, but I can feel what you feel. Reviews talking about the feelings when seeing the appearance are helpful for me to understand this pattern.\" It remains to be investigated how to retrieve the informative reviews to meet the specific needs of BLV users.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Leveraging customer reviews for extra visual description."
        },
        {
            "text": "Responding to active queries with the page resources. Participants expressed preferences to actively asking questions and getting summative answers of a product, which was usually what they did when consulting with a sighted helper. Meanwhile, any additional assistance mechanisms should be compatible with users' current screen reader experience to support users to access the original information as well. P6 commented \"I do not completely trust in answers selected by machines in case it may not cover very well \".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.3.3"
        },
        {
            "text": "The first-stage formative study uncovered the opportunities and challenges to support the unique needs of BLV population in visual information seeking in online shopping. Building upon these findings, we then seek to further understand:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "LEVERAGING REVIEWS TO ANSWER BLV CUSTOMERS' QUESTIONS"
        },
        {
            "text": "\u2022 What specific visual or other product-related questions are BLV users interested in? \u2022 How can we retrieve useful review snippets to answer these questions?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "LEVERAGING REVIEWS TO ANSWER BLV CUSTOMERS' QUESTIONS"
        },
        {
            "text": "It is beyond the scope of this paper to exhaustively cover the large number of product categories and all the consumer questions corresponding to each category. To narrow down our focus, we first distributed questionnaires to ten North American participants (P11 -P20) who mainly use Amazon to collect their most frequently shopped product categories, then conducted one-on-one interviews to formulate the understanding of their specific questions on three main categories: Home & Kitchen, Clothing, Shoes & Jewelry, and Electronics, based on which we derived rule-based solutions to extract informative review snippets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "LEVERAGING REVIEWS TO ANSWER BLV CUSTOMERS' QUESTIONS"
        },
        {
            "text": "The frequently shopped categories on Amazon by our participants are: Electronics (chosen by 90% participants), Home & Kitchen (50%), Pet Supplies (50%), Audible Books and CDs (50%), Clothing, Shoes & Jewelry (30%), Grocery and Gourmet Food (30%). Among these categories, we focused on three main categories whose appearances are as crucial as other information for making purchase decisions, including Home & Kitchen, Clothing, Shoes & Jewelry, and Electronics. We then gathered 168 questions from participants in these three categories. Each participant was shown representative products on the Amazon best-sellers list and was asked to raise as many questions they want to ask as possible after browsing the title and description provided by sellers. The questions were then labeled and divided into two groups, non-visual questions and visual questions. The distinct breakout of visual (57%) and non-visual (43%) questions is based on whether a question can be answered by directly observing the image.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Product Categories and the Corresponding Question Types"
        },
        {
            "text": "First, 72 out of the 168 questions were questions on non-visual information that shares common interests with sighted consumers, including (i) functionality and other non-visual attributes, e.g., material, price; and (ii) consumer feedback, e.g., whether the product was worth the price, or of high quality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Product Categories and the Corresponding Question Types"
        },
        {
            "text": "We are interested in the remaining 96 questions on visual information, which were of specific interests to BLV users, including (i) visual attributes of image, e.g., color, logo, shape, size; and (ii) high-level concepts that can be inferred from an image, e.g., usage method, style.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Product Categories and the Corresponding Question Types"
        },
        {
            "text": "For non-visual questions, all participants reported that the useful information usually could be found in the product description provided by the sellers, reviews or customer questions & answers, while for visual questions, they mainly rely on human helpers to get the answers. Thus we focus our investigation on the following sub-categories of visual questions and whether we can provide informative answers to these visual questions based on reviews, hence providing an alternative for BLV users to shop online independently.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Types of Visual Questions and How to Find Informative Answers"
        },
        {
            "text": "Most participants started with general appearance questions, such as \"how does it look like? \", followed by detailed questions around the basic visual attributes of the product, including color, shape, size and logo, which puts forward the need of providing a briefing of appearance covering these basic attributes first, then responding to specific queries. Participants mentioned that they are not satisfied with only knowing the basic category name of visual attributes, or just vague comments, e.g., \"Nice shape. This color looks great \". They feel interested in the specific detailed descriptions and impressions on appearance. P5 mentioned his story that once he wanted to buy a guitar and was curious about what exactly was the color attribute 'gold'. \"Is it a shining one or a matte one? I want to know more details. \" For shape and size, participants always want to know how to compare the product with daily objects, e.g., \"Does it fit on a tabletop? \", \"Is it like the size of a banana? \", which can help them better understand shape and size in the real world. Participants are also curious about if there are logos on the product.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Visual questions on basic attributes."
        },
        {
            "text": "To provide informative answers on basic attributes mentioned above, the key is to extract the descriptive and comparative details on appearance from the large amount of reviews. Particularly, clothes can have more nuanced design details to describe, such as sleeves, waist, neck, edition of clothing, graphic designs or patterns. These questions, e.g., \"Do the buttons go all the way down? What kind of neck design does the shirt have? \", are very categoryspecific and often require fashion knowledge to provide aesthetic descriptions, as P14 mentioned \"even my friends don't describe well \", hence descriptions in common customer reviews hardly meet their expectations. In this paper, we don't focus on these subdivided fashion knowledge which requires large amount of annotated data (e.g. DeepFashion [17] ), but focus on the common attributes shared by a wider range of products and possibly covered by customer reviews. The four attributes (color, shape, size, logo) are common across many products and can be concisely described in the alt-text compared to other aesthetic attributes (e.g., pattern, style).",
            "cite_spans": [
                {
                    "start": 803,
                    "end": 807,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Visual questions on basic attributes."
        },
        {
            "text": "2 Visual questions on high-level concepts. There are also highlevel questions not directly related to basic visual attributes but can be inferred from the appearances in the image with commonsense, including usage, style, quality, texture and specific accessibility requirements, e.g., , \"Does it look like it has good quality? How to open this bottle? Does the product have physical buttons? \" Such properties are similar to the \"signifier\" concept proposed by Don Norman 3 , which act as the indicator that can be interpreted meaningfully in the context of the social and physical world. These questions, usually framed as Visual Commonsense Reasoning, has been a challenging research topic in computer vision [40] . Yet currently it is not well-explored to address the need of inferring concepts and functions from product images for online shopping scenarios, as they require a large amount of annotated data from external knowledge sources beyond the shopping websites. However, the reviews commenting on the specific aspects (e.g. quality, physical buttons) can still indirectly provide insights on customers' opinions. Thus our expediency is treating these questions same as the nonvisual questions to provide the relevant reviews.",
            "cite_spans": [
                {
                    "start": 712,
                    "end": 716,
                    "text": "[40]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [],
            "section": "5.2."
        },
        {
            "text": "In this work, we mainly focused on addressing BLV users' questions on the common basic visual attributes, including color, logo, shape and size, which can be supported by augmenting keywords-based review searching with rule-based filtering (described below) to extract more informative snippets. Compared to data-driven models, rule-based solutions can work well without the need of manually labelled data. Further, rules are explainable and can be validated by our targeted users. Rules are also modular-existing rules can be edited or removed and new rules can be added without affecting the others. Note that currently our rules are not designed to answer questions related to high-level concepts, which we consider as future work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rules-Based Solution for Retrieving Informative Review Snippets"
        },
        {
            "text": "Out Less Informative Reviews. We first established several simple rules to filter out review sentences that cannot be used to further the understanding of visual attributes with specific rules:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rules for Filtering"
        },
        {
            "text": "(i) Short: sentence of 3 words or fewer; e.g., \"satisfied\", \"like the shape\", \"poor logo\";",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rules for Filtering"
        },
        {
            "text": "(ii) Reference to the image: reviews that comment whether the actual product is consistent with the photo provided by the seller, e.g., \"looks exactly as pictured\" \"not shown as picture\". These sentences mentioned by our participants as \"useless\" and \"annoying\" since they often occur in the reviews but contain little useful information for BLV customers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rules for Filtering"
        },
        {
            "text": "Attributes. All reviews containing the query keywords are included as candidates and annotated using basic Part-Of-Speech tags 4 , which are also known as word classes or lexical categories, e.g., noun, adjective. Based on the aforementioned analysis, a group of three researchers iteratively established several rules for extracting informative reviews as follows.",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 128,
                    "text": "4",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Rules for Basic Visual"
        },
        {
            "text": "Rule 1: Adjective + Keyword or Keyword + Verb + Adjective. The descriptive adjectives usually provide supplementary visual information. Examples include \"a shimmery purple\", \"crescent shape\", \"a very nice-looking etched logo\", \"squarish shape\", \"The bubblegum color is glossy and fun.\", etc. It is hard to enumerate all the possible descriptive words, instead we decided to obtain qualified snippets by differentiating those with evaluative adjectives.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rules for Basic Visual"
        },
        {
            "text": "The evaluative adjectives expressing subjective emotions can be vague hence not helpful to further understand the visual attributes. e.g., \"nice shape \", \"great color \". We collected a list of such vague adjectives, including \"great \", \"nice \", good, \"bad \", \"horrible \", \"disappointed \" etc. These snippets are less informative but still provide subjective opinions, so we still keep them as candidates, but as the lowest priority among reviews following our rules.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rules for Basic Visual"
        },
        {
            "text": "Rule 2: 1st pronoun + ... + Keyword + ... + that/which/but/because. Rather than the simple sentences only containing expressions on attitude e.g., \"I feel disappointed at the color.\", the sentences with clauses usually provide more detailed and useful information. Coordinating conjunction, e.g., with \"but\" emphasizes the two statements contrasting or contradicting each other. Subordinating conjunctions, e.g., with \"because\" often contain detailed reasons for customers' attitude towards the visual attribute. Examples include \"I love the color (Bubblegum), which I bought because it was the lowest cost for a color that would be difficult to misplace or forget while traveling\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rules for Basic Visual"
        },
        {
            "text": "Rule 3: Comparative Expressions. Other than the common rules above, there exist some specific rules that work for particular visual attributes. (1) Keyword (shape) + \"like/liked\", e.g., \"shaped like a Cola Bottle\". Comparing the shape of a product with a familiar daily object can be helpful for better understanding the shape; (2) Keyword (size) + \"fit/for/of\", e.g., \"size fits in all cup holders\", shows reviews containing details on how the product fits in the settings are informative; (3) \"than/more of\" + Keyword (color), e.g., \"it is a terra cotta than mocha\". Some products look different with the picture provided by the sellers. Sighted customers complaining about this kind of difference can also be informative for better understanding the actual appearance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rules for Basic Visual"
        },
        {
            "text": "Finally, we use the review snippets retrieved by the rules above to answer a BLV user's specific visual question or to compose a brief appearance description. As an answer to a BLV user's question, we provide original reviews retrieved by our rules divided into two lists-positive and negative-based on a sentiment analysis. We also provide a summary with the numbers of positive and negative reviews, and the top-3 informative review snippets from reviews across the two lists. To generate both the lists and the top-3 summary, we need to rank the review snippets. In particular, review snippets selected by Rule 1 with descriptive adjs, 2, and 3 have higher priority than those by Rule 1 withe evaluative adjs; further, Figure 2 : System overview of how Revamp responds to users' query. It first extract the keywords and process the query by its kind. After filtering out irrelevant reviews and reranking, Revamp uses the shortest candidate among the top-3 reviews to generate the visual description of a product and relevant answers to the query.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 722,
                    "end": 730,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Generating Answers and Image Descriptions"
        },
        {
            "text": "reviews of the same priority are ranked by their helpfulness (votes by other customers). The next level lower are reviews that contain the query keywords but do not meet any rules, within which they are ranked by relevance based on the concept of graph centrality following prior work [37] . For the brief appearance description, we select the shortest sentence in the top-3 candidates corresponding to each visual attribute (where each visual attribute is used as a keyword) since it is advised not to use long text for image alt-text. If users are interested in learning more details, they can move forward to ask specific questions on visual attributes.",
            "cite_spans": [
                {
                    "start": 285,
                    "end": 289,
                    "text": "[37]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Generating Answers and Image Descriptions"
        },
        {
            "text": "We present Revamp, an interactive information retrieval system to improve the shopping experience for BLV users. We implemented Revamp as a browser extension that works on Amazon. Revamp allows BLV users to interact with a simplified product page, access image description composed of relevant reviews, ask questions and receive a summary with original reviews as response, as shown in Figure 2 . On the front-end, Revamp uses Chrome API to automatically simplify the page and injects our search component to the current page, allowing user to send query and get a response from our back-end. On the back-end, Revamp extracts the keywords in user's query and matches the keywords with review snippets data. By ranking the filtered review snippets and applying sentimental classification, the back-end returns a review summary and a positive and a negative lists. Data source. Our data source consists of two main parts: (i) Basic attributes (title, color, price) from information provided by sellers; (ii) Reviews (containing title, content, rating, helpfulness, date, and author); and (iii) Customer Q&A; We scraped data from Amazon product pages using the Python library BeautifulSoup combined with Selenium and stored it in .csv format. If a product browsed by a user already exists in our database, we will use it directly; otherwise, we will run the scraper to retrieve the data and save it in the database. Our Python Flask based back-end API server will use these data to generate responses to our front-end's requests.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 386,
                    "end": 394,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "IMPLEMENTATION OF REVAMP-AN INFORMATION RETRIEVAL SYSTEM FOR ONLINE SHOPPING"
        },
        {
            "text": "Before our user study and evaluation, we pre-scraped the product data to avoid the potential latency of retrieving it in real-time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IMPLEMENTATION OF REVAMP-AN INFORMATION RETRIEVAL SYSTEM FOR ONLINE SHOPPING"
        },
        {
            "text": "Web page simplification We provide an an alternate view of the original Amazon pages to improve the screen reader browsing experience. Our extension firstly rearranges the elements on the Amazon product page by manipulating the DOM tree We removed the irreverent information such as advertisements and promotions, and keep only the product details, reviews and images from the original page. Revamp would generate brief descriptions as the alt-text of the images using the aforementioned rules. Then we use Chrome APIs to inject our Revamp module into the product page. We strictly followed WAI-ARIA 5 standards to make sure all components have proper attributes and keyboard interactions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IMPLEMENTATION OF REVAMP-AN INFORMATION RETRIEVAL SYSTEM FOR ONLINE SHOPPING"
        },
        {
            "text": "Review Snippets Extraction. Revamp responds to both keyword queries such as \"color \" as well as natural language questions such as \"Does the product have physical buttons? \". For keyword queries on four main visual attributes, including color, logo, shape, size, we extracted the reviews with the pre-defined extended keywords based on our formulated rules. Specifically, for color, the extended keywords included 'color', basic color name such as 'blue' and the special name provided by sellers such as 'surf the web'. For other natural language questions, we used the Rapid Automatic Keyword Extraction (RAKE) library 6 for keyword extraction, Synset from WordNet 7 to get the groupings of synonymous keywords that express the same concept, then extracted the reviews with the certain and synonymous keywords.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IMPLEMENTATION OF REVAMP-AN INFORMATION RETRIEVAL SYSTEM FOR ONLINE SHOPPING"
        },
        {
            "text": "Answer Generation. There are three types of information generated. (i) A supplementary textual description of product images; (ii) Two reviews lists: positive and negative; and (iii) A summary of reviews based on the two reviews lists. After we retrieved the ranked reviews, we divided them into positive and negative categories based on aspect-based sentimental classification by Mon-keyLearn API. Finally, we extract top three reviews from each list to generate our summary using a pre-defined template. In case there are too few reviews to populate the answers, we leverage the existing computer vision technique to provide basic answers, e.g., using Pythia [13] to answer the color, whether there is a logo on the image, and what is the shape. Users can either use text or voice input, when Revamp finishes generating answers, it will notify the user with a beep. Rather than automatically reading out the answers, we support users to use their own screen readers to scan the answers, which provides them with more freedom of control.",
            "cite_spans": [
                {
                    "start": 661,
                    "end": 665,
                    "text": "[13]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "IMPLEMENTATION OF REVAMP-AN INFORMATION RETRIEVAL SYSTEM FOR ONLINE SHOPPING"
        },
        {
            "text": "We separated evaluation into two parts: rule evaluation and system evaluation. Rule evaluation is to evaluate the performance of rulesbased solution for retrieving informative review snippets. System evaluation is to further evaluate how Revamp is integrated into BLV users' online shopping process. Modifying page structure and reviews-based interactions are non-separable, as they are coordinated to support a comprehensive information seeking flow. Hence we chose to evaluate Revamp as an integrated system.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "EVALUATION"
        },
        {
            "text": "We first evaluated how our rules cover the informative review snippets for understanding appearance and how our solution can be generalized to different products. 7.1.1 Coverage of Informative Reviews for Understanding Appearances. We randomly selected eight products covering three aforementioned main categories from the Amazon best-seller list and extracted review snippets covering four main visual attributes (color, logo, shape and size) following the above rules. We then distributed voting questionnaires to eight BLV participants (P13 -P20). Given each product, a participants was asked to vote for the snippet that could help them best understand the visual attributes of that product: half of the snippets were generated by our proposed rules and the other half from the top-ranked answers in the existing Amazon 'Have a Question' section; all snippets were presented in a random order. Figure 3 shows an example question and review snippets. The snippets selected by our rules gained 85% of the 144 votes. Notice that we are not filtering out the remaining 15%. Review snippets which were not selected by our rules may also contain informative details, but seldom directly for describing the visual attributes, e.g., a review \"We ordered 2 blue and black in size xl. The blue was ok and the black was small \" talking about color actually gave more details about the size differences between different colors. Rather than filtering out these reviews, we still show them but with lower ranking, while reviews that meet our rules and provide descriptive details such as \"color was off and not the true blue/normal blue that champion usually has \" will be assigned a higher priority.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 898,
                    "end": 906,
                    "text": "Figure 3",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Rule Evaluation"
        },
        {
            "text": "To further validate the generalization of these rules in a more real-world setting, we downloaded web data of 45 selected products from Amazon's up-to-date top-seller ranking list (top 15 in each of the aforementioned three categories with similar products removed to maintain variety). Two sighted research team members then browsed the product page and rated the quality of the generated alt-text and answers that describe four visual (color, logo, shape, size) in three levels: not applicable, providing related non-visual information, providing direct visual information. Results showed that our rules can provide informative results in most cases as long as there exist enough reviews, especially for the products whose appearances matter more in shopping decision, as shown in Figure 4 . Not applicable is due to no reviews mentioned the attribute if the specific attribute does not occur (there is no logo on the product), or has no details to provide (e.g., no reviews of a white bed sheet mentioned color details), or there exist too few informative reviews. In this case, Revamp uses Pythia Visual Question Asking [13] to provide back-up answers.",
            "cite_spans": [
                {
                    "start": 1124,
                    "end": 1128,
                    "text": "[13]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 783,
                    "end": 791,
                    "text": "Figure 4",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Generalization to Various Products."
        },
        {
            "text": "To further understand how the system is integrated into BLV users' online shopping process, we evaluated Revamp with BLV consumers. We selected six representative products from the aforementioned three main categories including Water Bottle, Men's Tee, Women's Dress, Bed Sheet, Bluetooth Speaker, and Chair Cushion as the testing products. Three representative products, retrieved reviews by rules and generated image description are shown in Figure 5 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 444,
                    "end": 452,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "System Evaluation"
        },
        {
            "text": "7.2.1 Participants, Design, and Procedure. We conducted interviews with the same eight blind participants (P13 -P20). Each user study lasted approximately 40 mins using Zoom after we received the IRB approval. Participants used their own laptops and screen readers and shared their feedback with the experimenters at the same time. We first asked the users to browse the products as they used to on original Amazon web pages without Revamp. The main task was to obtain information on four visual attributes (color, logo, shape, size) also other details they were curious about. We then introduced how Revamp worked, including how it simplified the original Amazon web page and what kinds of questions Revamp could answer, and asked the participants to repeat the same task using Revamp. Participants then filled out a survey with Likert-scale statements as shown in Table 1 . Participants then further explained their reasons of giving each specific ratings. Each study was audio recorded and transcribed and participants' qualitative response was summarized by affinity diagramming. We also included \"Not Applicable\" because of no relevant reviews or too few informative reviews. Table 1 : We collected participants' subjective ratings on Revamp. The scale was 1 to 7, in which 7 = I strongly agree with this statement, 4 = neutral, and 1 = I strongly disagree with this statement. The value represents the average of ratings (SD). Data was analyzed using Wilcoxon test and a statistical significant difference ( < 0.05) is marked with * -all ratings of Revamp significantly outperformed the current practice.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 866,
                    "end": 873,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 1181,
                    "end": 1188,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "System Evaluation"
        },
        {
            "text": "Revamp Current",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Statements"
        },
        {
            "text": "It is easy and efficient for me to locate product-related information I need ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Statements"
        },
        {
            "text": "We observed the common interaction pattern with Revamp proceeded as follows: The participants first navigated through four elements (product name, image with alt-text, info provided by sellers and QA) on Revamp. They then asked questions on visual attributes and other aspects they were curious about (no predefined questions were provided). Since personal interests differ, they asked 2~5 questions for each product, including both visual and non-visual questions. Besides the review summary, they also browsed the positive and negative review lists when needed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results."
        },
        {
            "text": "Overall participants considered Revamp to be a helpful alternative for shopping online independently when no sighted helper is available and will use Revamp regularly in daily life. The comparative ratings are shown in Figure 6 . Notice that one limitation of study is the bias of comparing a new system to an old one, participants would know which system was designed by the authors and can be influenced by this. Unfortunately, given the popularity of Amazon, participants would have known which one was ours even if we had intentionally de-identified the two systems. We further discuss participants' subjective comments below.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 219,
                    "end": 227,
                    "text": "Figure 6",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Results."
        },
        {
            "text": "Providing supplementary information for better understanding the product appearance. With Revamp, participants' ratings on their understanding of product appearance were improved comparing with their current practice, owing to the supplementary image description and responses to visual questions. Having a supplementary description of the product image is a new experience to them: \"You know the Amazon doesn't provide image descriptions for the products. I will definitely use this add-on in my life. \" (P13) When using Amazon without Revamp, participants often just quickly skipped the image web component since no informative information is provided; with Revamp, participants tended to use the shortcut command of screen readers to directly jump to image to first gain an overall understanding of the product's appearance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results."
        },
        {
            "text": "Participants felt surprised when Revamp provides useful answers on visual attributes, as mentioned by P19: \"It is really cool. It did answer my questions about product appearance. \" and even helps them learn about unfamiliar visual attributes, as shared by P13: \"Although I haven't seen colors before, I have a lot of fun in reading these descriptions of colors. For example, I don't know what is the 'Spice' color. It told me a review mentioned 'like a burnt orange', which is much more understandable. \" Participants also mentioned that descriptive details of reviews from the first-hand buyers sometimes felt more trustworthy than opinions of a friend or family member, as P15 commented \"Who can perform better than the customers who have bought the product themselves on describing the product? I really like the idea of using the reviews. \" Most participants agreed that they can better utilize the information from reviews. Only one participant gave a low rating of 2 because he personally preferred not to use any filter on the reviews in case something important is missed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results."
        },
        {
            "text": "Enhancing interaction flow of information seeking. With Revamp, the information seeking experience of participants was improved comparing with their current practice, owing to the reconstructed web page and better utilizing reviews. Revamp provided users with cleaner web page structure and is more user-friendly than the current Amazon page, as stated by P20: \"I don't need to worry of being stuck in useless information any more. \" Participants found the review summary and the review lists divided by positive and negative useful to access customers' opinions more efficiently and proactively. P14 mentioned that \"I like it that Revamp also keeps the original reviews accessible in the lists. After reading the summary, I can then make the decision to skip or look into the details of each relevant reviews in the list.\" Participants were more engaged in asking questions, as P19 commented \"Sometimes I could be inspired to ask more after I read the answers. \" Participants could choose to interact with the system using either voice commands or text input based on their preferences. Although participants in general liked the experience of screen readers augmented with voice input, they preferred to receive answers in text rather than speech: \"Using my screen reader to read out the answers is far more better than directly answering my questions by voice. I can adjust the speed and pause anytime. \" (P15)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results."
        },
        {
            "text": "Most web pages with vivid designs and a large amount of information are still not accessible enough to BLV users at present. We explored three aspects towards enhancing the information seeking experience on online shopping websites: (i) simplifying and reconstructing the web pages according to users' current task; (ii) providing coordinated experience of active query and passive reading to support flexible information seeking; (iii) leveraging relative text resources on the web page, such as reviews, to fill in the information gap. Besides, this work also inspired several exciting future directions as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "DISCUSSION"
        },
        {
            "text": "In this work, we focus on the task of understanding the product details, especially the appearance. Imagine another task: if the user has several product candidates in mind, we should also correspondingly meet a new information seeking need of comparing multiple products. To address this, future work can explore reconstructing the webpages to support efficient switching among products and answering comparative questions about multiple products. Currently, table is a common web page element for comparing different products and should be supported in future versions of Revamp.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Working with Multiple Product Pages of Online Shopping"
        },
        {
            "text": "For example, the system should guide the user to navigate a table by informing them which products are being compared in terms what attributes and further be able to answer a user's question by looking up the table.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Working with Multiple Product Pages of Online Shopping"
        },
        {
            "text": "Furthermore, future work can employ and test our methods on other product pages where there is often an overload of information inaccessible for BLV users to retrieve. For example, on Yelp, our methods can add description to popular dishes, whose images do not always convey all important information (e.g., how large is the portion); on Trip advisor, images taken by fellow travellers can be further described using our methods by extracting relevant descriptions from others' comments (e.g., whether a trail has shade); on Youtube, comments can be leveraged to generated video captions for eye-catching moments, which serve as a more vivid introduction before one decides whether to watch a given video.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Working with A Broader Variety of Product Pages"
        },
        {
            "text": "Although the subjective comments were useful and convey vivid details, our participants pointed out that the generated appearance descriptions might be too subjective since it only contained one snippet in each visual aspect. However, if we follow prior work on filtering out subjective comments [25] , we will lose the vivid and descriptive details that are crucial to appearance understanding. As such, participants preferred the image description to remain as concise as possible and the description provided by Revamp would serve more as \"a first step or a clue. \" (P16) Also, it remains to be explored how we can better retrieve useful information from reviews on the high-level visual concepts inferred by image. In this work, we formulated hand-crafted rules to explore the possibility of leveraging reviews as an additional information source to fill the visual information gap. In the future, we can collect and label data to deploy supervised methods such as Knowledge Graph which could extract more information to better support queries not limited in the four visual aspects (color, logo, shape and size) and provide better summative appearance description.",
            "cite_spans": [
                {
                    "start": 296,
                    "end": 300,
                    "text": "[25]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Working with Supervised methods"
        },
        {
            "text": "The question & answering experience of Revamp can be improved by providing prompting questions and involving more information sources. We observed that some participants had difficulty in formulating what questions to ask: P16 only asked about basic attributes, e.g., color, quality. She explained that \"Sometimes I don't know how to ask a 'good' question. Maybe if I change my wording, I can get more answers. \" and suggested us to provide more pre-defined questions as prompts. Meanwhile, participants noticed that there still exists a gap between their expectations and the answer quality in other details beyond the four main visual attributes. \"Revamp can support descriptive answers for basic visual questions, but when I want to ask about some concrete (non-visual) details such as the dimensions of the bottle, it cannot answer me directly.\"(P19) In the future, we can involve more information source into Revamp, also leveraging Optical Character Recognition to extract the text information on the images provided by sellers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Working with Better QA Experience"
        },
        {
            "text": "Similar to other information retrieval system, the performance of Revamp highly depends on the quality of existing data. Currently, we leverage existing visual question answering API to provide a back-up answer for visual questions when there are too few reviews or no informative reviews. In the future, we can also involve human helpers in the system by sending the questions to crowd workers. Our rules for extracting informative reviews can also give insights on formulating guidelines of describing products for BLV users: (i) include descriptive details rather than vague opinions (Rule 1); (ii) if have to express personal attitudes, elaborate the reasons (Rule 2); (iii) compare with daily or common objects to help understand new concepts (Rule 3). Besides, participants also hoped to see Revamp's functions integrated into the mobile application with Alexa since they often try the mobile application of Amazon when faced with accessibility problems shopping on the web.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Working in the Real-World Settings"
        },
        {
            "text": "There are several limitations of the system evaluation study. First, we did not counterbalance the conditions in the system evaluation. Using Revamp first would introduce a strong carry-over effect, as the information provided by Revamp is an augmentation of the baseline. As personal interests and product information differ, to compare intuitively how Revamp improves the experience, all participants first browsed the products as they used to on original Amazon web pages then on the modified web page of the same product by Revamp. Second, there exists the bias of comparing a new system to an old one when participants know which system was designed by the researchers [5, 33] . Third, only eight participants and a limited amount of products were involved in evaluation. This limitation can be addressed by a large-scale in-the-wild study, which we regard as future work beyond this initial study.",
            "cite_spans": [
                {
                    "start": 674,
                    "end": 677,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 678,
                    "end": 681,
                    "text": "33]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Limitations"
        },
        {
            "text": "We present Revamp, a system that employs information retrieval techniques to meet the unique information seeking requirements of BLV consumers when shopping online. Our main contribution is a rule-based approach that leverages rich customer reviews to serve as image description and to answer BLV users' questions related to product appearances. Evaluations with eight BLV consumers showed that Revamp provides useful subjective information for understanding the product appearance and enhances the accessible information seeking experience of online shopping. Although Revamp could not provide answers for all of the products (e.g., when there are too few reviews of a product), it can serve as an effective supplemental helper for BLV users to better access and understand a product before making a purchase decision. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CONCLUSION"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Remote Assistance for Blind Users in Daily Life: A Survey About Be My Eyes",
            "authors": [
                {
                    "first": "Mauro",
                    "middle": [],
                    "last": "Avila",
                    "suffix": ""
                },
                {
                    "first": "Katrin",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                },
                {
                    "first": "Anke",
                    "middle": [],
                    "last": "Brock",
                    "suffix": ""
                },
                {
                    "first": "Niels",
                    "middle": [],
                    "last": "Henze",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments",
            "volume": "85",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1145/2910674.2935839"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "VizWiz: Nearly Real-time Answers to Visual Questions",
            "authors": [
                {
                    "first": "Jeffrey",
                    "middle": [
                        "P"
                    ],
                    "last": "Bigham",
                    "suffix": ""
                },
                {
                    "first": "Chandrika",
                    "middle": [],
                    "last": "Jayant",
                    "suffix": ""
                },
                {
                    "first": "Hanjie",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "Greg",
                    "middle": [],
                    "last": "Little",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [
                        "C"
                    ],
                    "last": "Miller",
                    "suffix": ""
                },
                {
                    "first": "Robin",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                },
                {
                    "first": "Aubrey",
                    "middle": [],
                    "last": "Tatarowicz",
                    "suffix": ""
                },
                {
                    "first": "Brandyn",
                    "middle": [],
                    "last": "White",
                    "suffix": ""
                },
                {
                    "first": "Samual",
                    "middle": [],
                    "last": "White",
                    "suffix": ""
                },
                {
                    "first": "Tom",
                    "middle": [],
                    "last": "Yeh",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology",
            "volume": "",
            "issn": "",
            "pages": "333--342",
            "other_ids": {
                "DOI": [
                    "10.1145/1866029.1866080"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Visual Challenges in the Everyday Lives of Blind People",
            "authors": [
                {
                    "first": "Erin",
                    "middle": [],
                    "last": "Brady",
                    "suffix": ""
                },
                {
                    "first": "Meredith",
                    "middle": [
                        "Ringel"
                    ],
                    "last": "Morris",
                    "suffix": ""
                },
                {
                    "first": "Yu",
                    "middle": [],
                    "last": "Zhong",
                    "suffix": ""
                },
                {
                    "first": "Samuel",
                    "middle": [],
                    "last": "White",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [
                        "P"
                    ],
                    "last": "Bigham",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
            "volume": "",
            "issn": "",
            "pages": "2117--2126",
            "other_ids": {
                "DOI": [
                    "10.1145/2470654.2481291"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Reflecting on reflexive thematic analysis",
            "authors": [
                {
                    "first": "Virginia",
                    "middle": [],
                    "last": "Braun",
                    "suffix": ""
                },
                {
                    "first": "Victoria",
                    "middle": [],
                    "last": "Clarke",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Qualitative Research in Sport, Exercise and Health",
            "volume": "11",
            "issn": "",
            "pages": "589--597",
            "other_ids": {
                "DOI": [
                    "https:/arxiv.org/abs/https:/doi.org/10.1080/2159676X.2019.1628806"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Review of Quantitative Empirical Evaluations of Technology for People with Visual Impairments",
            "authors": [
                {
                    "first": "Emeline",
                    "middle": [],
                    "last": "Brul\u00e9",
                    "suffix": ""
                },
                {
                    "first": "Brianna",
                    "middle": [
                        "J"
                    ],
                    "last": "Tomlinson",
                    "suffix": ""
                },
                {
                    "first": "Oussama",
                    "middle": [],
                    "last": "Metatla",
                    "suffix": ""
                },
                {
                    "first": "Christophe",
                    "middle": [],
                    "last": "Jouffrais",
                    "suffix": ""
                },
                {
                    "first": "Marcos",
                    "middle": [],
                    "last": "Serrano",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
            "volume": "",
            "issn": "",
            "pages": "1--14",
            "other_ids": {
                "DOI": [
                    "10.1145/3313831.3376749"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Crowdsourcing Subjective Fashion Advice Using VizWiz: Challenges and Opportunities",
            "authors": [
                {
                    "first": "Michele",
                    "middle": [
                        "A"
                    ],
                    "last": "Burton",
                    "suffix": ""
                },
                {
                    "first": "Erin",
                    "middle": [],
                    "last": "Brady",
                    "suffix": ""
                },
                {
                    "first": "Robin",
                    "middle": [],
                    "last": "Brewer",
                    "suffix": ""
                },
                {
                    "first": "Callie",
                    "middle": [],
                    "last": "Neylan",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [
                        "P"
                    ],
                    "last": "Bigham",
                    "suffix": ""
                },
                {
                    "first": "Amy",
                    "middle": [],
                    "last": "Hurst",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility",
            "volume": "",
            "issn": "",
            "pages": "135--142",
            "other_ids": {
                "DOI": [
                    "10.1145/2384916.2384941"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Learning to Generate Personalized Product Descriptions",
            "authors": [
                {
                    "first": "Guy",
                    "middle": [],
                    "last": "Elad",
                    "suffix": ""
                },
                {
                    "first": "Ido",
                    "middle": [],
                    "last": "Guy",
                    "suffix": ""
                },
                {
                    "first": "Slava",
                    "middle": [],
                    "last": "Novgorodov",
                    "suffix": ""
                },
                {
                    "first": "Benny",
                    "middle": [],
                    "last": "Kimelfeld",
                    "suffix": ""
                },
                {
                    "first": "Kira",
                    "middle": [],
                    "last": "Radinsky",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management",
            "volume": "",
            "issn": "",
            "pages": "389--398",
            "other_ids": {
                "DOI": [
                    "10.1145/3357384.3357984"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Every Picture Tells a Story: Generating Sentences from Images",
            "authors": [
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                },
                {
                    "first": "Mohsen",
                    "middle": [],
                    "last": "Hejrati",
                    "suffix": ""
                },
                {
                    "first": "Mohammad",
                    "middle": [
                        "Amin"
                    ],
                    "last": "Sadeghi",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Young",
                    "suffix": ""
                },
                {
                    "first": "Cyrus",
                    "middle": [],
                    "last": "Rashtchian",
                    "suffix": ""
                },
                {
                    "first": "Julia",
                    "middle": [],
                    "last": "Hockenmaier",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Forsyth",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 11th European Conference on Computer Vision: Part IV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Twitter A11y: A Browser Extension to Make Twitter Images Accessible",
            "authors": [
                {
                    "first": "Cole",
                    "middle": [],
                    "last": "Gleason",
                    "suffix": ""
                },
                {
                    "first": "Amy",
                    "middle": [],
                    "last": "Pavel",
                    "suffix": ""
                },
                {
                    "first": "Emma",
                    "middle": [],
                    "last": "Mccamey",
                    "suffix": ""
                },
                {
                    "first": "Christina",
                    "middle": [],
                    "last": "Low",
                    "suffix": ""
                },
                {
                    "first": "Patrick",
                    "middle": [],
                    "last": "Carrington",
                    "suffix": ""
                },
                {
                    "first": "Kris",
                    "middle": [
                        "M"
                    ],
                    "last": "Kitani",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [
                        "P"
                    ],
                    "last": "Bigham",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
            "volume": "",
            "issn": "",
            "pages": "1--12",
            "other_ids": {
                "DOI": [
                    "10.1145/3313831.3376728"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Caption crawler: Enabling reusable alternative text descriptions using reverse image search",
            "authors": [
                {
                    "first": "Darren",
                    "middle": [],
                    "last": "Guinness",
                    "suffix": ""
                },
                {
                    "first": "Edward",
                    "middle": [],
                    "last": "Cutrell",
                    "suffix": ""
                },
                {
                    "first": "Meredith Ringel",
                    "middle": [],
                    "last": "Morris",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
            "volume": "",
            "issn": "",
            "pages": "1--11",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Extracting and ranking travel tips from user-generated reviews",
            "authors": [
                {
                    "first": "Ido",
                    "middle": [],
                    "last": "Guy",
                    "suffix": ""
                },
                {
                    "first": "Avihai",
                    "middle": [],
                    "last": "Mejer",
                    "suffix": ""
                },
                {
                    "first": "Alexander",
                    "middle": [],
                    "last": "Nus",
                    "suffix": ""
                },
                {
                    "first": "Fiana",
                    "middle": [],
                    "last": "Raiber",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 26th international conference on world wide web. International World Wide Web Conferences Steering Committee",
            "volume": "",
            "issn": "",
            "pages": "987--996",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "RevMiner: An Extractive Interface for Navigating Reviews on a Smartphone",
            "authors": [
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Oren",
                    "middle": [],
                    "last": "Etzioni",
                    "suffix": ""
                },
                {
                    "first": "Luke",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                },
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Clark",
                    "suffix": ""
                },
                {
                    "first": "Christian",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology",
            "volume": "",
            "issn": "",
            "pages": "3--12",
            "other_ids": {
                "DOI": [
                    "10.1145/2380116.2380120"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "NRC-Canada-2014: Detecting aspects and sentiment in customer reviews",
            "authors": [
                {
                    "first": "Svetlana",
                    "middle": [],
                    "last": "Kiritchenko",
                    "suffix": ""
                },
                {
                    "first": "Xiaodan",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Colin",
                    "middle": [],
                    "last": "Cherry",
                    "suffix": ""
                },
                {
                    "first": "Saif",
                    "middle": [],
                    "last": "Mohammad",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 8th international workshop on semantic evaluation",
            "volume": "",
            "issn": "",
            "pages": "437--442",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Structure-aware review mining and summarization",
            "authors": [
                {
                    "first": "Fangtao",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Chao",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Minlie",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Xiaoyan",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Ying-Ju",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "Shu",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Hao",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 23rd international conference on computational linguistics",
            "volume": "",
            "issn": "",
            "pages": "653--661",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "I Bought This for Me to Look More Ordinary\": A Study of Blind People Doing Online Shopping",
            "authors": [
                {
                    "first": "Guanhong",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Xianghua",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "Chun",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Lan",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Xingyu",
                    "middle": [],
                    "last": "Chi",
                    "suffix": ""
                },
                {
                    "first": "Yuanchun",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
            "volume": "372",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1145/3290605.3300602"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations",
            "authors": [
                {
                    "first": "Ziwei",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Ping",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "Shi",
                    "middle": [],
                    "last": "Qiu",
                    "suffix": ""
                },
                {
                    "first": "Xiaogang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Xiaoou",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "1096--1104",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Twitter A11y: A Browser Extension to Describe Images",
            "authors": [
                {
                    "first": "Christina",
                    "middle": [],
                    "last": "Low",
                    "suffix": ""
                },
                {
                    "first": "Emma",
                    "middle": [],
                    "last": "Mccamey",
                    "suffix": ""
                },
                {
                    "first": "Cole",
                    "middle": [],
                    "last": "Gleason",
                    "suffix": ""
                },
                {
                    "first": "Patrick",
                    "middle": [],
                    "last": "Carrington",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [
                        "P"
                    ],
                    "last": "Bigham",
                    "suffix": ""
                },
                {
                    "first": "Amy",
                    "middle": [],
                    "last": "Pavel",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The 21st International ACM SIGACCESS Conference on Computers and Accessibility",
            "volume": "",
            "issn": "",
            "pages": "551--553",
            "other_ids": {
                "DOI": [
                    "10.1145/3308561.3354629"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge",
            "authors": [
                {
                    "first": "Kenneth",
                    "middle": [],
                    "last": "Marino",
                    "suffix": ""
                },
                {
                    "first": "Mohammad",
                    "middle": [],
                    "last": "Rastegari",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                },
                {
                    "first": "Roozbeh",
                    "middle": [],
                    "last": "Mottaghi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3195--3204",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Addressing complex and subjective productrelated queries with customer reviews",
            "authors": [
                {
                    "first": "Julian",
                    "middle": [],
                    "last": "Mcauley",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 25th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee",
            "volume": "",
            "issn": "",
            "pages": "625--635",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Identifying helpful reviews based on customer's mentions about experiences",
            "authors": [
                {
                    "first": "Hye-Jin",
                    "middle": [],
                    "last": "Min",
                    "suffix": ""
                },
                {
                    "first": "Jong C",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Expert Systems with Applications",
            "volume": "39",
            "issn": "",
            "pages": "11830--11838",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Remote Shopping Advice: Enhancing In-store Shopping with Social Technologies",
            "authors": [
                {
                    "first": "Kori",
                    "middle": [],
                    "last": "Meredith Ringel Morris",
                    "suffix": ""
                },
                {
                    "first": "Gina",
                    "middle": [],
                    "last": "Inkpen",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Venolia",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &#38; Social Computing",
            "volume": "",
            "issn": "",
            "pages": "662--673",
            "other_ids": {
                "DOI": [
                    "10.1145/2531602.2531707"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Rich Representations of Visual Content for Screen Reader Users",
            "authors": [
                {
                    "first": "Jazette",
                    "middle": [],
                    "last": "Meredith Ringel Morris",
                    "suffix": ""
                },
                {
                    "first": "Cynthia",
                    "middle": [
                        "L"
                    ],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "Edward",
                    "middle": [],
                    "last": "Bennett",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Cutrell",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI '18)",
            "volume": "",
            "issn": "",
            "pages": "1--11",
            "other_ids": {
                "DOI": [
                    "10.1145/3173574.3173633"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Detecting experience revealing sentences in product reviews",
            "authors": [
                {
                    "first": "Quang",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "University of Amsterdam",
            "volume": "1",
            "issn": "",
            "pages": "1--78",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Generating Product Descriptions from User Reviews",
            "authors": [
                {
                    "first": "Slava",
                    "middle": [],
                    "last": "Novgorodov",
                    "suffix": ""
                },
                {
                    "first": "Ido",
                    "middle": [],
                    "last": "Guy",
                    "suffix": ""
                },
                {
                    "first": "Guy",
                    "middle": [],
                    "last": "Elad",
                    "suffix": ""
                },
                {
                    "first": "Kira",
                    "middle": [],
                    "last": "Radinsky",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The World Wide Web Conference",
            "volume": "",
            "issn": "",
            "pages": "1354--1364",
            "other_ids": {
                "DOI": [
                    "10.1145/3308558.3313532"
                ]
            }
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Autocaption: Automatic caption generation for personal photos",
            "authors": [
                {
                    "first": "Krishnan",
                    "middle": [],
                    "last": "Ramnath",
                    "suffix": ""
                },
                {
                    "first": "Simon",
                    "middle": [],
                    "last": "Baker",
                    "suffix": ""
                },
                {
                    "first": "Lucy",
                    "middle": [],
                    "last": "Vanderwende",
                    "suffix": ""
                },
                {
                    "first": "Motaz",
                    "middle": [],
                    "last": "El-Saban",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sudipta",
                    "suffix": ""
                },
                {
                    "first": "Anitha",
                    "middle": [],
                    "last": "Sinha",
                    "suffix": ""
                },
                {
                    "first": "Noran",
                    "middle": [],
                    "last": "Kannan",
                    "suffix": ""
                },
                {
                    "first": "Michel",
                    "middle": [],
                    "last": "Hassan",
                    "suffix": ""
                },
                {
                    "first": "Yi",
                    "middle": [],
                    "last": "Galley",
                    "suffix": ""
                },
                {
                    "first": "Deva",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Winter Conference on Applications of Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1050--1057",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "In-context Q&#38;A to Support Blind People Using Smartphones",
            "authors": [
                {
                    "first": "Andr\u00e9",
                    "middle": [],
                    "last": "Rodrigues",
                    "suffix": ""
                },
                {
                    "first": "Kyle",
                    "middle": [],
                    "last": "Montague",
                    "suffix": ""
                },
                {
                    "first": "Hugo",
                    "middle": [],
                    "last": "Nicolau",
                    "suffix": ""
                },
                {
                    "first": "Jo\u00e3o",
                    "middle": [],
                    "last": "Guerreiro",
                    "suffix": ""
                },
                {
                    "first": "Tiago",
                    "middle": [],
                    "last": "Guerreiro",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility",
            "volume": "",
            "issn": "",
            "pages": "32--36",
            "other_ids": {
                "DOI": [
                    "10.1145/3132525.3132555"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Sasayaki: Augmented Voice Web Browsing Experience",
            "authors": [
                {
                    "first": "Daisuke",
                    "middle": [],
                    "last": "Sato",
                    "suffix": ""
                },
                {
                    "first": "Shaojian",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Masatomo",
                    "middle": [],
                    "last": "Kobayashi",
                    "suffix": ""
                },
                {
                    "first": "Hironobu",
                    "middle": [],
                    "last": "Takagi",
                    "suffix": ""
                },
                {
                    "first": "Chieko",
                    "middle": [],
                    "last": "Asakawa",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
            "volume": "",
            "issn": "",
            "pages": "2769--2778",
            "other_ids": {
                "DOI": [
                    "10.1145/1978942.1979353"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "the Shadow of Misperception: Assistive Technology Use and Social Interactions",
            "authors": [
                {
                    "first": "Kristen",
                    "middle": [],
                    "last": "Shinohara",
                    "suffix": ""
                },
                {
                    "first": "Jacob",
                    "middle": [
                        "O"
                    ],
                    "last": "Wobbrock",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "705--714",
            "other_ids": {
                "DOI": [
                    "10.1145/1978942.1979044"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "What People with Vision Impairments Want in Image Descriptions",
            "authors": [
                {
                    "first": "Abigale",
                    "middle": [],
                    "last": "Stangl",
                    "suffix": ""
                },
                {
                    "first": "Meredith",
                    "middle": [
                        "Ringel"
                    ],
                    "last": "Morris",
                    "suffix": ""
                },
                {
                    "first": "Danna",
                    "middle": [],
                    "last": "Gurari",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
            "volume": "",
            "issn": "",
            "pages": "1--13",
            "other_ids": {
                "DOI": [
                    "10.1145/3313831.3376404"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "BrowseWithMe: An Online Clothes Shopping Assistant for People with Visual Impairments",
            "authors": [
                {
                    "first": "Abigale",
                    "middle": [
                        "J"
                    ],
                    "last": "Stangl",
                    "suffix": ""
                },
                {
                    "first": "Esha",
                    "middle": [],
                    "last": "Kothari",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Suyog",
                    "suffix": ""
                },
                {
                    "first": "Tom",
                    "middle": [],
                    "last": "Jain",
                    "suffix": ""
                },
                {
                    "first": "Kristen",
                    "middle": [],
                    "last": "Yeh",
                    "suffix": ""
                },
                {
                    "first": "Danna",
                    "middle": [],
                    "last": "Grauman",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gurari",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility (Galway, Ireland) (ASSETS '18)",
            "volume": "",
            "issn": "",
            "pages": "107--118",
            "other_ids": {
                "DOI": [
                    "10.1145/3234695.3236337"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
            "authors": [
                {
                    "first": "Chi",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Luyao",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Xipeng",
                    "middle": [],
                    "last": "Qiu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "1",
            "issn": "",
            "pages": "380--385",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/N19-1035"
                ]
            }
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Usage of Subjective Scales in Accessibility Research",
            "authors": [
                {
                    "first": "Shari",
                    "middle": [],
                    "last": "Trewin",
                    "suffix": ""
                },
                {
                    "first": "Diogo",
                    "middle": [],
                    "last": "Marques",
                    "suffix": ""
                },
                {
                    "first": "Tiago",
                    "middle": [],
                    "last": "Guerreiro",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility",
            "volume": "",
            "issn": "",
            "pages": "59--67",
            "other_ids": {
                "DOI": [
                    "10.1145/2700648.2809867"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "VERSE: Bridging Screen Readers and Voice Assistants for Enhanced Eyes-Free Web Search",
            "authors": [
                {
                    "first": "Alexandra",
                    "middle": [],
                    "last": "Vtyurina",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Fourney",
                    "suffix": ""
                },
                {
                    "first": "Meredith",
                    "middle": [
                        "Ringel"
                    ],
                    "last": "Morris",
                    "suffix": ""
                },
                {
                    "first": "Leah",
                    "middle": [],
                    "last": "Findlater",
                    "suffix": ""
                },
                {
                    "first": "Ryen",
                    "middle": [
                        "W"
                    ],
                    "last": "White",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The 21st International ACM SIGACCESS Conference on Computers and Accessibility",
            "volume": "",
            "issn": "",
            "pages": "414--426",
            "other_ids": {
                "DOI": [
                    "10.1145/3308561.3353773"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "DCU: Aspect-based Polarity Classification for SemEval Task 4",
            "authors": [
                {
                    "first": "Joachim",
                    "middle": [],
                    "last": "Wagner",
                    "suffix": ""
                },
                {
                    "first": "Piyush",
                    "middle": [],
                    "last": "Arora",
                    "suffix": ""
                },
                {
                    "first": "Santiago",
                    "middle": [],
                    "last": "Cortes",
                    "suffix": ""
                },
                {
                    "first": "Utsab",
                    "middle": [],
                    "last": "Barman",
                    "suffix": ""
                },
                {
                    "first": "Dasha",
                    "middle": [],
                    "last": "Bogdanova",
                    "suffix": ""
                },
                {
                    "first": "Jennifer",
                    "middle": [],
                    "last": "Foster",
                    "suffix": ""
                },
                {
                    "first": "Lamia",
                    "middle": [],
                    "last": "Tounsi",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation",
            "volume": "",
            "issn": "",
            "pages": "223--229",
            "other_ids": {
                "DOI": [
                    "10.3115/v1/S14-2036"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Modeling ambiguity, subjectivity, and diverging viewpoints in opinion question answering systems",
            "authors": [
                {
                    "first": "Mengting",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                },
                {
                    "first": "Julian",
                    "middle": [],
                    "last": "Mcauley",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE 16th international conference on data mining (ICDM)",
            "volume": "",
            "issn": "",
            "pages": "489--498",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "MRR: An Unsupervised Algorithm to Rank Reviews by Relevance",
            "authors": [
                {
                    "first": "Henrique",
                    "middle": [
                        "D P"
                    ],
                    "last": "Vinicius Woloszyn",
                    "suffix": ""
                },
                {
                    "first": "Leandro",
                    "middle": [],
                    "last": "Santos",
                    "suffix": ""
                },
                {
                    "first": "Karin",
                    "middle": [],
                    "last": "Krug Wives",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Becker",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the International Conference on Web Intelligence",
            "volume": "",
            "issn": "",
            "pages": "877--883",
            "other_ids": {
                "DOI": [
                    "10.1145/3106426.3106444"
                ]
            }
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Automatic alt-text: Computer-generated image descriptions for blind users on a social network service",
            "authors": [
                {
                    "first": "Shaomei",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Wieland",
                    "suffix": ""
                },
                {
                    "first": "Omid",
                    "middle": [],
                    "last": "Farivar",
                    "suffix": ""
                },
                {
                    "first": "Julie",
                    "middle": [],
                    "last": "Schiller",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. ACM, Association for Computing Machinery",
            "volume": "",
            "issn": "",
            "pages": "1180--1192",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "2017. I Didn'T Know That You Knew I Knew: Collaborative Shopping Practices Between People with Visual Impairment and People with Vision",
            "authors": [
                {
                    "first": "Benjamin",
                    "middle": [
                        "V"
                    ],
                    "last": "Chien Wen Yuan",
                    "suffix": ""
                },
                {
                    "first": "Sooyeon",
                    "middle": [],
                    "last": "Hanrahan",
                    "suffix": ""
                },
                {
                    "first": "Mary",
                    "middle": [
                        "Beth"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [
                        "M"
                    ],
                    "last": "Rosson",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Carroll",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. ACM Hum.-Comput. Interact. 1, CSCW, Article",
            "volume": "118",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1145/3134753"
                ]
            }
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "From recognition to cognition: Visual commonsense reasoning",
            "authors": [
                {
                    "first": "Rowan",
                    "middle": [],
                    "last": "Zellers",
                    "suffix": ""
                },
                {
                    "first": "Yonatan",
                    "middle": [],
                    "last": "Bisk",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                },
                {
                    "first": "Yejin",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "6720--6731",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Unsupervised tip-mining from customer reviews",
            "authors": [
                {
                    "first": "Di",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Theodoros",
                    "middle": [],
                    "last": "Lappas",
                    "suffix": ""
                },
                {
                    "first": "Juheng",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Decision Support Systems",
            "volume": "107",
            "issn": "",
            "pages": "116--124",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Sasayaki: An Augmented Voice-based Web Browsing Experience",
            "authors": [
                {
                    "first": "Shaojian",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Daisuke",
                    "middle": [],
                    "last": "Sato",
                    "suffix": ""
                },
                {
                    "first": "Hironobu",
                    "middle": [],
                    "last": "Takagi",
                    "suffix": ""
                },
                {
                    "first": "Chieko",
                    "middle": [],
                    "last": "Asakawa",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 12th International ACM SIGACCESS Conference on Computers and Accessibility",
            "volume": "",
            "issn": "",
            "pages": "279--280",
            "other_ids": {
                "DOI": [
                    "10.1145/1878803.1878870"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An example questionnaire for BLV users to vote for which one of the review snippets (retrieved by our rules or by default ranking of search result) best answers visual attributes questions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Performance of image description and visual questions on four basic attributes including shape, logo, size, color. Two sighted research team members rated the quality by three levels on 15 Amazon top-seller products in each of the three main categories. Results showed that our rules can directly provide visual info for 54 questions or image descriptions in Clothing, Shoes & Jewelry; 40 in Home & Kitchen; 29 in Electronics.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "confident in understanding the appearance of the product * 5.88 (1.25) 3.00 (0.76) I believe I can fully utilize the information from reviews * 5.75 (1.83) 5.13 (1.46) Revamp can be a helpful alternative for shopping online independently when no sighted helper is available 6.50 (0.53) -I will use Revamp regularly in my daily life 6.00 (0.76) -",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Examples of representative products in three main categories, showing the retrieved reviews, generated image description and comparing with the recognition results by SeeingAI, a common tool for BLV users to figure out what is in an image.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Subjective ratings on statements comparing current practice with experience of using Revamp. Revamp demonstrates a clear advantage in the experience of understanding the product appearance.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "P1 -P10: Chinese Users P11 -P20: North America Users",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors thank all participants who generously shared their time and experience for this work, Mengqi Li for helping with formative studies, and Prof. Jacob O. Wobbrock for valuable advice on the paper writing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "ACKNOWLEDGMENTS"
        }
    ]
}
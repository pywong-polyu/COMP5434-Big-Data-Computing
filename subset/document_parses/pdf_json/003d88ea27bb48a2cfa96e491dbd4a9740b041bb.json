{"paper_id": "003d88ea27bb48a2cfa96e491dbd4a9740b041bb", "metadata": {"title": "Adaptive Invariance for Molecule Property Prediction", "authors": [{"first": "Wengong", "middle": [], "last": "Jin", "suffix": "", "affiliation": {"laboratory": "", "institution": "Masssachusetts Institute of Technology", "location": {}}, "email": "wengong@csail.mit.edu"}, {"first": "Regina", "middle": [], "last": "Barzilay", "suffix": "", "affiliation": {"laboratory": "", "institution": "Masssachusetts Institute of Technology", "location": {}}, "email": "regina@csail.mit.edu"}, {"first": "Tommi", "middle": [], "last": "Jaakkola", "suffix": "", "affiliation": {"laboratory": "", "institution": "Masssachusetts Institute of Technology", "location": {}}, "email": ""}]}, "abstract": [{"text": "Effective property prediction methods can help accelerate the search for COVID-19 antivirals either through accurate in-silico screens or by effectively guiding ongoing at-scale experimental efforts. However, existing prediction tools have limited ability to accommodate scarce or fragmented training data currently available. In this paper, we introduce a novel approach to learn predictors that can generalize or extrapolate beyond the heterogeneous data. Our method builds on and extends recently proposed invariant risk minimization, adaptively forcing the predictor to avoid nuisance variation. We achieve this by continually exercising and manipulating latent representations of molecules to highlight undesirable variation to the predictor. To test the method we use a combination of three data sources: SARS-CoV-2 antiviral screening data, molecular fragments that bind to SARS-CoV-2 main protease and large screening data for SARS-CoV-1. Our predictor outperforms state-of-the-art transfer learning methods by significant margin. We also report the top 20 predictions of our model on Broad drug repurposing hub. arXiv:2005.03004v1 [q-bio.QM] 5 May 2020", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "The race to identify promising repurposing drug candidates against COVID-19 calls for improvements in the underlying property prediction methodology. The accuracy of many existing techniques depends heavily on access to reasonably large, uniform training data. Such high-throughput, on target screening data is not yet publicly available for COVID-19. Indeed, we have only 48 drugs with measured in-vitro SARS-CoV-2 activity shared with the research community [6] . This limited data scenario is not unique to the current pandemic but likely to recur with each evolving or new viral challenge. The ability to make accurate predictions based on all the available data, however limited, is also helpful in guiding later high-throughput targeted experimental effort.", "cite_spans": [{"start": 460, "end": 463, "text": "[6]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Introduction"}, {"text": "We can supplement scarce on-target data with other related data sources, either related screens pertaining to COVID-19 or screens involving related viruses. For instance, we can use additional data pertaining to molecular fragment screens that measure binding to SARS-CoV-2 main protease, obtained via crystallography screening [9] . On average, these fragments consist of only 14 atoms, comprising roughly 37% of full drug size molecules. Another source of data is SARS-CoV-1 screens. Since SARS-CoV-1 and SARS-CoV-2 proteases are similar (more than 79% sequence identity) [12] , drugs screened against SARS-CoV-1 can be expected to be relevant for SARS-CoV-2 predictions.", "cite_spans": [{"start": 328, "end": 331, "text": "[9]", "ref_id": "BIBREF6"}, {"start": 574, "end": 578, "text": "[12]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Introduction"}, {"text": "These two examples highlight the challenges for property prediction tools: much of the available training data comes from either different chemical space (molecular fragments) or different viral species (SARS-CoV-1).", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "The key technological challenge is to be able to estimate models that can extrapolate beyond their training data, e.g., to different chemical spaces. The ability to extrapolate implies a notion of invariance (being impervious) to the differences between the available training data and where predictions are sought. A recently proposed approach known as invariant risk minimization (IRM) [1] seeks to find predictors that are simultaneously optimal across different such scenarios (called environments). Indeed, the differences in chemical spaces can be thought as \"nuisance variation\" that the predictor should be explicitly forced to ignore. One possible way to automatically define this type of environment variability for molecules is scaffolds [2] . But the setting is challenging since scaffolds are combinatorial descriptors (substructures) and can potentially uniquely identify each compound in the training data. Useful environments for estimation should enjoy some statistical support.", "cite_spans": [{"start": 388, "end": 391, "text": "[1]", "ref_id": null}, {"start": 749, "end": 752, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Introduction"}, {"text": "In this paper we propose a novel variant of invariant risk minimization specifically tailored to rich, combinatorially defined environments typical in molecular contexts. Indeed, unlike in standard IRM, we introduce two dynamic (in contrast to many static) environments. These are defined over the same set of training examples, but differ in terms of their associated latent representations. The difference between them arises from continually adjusted perturbations that manipulate the latent representations of compounds towards more \"generic\" versions with the help of a scaffold classifier. The idea is to explicitly highlight to the property predictor that operates on these latent representations what the nuisance variability is that it should not rely on.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Our method is evaluated on existing SARS-CoV-2 screening data [9, 6] . The training utilizes three sources of data: SARS-CoV-2 screened molecules, SARS-CoV-2 fragments and SARS-CoV-1 screening data described above. We compare against multiple transfer learning techniques such as domain adversarial training [5] and conditional domain adversarial network [7] . On two SARS-CoV-2 datasets, the proposed approach outperforms the best performing baseline with 8-16% relative AUROC improvement. Finally, we apply our model on Broad drug repurposing hub [4] and report the top 20 predictions for further investigation.", "cite_spans": [{"start": 62, "end": 65, "text": "[9,", "ref_id": "BIBREF6"}, {"start": 66, "end": 68, "text": "6]", "ref_id": "BIBREF3"}, {"start": 355, "end": 358, "text": "[7]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "Introduction"}, {"text": "Training data in many emerging applications is necessarily limited, fragmented, or otherwise heterogeneous. It is therefore important to ensure that model predictions derived from such data generalize substantially beyond where the training samples lie. In other words, the trained model should have the ability to extrapolate. For instance, in computational chemistry, it is desirable for property prediction models to perform well in time-split scenarios where the evaluation concerns compounds that were created after those in the training set. Another way to simulate evaluation on future compounds is through a scaffold split [2] . A scaffold split between training and test introduces some structural separation between the chemical spaces of the two sets of compounds, hence evaluating the model's ability to extrapolate to a new domain.", "cite_spans": [{"start": 631, "end": 634, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Domain Extrapolation"}, {"text": "One way to ensure domain extrapolation is to enforce an appropriate invariance criterion during training. We envision here that the compounds X can be divided into potentially a large number of domains or \"environments\" E, for example, based on their scaffold. The goal is then to learn a parametric mapping Z = \u03c6(X) of compounds X to their latent representations Z in a manner that satisfies the chosen invariance criterion. A number of such strategies relevant to extrapolation have been proposed. They can be roughly divided into the following three categories:", "cite_spans": [], "ref_spans": [], "section": "Domain Extrapolation"}, {"text": "\u2022 Domain adversarial training [5] enforces the latent representation Z = \u03c6(X) to have the same distribution across different domains E. If we denote by P (X|E) the conditional distribution of compounds in environment E, then we want P (\u03c6(X)|E) = P (\u03c6(X)) for all E. With some abuse of notation, we can write this condition as Z \u22a5 E. A single predictor is learned based on Z = \u03c6(X), i.e., all the domains share the same predictor. As a result, the predicted label distribution P (Y ) will also be the same across the domains. This can be problematic when the training and test domains have very different label distributions [11] . The independence condition itself can be challenging to satisfy when the chemical spaces overlap across the environments. \u2022 Conditional domain adaptation [7] relaxes the requirement that the label distributions must agree across the environments. The key idea is to condition the invariance criterion on the label. In other words, we require that P (\u03c6(X)|E, Y ) = P (\u03c6(X)|Y ) for all E and Y , i.e., we aim to satisfy the independence statement Z \u22a5 E | Y . The formulation allows the label distribution to vary between domains since E and Y can depend on each other. The constraint remains, however, too restrictive about the latent representation. To illustrate this, consider a simple case where the environments share the same chemical space and differ only in terms of proportions of different types of compounds in them. These type proportions play roles analogous to label proportions in domain adversarial training. Hence, the only way to achieve Z \u22a5 E | Y would be if the proportions were the same across environments. To state the example differently, a functional mapping Z = \u03c6(X) cannot fractionally assign probability mass placed on X to different latent ", "cite_spans": [{"start": 624, "end": 628, "text": "[11]", "ref_id": "BIBREF8"}, {"start": 785, "end": 788, "text": "[7]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "Domain Extrapolation"}, {"text": "using the gradient from scaffold classifier g. The predictor f is trained to be simultaneously optimal across two environments. The scaffold s is a subgraph of a molecule x with its side chains removed [2] .", "cite_spans": [{"start": 202, "end": 205, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Domain Extrapolation"}, {"text": "space locations Z; it all has to be mapped to a single location. To reduce the impact of the strict condition, we would have to introduce P (Z|\u03c6(X), E) in place of the simpler functional mapping Z = \u03c6(X), further complicating the approach. \u2022 Invariant risk minimization (IRM) [1] seeks a different notion of invariance, focusing less on aligning distributions of latent representations, and instead shifting the emphasis on how those representations can be consistently used for predictions. The IRM principle requires that the predictor f operating on Z = \u03c6(X) is simultaneously optimal across different environments or domains. For example, this holds if our representation explicates only features that are (causally) necessary for the correct prediction. How \u03c6(X) is distributed across the environments is then immaterial. The associated conditional independence criterion is Y \u22a5 E | Z. In other words, knowing the environment shouldn't provide any additional information about Y beyond the features Z = \u03c6(X). The distribution of labels can differ across the environments.", "cite_spans": [{"start": 276, "end": 279, "text": "[1]", "ref_id": null}], "ref_spans": [], "section": "Domain Extrapolation"}, {"text": "While the IRM principle provides a natural framework for domain extrapolation, it needs to be extended in several ways for our setting. The main limitation of the original framework is that the environments E themselves are fixed and pre-defined. Their role in the pricinple is to illustrate \"nuisance\" variation, i.e., variability that the predictor should learn not to rely on. In order to enforce the associated independence criterion, we need a fair number of examples within each such environment. The approach therefore becomes unsuitable when the natural environments such as scaffolds are combinatorially defined or otherwise have high cardinality. Indeed, we might have only a single molecule per environment in our training set, making the independence criterion vacuous (E would uniquely specify X, thus also Z and Y ). A straightforward remedy for the high cardinality environments would be to introduce a coarser definition, and enforce the principle at this coarse level instead. Since environments represent constraints on the predictor, their role in estimation is adversarial. What is then the appropriate trade-off between such a coarser definition (relaxation of constraints) and our ability to predict? We side-step having to answer this question, and instead propose to dynamically map the large number of environments to just two. These two environments are designed to nevertheless highlight the nuisance variation the predictor should avoid but do so in a tractable manner.", "cite_spans": [], "ref_spans": [], "section": "Domain Extrapolation"}, {"text": "Our goal is to adaptively highlight to the predictor the type of variability that it ought not to rely on. We do this by replacing high cardinality environments such as those based on scaffold with just two new environments. These two new environments are unusual in the sense that they share the exact same set of examples. Indeed, they only differ in terms of the representation that the predictor operates on. The first environment simply corresponds to the representation we are trying to learn, i.e., z = \u03c6(x), where the lowercase letters refer to specific instances rather than random variables. The second environment is defined in terms of a modified representation \u03c6(x) + \u03b4(x) that is a perturbed version of \u03c6(x) and constructed with the help of the environment or scaffold classifier. More formally, our two environments correspond to a choice of perturbation h \u2208 {0, \u03b4} used to derive the latent representation z from x, i.e., \u03c6(x) + h(x). The associated target labels are clearly the same regardless of which perturbation (none or \u03b4(x)) was chosen. The key part of our approach pertains to how \u03b4(x) is defined. To this end, let g(\u03c6(x))) be a parametric environment classifier that we will instantiate in detail later. The associated classification loss is (s(x), g(\u03c6(x))) where s(x) is the correct original environment label (here a scaffold) for x. The scaffold classifier is evolved together with the feature mapping \u03c6 and the associated predictor f . We define the non-zero perturbation \u03b4(x) in terms of the gradient:", "cite_spans": [], "ref_spans": [], "section": "IRM with adaptive environments"}, {"text": "where \u03b1 is a step size parameter. The goal of this perturbation is to turn \u03c6(x) into its \"generic\" version \u03c6(x) + \u03b4(x) which contains less information of the environment (e.g., scaffold). Note that if we were to perform adversarial domain alignment, \u03b4(x) would represent a reverse gradient update to modify \u03c6(x). We do not do that, instead we are using the perturbation to highlight directions of variability to avoid for the predictor f within an overall IRM formulation. The degree to which \u03c6(x) is adjusted in response to \u03b4(x) arises from the IRM principle, not from a direct alignment objective.", "cite_spans": [], "ref_spans": [], "section": "IRM with adaptive environments"}, {"text": "We begin by building the overall training objective which is then optimized in batches as described in Algorithm 1. Let (x i , y i ) be a pair of training example + the associated label to predict. Each x i also has an environment label/features given by s i = s(x i ) (the original mapping of examples to environments is assumed given and fixed, defined by s(x)). The environment classifier is trained to minimize", "cite_spans": [], "ref_spans": [], "section": "IRM with adaptive environments"}, {"text": "As we will explain later on, the environment classifier remains \"unaware\" of how the perturbation is derived on the basis of its predictions. The loss of the predictor f , now operating on \u03c6(", "cite_spans": [], "ref_spans": [], "section": "IRM with adaptive environments"}, {"text": "The specific form of the loss depends on the prediction task. In accordance with the IRM principle, we enforce that the predictor operating on z = \u03c6(x) + h(x) remains optimal whether its input is \u03c6(x) or the perturbed version \u03c6(x) + \u03b4(x). In other words, we require that", "cite_spans": [], "ref_spans": [], "section": "IRM with adaptive environments"}, {"text": "where f h is a predictor in the same parametric family as f but trained separately with the knowledge of h (perturbed or not). By relaxing the constraints via Lagrange multipliers, we express the overall training objective as", "cite_spans": [], "ref_spans": [], "section": "IRM with adaptive environments"}, {"text": "This minimax objective is minimized with respect to \u03c6, g, and f , and maximized with respect to f h , h \u2208 {0, \u03b4}. A few remarks are necessary concerning this objective:", "cite_spans": [], "ref_spans": [], "section": "IRM with adaptive environments"}, {"text": "\u2022 Even though \u03b4 is defined on the basis of \u03c6 and the environment classifier g, we view it as a functionally independent player. The goal of \u03b4 is to enforce optimality of f and therefore it plays an adversarial role relative to f . Similarly to GAN objectives where the discriminator has a separate objective function, different from the generator, we separate out \u03b4 as another player in an overall game theoretic objective. Specifically, \u03b4 takes input from \u03c6 and g but does not inform them in return in back-propagation. 1 \u2022 \u03c6 in our objective is adjusted to also help the auxiliary environment classifier. This is contrary to domain alignment where the goal would be to take out any dependence on the environment. The benefit in our formulation is two-fold. First, the term grounds \u03c6 also based on the auxiliary Sample a batch of molecules {x 1 , \u00b7 \u00b7 \u00b7 , x n } with their environments {s 1 , \u00b7 \u00b7 \u00b7 , s n } 3:", "cite_spans": [], "ref_spans": [], "section": "IRM with adaptive environments"}, {"text": "Encode molecules {x 1 , \u00b7 \u00b7 \u00b7 , x n } into vectors {\u03c6(x 1 ), \u00b7 \u00b7 \u00b7 , \u03c6(x n )}", "cite_spans": [], "ref_spans": [], "section": "IRM with adaptive environments"}, {"text": "Computed environment classification loss L s (g \u2022 \u03c6) = i (s i , g(\u03c6(x i ))).", "cite_spans": [], "ref_spans": [], "section": "4:"}, {"text": "Construct perturbed representation \u03b4(x i ) = \u2212\u03b1\u2207 \u03c6(xi) (s i , g(\u03c6(x i ))) 6: Compute invariant predictor loss L(f \u2022 \u03c6) 7: Compute competing predictor loss L(f h \u2022 (\u03c6 + h)) h \u2208 {0, \u03b4} 8:", "cite_spans": [{"start": 74, "end": 76, "text": "6:", "ref_id": null}, {"start": 119, "end": 121, "text": "7:", "ref_id": null}], "ref_spans": [], "section": "5:"}, {"text": "Update \u03c6, f, g to minimize Eq. (5) 9: Update f 0 , f \u03b4 to maximize Eq. (5) 10: end for objective, helping it to retain useful information about each example x. Second, the term grounds and stabilizes the definition of \u03b4 as the gradient of the environment predictor since g no longer approaches a random predictor. It would be weak if \u03c6 contains no information about the environment as in domain alignment. Thus \u03b4 remains well-defined as a direction throughout the optimization.", "cite_spans": [{"start": 35, "end": 37, "text": "9:", "ref_id": null}], "ref_spans": [], "section": "5:"}, {"text": "The training procedure is shown in Algorithm 1.", "cite_spans": [], "ref_spans": [], "section": "5:"}, {"text": "In molecule property prediction, the training data is a collection of pairs {(x i , y i )}, where x i is a molecular graph and y i is its activity score, typically binary (active/inactive). The feature extractor \u03c6(\u00b7) is a graph convolutional network (GCN) which translates a molecular graph into a continuous vector through directed message passing operations [10] . The predictor f is a feed-forward network that takes \u03c6(x) or \u03c6(x) + \u03b4(x) as input and yields predicted activity y.", "cite_spans": [{"start": 360, "end": 364, "text": "[10]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Adapting the framework to molecule property prediction"}, {"text": "The original environment of each compound x i is defined as its Murcko scaffold [2] , which is a subgraph of x i . Since scaffold is a combinatorial object with a large vocabulary of possible values, we define and train the environment classifier in a contrastive fashion [8] . Specifically, for a given molecule x i with scaffold s i , we randomly sample n other molecules and take their associated scaffolds {s k } as negative examples, as the contrastive set C. The environment classifier g makes use of a feed-forward network g s that maps each compound or a scaffold (subgraph) to a feature vector. The probability that x i is mapped to its correct scaffold s i is then defined as", "cite_spans": [{"start": 80, "end": 83, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 272, "end": 275, "text": "[8]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Adapting the framework to molecule property prediction"}, {"text": "where sim(\u00b7, \u00b7) stands for cosine similarity. In practice, we use the molecules within the same batch as negative examples.", "cite_spans": [], "ref_spans": [], "section": "Adapting the framework to molecule property prediction"}, {"text": "Our experiments consist of two settings. To compare our method with existing transfer learning techniques, we first evaluate our methods on a standard unsupervised transfer setup. All the models are trained on SARS-CoV-1 data and tested on SARS-CoV-2 compounds. Next, in order to identify drug candidates for SARS-CoV-2, we extend our method by incorporating labeled SARS-CoV-2 data to maximize prediction accuracy and perform virtual screening over Broad drug repurposing hub [4] .", "cite_spans": [{"start": 477, "end": 480, "text": "[4]", "ref_id": null}], "ref_spans": [], "section": "Experiments"}, {"text": "Training data Our training data consist of three screens related to SARS-CoV. All the data can be found at https://github.com/yangkevin2/coronavirus_data.", "cite_spans": [], "ref_spans": [], "section": "Experiments"}, {"text": "\u2022 SARS-CoV-2 MPro inhibition 881 fragments screened for SARS-CoV-2 main protease (Mpro) collected by the Diamond Light Source group [9] . The dataset contains 78 hits. \u2022 SARS-CoV-2 antiviral activity 48 FDA-approved drugs screened for antiviral activity against SARS-CoV-2 in vitro [6] , including reference drugs such as Remdesivir, Lopinavir and Chloroquine. The dataset contains 27 hits. \u2022 SARS-CoV-1 3CLpro inhibition Over 290K molecules screened for activity against SARS-CoV-1 3C-like protease (3CLpro) in PubChem AID1706 assay. There are 405 active compounds.", "cite_spans": [{"start": 132, "end": 135, "text": "[9]", "ref_id": "BIBREF6"}, {"start": 282, "end": 285, "text": "[6]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Experiments"}, {"text": "Baselines We compare the proposed approach with the following baselines:", "cite_spans": [], "ref_spans": [], "section": "Experiments"}, {"text": "\u2022 Direct transfer: We train a GCN on SARS-CoV-1 data and directly test it on SARS-CoV-2 data.", "cite_spans": [], "ref_spans": [], "section": "Experiments"}, {"text": "\u2022 Domain adversarial training (DANN) [5]: Since distribution of molecules is different between SARS-CoV-1 and SARS-CoV-2 datasets, we use domain adversarial training to facilitate transfer. Specifically, we augment our GCN with additional domain classifier g to enforce the distribution of \u03c6(x) to be the same across training (SARS-CoV-1) and test set (SARS-CoV-2). \u2022 Conditional adversarial domain adaptation (CDAN) [7] conditions the domain classifier g with predicted labels f (\u03c6(x)). In particular, we adopt their multilinear conditioning strategy: the input to g becomes a vector outer-product \u03c6(x) \u2297 f (\u03c6(x)), which has the same dimension as \u03c6(x) for binary classification tasks. \u2022 Scaffold adversarial training (SANN): This is an extension of DANN where the domain classifier g is replaced with our scaffold classifier g s in Eq. (6) . SANN seeks to learn a scaffold-invariant representation \u03c6(x) through the following minimax game (L e is scaffold classification loss):", "cite_spans": [{"start": 417, "end": 420, "text": "[7]", "ref_id": "BIBREF4"}, {"start": 837, "end": 840, "text": "(6)", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Experiments"}, {"text": "\u2022 Invariant risk minimization (IRM): The original IRM [1] requires the predictor f to be constant, which does not work well in our setting. Therefore, we adopt an adversarial formulation for IRM proposed in [3] , allowing us to use powerful neural predictors:", "cite_spans": [{"start": 54, "end": 57, "text": "[1]", "ref_id": null}, {"start": 207, "end": 210, "text": "[3]", "ref_id": null}], "ref_spans": [], "section": "Experiments"}, {"text": "Here each of the K environments consists of molecules with the same scaffold. Since the number of environments is large, we impose parameter sharing among the competing predictors f 1 , \u00b7 \u00b7 \u00b7 , f K . Specifically, the input of f j = f (\u03c6(x), j) is a concatenation of \u03c6(x) and one-hot encoding of j.", "cite_spans": [], "ref_spans": [], "section": "Experiments"}, {"text": "Model hyperparameters For our model, we set \u03bb h = \u03bb e = 0.1 and perturbation learning rate \u03b1 = 0.1, which worked well across all experiments. All methods are trained with Adam using its default configuration. Our GCN implementation is based on chemprop [10] .", "cite_spans": [{"start": 253, "end": 257, "text": "[10]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Experiments"}, {"text": "\u2022 For unsupervised transfer, we use their default hyper-parameter setting. For all methods, the GCN \u03c6 has three layers with hidden layer dimension 300. The predictor f is a two-layer MLP. \u2022 For supervised transfer, we perform hyper-parameter optimization to identify the best architecture for the multitask GCN. The GCN \u03c6 has two layers with hidden layer dimension 2000. The predictor f is a three-layer MLP. The dropout rate is 0.1. For fair comparison, all the methods use the same architecture in this setting.", "cite_spans": [], "ref_spans": [], "section": "Experiments"}, {"text": "Setup Our model is a single-task binary classification model which predicts the SARS-CoV-1 3CLpro inhibition. After training, the model is tested on SARS-CoV-2 Mpro and antiviral data. Each model is evaluated under five independent runs and we report the average AUROC score.", "cite_spans": [], "ref_spans": [], "section": "Unsupervised transfer"}, {"text": "Our results are shown in Table 1 . The proposed method significantly outperformed all the baselines, especially on the Mpro inhibition prediction dataset (0.756 versus 0.653 AUROC).", "cite_spans": [], "ref_spans": [{"start": 25, "end": 32, "text": "Table 1", "ref_id": "TABREF2"}], "section": "Results"}, {"text": "Ablation study Indeed, the improvement of our model comes from two sources: the additional auxiliary task and IRM principle. To show individual contribution of each component, we conduct an ablation study of our method without the IRM principle. The loss function in this case is the scaffold classification loss plus property prediction loss \u03bb e L e (g \u2022 \u03c6) + L(f \u2022 \u03c6). The performance of this method is shown in the end of Table 1 (\"without IRM\"). The auxiliary scaffold classifier shows quite significant improvement, but is still inferior to our full model trained with IRM principle.", "cite_spans": [], "ref_spans": [{"start": 425, "end": 432, "text": "Table 1", "ref_id": "TABREF2"}], "section": "Results"}, {"text": "Setup We extend all the methods to multitask binary classification models that predict three different properties for each new compound: 1) probability of inhibiting the SARS-CoV-2 Mpro; 2) antiviral activity against SARS-CoV-2; 3) probability of inhibiting SARS-CoV-1 3CLpro. [7] 0.625 \u00b1 0.013 0.639 \u00b1 0.067 IRM [1] 0.653 \u00b1 0.022 0.391 \u00b1 0.086", "cite_spans": [{"start": 277, "end": 280, "text": "[7]", "ref_id": "BIBREF4"}, {"start": 313, "end": 316, "text": "[1]", "ref_id": null}], "ref_spans": [], "section": "Drug repurposing for SARS-CoV-2"}, {"text": "Our method 0.756 \u00b1 0.012 0.695 \u00b1 0.068 -without IRM 0.736 \u00b1 0.007 0.678 \u00b1 0.057 Each model is evaluated under 5-fold cross validation with the same splits. In each fold, the training set contains the SARS-CoV data and 60% of the SARS-CoV-2 data (Mpro + antiviral), and the test set contains the rest 40% of the SARS-CoV-2 compounds. We report the mean and standard deviation of AUROC score evaluated on five different folds.", "cite_spans": [], "ref_spans": [], "section": "Drug repurposing for SARS-CoV-2"}, {"text": "Our results are shown in Table 2 . The proposed method significantly outperformed the two baselines, especially on the antiviral activity prediction dataset (0.89 versus 0.82 AUROC). As an ablation study, we also trained a GCN on only SARS-CoV-2 data (the first row in Table 2 ). Indeed, the multitask GCN trained with additional SARS-CoV-1 data performs better (0.740 vs 0.807 on antiviral prediction), indicating that the two virus are closely related.", "cite_spans": [], "ref_spans": [{"start": 25, "end": 32, "text": "Table 2", "ref_id": "TABREF3"}, {"start": 269, "end": 276, "text": "Table 2", "ref_id": "TABREF3"}], "section": "Results"}, {"text": "The best model is then used to predict the SARS-CoV-2 Mpro inhibition and antiviral activity of compounds in Broad drug repurposing hub. In order to utilize maximal amount of labeled data, the model is re-trained under 10-fold cross validation with 90%/10% split (instead of 60%/40%). The resulting 10 models are combined together as an ensemble to predict properties for new compounds. We report the top 20 predicted molecules for MPro inhibition and antiviral activity in Table 3 and 4.", "cite_spans": [], "ref_spans": [{"start": 474, "end": 481, "text": "Table 3", "ref_id": "TABREF4"}], "section": "Results"}, {"text": "In this paper, we investigate existing domain extrapolation paradigms and their limitations. To allow the method to extrapolate across combinatorially many environments, we propose a new method which complements invariant risk minimization with adaptive environments. The method is evaluated on molecule property prediction tasks and shows significant improvements over strong baselines. ", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}], "bib_entries": {"BIBREF1": {"ref_id": "b1", "title": "The properties of known drugs. 1. molecular frameworks", "authors": [{"first": "W", "middle": [], "last": "Guy", "suffix": ""}, {"first": "", "middle": [], "last": "Bemis", "suffix": ""}, {"first": "A", "middle": [], "last": "Mark", "suffix": ""}, {"first": "", "middle": [], "last": "Murcko", "suffix": ""}], "year": 1996, "venue": "Journal of medicinal chemistry", "volume": "39", "issn": "15", "pages": "2887--2893", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Identification of antiviral drug candidates against sars-cov-2 from fda-approved drugs. bioRxiv", "authors": [{"first": "Sangeun", "middle": [], "last": "Jeon", "suffix": ""}, {"first": "Meehyun", "middle": [], "last": "Ko", "suffix": ""}, {"first": "Jihye", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Inhee", "middle": [], "last": "Choi", "suffix": ""}, {"first": "Soo", "middle": ["Young"], "last": "Byun", "suffix": ""}, {"first": "Soonju", "middle": [], "last": "Park", "suffix": ""}, {"first": "David", "middle": [], "last": "Shum", "suffix": ""}, {"first": "Seungtaek", "middle": [], "last": "Kim", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Conditional adversarial domain adaptation", "authors": [{"first": "Mingsheng", "middle": [], "last": "Long", "suffix": ""}, {"first": "Zhangjie", "middle": [], "last": "Cao", "suffix": ""}, {"first": "Jianmin", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Michael I Jordan", "middle": [], "last": "", "suffix": ""}], "year": 2018, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "1640--1650", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Representation learning with contrastive predictive coding", "authors": [{"first": "Aaron", "middle": [], "last": "Van Den Oord", "suffix": ""}, {"first": "Yazhe", "middle": [], "last": "Li", "suffix": ""}, {"first": "Oriol", "middle": [], "last": "Vinyals", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1807.03748"]}}, "BIBREF6": {"ref_id": "b6", "title": "Sars-cov-2 main protease structure and xchem fragment screen", "authors": [], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Analyzing learned molecular representations for property prediction", "authors": [{"first": "Kevin", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Kyle", "middle": [], "last": "Swanson", "suffix": ""}, {"first": "Wengong", "middle": [], "last": "Jin", "suffix": ""}, {"first": "Connor", "middle": [], "last": "Coley", "suffix": ""}, {"first": "Philipp", "middle": [], "last": "Eiden", "suffix": ""}, {"first": "Hua", "middle": [], "last": "Gao", "suffix": ""}, {"first": "Angel", "middle": [], "last": "Guzman-Perez", "suffix": ""}, {"first": "Timothy", "middle": [], "last": "Hopper", "suffix": ""}, {"first": "Brian", "middle": [], "last": "Kelley", "suffix": ""}, {"first": "Miriam", "middle": [], "last": "Mathea", "suffix": ""}], "year": 2019, "venue": "Journal of chemical information and modeling", "volume": "59", "issn": "8", "pages": "3370--3388", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "On learning invariant representation for domain adaptation", "authors": [{"first": "Han", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Remi", "middle": [], "last": "Tachet", "suffix": ""}, {"first": "Kun", "middle": [], "last": "Combes", "suffix": ""}, {"first": "Geoffrey", "middle": ["J"], "last": "Zhang", "suffix": ""}, {"first": "", "middle": [], "last": "Gordon", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1901.09453"]}}, "BIBREF9": {"ref_id": "b9", "title": "A pneumonia outbreak associated with a new coronavirus of probable bat origin", "authors": [{"first": "Peng", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "Xing-Lou", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Xian-Guang", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Ben", "middle": [], "last": "Hu", "suffix": ""}, {"first": "Lei", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Wei", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "", "middle": [], "last": "Hao-Rui", "suffix": ""}, {"first": "Yan", "middle": [], "last": "Si", "suffix": ""}, {"first": "Bei", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "Chao-Lin", "middle": [], "last": "Li", "suffix": ""}, {"first": "", "middle": [], "last": "Huang", "suffix": ""}], "year": 2020, "venue": "Nature", "volume": "579", "issn": "7798", "pages": "270--273", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Left: Illustration of base GCN model for molecule property prediction. Right: IRM with adaptive environments. The model perturbs the representation", "latex": null, "type": "figure"}, "FIGREF1": {"text": "20 SARS-CoV-2 antiviral molecules in the Broad drug repurposing hub. SMILES Antiviral C[C@]12CC(=O)[C@H]3[C@@H](CCC4=CC(=O)CC[C@]34C)[C@@H]1CC[C@]2(O)C(=O)CO 0.955 C[C@]1(O)CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C 0.953 C[C@]12C[C@H](O)[C@H]3[C@@H](CCC4=CC(=O)CC[C@]34C)[C@@H]1CC[C@]2(O)C(=O)CO 0.948 CC(=O)OCC(=O)[C@@]1(O)CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3C(=O)C[C@]12C 0.945 C[C@H]1C[C@H]2[C@@H]3CC[C@](O)(C(C)=O)[C@@]3(C)CC[C@@H]2[C@@]2(C)CCC(=O)C=C12 0.945 CC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C 0.944 C[C@@]12[C@H](CC[C@]1(O)[C@@H]1CC[C@@H]3C[C@@H](O)CC[C@]3(C)[C@H]1C[C@H]2O)C1=CC(=O)OC1 0.936 C[C@]12CC[C@H]3[C@@H](CCC4=CC(=O)CC[C@]34C)[C@@H]1CC[C@@H]2C(=O)CO 0.932 CC(C)(C)C(=O)OCC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C 0.931 C[C@]12CC(=O)C3C(CCC4=CC(=O)CC[C@]34C)C1CC[C@]2(O)C(=O)CO 0.924 CC(=O)O[C@@]1(CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C)C(C)=O 0.924 CC(=O)OCC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C 0.920 C\\C=C1/C(=O)C[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C 0.919 C[C@]12CC[C@H]3[C@@H](CC[C@@H]4C[C@@H](O)CC[C@]34C)[C@@]1(O)CC[C@@H]2C1=CC(=O)OC1 0.918 CCCC(=O)O[C@@]1(CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3[C@@H](O)C[C@]12C)C(=O)CO 0.902 CCC(=O)O[C@@]1(CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C)C(=O)CO 0.894 CC(=O)[C@@]12OC(C)(O[C@@H]1C[C@H]1[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]21C)c1ccccc1 0.880 CC(=O)OCC(=O)[C@@]1(O)CCC2C3CCC4=CC(=O)CC[C@]4(C)C3[C@@H](O)C[C@]12C 0.877 CC(=O)[C@@]1(O)CCC2C3CCC4=CC(=O)CC[C@]4(C)C3CC[C@]12C 0.876 C[C@]12CCC3C(CCC4=CC(=O)CC[C@]34C)C1CC[C@@H]2O 0.872 [4] Steven M Corsello, Joshua A Bittker, Zihan Liu, Joshua Gould, Patrick McCarren, Jodi E Hirschman, Stephen E Johnston, Anita Vrcic, Bang Wong, Mariya Khan, et al. The drug repurposing hub: a next-generation drug library and information resource. Nature medicine, 23 (4):405-408, 2017. [5] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.", "latex": null, "type": "figure"}, "TABREF0": {"text": "The goal of \u03b4(x) is to explicate directions in the latent representation that the predictor should avoid paying attention to. While traditional IRM environments divide examples x into environments, often exclusively, we instead exercise different latent representations over the same set of examples.", "latex": null, "type": "table"}, "TABREF2": {"text": "Unsupervised transfer results. Models are trained on SARS-CoV-1 data and tested on SARS-CoV-2 compounds. We report average of AUROC under 5-fold cross validation.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>Mpro inhibition AUC </td><td>Antiviral activity AUC\n</td></tr><tr><td>Direct Transfer </td><td>0.642\u00b1 0.021 </td><td>0.415\u00b1 0.081\n</td></tr><tr><td>DANN [5] </td><td>0.646\u00b1 0.012 </td><td>0.607\u00b1 0.121\n</td></tr><tr><td>SANN </td><td>0.630\u00b1 0.079 </td><td>0.570\u00b1 0.096\n</td></tr><tr><td>CDAN [7] </td><td>0.625\u00b1 0.013 </td><td>0.639\u00b1 0.067\n</td></tr><tr><td>IRM [1] </td><td>0.653\u00b1 0.022 </td><td>0.391\u00b1 0.086\n</td></tr><tr><td>Our method </td><td>0.756\u00b1 0.012 </td><td>0.695\u00b1 0.068\n</td></tr><tr><td>- without IRM </td><td>0.736\u00b1 0.007 </td><td>0.678\u00b1 0.057\n</td></tr></table></body></html>"}, "TABREF3": {"text": "", "latex": null, "type": "table"}, "TABREF4": {"text": "Top 20 SARS-CoV-2 Mpro inhibiting molecules (non-covalent) in the Broad drug repurposing hub.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>SMILES </td><td>MPro\n</td></tr><tr><td>Nc1ccc(cc1)S(N)(=O)=O </td><td>0.797\n</td></tr><tr><td>N#Cc1ccncn1 </td><td>0.769\n</td></tr><tr><td>Nc1ccccc1S(N)(=O)=O </td><td>0.747\n</td></tr><tr><td>Cc1ccc(cc1)S(N)(=O)=O </td><td>0.727\n</td></tr><tr><td>NS(=O)(=O)c1cc(Cl)c(Cl)c(c1)S(N)(=O)=O </td><td>0.695\n</td></tr><tr><td>NCc1ccc(cc1)S(N)(=O)=O </td><td>0.694\n</td></tr><tr><td>NC(=N)NCCNS(=O)(=O)c1cccc2cnccc12 </td><td>0.679\n</td></tr><tr><td>NC(=N)NS(=O)(=O)c1ccc(N)cc1 </td><td>0.662\n</td></tr><tr><td>NS(=O)(=O)NCc1csc2ccccc12 </td><td>0.651\n</td></tr><tr><td>NS(=O)(=O)C#Cc1ccccc1 </td><td>0.614\n</td></tr><tr><td>Nc1cc(C(Cl)=C(Cl)Cl)c(cc1S(N)(=O)=O)S(N)(=O)=O </td><td>0.613\n</td></tr><tr><td>NC(=N)NCC1COC2(CCCCC2)O1 </td><td>0.582\n</td></tr><tr><td>NC(=N)NC(=O)c1cnccn1 </td><td>0.580\n</td></tr><tr><td>NC(=N)Nc1ccc(cc1)C(=O)Oc1ccc2cc(ccc2c1)C(N)=N </td><td>0.560\n</td></tr><tr><td>N#Cc1ncccn1 </td><td>0.524\n</td></tr><tr><td>Nc1ccncc1N </td><td>0.521\n</td></tr><tr><td>N#Cc1ccc(cc1)C1CCCc2cncn12 </td><td>0.450\n</td></tr><tr><td>NS(=O)(=O)Cc1noc2ccccc12 </td><td>0.443\n</td></tr><tr><td>NS(=O)(=O)c1ccc(NC(=O)Nc2ccc(F)cc2)cc1 </td><td>0.442\n</td></tr></table></body></html>"}, "TABREF5": {"text": "Table 2: 5-fold cross validation of different methods. CoV-2 means that the model is trained on\nSARS-CoV-2 data. CoV-1 means that the model is additionally trained on SARS-CoV-1 data.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Method </td><td>CoV-1 </td><td>CoV-2 </td><td>Mpro inhibition AUC </td><td>Antiviral activity AUC\n</td></tr><tr><td>Multitask GCN </td><td>\u00a0</td><td>X </td><td>0.7646\u00b1 0.0398 </td><td>0.7404\u00b1 0.1117\n</td></tr><tr><td>Multitask GCN </td><td>X </td><td>X </td><td>0.7841\u00b1 0.0416 </td><td>0.8067\u00b1 0.0690\n</td></tr><tr><td>DANN </td><td>X </td><td>X </td><td>0.7785\u00b1 0.0321 </td><td>0.8146\u00b1 0.1068\n</td></tr><tr><td>IRM [1] </td><td>X </td><td>X </td><td>0.7778\u00b1 0.0426 </td><td>0.8198\u00b1 0.1072\n</td></tr><tr><td>Our method </td><td>X </td><td>X </td><td>0.7955\u00b1 0.0489 </td><td>0.8930\u00b1 0.0267\n</td></tr></table></body></html>"}, "TABREF6": {"text": "Table 4: Top 20 SARS-CoV-2 antiviral molecules in the Broad drug repurposing hub.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>SMILES </td><td>Antiviral\n</td></tr><tr><td>C[C@]12CC(=O)[C@H]3[C@@H](CCC4=CC(=O)CC[C@]34C)[C@@H]1CC[C@]2(O)C(=O)CO </td><td>0.955\n</td></tr><tr><td>C[C@]1(O)CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C </td><td>0.953\n</td></tr><tr><td>C[C@]12C[C@H](O)[C@H]3[C@@H](CCC4=CC(=O)CC[C@]34C)[C@@H]1CC[C@]2(O)C(=O)CO </td><td>0.948\n</td></tr><tr><td>CC(=O)OCC(=O)[C@@]1(O)CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3C(=O)C[C@]12C </td><td>0.945\n</td></tr><tr><td>C[C@H]1C[C@H]2[C@@H]3CC[C@](O)(C(C)=O)[C@@]3(C)CC[C@@H]2[C@@]2(C)CCC(=O)C=C12 </td><td>0.945\n</td></tr><tr><td>CC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C </td><td>0.944\n</td></tr><tr><td>C[C@@]12[C@H](CC[C@]1(O)[C@@H]1CC[C@@H]3C[C@@H](O)CC[C@]3(C)[C@H]1C[C@H]2O)C1=CC(=O)OC1 </td><td>0.936\n</td></tr><tr><td>C[C@]12CC[C@H]3[C@@H](CCC4=CC(=O)CC[C@]34C)[C@@H]1CC[C@@H]2C(=O)CO </td><td>0.932\n</td></tr><tr><td>CC(C)(C)C(=O)OCC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C </td><td>0.931\n</td></tr><tr><td>C[C@]12CC(=O)C3C(CCC4=CC(=O)CC[C@]34C)C1CC[C@]2(O)C(=O)CO </td><td>0.924\n</td></tr><tr><td>CC(=O)O[C@@]1(CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C)C(C)=O </td><td>0.924\n</td></tr><tr><td>CC(=O)OCC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C </td><td>0.920\n</td></tr><tr><td>C\\C=C1/C(=O)C[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C </td><td>0.919\n</td></tr><tr><td>C[C@]12CC[C@H]3[C@@H](CC[C@@H]4C[C@@H](O)CC[C@]34C)[C@@]1(O)CC[C@@H]2C1=CC(=O)OC1 </td><td>0.918\n</td></tr><tr><td>CCCC(=O)O[C@@]1(CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3[C@@H](O)C[C@]12C)C(=O)CO </td><td>0.902\n</td></tr><tr><td>CCC(=O)O[C@@]1(CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]12C)C(=O)CO </td><td>0.894\n</td></tr><tr><td>CC(=O)[C@@]12OC(C)(O[C@@H]1C[C@H]1[C@@H]3CCC4=CC(=O)CC[C@]4(C)[C@H]3CC[C@]21C)c1ccccc1 </td><td>0.880\n</td></tr><tr><td>CC(=O)OCC(=O)[C@@]1(O)CCC2C3CCC4=CC(=O)CC[C@]4(C)C3[C@@H](O)C[C@]12C </td><td>0.877\n</td></tr><tr><td>CC(=O)[C@@]1(O)CCC2C3CCC4=CC(=O)CC[C@]4(C)C3CC[C@]12C </td><td>0.876\n</td></tr><tr><td>C[C@]12CCC3C(CCC4=CC(=O)CC[C@]34C)C1CC[C@@H]2O </td><td>0.872\n</td></tr></table></body></html>"}}, "back_matter": []}
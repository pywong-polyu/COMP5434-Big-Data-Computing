{
    "paper_id": "1d08f2054d00d18d8f26dc18f93685f5953b268f",
    "metadata": {
        "title": "An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ",
        "authors": [
            {
                "first": "Matthew",
                "middle": [],
                "last": "Montebello",
                "suffix": "",
                "affiliation": {},
                "email": "matthew.montebello@um.edu"
            },
            {
                "first": "",
                "middle": [],
                "last": "Mt",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Dylan",
                "middle": [],
                "last": "Seychell",
                "suffix": "",
                "affiliation": {},
                "email": "dylan.seychell@um.edu"
            },
            {
                "first": "Michele",
                "middle": [
                    "La"
                ],
                "last": "Ferla",
                "suffix": "",
                "affiliation": {},
                "email": "michele.laferla.05@um.edu.mt"
            }
        ]
    },
    "abstract": [
        {
            "text": "During the last decade or so, there has been an insurgence in the deep learning community to solve health-related issues, particularly breast cancer. Following the Camelyon-16 challenge in 2016, several researchers have dedicated their time to build Convolutional Neural Networks (CNNs) to help radiologists and other clinicians diagnose breast cancer. In particular, there has been an emphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage breast cancer. Large companies have given their fair share of research into this subject, among these Google Deepmind who developed a model in 2020 that has proven to be better than radiologists themselves to diagnose breast cancer correctly.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Although the above proves interesting, very few hospitals have embraced these models and started using their diagnostic procedures. As part of our research, we asked the local medical community of surgeons and radiologists their opinion on such. We found that among the issues which exist, there is the need for an explanatory system which goes through the hidden layers of a CNN to highlight those pixels that contributed to the classification of a mammogram.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "We then chose an open-source, reasonably successful project developed by Prof. Shen [1], using the CBIS-DDSM image database to run our experiments on. Initially, this project was submitted to the DREAM2016 Digital Mammography challenge using the YaroslavNet patch-classifier. However, it was later improved using the Resnet-50 and VGG-16 patch-classifiers, analytically comparing the outcome of both. The results showed that the Resnet-50 one converged earlier in the experiments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Following the research by Montavon and Binder [2], [3], we used the DeepTaylor Layer-wise Relevance Propagation (LRP) model to highlight those pixels and regions within a mammogram which contribute most to its classification. This is represented as a map of those pixels in the original image, which contribute to the diagnosis and the extent to which they contribute to the final classification. The most significant advantage of this algorithm is that it performs exceptionally well with the Resnet-50 patch classifier architecture.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "According to the WHO reports, breast cancer remains the most typical cancer among women globally. Therefore, it is not surprising that many researchers have focused substantial time and resources to construct models that can assist radiologists in the correct and early diagnosis of this disease. [4] Work on deep learning models to assist in the prediction and diagnosis of breast cancer started in the 2010's but culminated during the Camelyon16 challenge between October 2015 and November 2016. This online challenge brought together leading research institutes and large companies, all of whom tried their best to build a deep learning system that was able to help clinicians in their field of expertise. [5] , [6] The Google Deepmind team emerged at the forefront of the challenge and in the following years, improved on their initial architecture to identify traits of breast cancer even through mammographic images, thus helping clinicians even in the first stage of the diagnosis.",
            "cite_spans": [
                {
                    "start": 297,
                    "end": 300,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 709,
                    "end": 712,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 715,
                    "end": 718,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "This research aims to build a proof of concept that will allow researchers from both the Artificial Intelligence and Medical fields to prove that an assistive model can help diagnose breast cancer through mammograms at the earliest possible stage from a scientific perspective. The research also gives a basic understanding of how the classification by the model took place through the Deep Taylor LRP methodology. This will be achieved through the following objectives:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. AIMS AND OBJECTIVES"
        },
        {
            "text": "\u2022 Collect and analyse information about the general attitude of clinicians who specialise in breast cancer in the use of artificial intelligence to assist them in the detection of DCIS. \u2022 Research into the functionality within the hidden layers of existent CNN models on how they predict whether a mammogram contains radiologically detectable foci of DCIS or not. We will give particular attention to Resnet-50 networks for this task. The reason for this is that Resnet-50 models tend to perform better than others when detecting breast cancer. \u2022 Employ the use of XAI, in particular the Deep Taylor Decomposition methodology to generate a rational justification of how images are processed through the neural network's different hidden layers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. AIMS AND OBJECTIVES"
        },
        {
            "text": "\u2022 Compare the model's performance proposed in this research to other similar existent models within the breast cancer and XAI fields of study.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. AIMS AND OBJECTIVES"
        },
        {
            "text": "We chose the end2end-all-conv open-source system developed by Prof. Li Shen to train the model so that it learns to differentiate between normal, benign or malignant mammographic breast images. The model uses the CBIS-DDSM dataset which consists of 2,620 publicly available images. The dataset is already equipped with relevant annotations and specified Regions of Interest. We fed these images into a CNN, which classified the images accordingly. We split the dataset of mammographic images into a 60 -40 K-fold cross-validation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. The chosen CNN model"
        },
        {
            "text": "B. The use of XAI AI may find considerable use in the healthcare industry's applications; however, its use remains difficult even in the modern worlds. It falls short of its true potential in many highrisk applications such as early-stage breast cancer detection. There is an expectation of understanding the logic behind the AI model's outcome, especially when dealing with vital issues involving a patient's health and clinical management. Hence, the need to use XAI to achieve insight into how the classification of the model occurs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. The chosen CNN model"
        },
        {
            "text": "For this research project, the Deep Taylor Decomposition Layer-wise Relevance Propagation (LRP) developed by A. Binder and G. Montavon et al. [2] , [3] will be used. The presented approach is based on first (or higher) order Deep Taylor expansion as an activation function. The Deep Taylor based approach was used for decomposing ReLU neurons by exploiting their local linearity. [7] Through this algorithm, the quality of the heatmaps in the source image is measured by perturbing the highest pixels first and computing the Area Under Curve (AUC). Lower AUC over a substantial number of source images would better identify pixel relevance by the heatmap. The Taylor method improves the heatmap AUC score. This shows its effectiveness for dealing with non-linear neuron layers.",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 145,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 148,
                    "end": 151,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 380,
                    "end": 383,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "A. The chosen CNN model"
        },
        {
            "text": "In the early 1990s, a text-based public dataset was made available on the Machine Learning Repository site containing a total of 699 instances of information from women who had undergone tests to verify if they had breast cancer. The dataset was curated by the University of Wisconsin hospital in the US. It had data for women tested between 1989 and 1992. [8] The dataset was profoundly used by several projects over the next couple of years, using its features in an attempt to predict if a person who showed similar symptoms had a higher possibility to have breast cancer. The dataset has ten attributes and is to date referenced in over 670,000 projects since it was donated. The problem with the Wisconsin dataset is that apart from having a limited number of instances and attributes, these were text-based. Although it was an excellent start for different researchers who wanted to experiment with this dataset, the results obtained could never be presented to clinicians or medical professionals to use in the actual detection of breast cancer.",
            "cite_spans": [
                {
                    "start": 357,
                    "end": 360,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "A. Early Works"
        },
        {
            "text": "In a typical environment, activation maximisation can be used as part of an analysis framework that searches for a concurrent pattern in the input images to produce a maximum model response for a specific quantity of interest. [9] , [10] The class probabilities modelled in the neural network are functions implemented using a well-known AI technique, gradient ascent. This technique aims at maximising an objective function and does the opposite of gradient descent, which on the other hand, aims to minimise an objective function. When applied to image classification, this formula allows to mostly take images, with only a few edges and coloured patterns at strategic locations of the image. This would be very beneficial for mammogram images used in detecting breast cancer at its earliest stage. [2] , [11] , [12] Fig. 1. The above are 4 cases illustrating how the \"expert\" p(x) affects the prototype p* found through the activation maximisation technique. The horizontal axis represents the input space, and the vertical axis represents the probability. In the example given above, the choice of \"expert\" d is over-fitted. [2] The choice of expert d in figure 1 must be avoided to resolve the problem of over-fitting the images in the input. This would skew the results of the ultimate output in the subsequent layers and final output of the neural network. Over-fitting could also result in hidden failure modes of the model p(x). Based on the figure above, an expert who is somewhere between choices a and b can already be sufficient to classify the mammogram images inputted. This is also true because, on the other hand, an under fitted expert would expose optima of p(x) potentially distant from the data. Therefore, the prototype x * would not be truly representative of w c . Nguyen et al. proposed a prototype for w c by incorporating a generative model in the activation maximisation framework. [13] The idea of using an alternative class consisting of unsupervised models such as those in Generative Adversarial Networks was already researched by Goodfellow in 2014. [14] The training procedure proposed by Goodfellow is for the generative model G to maximise the probability of the discriminative model D making an error; therefore working as a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere; in the case where multi-layer perceptrons define G and D, the entire system can be trained with back-propagation. [13] The optimisation problem proposed by Nguyen is redefined as:",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 230,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 233,
                    "end": 237,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 801,
                    "end": 804,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 807,
                    "end": 811,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 814,
                    "end": 818,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1129,
                    "end": 1132,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1910,
                    "end": 1914,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 2083,
                    "end": 2087,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 2422,
                    "end": 2425,
                    "text": "1 2",
                    "ref_id": null
                },
                {
                    "start": 2552,
                    "end": 2556,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [
                {
                    "start": 1159,
                    "end": 1167,
                    "text": "figure 1",
                    "ref_id": null
                }
            ],
            "section": "B. Deep Learning"
        },
        {
            "text": "Nguyen had come up with a formula to build a prototype for w c by incorporating a generative model in the activation maximisation framework. In this formula, the first term is a composition of the newly introduced decoder and the original classifier. The second term is an l 2 -norm regulator in the code space. Once a solution z * to the optimization problem is found, the prototype for w c is achieved by decoding the solution being: x * = g(z * ). Mantovan gives an excellent explanation of the l 2 -norm regulator specifying that it can be understood in the context of image data as favouring grey images, such as those obtained in mammograms. The effect of the regulator in the code space can instead be understood as encouraging codes that have high probability. [2] C. Related Works An important contribution to breast cancer identification using screening mammography was that by Prof Li Shen [1] . His team developed a deep learning algorithm that can accurately detect breast cancer on screening mammograms using the so-called \"end-to-end\" training approach. This efficiently leverages training datasets with either complete clinical annotation or only the whole image's cancer status (label). Through this approach, lesion annotations are required only in the initial training stage, and subsequent stages require only image-level labels, hence eliminating the reliance on rarely available lesion annotations. [1] The importance of this project is that its source code is publicly available on Github and can be used for improvements by others. [15] Shen et al. use the CBIS-DDSM dataset to train their model. This is a freely available dataset on around 6,000 breast images from 2,620 scanned film mammography studies. [16] Assuming that we have an input patch X \u2208 IR p\u00d7q and a patch classifier which is a function f so that f (X) \u2208 IR c , where the function's output satisfies f (X) i \u2208 [0, 1] and c t=1 f (X) t = 1 and c is the number of classes of the patches. Here, c = 5 and the classes are: benign calcification, malignant calcification, benign mass, malignant mass and background for each patch from a mammogram. Assuming also that the input patch is extracted from an image M \u2208 IR r\u00d7s where p << r and q << s. If the function f represents a CNN, then f can be applied to M without changing the network parameters so that f (M ) \u2208 IR u \u00d7 v \u00d7 c, where u \u00bf 1 and v \u00bf 1 depend on the image size and the stride of the patch classifier. This is possible because of the weight sharing and locality properties of a CNN. [1] The function h then accepts whole images as inputs and produces label at the whole image level. Therefore it can be trained throughout all the internal layers of a CNN. This approach allows researchers to significantly reduce the requirements for Region OF Interest (ROI) annotations and has many applications in medical imaging in addition to just breast cancer.",
            "cite_spans": [
                {
                    "start": 769,
                    "end": 772,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 901,
                    "end": 904,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1421,
                    "end": 1424,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1731,
                    "end": 1735,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 2532,
                    "end": 2535,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "B. Deep Learning"
        },
        {
            "text": "The figure below shows that the best architecture for diagnosing breast cancer using the DDSM dataset was the Yaroslav model for the patch-network feature map (function f). Additional CNN top-layers were added at the end of the model (function g) to complete the image classifier (function h). ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Deep Learning"
        },
        {
            "text": "The need to explain how algorithms are used to solve nonmathematically related problems started to be explored in the 1980s, with several reasoning systems created to explain machine learning models used in several computer systems. The initial systems provided a logical explanation of how different models worked. Such models, for example, helped in Computer-based medical decision making to diagnose bacteria in a patient's bloodstream. Other applications of the earliest explainable AI models were in education [17] , [18] and engineering [19] . As deep learning concepts were introduced into artificial intelligence, it became even more critical to develop the explainable element, especially when these models were developed for patient diagnosis in the medical field because of the high risk involved. Researchers within the clinical expert systems who were creating neural networks for decision support systems have sought to provide dynamic explanations that allowed them to increase trust in such technologies on a practical level. [20] The increased fear of bias in several machine learning applications has led to a greater demand for these models' transparency. Therefore the need for a more explainable version of AI arose. [21] The term eXplainable AI (XAI) was coined in 2018, even though it has been developed since the 1970s; and nowadays tends to focus more on exploring the reasoning behind deep learning, GANs and other novel techniques. Since the way these systems work is by dissecting the input into sections and passing it through several layers, the latest XAI techniques are developed to read into these layers and roll back the process made by the model. This makes the process more interpretable and explainable to an outsider who knows little about deep learning. An example of such an XAI method is that of LRP, a technique for determining which features found in the input have the most significant contribution to the model's output. [22] , [23] Besides, a few researchers have also worked in XAI within the field of decision trees and Bayesian networks, which are more transparent in a way and easier to inspect. [24] In their paper published in 2018, Adadi and Berrada have deduced some general cases where explanations may be needed for inferences from complex machine learning methods such as convolutional neural networks and other deep learning techniques. They give four general cases when explainability is needed; explain, justify, explain to control, and explain to improve and explain to discover. [25] For our research purposes, explainability is needed to justify why the model has classified mammograms as normal, benign or malignant. Explaining to justify is typically for decisions or inferences that need to comply with legislation or regulation. However, the XAI element may also be used to improve the explanations about the relationships between inputs and outputs to improve the model and a technique that uses explanations to obtain new information or perform ulterior diagnosis on the input image. The detection of features in the input image and the different layers of the CNN is one of the most common methods used to explain the decision-making process made by the model. Adadi and Berrada provide a list of learners designed to be explainable such as Bayesian Rule Lists and Sparse Linear Models.",
            "cite_spans": [
                {
                    "start": 515,
                    "end": 519,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 522,
                    "end": 526,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 543,
                    "end": 547,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1042,
                    "end": 1046,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1967,
                    "end": 1971,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 2147,
                    "end": 2151,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 2542,
                    "end": 2546,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "D. Developments in XAI"
        },
        {
            "text": "One final approach to consider was using Layer-wise Relevance Propagation, which works backwards to redistribute the input image back to all the inputs. The redistribution process is simple from layer to layer using the formula below:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Developments in XAI"
        },
        {
            "text": "The formula above basically weights the relevances based on neuron activation and weight connection. x j represents the activation value for the neuron j in a given layer. In contrast, w ( j, k) is the weighting of the connection between neuron j in the given layer and neuron k in the given layer + 1 coming after it. R j is the relevance score for each neuron in the first given layer, and R k is the relevance score for each neuron in the first given layer + 1. [3] IV. METHODOLOGIES",
            "cite_spans": [
                {
                    "start": 465,
                    "end": 468,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "D. Developments in XAI"
        },
        {
            "text": "The rapid development of deep learning has spurred much interest in its application to medical imaging problems, partic-ularly in detecting early-stage breast cancer. The most remarkable step taken is the shift from the reading of MRI images to that of mammogram ones. This meant that AI solutions were not only researched using MRI images taken during biopsies but also at the first stage of clinical examinations. Several initiates are taken in different countries to encourage women who have reached 55 to be screened, so that breast cancer at the first stage can be detected. Locally, in particular, health authorities do take advantage of the Pink October tradition to encourage screening.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Developments in XAI"
        },
        {
            "text": "The CBIS-DDSM dataset had its beginnings in 2016. Its curative selection of mammographic images was published in a paper [26] following a series of instances where CAD images lacked correct maintenance and updating. The CBIS-DDSM dataset is curated by the Cancer Imaging Archive and is a subset of the DDSM Breast Imaging Subset, first published in 1997. It is updated from time to time from the standardised version of the DDSM and is maintained by The Cancer Imaging Archive (TCIA). [27] The image data for this collection is structured such that each participant has multiple patient IDs, hence making it appear as though there are 6,671 patients according to the DICOM metadata. However, there are only 1,566 actual participants in the cohort. [28] The final dataset has a size of 163.6 GB, containing mammograms from 1,566 patients over 6,775 cases studied. The curated dataset is also available on popular dataset websites such as Kaggle or Tensorflow Datasets Catalogs for researchers to create and improve upon it.",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 125,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 485,
                    "end": 489,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 748,
                    "end": 752,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "A. CBIS-DDSM Dataset"
        },
        {
            "text": "A CNN usually works best when fed with an input that is then processed through different layers; each performs one single mathematical function and then finally a classifier, which determines the output of the image. Max pooling layers are often used amid the different layers to improve translational invariance and reduce feature map size. In this study, we focused mainly on the residual network (Resnet). [29] Consecutive network layers can be naturally grouped into smaller blocks so that the feature map size is reduced at both ends of the blocks remaining the same elsewhere. A Resnet-50 block uses a stride value of two in the first convolutional layer instead of 2 \u00d7 2 max-pooling to reduce feature map size at the beginning of the block, followed by the stacking of several convolutional layers. We used the bottleneck design for this model, which consists of repeated units of three convolutional layers with filter sizes of 1 \u00d7 1, 3 \u00d7 3 and 1 \u00d7 1, respectively. A vital feature of the Resnet block is that a shortcut is made between the two ends of each unit. The features are directly carried over, and therefore each unit can focus on learning the residual information. Batch normalisation is used in every convolutional layer in the Resnet patch-classifier, which is known to speed up the convergence and has a regularisation effect. A Resnet-50 block can be represented by the pattern of [L \u2212 M \u2212 N ] \u00d7 K, where L, M and N represent the depths of the three convolutional layers in a unit and K represents the number of units. For the experiments made on the CBIS-DDSM dataset used for this research, the Resnet-50 model was built, meaning that 50 layers were used in our CNN before the output was determined.",
            "cite_spans": [
                {
                    "start": 409,
                    "end": 413,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "B. Resnet-50 CNN"
        },
        {
            "text": "To construct a whole image classifier from a patch classifier, we flattened the heatmap and connected it to the image's classification output using fully connected layers. A maxpooling layer is used after the heatmap to increase the model's translational invariance to the patch classifier's output. Furthermore, a shortcut is made between the heatmap and the output to make the training easier. The heatmap results directly from the patch classifier's output which uses the softmax activation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Resnet-50 CNN"
        },
        {
            "text": "The data is divided into train and test sets using a 0.85-0.15 split on patients. Within the training set, 10% of the patients were used as validation. For each image, 100 background patches were generated with no overlap with the region of interest (ROI). For each ROI, 100 patches were generated with an overlap of at least 50% within the ROI. This process generated about a half-million image patches for training and validating the patch classifier. The input to the residual net had a size of 3x256x256 with the three channels representing red, green and blue colours in the CBIS-DDSM pre-trained model. A patch from a mammogram contains only one channel, so each patch is replicated onto the three colour channels. Since the ImageNET data has 1000 classes, the last output layer, which contains the softmax algorithm, is removed and replaced with a three-class softmax layer for the normal, benign and malignant classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Resnet-50 CNN"
        },
        {
            "text": "The Nadam algorithm from Keras was used as the optimiser with the default parameter setting. We trained the model on a local machine with a single NVIDIA Quadro M4000 GPU, setting the batch size to 32. We also adjusted the class weights within each batch to keep a class balance. Data augmentation was done by flipping an image both horizontally and vertically, rotating up to 45 degrees and shearing up to 1/8 * P i. The trained patch classifier achieved an accuracy of 0.84 on the validation set and 0.82 on the test set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Resnet-50 CNN"
        },
        {
            "text": "Following the research by Montavon and Binder [2] , [3] , it was found that the LRP is, to date, one of the most successful methods to explain predictions in CNNs, especially when it comes to reading Resnet-50 patch-classifiers. The purpose of LRP is to provide a classification explanation to work done by the inner layers of any neural network, which ultimately classifies the output. The explanation given by LRP is represented as a map of those pixels in the original image, which contribute to the diagnosis. Their colour determines the extent to which they contribute towards the final classification. The most significant advantage of this method is that it does not interact with the network's training, so we could easily apply it to already trained classifiers of our dataset. Based on another study, LRP could in the coming future enable clinicians to use mammograms in an attempt to verify if patients are at a greater risk to have other health problems related to cardiovascular disease or other cancers in different body parts. Researchers are eager to apply the models to other diseases and ailments, and especially those with less effective risk models, like pancreatic cancer. [30] The function of LRP is to use the network weights and the neural activations created by the forward-pass to backpropagate the output back through the network up until the input layer. There, we can visualise which pixels contributed to the classification at each step of the CNN; and ultimately to the final diagnosis. That way, when clinicians study the propagation, they would understand how the model worked, evaluate its intuition and maybe even improve it. In terms of LRP, we call the contribution magnitude of each pixel or intermediate neuron the relevance value and is denominated as R.",
            "cite_spans": [
                {
                    "start": 46,
                    "end": 49,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 52,
                    "end": 55,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1194,
                    "end": 1198,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "C. Layer-Wise Relevance Propagation"
        },
        {
            "text": "Since its introduction into deep learning in 2017, several improvements were made both by the authors and even by other researchers. [2] , [10] , [31] The critical part is to leverage efficient frameworks, such as PyTorch and Tensorflow, to do the backward pass for us. In our implementation of the four layers within an LRP-0 rule, we used the iNNvestigate library on Github and incorporated it into our project. [32] This library provides a standard interface and out-of-the-box implementation for many analysis methods, one of which being the Deep Taylor LRP decomposition. The developers' goal is to make analysing neural networks' predictions easier for researchers.",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 136,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 139,
                    "end": 143,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 146,
                    "end": 150,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 414,
                    "end": 418,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "C. Layer-Wise Relevance Propagation"
        },
        {
            "text": "The iNNvestigate library is based on Keras and therefore requires a supported Keras-backend. Currently, only the Tensorflow backend is supported. We tested our implementation with Python 3.6, Tensorflow 1.12 and Cuda 9.x. It applies the LRP-0 algorithm to analyse the model based on the gradient times the input formula. This formula holds only for ReLU networks, for which LRP-0 collapses into the stated formula.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Layer-Wise Relevance Propagation"
        },
        {
            "text": "Based on the finding published in the CBIS-DDSM dataset and the fact that LRP is not used to back-propagate the VGG-16 architecture, we decided to focus on the Resnet-50, paving the way to inject the element of XAI onto the CNN. In the original design of the Resnet-50 as proposed by Hu et al., L \u2261 M, N is four times L, and K is three or more; the L of the current block is also double of the L of the previous block. [33] However, this design was found to exceed the available GPU memory limit when used for the top layers of the whole image classifier. The whole image classifier trained using the S10 set (mean AUC = 0.85) performed much better than trained using the S1 set (mean AUC = 0.63), despite its insufficient patch classification accuracy.",
            "cite_spans": [
                {
                    "start": 419,
                    "end": 423,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "V. RESULTS"
        },
        {
            "text": "The S10 dataset contains more information about the ROIs and their adjacent regions and other background regions on the image than the S1 dataset. This allows a patch classifier to extract more features that can be important for whole image classification. For the rest of the study, only patch classifiers trained on the S10 dataset were used. Varying the configuration by using two Resnet-50 blocks of [512 -512 -1024] \u00d7 2 yielded a mean AUC of 0.86 while reducing the depths and K of the two Resnet-50 blocks to [256 -256 -256] \u00d7 1 and [128 -128 -128] \u00d7 1 did not significantly decrease the AUC. This result showed that the depths of the Resnet-50 blocks were relatively uncorrelated with the performance of the whole image classifiers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. RESULTS"
        },
        {
            "text": "After analysing the CNN model's performance using both Resnet-50 and VGG-16 patch-classifiers and concluded that the former converges earlier, we will now move on to perform experiments with LRP to analyse how the multiple pixels within a mammogram have contributed to classifying it. In the previous chapter, we have already asserted that the LRP algorithm works perfectly well when used in back-propagation in CNN models. Gregoire Montavon has proved this to be true in a paper he wrote in 2017. [7] Since then, LRP has been a catalyst in deep learning models to improve the element of explainability and move forward the agenda proposed by XAI of increasing the element of trust in AI and introducing a well researched peer-reviewed model to the industry. LRP has been used multiple times to explain how CNN models detect pulmonary issues in a person's lungs to help detect the COVID-19 virus and concerning Alzheimer's disease. [34] - [36] Nonetheless, it has never been used to assist in the detection of breast cancer, hence the importance of this research study to explain how a Resnet-50 patch-classifier model can explain the classification and diagnosis of a benign, malignant or normal breast.",
            "cite_spans": [
                {
                    "start": 498,
                    "end": 501,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 932,
                    "end": 936,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 939,
                    "end": 943,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "V. RESULTS"
        },
        {
            "text": "The iNNvestigate library on Github addresses the lack of a reference implementation in several methods, which have been recently proposed by providing a familiar interface and out-of-the-box implementation for many analysis methods. Among the methods implemented in this library is the LRP one. [32] The library is based on Keras and therefore requires a supported Keras-backend such as Tensorflow, the one we used. All the available methods have in common that they try to analyse the output of a specific neuron concerning input to the neural network. Typically one analyses the neuron with the most significant activation in the output layer. For example, given a Keras model, one can create a 'gradient' analyser. In our case, we used the Deep Taylor for ReLU-networks with unbounded input by implementing the Deep Taylor algorithm for neural networks with ReLU activation with either bound or unbounded input ranges. The Deep Taylor method uses a Keras model as its input parameter, with the bounded method also taking the highest and lowest input range. For our experiments, we used the unbounded method, hence inputting only the CBIS-DDSM trained Keras model. The core idea of LRP is to compute a relevance score for each input pixel layer by layer in the backward direction. It first forward-passes the image to collect activation maps and backpropagates the error taking into account the network weights and activations.",
            "cite_spans": [
                {
                    "start": 295,
                    "end": 299,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "V. RESULTS"
        },
        {
            "text": "The Deep Taylor method identifies the contribution of input features as the first order of a Taylor expansion through Taylor decomposition. It can estimate the attribution of each neuron one by one. In the classification setting, a saliency map estimates how much each pixel contributes to the class prediction. In the regression setting, the saliency map will estimate how much each pixel is impacting the model and contribute to decreasing the prediction error, as measured by the loss function. Fig. 4 . We used the iNNvestigate library LRP to highlight those areas which have contributed to the classification of a mammogram as having a normal, benign or malignant breast. Fig. 5 . The mammogram above is that of benign breast cancer, one that will need to be monitored but not necessarily operated on. The heatmap visualisation correctly classified this mammogram as containing a benign breast cancer, having only a few small purple spots in it. Fig. 6 . The heatmap visualisation used to identify the areas with the greatest contribution to the classification did not have any purple areas in the image above, hence correctly classifying it as that of a normal non-cancerous breast mammogram.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 498,
                    "end": 504,
                    "text": "Fig. 4",
                    "ref_id": null
                },
                {
                    "start": 677,
                    "end": 683,
                    "text": "Fig. 5",
                    "ref_id": null
                },
                {
                    "start": 951,
                    "end": 957,
                    "text": "Fig. 6",
                    "ref_id": null
                }
            ],
            "section": "V. RESULTS"
        },
        {
            "text": "When comparing the example in fig. 4 ; which is that of malignant breast, to the ones in figs. 5 and 6, which are of a benign and routine breast mammogram, respectively, the blue areas in figures 5 and 6 are smaller than the ones in fig. 4 . Notably, in fig. 6 , the image was unable to detect DCIS since the CNN model indeed classified this as a normal breast with no signs of tumorous cells.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 30,
                    "end": 36,
                    "text": "fig. 4",
                    "ref_id": null
                },
                {
                    "start": 233,
                    "end": 239,
                    "text": "fig. 4",
                    "ref_id": null
                },
                {
                    "start": 254,
                    "end": 260,
                    "text": "fig. 6",
                    "ref_id": null
                }
            ],
            "section": "V. RESULTS"
        },
        {
            "text": "VI. EVALUATION Through the Deep Taylor LRP method, we managed to achieve a proof of concept that highlights the crucial regions within a mammogram and defines how they have contributed to the classification of that image. Our implementation of this method highlighted areas with a high possibility of being affected by DCIS in purple, emphasizing a high density of abnormal tissue in the region. If presented with such an explanation, clinicians can better interpret the decisions taken by the CNN model and improve it. The problem here is that Deep Taylor has not performed that well previously in clinical adaptations. That is maybe why there are still no models that can explain the classification made by a CNN.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. RESULTS"
        },
        {
            "text": "Deep Taylor decomposition performed less well in our study, hinting at a disagreement between their rating-based evaluation and our segmentation-based evaluation. In a recent study [37] , an instance of Inception-V3 was trained to predict the presence of several diseases from OCT images. Then, three experts graded saliency maps for their decisions on a scale between 0 and 5 according to their clinical relevance. According to the subjective expert rating, Deep Taylor decomposition and Guided Backprop produced the most relevant saliency maps. Deep Taylor decomposition provided slightly better visualizations than Guided Backprop due to clinically coherent explanations, better coverage of pathology, and lack of high-frequency noise. [38] This paper was only published in May 2021 and still needs to be peer-reviewed; however, it does emphasize the usage of the Deep Taylor LRP method when used in clinical implementations. There is still room for improvement in the Deep Taylor method to be used throughout different adaptations within the medical environment. However, this study proves that it can be helpful in radiology for the detection of DCIS.",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 185,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 739,
                    "end": 743,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "V. RESULTS"
        },
        {
            "text": "VII. CONCLUSIONS Our research, motivated by the lack of implementations of such rigorous research made in the past decade through AI and deep learning, has taken us through a humbling journey to understand the medical and mathematical input into creating a model where both sciences can benefit. This study allowed us to research a solution that can increase trust in deep learning. It helped us understand better how mathematical algorithms can be beneficial in solving real-life health problems that affect us all and propose a solution that we could all ultimately profit from.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. RESULTS"
        },
        {
            "text": "This research contributes to medical science and the diagnosis of breast cancer in particular because both our approach and the clinical expertise can be used in parallel to help each domain improve. Clinicians can contribute to the improvement of the model through their expertise. In contrast, the model can help clinicians determine specific ROIs and patches within them, which might improve the diagnosis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Questionnaire Conclusions"
        },
        {
            "text": "The challenge here is proposing a legal framework that permits surgeons and radiologists to use such deep learning models in their daily work. Without such a framework, medical practitioners would be exposed to legal complaints by patients in case of false positives or false negatives. For this reason, we propose that a legal framework is set up consisting of professionals from different areas so that this issue can be addressed. We believe that this, together with an improved proof of concept of our idea, can help integrate AI systems into our hospitals.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Questionnaire Conclusions"
        },
        {
            "text": "Our study demonstrates that deep learning models trained in an end-to-end fashion can be highly accurate. Deep learning methods have enormous potential to further improve breast cancer detection accuracy on screening mammography as the available training datasets and computational resources expand.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. End2end-all-conv Conclusions"
        },
        {
            "text": "This is especially true if we had access to a larger dataset than the CBIS-DDSM one, which contains a limited amount of just under 7,000 mammograms to test and train models. The challenge here is to gather more mammograms since not many patients are willing to provide researchers with their medical images and data, even if this is anonymised. Further to that, one would still need to go through the mammograms and curate them to identify the ROIs and other medical annotations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. End2end-all-conv Conclusions"
        },
        {
            "text": "We have discussed in detail the contribution of LRP and the Deep Taylor method to reverse-engineer a Resnet-50 patch-classifier CNN. Theoretically speaking, this is a perfect solution to the challenge being addressed in this thesis because it meets all the necessary criteria and functions within Li Shen's End2end-all-conv model. This has been proved through the experiments we have conducted and, if improved, will be helpful to clinicians and researchers within the field of AI alike; however, it does have its limitations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Deep Taylor LRP Conclusions"
        },
        {
            "text": "As Ayhan et al. discuss in the paper, they wrote in May 2021 that Guided Backprop methods performed better than Deep Taylor since they generated saliency maps explaining decisions taken by Deep Learning models in a better way. They believe that Guided Backdrop should be further studied to understand its distinct behaviours when explaining these classifications on medical images. Moreover, its restriction by design to ReLU networks (see Methods) should be relaxed to extend its applicability to new architectures beyond ReLUbased designs. [38] Nonetheless, the Guided Backdrop method was never used as a saliency map to reverse engineer mammograms used to interpret breast cancer. So one still needs to determine in a future study whether Guided Backdrop can find those pixels mammogram which contributes most to its final classification of benign, malignant or normal breast tissue. One has also to prove whether this method works well with the Resnet-50 patch-classifier models or whether one should use a different model to build a CNN that can effectively diagnose breast cancer.",
            "cite_spans": [
                {
                    "start": 542,
                    "end": 546,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "C. Deep Taylor LRP Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Deep learning to improve breast cancer detection on screening mammography",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Margolies",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Rothstein",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fluder",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mcbride",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Sieh",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Scientific Reports",
            "volume": "9",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Methods for interpreting and understanding deep neural networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Montavon",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Samek",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "R"
                    ],
                    "last": "M\u00fcller",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Digital Signal Processing",
            "volume": "73",
            "issn": "",
            "pages": "1--15",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Layer-wise relevance propagation for neural networks with local renormalization layers",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Binder",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Montavon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lapuschkin",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "R"
                    ],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Samek",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "25th International Conference on Artificial Neural Networks and Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "63--71",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Incidence of female breast cancer per 100,000",
            "authors": [],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Home -grand challenge",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Litjens",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "B"
                    ],
                    "last": "Bejnordi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Veta",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Van Diest",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Van Ginneken",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Karssemeijer",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Litjens",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A W"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "JAMA",
            "volume": "318",
            "issn": "22",
            "pages": "2199--2210",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Explaining nonlinear classification decisions with deep taylor decomposition",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Montavon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lapuschkin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Binder",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Samek",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Pattern Recognition",
            "volume": "65",
            "issn": "",
            "pages": "211--222",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Cancer diagnosis and prognosis via linear-programmingbased machine learning",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "N"
                    ],
                    "last": "Street",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "On the analysis and interpretation of inhomogeneous quadratic forms as receptive fields",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Berkes",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wiskott",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Neural Computation",
            "volume": "18",
            "issn": "8",
            "pages": "1868--1895",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Very deep convolutional networks for large-scale image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Online convex programming and generalized infinitesimal gradient ascent",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zinkevich",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ser. ICML'03",
            "volume": "",
            "issn": "",
            "pages": "928--935",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Pixel recurrent neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oord",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kalchbrenner",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kavukcuoglu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dosovitskiy",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yosinski",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Clune",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "30th Conference on Neural Information Processing Systems (NIPS 2016)",
            "volume": "",
            "issn": "",
            "pages": "3395--3403",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Generative adversarial nets",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pouget-Abadie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mirza",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Warde-Farley",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ozair",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Courville",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "27",
            "issn": "",
            "pages": "2672--2680",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "lishen/end2end-all-conv",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "University of south florida digital mammography home page",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Rose",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Turi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Williams",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Wolstencroft",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Taylor",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Knowledge-based tutoring: The GUIDON program",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "J"
                    ],
                    "last": "Clancey",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Protos: an exemplar-based learning apprentice",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "R"
                    ],
                    "last": "Bareiss",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "W"
                    ],
                    "last": "Porter",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Wier",
                    "suffix": ""
                }
            ],
            "year": 1988,
            "venue": "International Journal of Man-Machine Studies",
            "volume": "29",
            "issn": "5",
            "pages": "549--561",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Pedagogical, natural language, and knowledge engineering techniques in SOPHIE-I, II and III",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Burton",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "De Kleer",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "Peabody Journal of Education",
            "volume": "",
            "issn": "",
            "pages": "1--54",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Slave to the algorithm? why a right to explanationn is probably not the remedy you are looking for",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Edwards",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Veale",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "SSRN Electronic Journal",
            "volume": "16",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Computer says no: why making AIs fair, accountable and transparent is crucial",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sample",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Understanding neural networks with layerwise relevance propagation and deep taylor series",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shiebler",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lapuschkin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Binder",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Montavon",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Klauschen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "R"
                    ],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Samek",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "PLoS ONE",
            "volume": "10",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "The Cambridge Handbook of Artificial Intelligence",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Bostrom",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Yudkowsky",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "316--334",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Adadi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Berrada",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Access",
            "volume": "6",
            "issn": "",
            "pages": "52--138",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "A curated mammography data set for use in computer-aided detection and diagnosis research",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Sawyer Lee",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Gimenez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hoogi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Miyake",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gorovoy",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rubin",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Scientific Data",
            "volume": "4",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "The cancer imaging archive (tcia): Maintaining and operating a public information repository",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Clark",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Vendt",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Smith",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Freymann",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kirby",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Koppel",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Moore",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Phillips",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Maffitt",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pringle",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Tarbox",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Prior",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Journal of digital imaging",
            "volume": "26",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Curated breast imaging subset of ddsm",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Sawyer Lee",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Gimenez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hoogi",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rubin",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Mammographic breast density assessment using deep learning: Clinical implementation",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Lehman",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Yala",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Schuster",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Dontchos",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bahl",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Swanson",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Barzilay",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Radiology",
            "volume": "290",
            "issn": "1",
            "pages": "52--58",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Layer-Wise Relevance Propagation: An Overview",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Montavon",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Binder",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lapuschkin",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Muller",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Samek",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "K"
                    ],
                    "last": "Hansen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "193--209",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "innvestigate neural networks!",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Alber",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lapuschkin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Seegerer",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "H\u00e4gele",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "T"
                    ],
                    "last": "Sch\u00fctt",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Montavon",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Samek",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "D\u00e4hne",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kindermans",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of Machine Learning Research",
            "volume": "20",
            "issn": "93",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Discriminative localization in cnns for weakly-supervised segmentation of pulmonary nodules",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Laine",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Angelini",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Lecture Notes in Computer Science",
            "volume": "",
            "issn": "",
            "pages": "568--576",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "A deep convolutional neural network for covid-19 detection using chest x-rays",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bassi",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Attux",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "04",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Testing the robustness of attribution methods for convolutional neural networks in mri-based alzheimer's disease classification",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Eitel",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ritter",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "geb. Hackmack)",
            "volume": "09",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "What is the optimal attribution method for explainable ophthalmic disease classification",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sengupta",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "R"
                    ],
                    "last": "Mohammed",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Faruq",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Jayakumar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zelek",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lakshminarayanan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Workshop on Ophthalmic Medical Image Analysis",
            "volume": "",
            "issn": "",
            "pages": "21--31",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Clinical validation of saliency maps for understanding deep neural networks in ophthalmology",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ayhan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "B"
                    ],
                    "last": "K\u00fcmmerle",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "K\u00fchlewein",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Inhoffen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Aliyeva",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ziemssen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Berens",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Converting a patch classifier to an end-to-end trainable whole image classifier using an all-convolutional design. The function f was first trained on patches and then refined on whole images.[1]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "A diagram showing how the Deep Taylor method works in LRP together with the formulas implemented.",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}
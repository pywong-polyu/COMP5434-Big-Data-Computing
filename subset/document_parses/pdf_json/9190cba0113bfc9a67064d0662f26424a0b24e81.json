{
    "paper_id": "9190cba0113bfc9a67064d0662f26424a0b24e81",
    "metadata": {
        "title": "Using growth transform dynamical systems for spatio-temporal data sonification",
        "authors": [
            {
                "first": "Oindrila",
                "middle": [],
                "last": "Chatterjee",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Washington University in St. Louis",
                    "location": {
                        "postCode": "63130",
                        "region": "Missouri",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Shantanu",
                "middle": [],
                "last": "Chakrabartty",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Washington University in St. Louis",
                    "location": {
                        "postCode": "63130",
                        "region": "Missouri",
                        "country": "USA"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Sonification, or encoding information in meaningful audio signatures, has several advantages in augmenting or replacing traditional visualization methods for human-in-the-loop decision-making. Standard sonification methods reported in the literature involve either (i) using only a subset of the variables, or (ii) first solving a learning task on the data and then mapping the output to an audio waveform, which is utilized by the end-user to make a decision. This paper presents a novel framework for sonifying high-dimensional data using a complex growth transform dynamical system model where both the learning (or, more generally, optimization) and the sonification processes are integrated together. Our algorithm takes as input the data and optimization parameters underlying the learning or prediction task and combines it with the psychoacoustic parameters defined by the user. As a result, the proposed framework outputs binaural audio signatures that not only encode some statistical properties of the high-dimensional data but also reveal the underlying complexity of the optimization/learning process. Along with extensive experiments using synthetic datasets, we demonstrate the framework on sonifying Electro-encephalogram (EEG) data with the potential for detecting epileptic seizures in pediatric patients.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "With the multitude of high-dimensional data available today, the search for better techniques for perceptualizing data before applying it to a learning/predictive task has emerged as an important research area. Though visualization remains the modality of choice for most applications, sonification (representation of data using human-recognizable audio signatures [1] ) is gaining momentum as a complementary or alternative modality.",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 368,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "II. INTRODUCTION"
        },
        {
            "text": "Some of the advantages of data sonification, that make it an ideal candidate for augmenting or replacing visualization for analyzing the dataset's properties, are as follows [1] - [3] : 1) Temporal resolution for auditory perception is better than that for visual perception. This makes it suitable for perceptualizing time-varying data with complex patterns that visual displays might otherwise miss. 2) Audio is orientation-agnostic and has a wider spatial range since the user need not be oriented towards a particular direction. In contrast, for visualization, objects need to be within the field of vision of the user. 3) Humans typically respond faster to auditory feedback when compared to visual feedback, making sonification attractive for human-in-the-loop control applications. 4) In scenarios where the visual scene is crowded due to multiple displays or when attention is lacking, auditory signals can be used as a means of drawing the user's attention to a particular segment of the visual field. 5) Auditory perception provides a natural alternative to shrinking display sizes, especially for monitoring and alerting applications. 6) Auditory perception and auditory memory remain resilient to many neurodegenerative diseases, as a result sonification is an attractive choice for rehabilitation technologies. The most common sonification scheme used in literature involves mapping only a subset of the most important variables in the data directly within the audible range [4] , [5] . A second strategy involves using a machine learning algorithm to perform a classification task and then sonifying the output of the classifier [6] . While the first approach does not utilize the entire information available at hand, the second approach lacks a human-in-the-loop component that might be crucial for tasks involving monitoring and feedback. In this paper, we propose a novel sonification technique that incorporates both the learning task at hand, as well as psychoacoustic parameters defined by the end-user in the form of constraints placed on the task. The algorithm finally produces an audio signature that can be utilized by the end-user for identifying both spatial and temporal patterns in the dataset. The scope of the paper is illustrated in Fig 1. At the core of the approach is a variant of the complex growth transform dynamical system, proposed in our previous work [7] , [8] based on Baum-Eagon growth transforms [9] . Effectively, the highdimensional space encompassed by the learning task is projected into a network of interacting limit cycle oscillators that form a set of complex basis functions encoding the low-dimensional space of the audio signal. The main model along with its properties is presented in Section IV, while Section IV-A shows the effects of the learning task on the sonified signal. Section IV-B presents the effects of the psychoacoustic parameters and sonification strategies that can be adopted for the sonification module. Section V presents the application of the sonification framework on synthetic datasets to highlight the role of different parameters and sonification strategies in the process. Section VI, finally, shows how our model can be applied to the CHB-MIT scalp EEG dataset [10] , [11] for the real-time detection of epileptic seizures.",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 177,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 180,
                    "end": 183,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1488,
                    "end": 1491,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1494,
                    "end": 1497,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1643,
                    "end": 1646,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 2394,
                    "end": 2397,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 2400,
                    "end": 2403,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 2442,
                    "end": 2445,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 3247,
                    "end": 3251,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 3254,
                    "end": 3258,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 2266,
                    "end": 2272,
                    "text": "Fig 1.",
                    "ref_id": null
                }
            ],
            "section": "II. INTRODUCTION"
        },
        {
            "text": "Cost function, gradients, constraints etc.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Psychoacoustics Sonification"
        },
        {
            "text": "Sampling frequency, bandwidth, maximum loudness etc.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Psychoacoustics Sonification"
        },
        {
            "text": "Sonified Signal Figure 1 : Proposed approach for data sonification. The proposed sonification module takes as input (i) a generic optimization/learning problem defined by a cost function, gradients, and constraints on one hand, and (ii) psychoacoustic parameters like sampling frequency etc. on the other. It then outputs a single or dual-channel audio waveform that encodes the underlying optimization problem.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 16,
                    "end": 24,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Data Psychoacoustics Sonification"
        },
        {
            "text": "In summary, the key contributions of the paper can be highlighted as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Psychoacoustics Sonification"
        },
        {
            "text": "1) The proposed method inherently maps high-dimensional data into a lower-dimensional single-channel or dualchannel audio signal. This is particularly suitable when dealing with low throughput systems having limited capacity/ bandwidth. 2) The method utilizes the whole spectrum of information available for a task and enables the end-user to arrive at a decision instead of relying on a machine learning model. 3) The algorithm automatically combines the learning and sonification stages into a single module. This is unlike the sonification-based decision-making algorithms existing in literature, that typically involve learning the decision parameters and then mapping the same to different sound parameters in subsequent stages. 4) The method is versatile and can be tailored to accommodate various learning models, problem dimensionality, and dataset sizes. Additionally, the complex growth transform dynamical system provides a wide range of tunable parameters which can be customized for different applications.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Psychoacoustics Sonification"
        },
        {
            "text": "III. BACKGROUND AND RELATED WORK 1) Visualization of High-Dimensional Data: Elementary visualization schemes use different visual attributes like color, shape, size, spatial location, etc., for representing information. However, almost all the standard visualization techniques typically use these attributes after mapping the data to a lower-dimensional (usually two or threedimensional) space. Some of the most commonly used mapping techniques include linear methods like PCA, LDA and their variants [12] that only preserve the global structure of the data. In contrast, nonlinear methods like Locally Linear Embedding, [13] , Isomap [14] , Laplacian EigenMap [15] , tSNE [16] , PHATE [17] etc, have been designed to preserve both global and local information.",
            "cite_spans": [
                {
                    "start": 502,
                    "end": 506,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 622,
                    "end": 626,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 636,
                    "end": 640,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 662,
                    "end": 666,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 674,
                    "end": 678,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 687,
                    "end": 691,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Data Psychoacoustics Sonification"
        },
        {
            "text": "However, these techniques are more suitable for time-invariant or static data, and analysis of time-varying, highdimensional data using these methods would involve frame-wise visualization in a lower-dimensional space. Several visualization methods based on variants of the techniques discussed earlier have been proposed, e.g., m-tSNE [18] . However, they lack interpretability and suffer from a similar limitation of being restricted to only three dimensions for representing the data. Moreover, though other visual attributes like color, size, shape, etc., can be used in conjunction with the three spatial dimensions, these may not be sufficient for complex datasets with very high dimensionality. This can thus potentially lead to information overload of the visual system, thereby pushing the users to the limits of comprehension. Additionally, complex, time-varying patterns in the data are sometimes difficult to capture using only the visual faculties. Hence, employing sonification might help users interpret the data more effectively or draw their attention to the most important aspects of the data that can then be monitored/analyzed by visual inspection [1] .",
            "cite_spans": [
                {
                    "start": 336,
                    "end": 340,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1168,
                    "end": 1171,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Data Psychoacoustics Sonification"
        },
        {
            "text": "2) Data Sonification and its Applications: Sonification, similar to its visual counterpart, provides several attributes that can be used for representing information, like pitch, loudness, timbre, rhythm, duration, harmonic content, etc. Broadly, two different sonification methods exist in the literature depending on how the data is mapped into an audio signal: (i) parameter mapping sonification and (ii) model-based sonification [1] . Of these, the parameter mapping method, where variables can be mapped into different attributes of the sound signal to create a sonified signature, is the most commonly used. Parameter mapping based sonification has been used in a wide range of applications, ranging from sonifying astronomical data like gravitational waves [19] and photons emitted by the Higgs bosons [20] , to synthesizing novel protein structures [21] , [22] and detecting anomalies in medical data (e.g., CT scans of Alzheimer's patients [23] , EEG [24] and ECG [4] signals, skin cancer detection [25] , epileptic seizure detection [5] ), detection of anomalous events [26] etc. Sonification based techniques have also been used for intrusion detection in networks [27] and network traffic flow [28] , analysis of stock market data [29] , and in therapeutic treatment of freezing of gait in Parkinson's patients [30] . More recently, sonification has been applied to the analysis of RNA sequences in different strains of the Covid19 virus [31] . Unlike the visualization schemes, sonification offers a much wider parameter space for the variables to be mapped into. For example, humans can perceptualize sound signals anywhere from 20 to 20kHz, and frequency differences as low as 3 Hz are easily discernible by the human auditory system [1] . In contrast, the model-based sonification technique involves using a virtual model whose sonic responses are altered according to the data provided. This might involve first passing the data through a machine learning or optimization module which learns the manifold the data resides in, and then mapping the output to different properties of the sound signal. While the parameter mapping based method is restricted in terms of the dimensionality of the data it can handle, the model-based approach uses the entire data available, but does not rely on the end user for the decision-making [1] .",
            "cite_spans": [
                {
                    "start": 433,
                    "end": 436,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 764,
                    "end": 768,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 809,
                    "end": 813,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 857,
                    "end": 861,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 864,
                    "end": 868,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 949,
                    "end": 953,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 960,
                    "end": 964,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 973,
                    "end": 976,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1008,
                    "end": 1012,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1043,
                    "end": 1046,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1080,
                    "end": 1084,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1176,
                    "end": 1180,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1206,
                    "end": 1210,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1243,
                    "end": 1247,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1450,
                    "end": 1454,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1749,
                    "end": 1752,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 2344,
                    "end": 2347,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Data Psychoacoustics Sonification"
        },
        {
            "text": "In this section, we introduce a novel framework for sonifying high-dimensional data using a variant of the complex domain dynamical system model proposed in our previous work [8] . Our method utilizes the entire information available at hand to perform a combination of feature extraction and dimensionality reduction. Finally, it allows the human user to arrive at a decision based on the output sound signature. The sonification algorithm has been summarized in Table 2 , while the proof has been outlined in VIII-B. The inputs to the sonification process are: (i) the learning parameters and data under consideration; and (ii) the psychoacoustic parameters defined by the user. We then develop a time-evolution operator U that depends on both the learning task and the psychoacoustics by incorporating the latter as additional constraints on the learning/optimization process. U is chosen to be a nonlinear unitary transformation that ensures that the total energy of the sonified signal remains conserved over time. This effect mimics an automatic gain control mechanism observed in bioacoustics [32] .",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 178,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [
                {
                    "start": 464,
                    "end": 471,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "IV. SONIFICATION FRAMEWORK USING COMPLEX GROWTH TRANSFORMS"
        },
        {
            "text": "Our proposed technique is thus a blend of both the parameter mapping-based and model-based sonification strategies. This is because the entire range of the data is mapped to different parameters of the output sound signal based on a dynamical system model. However, the end-user takes the decision, as illustrated in Fig 1. Within the framework, each variable or data point is mapped to a set of complex growth transform limit cycle oscillators globally coupled together by the conservation constraint on the signal energy. The oscillators can be thought of as complex basis functions defining the lower-dimensional space of the audio output to which we want to project our high-dimensional data, and are dictated by the psychoacoustic parameters. This is illustrated in Fig 2 for a group of K oscillators represented by complex oscillator variables or waveforms \u03c8 1 (t), . . . , \u03c8 K (t) having different frequencies and amplitudes. The variables involved are assigned both baseline and relative frequencies as outlined in Appendix VIII-B. As a result of the time evolution of the complex dynamical system, both the amplitudes and frequencies of the oscillators get modulated during the optimization process. For example, if all the variables get mapped to the same constant value of baseline and relative frequencies, each oscillator frequency drifts from this constant value during the transient phase, following which it again returns to the original trajectory. The frequency deviation during the transient phase should ideally encode the complexity of the optimization problem. The output sonified signal is thus a complex single or dual-channel waveform obtained by a superposition of all the oscillator waveforms, ie., \u03c8 sum (t) = ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 317,
                    "end": 323,
                    "text": "Fig 1.",
                    "ref_id": null
                },
                {
                    "start": 771,
                    "end": 780,
                    "text": "Fig 2 for",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "IV. SONIFICATION FRAMEWORK USING COMPLEX GROWTH TRANSFORMS"
        },
        {
            "text": "In this section, we will show how the learning problem affects the sonified signal. Any statistical learning task can be expressed in terms of an objective function H that we are trying to minimize over a constraint space. We will therefore investigate the role of the objective function on the sound signature. Considering default temporal constraints on the total signal energy, the complex growth transform-based sonification technique (please see Table 2 ) provides a natural framework for optimizing the cost function H subject to constraints defined by the psychoacoustics. Without loss of generality, in this section, we will focus only on the effect of H and keep the psychoacoustic parameters constant for all the experiments. In particular, we will consider here that (a) the total energy of the sonified signal remains conserved over time, and (b) all the oscillators or basis waveforms in the network have identical baseline frequencies for better visualization of the network dynamics. The sonified output signal for all cases, as stated earlier, is obtained by the superposition of the wave functions generated by all the oscillators, i.e., \u03a8 sum,n = K k=1 \u03a8 k,n . Figure 3 : Illustration of the superimposition of oscillators. Each multidimensional variable/data vector is mapped into an individual growth transform limit cycle oscillator. Local and global couplings lead to a constant phase difference between each pair of oscillators in a steady-state. This results in a sudden shift of the frequency of the superimposed waveform \u03c8 sum (t) from the baseline frequency during the transient state, following which the frequency is restored to its baseline value in the steady-state.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 451,
                    "end": 458,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1179,
                    "end": 1187,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "Algorithm 2: Sonification using complex growth transform dynamical system (Proof in Appendix VIII-B)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "\u2022 Learning task: Consider a statistical learning task defined by a cost function and gradients.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "\u2022 Psychoacoustics: Consider a set of complex basis functions \u03a8 = [\u03c8 1 , . . . , \u03c8 K ] \u2208 C K defined by the user using psychoacoustic parameters like sampling frequency F s, bandwidth (f 1 , f 2 ), frequency mapping strategy etc.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "\u2022 Sonification module: The growth transform-based sonification method produces an output signal of the following form in steady-state:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "where \u03c9 n and \u03be k,n are the instantaneous baseline and relative frequencies of the k th oscillator variable \u03c8 k,n \u2208 C at the n th time step respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "\u2022 Update rule: The corresponding instantaneous time evolution equation for the oscillators is given by the following equation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "where the mathematical form of U n is given in Appendix VIII-B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "\u2022 Conservation of signal power: The operator U n : C K \u2192 C K represents an instantaneous nonlinear unitary transformation that ensures that the total signal energy remains conserved over time and thus imposes a temporal constraint brought about by the psychoacoustics as well, i.e., K k=1 |\u03c8 k,n | 2 = \u03b3 \u2200n, \u03b3 > 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "Example 1: Consider the following one-dimensional quadratic optimization problem:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "We can map the above to the sonification framework in Table 2 by taking p = |\u03c8 1 | 2 \u2212|\u03c8 2 | 2 , where \u03c8 1 , \u03c8 2 \u2208 C are complex waveforms. (Please see Appendix VIII-B for a detailed outline of the mapping procedure.) The equivalent optimization problem in the complex domain thus becomes:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 54,
                    "end": 61,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "where \u03a8 = [\u03c8 1 , \u03c8 2 ]. For this simple problem, the expression for the unitary operator U using the procedure outlined in Table 2 (please see Appendix VIII-B for a detailed derivation) is given by: Figure 4 : Effect of \u03b3. Dynamics of growth transform-based complex dynamical system consisting of two oscillators for two different cases corresponding to problem H 1 : (a) both oscillators asymptotically reach their respective stable limit cycles in steady-state (\u03b3 = 1) and (b) one reaches a stable limit cycle, while the other goes to a stationary point (\u03b3 = 0.01).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 123,
                    "end": 130,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 199,
                    "end": 207,
                    "text": "Figure 4",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "Example 2: Consider a second quadratic optimization problem:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "and c \u2208 R M , Q \u2208 R M \u00d7M . a \u2208 R + is a parameter that controls the curvature of the optimization surface, and hence controls the rate of convergence to the optimal solution. Taking",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "we can again follow the mapping procedure in Appendix VIII-C. The equivalent optimization problem in the complex domain is given by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "where \u03a8 1 = {\u03c8 11 , . . . , \u03c8 i1 , . . . , \u03c8 M 1 } and \u03a8 2 = {\u03c8 12 , . . . , \u03c8 i2 , . . . , \u03c8 M 2 }. Again, defining the unitary operator U according to Appendix VIII-B, we have:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "The final sonified output is then given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "Time evolution and phase portrait of the sonified signal: Fig 5 shows the time evolutions and phase portrait for a one-dimensional problem (M = 1), with a = 1, Q = 1, c = 0.8 and \u03b3 = 2. The sampling and natural/ baseline frequencies were considered to be F s = 22kHz and \u03c9 n = 600Hz respectively, while the relative frequency was considered to be \u03be ik,n = \u03be k = 0 for all the oscillators. In the sonification framework, t = n\u2206t, where t seconds is the time duration corresponding to the n\u2212th time step, and each time step \u2206t = (t+\u2206t)\u2212\u2206t = (n+1)\u2206t\u2212n\u2206t = (1/F s)s. The simulation duration was 0.1s, with the optimization onset being at 0.03s for same initial amplitudes of \u03c8 1 and \u03c8 2 , and zero initial phase difference between the waveforms. Fig 5(a) shows the time evolutions for the waveforms \u03c8 11 = \u03c8 1 , \u03c8 12 = \u03c8 2 and \u03c8 sum , along with their zoomed-in views during the initial (I), transient (T) and steady-state (S) stages respectively. Fig 5(b) shows the phase portrait between the real parts of \u03c8 1 and \u03c8 2 . It can be seen that the two waveforms evolve over time from an initial state (I) of same amplitude and zero phase difference, through a transient phase (T) of varying amplitudes and phase difference, to a final steady state where \u03c8 1 and \u03c8 2 have constant values of amplitudes, and a constant phase difference between them. This indicates in a non-zero phase shift of the final trajectory of \u03c8 sum (t) from the initial trajectory.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 58,
                    "end": 63,
                    "text": "Fig 5",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 742,
                    "end": 750,
                    "text": "Fig 5(a)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 944,
                    "end": 952,
                    "text": "Fig 5(b)",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "Effect of H and \u03b3 on the sonified signal: Next, we investigate the performance of the model for different values of curvature of the optimization surface as shown in Fig 6, as well as different levels of total energy available to the system at any instant of time. during the same decreases with an increase in the value of a, (i.e., a steeper curve implies a faster optimization process and hence smaller values of frequency deviation); (ii) higher value of the total energy \u03b3 leads to shorter transient phases with smaller frequency drifts. Thus, the complexity of the optimization problem is encoded in the final phase shift of the output signal from its initial phase. Effect of cost function-induced frequency perturbations on the sonified signal: Next, we examine the effect of variations in the relative frequencies on the system's performance. Fig 8 shows the spectrogram and instantaneous frequency plots for different choices of time evolution of the relative frequency \u03be k (t) \u2200k, for the optimization problem in Example 2 and same parameter settings as used in the simulations in Fig 7. The baseline frequency was chosen to be \u03c9 n = 100 Hz for all the oscillators. Additionally, the total simulation duration and the onset of show similar results for the case when each relative frequency trajectory \u03be k is modulated by the corresponding value of \u03c3 k , where \u03c3 k \u2192 1 in steady state, i.e., \u03be k,n = \u03be\u03c3 k,n . This type of frequency modulation thus incorporates information about the optimization process in the output signal, and leads to faster convergence and larger frequency deviation compared to the output in Case 1. Case 3 (Amplitude decay and constant relative frequency): In this case, a varying amplitude term constant for each oscillator was added to the relative frequency term, and no frequency modulation was added. The multiplicative term in the update equation is thus replaced by exp(j\u03be k,n \u2206t) \u2190 exp((\u2212\u03c1 + j\u03be)\u2206t), where \u03c1 = 1 was chosen for the experiment. Figs 8(g)-(i) show the relative frequency modulations, spectrogram, and frequency deviation respectively for this case. Case 4 (Amplitude decay and frequency modulation): In the last set of experiments, both the amplitude and frequency of the relative frequency trajectories were modulated for all the oscillators, i.e., exp(j\u03be k,n \u2206t) \u2190 exp((\u2212\u03c1 + j\u03be\u03c3 k,n\u22121 )\u2206t). Figs 8(j)-(l) show the relative frequency modulations, spectrogram, and frequency deviation respectively for this case. In both Cases 3 and 4, the relative frequencies were applied only during the transient phase of the simulation, and set to zero both during the initial stage as well as after convergence to a steady-state. It can be seen that the frequency shift during the transient phase in Case 4 is significantly higher than all the other stages. Based on the above experiments, we can infer the following:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 166,
                    "end": 172,
                    "text": "Fig 6,",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 852,
                    "end": 863,
                    "text": "Fig 8 shows",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 1092,
                    "end": 1098,
                    "text": "Fig 7.",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "\u2022 The framework can encode information about the optimization problem's complexity and the total energy available to the network. \u2022 Different encoding strategies can be employed by exploiting the relative frequencies of the oscillators. \u2022 Frequency selectors/tuners can also be implemented by assigning sufficiently lower levels of energy to the oscillator network, such that only the oscillator with the maximum energy content is sustained.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Effect of the learning problem on sonified output"
        },
        {
            "text": "On the psychoacoustics side, we can incorporate both temporal and spatial attributes, as well as different choices of the sampling frequency, frequency mapping schemes, bandwidth, etc. Table 1 presents a summary of the various psychoacoustic parameters at our disposal. A temporal constraint is imposed on the sonified output by default because of the inherent power normalization provided by the growth transform framework. The temporal constraint thus imparts an automatic gain control feature [32] on the sonified signal. Since the output signal is inherently complex, spatial constraints can be imposed on the model using the left and right channels of the audio for the real and imaginary parts respectively, or vice versa. Finally, different sonification strategies can be adopted by considering different mapping schemes for both the baseline frequencies \u03c9 i 's, and the relative frequencies \u03be k 's. In a sense, the sonification process can be thought of as projecting high-dimensional data into a low-dimensional basis space created by predetermined frequency trajectories. We will present here three different sonification strategies: (a) using frequencies equally spaced on the Bark-scale, (b) creating chords based on a musical scale of choice, and (c) extracting dominant frequency trajectories from a chosen musical piece and mapping the basis set of frequencies to these trajectories. For generating human recognizable auditory signatures, all the baseline and relative frequencies should lie within the range of human perception, i.e., 20 Hz-20 kHz. Furthermore, the largest frequency assigned to a variable should be less than the Nyquist frequency to avoid aliasing. Some of the desirable characteristics of a candidate sonification strategy are as follows: 1) Different data distributions can be encoded by different sound signatures, depending on the underlying optimization task.",
            "cite_spans": [
                {
                    "start": 496,
                    "end": 500,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [
                {
                    "start": 185,
                    "end": 192,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "B. Effect of Psychoacoustics on Sonified Output"
        },
        {
            "text": "2) The complexity of the dataset or the underlying optimization problem affects the output sound signature. 3) For time-varying data, drift in the data distribution over time would lead to a drift in the sonified signal as well. For example, if we consider clustering as the underlying optimization problem, where the number of allowable clusters (K) is fixed apriori, then we would ideally want the sonified output to give an indication of (a) the instantaneous cluster densities, (b) the orientation of the clusters and (c) the time it takes for the optimization problem to converge to the optimal cluster assignments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Effect of Psychoacoustics on Sonified Output"
        },
        {
            "text": "1) Bark scale-based sonification: This method involves mapping the relative/baseline frequencies to equally spaced frequencies on the Bark scale. Any number of frequency trajectories can be selected if masking is acceptable in the end application. However, if we want to eliminate masking effects from the output sonified signal, the Bark scale frequencies should be chosen in a way such that the critical bands around these frequencies do not overlap with each other. Since the Bark scale has 24 critical bands, this implies that the number of frequencies in the basis set is limited to a maximum of 24 if we want to avoid masking. Additionally, the sonification module should be designed so that each frequency trajectory remains within its critical band on the Bark scale, even during the transient phase. This method's advantage is that changes in each frequency trajectory (i.e., each sonified variable) can be discerned unambiguously since there is no mixing of trajectories throughout the duration of sonification.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Effect of Psychoacoustics on Sonified Output"
        },
        {
            "text": "2) Musical chord-based sonification: This approach is similar to the Bark scale-based approach, with the relative (or baseline) frequencies being mapped to a predetermined musical scale, e.g., the equally-tempered Western scale. Depending on the user requirements, we may create a chord by choosing notes in a single octave as the basis set. We can also select notes over multiple octaves, associating a different timbre to each, giving the impression of multiple different instruments being played. For example, we can map the frequencies to every other note in a diatonic scale such that they form a triad (e.g., the notes C, E, and G form the C major triad in the equally-tempered scale around A 4 = 440 Hz). Similarly, four or more random notes from the same octave create a generic chord. The method also allows for the creation of arpeggios (or a broken chord being played multiple times in succession) by suitably defining periodic repetitions of the set of chords. Overlapping of the frequency trajectories corresponding to different sonification variables may or may not occur, depending on (a) the pairwise distances between the frequency trajectories and (b) the maximum extent of frequency perturbation caused by the sonification module. Additionally, since humans are more adept at recognizing time-varying audio signatures than static tones, a small, slowly varying sinusoidal variation may be added on top of the original frequency trajectories. The amplitudes and frequencies of these sinusoidal variations may be either (a) kept constant, or made to vary based on (b) the convergence properties of the optimization problem, or on (c) the instantaneous statistical properties of the sonification variables.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Effect of Psychoacoustics on Sonified Output"
        },
        {
            "text": "3) Sonification using an existing musical piece: This method of sonification involves extracting a certain number of dominant frequency trajectories (depending on the desired size K of the basis set) from an existing musical piece. Additionally, the following steps need to be taken for extracting the dominant frequency trajectories from the chosen musical piece: 1) Typically, a musical piece may be much longer in duration than the simulation duration and has a much higher sampling rate (44 kHz). Thus we would need to extract a segment of the entire composition and compress its time scale to ensure a proper mapping. 2) Next, we analyze the spectrogram of the time-scaled musical sample and extract K dominant frequency components in each time window of the spectrogram, based on the power content of each in that window. 3) Finally, we carry out upsampling and interpolation to form continuous frequency trajectories that represent K most dominant components of the musical composition. The sonification variables are then mapped to these frequency trajectories. The trajectories undergo perturbations in the transient phase during the sonification process, depending on the dataset complexity and the optimization problem being solved. The baseline frequency perturbations may remain unmodified or can be made a function of the convergence properties only. They can also be made to depend on certain statistical properties of the dataset or the optimization variables. Finally, for better interpretability, the sonification module's output can be treated as a \"noise\" signal and superimposed on the original musical composition. The degree of deviation of this superimposed signal from the original composition thus encodes the dataset's complexity and the optimization process. Fig 9 shows an overview of the musical composition-based sonification technique for a sample optimization problem, considering a basis set of size 3. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1787,
                    "end": 1798,
                    "text": "Fig 9 shows",
                    "ref_id": "FIGREF9"
                }
            ],
            "section": "B. Effect of Psychoacoustics on Sonified Output"
        },
        {
            "text": "As a case study, we will consider a data clustering problem. The goal is to sonify the data by mapping each cluster to a particular tone or frequency trajectory, with the amplitude of each trajectory being proportional to the cluster density. Since the focus of the paper is on advocating a new sonification framework and not improving the efficiency of the particular clustering algorithm chosen, for the sake of simplicity, we will consider a similaritybased probabilistic clustering approach proposed in [33] . This involves solving a non-negative matrix factorization problem that minimizes the distance between a similarity matrix computed pairwise between the data points, and the actual likelihoods of the different data points to be clustered together in space. Consider a D\u2212dimensional dataset X \u2208 R M \u00d7D and a similarity matrix W \u2208 R M \u00d7M + computed using a pairwise distance metric between the data points. Then the following optimization problem assigns to the i\u2212th data point a K\u2212dimensional vector p i consists of probability values of the data point of belonging to each of K possible clusters:",
            "cite_spans": [
                {
                    "start": 507,
                    "end": 511,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "Thus, p ik \u2208 R + denotes the probability of the ith data point of belonging to the kth cluster, and \u03b2 denotes a scaling factor such that \u03b2p T i p j represents the true likelihood of the ith and j\u2212th data points to be clustered together. In this approach, each w ij is assumed to be normally distributed about its corresponding true likelihood \u03b2p T i p j with a constant variance. Here, the similarity matrix W is chosen to be the RBF kernel computed pairwise between the data points by mapping them to a high-dimensional space. We can use the procedure outlined in Table  2 by applying the mapping p ik = |\u03c8 ik | 2 , \u03c8 ik \u2208 C. Sonification of the clustering problem can then be achieved in the following manner:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 565,
                    "end": 573,
                    "text": "Table  2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "\u2022 We assign the same baseline frequency to all the subgroups (individual data points in this case), i.e., \u03c9 i = \u03c9 \u2200i. \u2022 Each cluster is assigned to a particular relative frequency trajectory according to the sonification strategy chosen, i.e., \u03be ik = \u03be k \u2200i. Depending on the sonification strategy, the unperturbed version of the relative frequencies \u03be k (t)'s may be chosen as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "(a) Bark scale based: Each of the K clusters is mapped to a distinct frequency on the Bark scale (with or without masking effects, depending on the frequency spacing). A slow sinusoidal variation may be added about each original frequency trajectory depending on the instantaneous cluster density, as well as the convergence characteristics of the optimization problem. The relative frequencies are thus modulated over time governed by the following equation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "where \u2206f",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "a k , b k \u2208 R + are constants, c k (0) = M/K is the initial cluster density (assuming the data points to be uniformly distributed among the clusters), and c k (t) = M i=1 |\u03c8 ik | 2 is the instantaneous cluster density of the k\u2212th cluster.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "(b) Musical chord-based: Each of the K clusters is mapped to a distinct frequency on a chosen musical scale. In this case, too, a slow sinusoidal variation may be added about each original frequency trajectory depending on the instantaneous cluster density and the convergence characteristics of the optimization problem. The evolution equations for the \u03be k (t)'s are similar to those used for the Bark scale-based method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "(c) Using an existing musical composition: Each of the K clusters is mapped to a distinct frequency trajectory extracted from a musical composition. The instantaneous statistical properties of each cluster and the convergence properties of the optimization variables may be used as a scaling factor, to enhance the perturbations caused by the growth transform optimization framework. In this case, the evolution equations may have a sinusoidal variation as in the previous two approaches, or maybe of the following form:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "where a k , \u2206f k (t) and s k have the same definitions as before. This is because the original frequency trajectories vary over time, and hence an additional sinusoidal perturbation for recognizing the audio signature in the steady state might not be necessary. \u2022 The update equation for \u03b2 is given by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "while those of the complex waveforms \u03c8 ik 's are obtained using the complex growth transform updates in Table  2 (please see Appendix VIII-B for details). The time evolution of \u03be k (t) \u2200k occurs according to the update rules in (12) or (13) , depending on the chosen sonification strategy. \u2022 The final output of the sonification module is obtained by superimposing the waveforms of all the oscillators,",
            "cite_spans": [
                {
                    "start": 228,
                    "end": 232,
                    "text": "(12)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 236,
                    "end": 240,
                    "text": "(13)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [
                {
                    "start": 104,
                    "end": 112,
                    "text": "Table  2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "Next, we demonstrate the properties of the sonification technique when applied to the clustering problem for different sonification strategies, the actual number of clusters in the dataset, dataset complexity (i.e., cluster alignments), and the number of clusters assigned apriori (i.e., the size of the basis set of frequencies K). Effect of different frequency mapping strategies: Fig 10 shows the results on the 'Iris' dataset (M = 150), considering a basis of size K = 3, for all three types of sonification strategies discussed in Section IV-B. Fig 10(a) shows a PCA plot of the data for reference, where the data points have been colored according to their cluster assignments by the complex growth transform-based clustering algorithm. Fig 10(b) shows the frequency evolution plots of the relative frequency trajectories are chosen according to the Bark scale, with added sinusoidal variations, as discussed before. Fig 10(c) and (d) show the corresponding time evolution plot and the spectrogram respectively of the sonified output signal \u03c8 sum (t). Effect basis set size and number of clusters: Fig 11 shows the results of the sonification approach on synthetic Datasets I, II, and III, each containing M = 500 data points, but the number of underlying clusters being 2,3 and 5 respectively. The synthetic datasets are generated using a Gaussian mixture model consisting of 2, 3 and 5 different clusters respectively, with a fixed cluster mean and variance associated with each cluster. We consider the basis set to consist of 3 frequency trajectories (i.e., K = 3), and the musical composition-based sonification strategy for our experiments. Fig 11(a) shows a scatter plot of Dataset I, with the points color-coded according to their cluster assignment, Fig 11( ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 383,
                    "end": 395,
                    "text": "Fig 10 shows",
                    "ref_id": "FIGREF12"
                },
                {
                    "start": 550,
                    "end": 559,
                    "text": "Fig 10(a)",
                    "ref_id": "FIGREF12"
                },
                {
                    "start": 743,
                    "end": 752,
                    "text": "Fig 10(b)",
                    "ref_id": "FIGREF12"
                },
                {
                    "start": 923,
                    "end": 940,
                    "text": "Fig 10(c) and (d)",
                    "ref_id": "FIGREF12"
                },
                {
                    "start": 1104,
                    "end": 1116,
                    "text": "Fig 11 shows",
                    "ref_id": "FIGREF13"
                },
                {
                    "start": 1653,
                    "end": 1662,
                    "text": "Fig 11(a)",
                    "ref_id": "FIGREF13"
                },
                {
                    "start": 1765,
                    "end": 1772,
                    "text": "Fig 11(",
                    "ref_id": "FIGREF13"
                }
            ],
            "section": "V. EXPERIMENTS ON SYNTHETIC DATASETS"
        },
        {
            "text": "In this section, we demonstrate how our framework can be applied for sonifying non-invasive electro-encephalogram (EEG) recordings for the detection of epileptic seizures. This is particularly useful in scenarios where trained physicians may not be readily available for analyzing the patterns in the EEG waveforms, as in the case of sub-clinical seizure onset. In instances of refractory epilepsy in particular, where patients are non-responsive to anti-epileptic drugs, neurostimulation or surgery is an option if the epileptogenic focus/foci can be identified [34] , [35] . Sonification can be useful in such scenarios since it usually implies faster feedback than visualization, leading to faster injection of the radiotracer for effective localization of the epileptogenic foci.",
            "cite_spans": [
                {
                    "start": 563,
                    "end": 567,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 570,
                    "end": 574,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "VI. EXPERIMENTS ON REAL DATASETS: SONIFICATION OF EEG RECORDINGS FOR EPILEPTIC SEIZURE DETECTION"
        },
        {
            "text": "Traditional approaches for seizure detection from EEG recordings include a sliding window-based feature extraction stage. This involves inferring network connectivities or frequency components of the EEG channels in each window [36] . The feature extraction stage is usually followed by a classification stage formed by support vector machine or neural network-based classifiers, and more recently, deeper architectures like CNNs [6] . This is illustrated in Fig 14(c) . On the other hand, sonification-based approaches for seizure detection usually adopt a parameter mapping approach where the voltage values of a single channel of the EEG signal are mapped to the auditory domain [5] . There exists a second approach for sonification of EEG signal. This involves mapping the output of the binary classifier into two distinct audio waveforms representing the presence or absence of seizure. While the first approach discards valuable information in terms of the network interactions during a seizure event, the second approach conveys the decision of the classifier using sound [37] . It thus does not employ the end-user (clinician or caregiver) in the decision-making process. (a) (e) (i) In this section, we propose a novel sonification method that considers the interactions between all EEG channels to detect a seizure event, and creates a sonified output that enables the listener to make a decision. The proposed sonification module thus acts as a feature extractor in this case, and the task of detecting a seizure event still lies with the human-in-the-loop. We applied the sonification strategy to scalp EEG recordings of pediatric patients provided by the CHB-MIT database collected at the Children's Hospital Boston [10] , [11] . The recordings were collected from 23 channels placed according to the International 10-20 system of electrode positions, and all signals were sampled at 256 Hz. The original dataset contains EEG recordings from 23 patients (5 males aged 3-22 years, 17 females aged 1.5-19 years). Only the recordings that had one or more instances of the seizure (as marked by domain experts in the dataset) were used for our purpose.",
            "cite_spans": [
                {
                    "start": 228,
                    "end": 232,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 430,
                    "end": 433,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 682,
                    "end": 685,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1079,
                    "end": 1083,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 1729,
                    "end": 1733,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 459,
                    "end": 468,
                    "text": "Fig 14(c)",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "VI. EXPERIMENTS ON REAL DATASETS: SONIFICATION OF EEG RECORDINGS FOR EPILEPTIC SEIZURE DETECTION"
        },
        {
            "text": "The first step in the sonification process involves (i) removing the baseline, (ii) re-referencing the electrodes to the average potential, and finally (iii) applying a bandwidth filter with a passband of 0.5-50 Hz for noise reduction. All of these operations were performed using the publicly available EEGLAB toolbox [38] . The EEG waveforms were then normalized channel-wise in the range [\u22121, +1]. Fig 14(a) shows a sample EEG recording with a seizure event lasting from 1467-1494s. Fig 14(b) shows a zoomed-in version of the seizure event. The EEG channel signals were then analyzed using a non-overlapping sliding window of 2s duration (denoted by the matrix E). Note that 2s sliding windows have been used extensively in literature for epileptic seizure detection from EEG signals since they are short enough for retaining relevant information about the dynamics of the signal [10] . Fig 14(c) shows the traditional supervised approach where the output of a classifier trained on features extracted from each of the 2s windows is converted to an audio waveform. In contrast, our proposed unsupervised approach (shown in Fig  14(d) ) involves passing each such window through the sonification module to produce an audio signal. The output signal is finally used by the user to decide whether the window belongs to a seizure or a non-seizure event. In this framework, for each 2s epoch, we solve a quadratic optimization problem of the following form for inferring the ",
            "cite_spans": [
                {
                    "start": 319,
                    "end": 323,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 883,
                    "end": 887,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [
                {
                    "start": 401,
                    "end": 410,
                    "text": "Fig 14(a)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 486,
                    "end": 495,
                    "text": "Fig 14(b)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 890,
                    "end": 899,
                    "text": "Fig 14(c)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1126,
                    "end": 1136,
                    "text": "Fig  14(d)",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "VI. EXPERIMENTS ON REAL DATASETS: SONIFICATION OF EEG RECORDINGS FOR EPILEPTIC SEIZURE DETECTION"
        },
        {
            "text": "where n c = 23 is the number of EEG channels under consideration. Q \u2208 R nc\u00d7nc represents a measure of the pairwise connectivity between the EEG channels, and is computed as Q = f (EE ). Here, f (\u00b7) : R nc\u00d7nc \u2192 R nc\u00d7nc represents an element-wise square-root operator, and E represents the EEG data over the 2s window. Fig 14(e) illustrates the proposed approach. We can apply the complex growth transforms to arrive at the optimal solution of Eq (15)- (16) . For the simulations presented in this section, the baseline frequency of all the oscillators is chosen to be \u03c9 n = 300 Hz. The relative frequencies \u03be k,n 's of the oscillator variables \u03c8 k 's (where p k = \u03c8 k \u03c8 * k , \u03c8 k \u2208 C) are chosen according to the relative placements of the electrodes from which the corresponding channels are recorded. For example, oscillators corresponding to channels recorded from the frontal, temporal, parietal, and occipital lobes of the brain are can be mapped to the notes G, E, C, and A based on the equally-tempered Western musical scale around A = 440 Hz. This mapping scheme based on the electrode locations is illustrated in Fig 15. Similar mappings can also be achieved using the Bark scale-based and musical composition-based approaches. Note that higher relative frequencies are assigned to the channels from the frontal and parietal lobes, as these brain regions are affected more by epileptic seizure events in pediatric patients than in the other two regions [39] . Thus allocating higher frequencies to the oscillators/channels mapping to these lobes would result in greater perturbation in the corresponding frequency bands. This would potentially lead to better discrimination between seizure and non-seizure events. The framework thus allows for incorporating spatial information about the electrode locations because of the proposed mapping strategy, in addition to the temporal information captured by the sonification process. Spatial information can also be embedded in the sonification process by utilizing a dual-channel audio representation instead of the single-channel audio representation discussed until now in the paper. Since the output sonified signal is inherently complex, we can use the left and right channels for outputting the real and imaginary part respectively of the signal, or vice versa. show similar figures for 2s windows corresponding to seizure events in the same five recordings. Based on the above results, we can conclude that the spectrograms for the sonified windows corresponding to seizure events seem to have a higher level of entropy than those corresponding to non-seizure events. Fig 17, on the other hand, presents a comparison of the results of the sonification process between seizure recordings across ten different patients (Patient #01, #03 #05, #08, #11, #14, #19, #20, #21 and #22 respectively). Figs 17(a)-(j) show the spectrograms for 2s windows corresponding to non-seizure events. Figs 17(k)-(t) show similar results for 2s windows corresponding to seizure events across the ten patients. The sonified signals corresonding to seizure vs non-seizure windows thus show consistent patterns not only across different recordings for the same patient, but also across patients. Our proposed sonification framework is thus capable of identifying both spatial and temporal patterns in the time-varying EEG signal, which can be potentially used for real-time detection of epileptic seizure events.",
            "cite_spans": [
                {
                    "start": 451,
                    "end": 455,
                    "text": "(16)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [
                {
                    "start": 317,
                    "end": 326,
                    "text": "Fig 14(e)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1121,
                    "end": 1128,
                    "text": "Fig 15.",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 2627,
                    "end": 2634,
                    "text": "Fig 17,",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "VI. EXPERIMENTS ON REAL DATASETS: SONIFICATION OF EEG RECORDINGS FOR EPILEPTIC SEIZURE DETECTION"
        },
        {
            "text": "This paper presented a novel technique for the sonification of high-dimensional data that incorporates both learning and sonification stages into the same module. The end-user then uses the sonified audio output for making a decision. This is in contrast to existing sonification techniques that either involve (a) using a learning algorithm upstream that inherently controls the decision-making process and maps the outcome to an audio signature, or (b) directly Figure 17 : Sonification results on arbitrary 2s windows for a single recording of each of 10 different patients (Patient #01, #03 #05, #08, #11, #14, #19, #20, #21 and #22 respectively). (i) Non-seizure events: (a)-(j): Spectrograms for arbitrary non-seizure windows in each of the ten patients; (ii) Seizure events : (k)-(t): similar results for arbitrary seizure windows in each of the ten patients.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 464,
                    "end": 473,
                    "text": "Figure 17",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "VII. DISCUSSION AND CONCLUSIONS"
        },
        {
            "text": "mapping the underlying variables to different parameters of the sound wave, without accounting for the correlations and patterns in the data. At the core of the framework lies the complex growth transform dynamical system, which simultaneously utilizes the learning variables and psychoacoustic parameters defined by the user. Thus, our proposed sonification module outputs a binaural audio signature that can be used for human-in-the-loop decisionmaking. The output sonified signal encodes the high-dimensional space data, which might be particularly useful for low throughput systems. Additionally, the method can be utilized for solving a range of learning problems of varying dimensionality and provides several tunable parameters that can be customized to adapt different sonification strategies. Experiments on synthetic and real data show encouraging results, proving that the method can be used for real-time applications involving high-dimensional, temporally varying data. Future directions for this work involve explore using sonification in multimodal perceptualization tasks [40] , [41] with potential applications in visual rehabilitation [42] , neuroprosthesis [43] , and for cohesive perception in autonomous robots [44] .",
            "cite_spans": [
                {
                    "start": 1088,
                    "end": 1092,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 1095,
                    "end": 1099,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 1176,
                    "end": 1180,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [],
            "section": "VII. DISCUSSION AND CONCLUSIONS"
        },
        {
            "text": "A. Nomenclature ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "In [7] , we used the Baum-Eagon inequality [9] to show that the optimal point of the optimization problem represented by Eqs (18)- (19) for a generic Lipschitz continuous cost function H (P)(P \u2208 D \u2282 R M \u00d7K ) corresponds to the steady-state solution of a multiplicative update-based discrete-time growth transform dynamical system model given by: p ik,n \u2190 (1 \u2212 \u03b1 i,n )p ik,n\u22121 + \u03b1 i,n p ik,n\u22121 g ik,n\u22121 (P n\u22121 ),",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 6,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 43,
                    "end": 46,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 131,
                    "end": 135,
                    "text": "(19)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "where g ik,n\u22121 (P n\u22121 ) = (\u2212 \u2202H \u2202p ik,n\u22121 + \u03bb) K l=1 p il,n\u22121 (\u2212 \u2202H \u2202p il,n\u22121 + \u03bb)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": ", \u2200i = 1, . . . , M , and 0 \u2264 \u03b1 i,n \u2264 1 \u2200i. The constant \u03bb \u2208 R + is chosen to ensure that |\u2212 \u2202H (P) \u2202p ik,n\u22121 + \u03bb| > 0, \u2200i, k. Note that convergence to the optimal solution is guaranteed even though \u03b1 i,n 's are time-varying, since this would still ensure the invariance of the manifold D.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "Taking g ik,n\u22121 (P n\u22121 ) = \u03c3 2 ik,n\u22121 (P n\u22121 ) \u2200i, k and \u03b1 i,n = sin 2 (\u03b8 i,n ), we get:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "p ik,n \u2190 [cos 2 (\u03b8 i,n )p ik,n\u22121 + sin 2 (\u03b8 i,n )p ik,n\u22121 \u03c3 2 ik,n\u22121 (P n\u22121 )]",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "Representing p ik,n = \u03c8 ik,n \u03c8 * ik,n , \u03c8 ik,n \u2208 C, the update equations become: \u03c8 ik,n \u03c8 * ik,n \u2190 \u03c8 ik,n\u22121 [cos(\u03b8 i,n ) + j sin(\u03b8 i,n )\u03c3 ik,n\u22121 (P n\u22121 )] \u00d7\u03c8 * ik,n\u22121 [cos(\u03b8 i,n ) + j sin(\u03b8 i,n )\u03c3 ik,n\u22121 (P n\u22121 )] *",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "Considering H (P) = H(\u03a8) to be analytic in D C , since by Wirtinger's calculus, \u2202H \u2202\u03c8 ik,n\u22121 = \u2202H \u2202\u03c8 ik,n\u22121 \u03c8 * ik,n\u22121 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "\u2202\u03c8 ik,n\u22121 \u03c8 * ik,n\u22121 \u2202\u03c8 ik,n\u22121 = \u2202H \u2202\u03c8 ik,n\u22121 \u03c8 * ik,n\u22121 .\u03c8 * ik,n\u22121 , ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "The discrete time update equations for \u03c8 ik,n is thus given by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "\u03c8 ik,n \u2190 \u03c8 ik,n\u22121 [cos(\u03b8 i,n ) + j sin(\u03b8 i,n )\u03c3 ik,n\u22121 (\u03a8 n\u22121 )],",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "Now, let us define \u03b8 i,n = \u03c9 i,n \u2206t, where \u03c9 i,n is the baseline frequency for the i\u2212th group of oscillator variables at the n\u2212th time step, and \u2206t is a unit time step. Eq (25) can then be rewritten as:",
            "cite_spans": [
                {
                    "start": 172,
                    "end": 176,
                    "text": "(25)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "\u03c8 ik,n \u2190 \u03c8 ik,n\u22121 [cos(\u03c9 i,n \u2206t) + j sin(\u03c9 i,n \u2206t)\u03c3 ik,n\u22121 (\u03a8 n\u22121 )],",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "Theorem 2: Different oscillation frequencies can be assigned to each element in the dynamical system represented by Eq (17) by adding an instantaneous relative phase term, i.e., the following dynamical system \u03c8 ik,n \u2190 \u03c8 ik,n\u22121 [cos(\u03c9 i,n \u2206t) + j sin(\u03c9 i,n \u2206t)\u03c3 ik,n\u22121 ] exp(j\u03be ik,n \u2206t),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "converges to the same solution as that attained by the one described by Eq (17) . Proof: Considering \u03d5 ik,n to be the instantaneous relative phase of the ik\u2212th element with respect to an absolute reference at the n\u2212th time instance, we have: \u03c8 ik,n \u2190 \u03c8 ik,n\u22121 [cos(\u03c9 i,n \u2206t) + j sin(\u03c9 i,n \u2206t)\u03c3 ik,n\u22121 ] exp(j\u03d5 ik,n ),",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 79,
                    "text": "(17)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "Let \u03d5 ik,n be the instantaneous relative phase of the ik-th oscillator variable. Then we can define \u03d5 ik,n = \u03be ik,n \u2206t, such that \u03be ik,n is the corresponding relative frequency. The addition of the relative frequency term only alters the oscillator dynamics without altering the instantaneous solution of the optimization problem. Additionally, we can incorporate information about the instantaneous oscillator dynamics in the relative phase by using \u03d5 ik,n = c ik \u03c3 ik,n \u03be ik,n \u2206t, where c ik 's are arbitrary scaling coefficients. Note that as before, this mapping only perturbs the dynamics, without impacting the true solution. Eq (28) can be rewritten as \u03c8 ik,n \u2190 \u03c8 ik,n\u22121 cos 2 (\u03c9 i,n \u2206t) + sin 2 (\u03c9 i,n \u2206t)\u03c3 2 ik,n\u22121 \u00d7 exp j arctan(\u03c3 ik,n\u22121 tan(\u03c9 i,n \u2206t)) + \u03be ik,n \u2206t ,",
            "cite_spans": [
                {
                    "start": 635,
                    "end": 639,
                    "text": "(28)",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "Finally, in the steady state, since \u03c3 ik,n\u22121 n\u2192\u221e \u2212\u2212\u2212\u2192 1 \u2200i, k, and taking \u2206t = 1 F s (F s is the sampling frequency), we have: \u03c8 ik,n \u2190 \u03c8 ik,n\u22121 exp j(\u03c9 i,n + \u03be ik,n ) 1 F s . 24 The output sonified signal in the steady-state is then obtained by the superposition of all the oscillator waveforms, i.e.,",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 178,
                    "text": "24",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "\u03c8 ik,n\u22121 exp j(\u03c9 i,n + \u03be ik,n ) 1 F s (31) In particular, when there is a single global constraint involving all the variables, i.e., when M = 1, then \u03c8 ik,n = \u03c8 k,n , \u03c9 i,n = \u03c9 n and \u03be ik,n = \u03be k,n . The complex growth transform dynamical system for this special case can be written as:",
            "cite_spans": [
                {
                    "start": 38,
                    "end": 42,
                    "text": "(31)",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "\u03c8 k,n \u2190 \u03c8 k,n\u22121 exp j(\u03c9 i,n + \u03be ik,n ) 1 F s (32) \u03c8 sum,n = K k=1 \u03c8 k,n\u22121 exp j(\u03c9 n + \u03be k,n ) 1 F s . ",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 49,
                    "text": "(32)",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "where d ik,n = [cos(\u03c9 i,n \u2206t) + j sin(\u03c9 i,n \u2206t)\u03c3 ik,n\u22121 ] exp(j\u03be ik,n \u2206t),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "and using Eq (28), we have: \u03a8 n \u2190 U n (\u03a8 n\u22121 ) \u03a8 n\u22121 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "Additionally, since ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "and K k=1 |\u03c8 k,n | 2 = K k=1 |\u03c8 k,n\u22121 | 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        },
        {
            "text": "C. Mapping procedure Let us consider an optimization problem of the following generic form:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. APPENDIX"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "The sonification handbook",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hermann",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hunt",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "G"
                    ],
                    "last": "Neuhoff",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Sonification report: Status of the field and research agenda",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Kramer",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Walker",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Bonebright",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cook",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Flowers",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Miner",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Neuhoff",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A systematic review of mapping strategies for the sonification of physical quantities",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Dubus",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bresin",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "PloS one",
            "volume": "8",
            "issn": "12",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Polyphonic sonification of electrocardiography signals for diagnosis of cardiac pathologies",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "N"
                    ],
                    "last": "Kather",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hermann",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bukschat",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kramer",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "R"
                    ],
                    "last": "Schad",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "G"
                    ],
                    "last": "Z\u00f6llner",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Scientific reports",
            "volume": "7",
            "issn": "1",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Rapidly learned identification of epileptic seizures from sonified eeg",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Loui",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Koplin-Green",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Frick",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Massone",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Frontiers in human neuroscience",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Epileptic seizure detection based on eeg signals and cnn",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Frontiers in neuroinformatics",
            "volume": "12",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Decentralized global optimization based on a growth transform dynamical system model",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chatterjee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chakrabartty",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Resonant machine learning based on complex growth transform dynamical systems",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chatterjee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chakrabartty",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "volume": "",
            "issn": "",
            "pages": "1--15",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Growth transformations for functions on manifolds",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "E"
                    ],
                    "last": "Baum",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Sell",
                    "suffix": ""
                }
            ],
            "year": 1968,
            "venue": "Pacific Journal of Mathematics",
            "volume": "27",
            "issn": "2",
            "pages": "211--227",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Application of machine learning to epileptic seizure detection",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "H"
                    ],
                    "last": "Shoeb",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "V"
                    ],
                    "last": "Guttag",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10)",
            "volume": "",
            "issn": "",
            "pages": "975--982",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Goldberger",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "A"
                    ],
                    "last": "Amaral",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Glass",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Hausdorff",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "C"
                    ],
                    "last": "Ivanov",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "G"
                    ],
                    "last": "Mark",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Mietus",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "B"
                    ],
                    "last": "Moody",
                    "suffix": ""
                },
                {
                    "first": "C.-K",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "E"
                    ],
                    "last": "Stanley",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "",
            "volume": "101",
            "issn": "",
            "pages": "215--220",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Pca versus lda",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Martinez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Kak",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "IEEE transactions on pattern analysis and machine intelligence",
            "volume": "23",
            "issn": "",
            "pages": "228--233",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Nonlinear dimensionality reduction by locally linear embedding",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Roweis",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "K"
                    ],
                    "last": "Saul",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "science",
            "volume": "290",
            "issn": "5500",
            "pages": "2323--2326",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "The isomap algorithm and topological stability",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Balasubramanian",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "L"
                    ],
                    "last": "Schwartz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Silva",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Langford",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "295",
            "issn": "",
            "pages": "7--7",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Laplacian eigenmaps for dimensionality reduction and data representation",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Belkin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Niyogi",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Neural computation",
            "volume": "15",
            "issn": "6",
            "pages": "1373--1396",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Visualizing data using t-sne",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "V D"
                    ],
                    "last": "Maaten",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Journal of machine learning research",
            "volume": "9",
            "issn": "",
            "pages": "2579--2605",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Visualizing structure and transitions in high-dimensional biological data",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "R"
                    ],
                    "last": "Moon",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Van Dijk",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gigante",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "B"
                    ],
                    "last": "Burkhardt",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "S"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yim",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Van Den Elzen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Hirn",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "R"
                    ],
                    "last": "Coifman",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Nature Biotechnology",
            "volume": "37",
            "issn": "12",
            "pages": "1482--1492",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "m-tsne: A framework for visualizing high-dimensional multivariate time series",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Purushotham",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "To",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shahabi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1708.07942"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "The Steering Committee of The World Congress in Computer Science",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "St George",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "Y"
                    ],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "H.-P",
                    "middle": [],
                    "last": "Bischof",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the International Conference on Modeling, Simulation and Visualization Methods (MSV)",
            "volume": "",
            "issn": "",
            "pages": "3--9",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Sublime frequencies: The construction of sublime listening experiences in the sonification of scientific data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Supper",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Social Studies of Science",
            "volume": "44",
            "issn": "1",
            "pages": "34--58",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Life music: the sonification of proteins",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dunn",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Clark",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Leonardo",
            "volume": "32",
            "issn": "1",
            "pages": "25--32",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Sonification based de novo protein design using artificial intelligence, structure prediction, and analysis using molecular modeling",
            "authors": [
                {
                    "first": "C.-H",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Buehler",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "APL bioengineering",
            "volume": "4",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "A novel sonification approach to support the diagnosis of alzheimer's dementia",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Gionfrida",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Roginska",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Frontiers in neurology",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "A review of real-time eeg sonification research",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "V\u00e4ljam\u00e4e",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Steffert",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Holland",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Marimon",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Benitez",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mealla",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oliveira",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jord\u00e0",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Skin cancer detection by deep learning and sound analysis algorithms: A prospective clinical study of an elementary dermoscope",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dascalu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "EBioMedicine",
            "volume": "43",
            "issn": "",
            "pages": "107--113",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Use of sonification in the detection of anomalous events",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ballora",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Cole",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kruesi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Greene",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Monahan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "L"
                    ],
                    "last": "Hall",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Multisensor, Multisource Information Fusion: Architectures, Algorithms, and Applications 2012",
            "volume": "8407",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "A formalised approach to designing sonification systems for network-security monitoring",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Axon",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Nurse",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Goldsmith",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Creese",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Journal on Advances in Security",
            "volume": "10",
            "issn": "1-2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Sonification of network traffic flow for monitoring and situational awareness",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Debashi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Vickers",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "PloS one",
            "volume": "13",
            "issn": "4",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Marketbuzz: Sonification of real-time financial dataa",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Janata",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Childs",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Action observation plus sonification. a novel therapeutic protocol for parkinson's patient with freezing of gait",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mezzarobba",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Grassi",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Pellegrini",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Catalan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kruger",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Furlanis",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Manganotti",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bernardis",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Frontiers in neurology",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Nanomechanical sonification of the 2019-ncov coronavirus spike protein through a materiomusical approach",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Buehler",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.14258"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Automatic gain control in cochlear mechanics",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "F"
                    ],
                    "last": "Lyon",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "The mechanics and biophysics of hearing",
            "volume": "",
            "issn": "",
            "pages": "395--402",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Probabilistic clustering using the baum-eagon inequality",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Bulo",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pelillo",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "2010 20th International Conference on Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1429--1432",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Detecting silent seizures by their sound",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Parvizi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Gururangan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Razavi",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chafe",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Epilepsia",
            "volume": "59",
            "issn": "4",
            "pages": "877--884",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Time-evolving controllability of effective connectivity networks during seizure progression",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "H"
                    ],
                    "last": "Scheid",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ashourvan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Stiso",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "A"
                    ],
                    "last": "Davis",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Mikhail",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pasqualetti",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Litt",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "S"
                    ],
                    "last": "Bassett",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the National Academy of Sciences",
            "volume": "118",
            "issn": "5",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Real-time inference and detection of disruptive eeg networks for epileptic seizures",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Bomela",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "C.-A",
                    "middle": [],
                    "last": "Chou",
                    "suffix": ""
                },
                {
                    "first": "J.-S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Scientific Reports",
            "volume": "10",
            "issn": "1",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Visualization and sonification of long-term epilepsy electroencephalogram monitoring",
            "authors": [
                {
                    "first": "J.-W",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "C.-P",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "M.-J",
                    "middle": [],
                    "last": "Chiu",
                    "suffix": ""
                },
                {
                    "first": "Y.-H",
                    "middle": [],
                    "last": "Kao",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cichocki",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Journal of Medical and Biological Engineering",
            "volume": "38",
            "issn": "6",
            "pages": "943--952",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Eeglab: an open source toolbox for analysis of single-trial eeg dynamics including independent component analysis",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Delorme",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Makeig",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Journal of neuroscience methods",
            "volume": "134",
            "issn": "1",
            "pages": "9--21",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Childhood epilepsy: The brain",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Do sensory cortices process more than one sensory modality during perceptual judgments",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lemus",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hern\u00e1ndez",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Luna",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zainos",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Romo",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Neuron",
            "volume": "67",
            "issn": "2",
            "pages": "335--348",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Interactions of auditory and visual stimuli in space and time",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "H"
                    ],
                    "last": "Recanzone",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Hearing research",
            "volume": "258",
            "issn": "1-2",
            "pages": "89--99",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Multisensory training reverses midbrain lesion-induced changes and ameliorates haemianopia",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "E"
                    ],
                    "last": "Stein",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "G"
                    ],
                    "last": "Mchaffie",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Nature communications",
            "volume": "6",
            "issn": "1",
            "pages": "1--11",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Multisensory perception in argus ii retinal prosthesis patients: Leveraging auditory-visual mappings to enhance prosthesis outcomes",
            "authors": [
                {
                    "first": "N",
                    "middle": [
                        "R"
                    ],
                    "last": "Stiles",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "R"
                    ],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Weiland",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Vision Research",
            "volume": "182",
            "issn": "",
            "pages": "58--68",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "A generalized model for multimodal perception",
            "authors": [
                {
                    "first": "S.-R",
                    "middle": [],
                    "last": "Shiang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gershman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Oh",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of AAAI '17 Fall Symposium",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Nonlinear operators. ii",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Schwartz",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Journal of Mathematical Physics",
            "volume": "38",
            "issn": "7",
            "pages": "3841--3862",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "(t), as. illustrated inFig 3.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Illustration of the sonification process. K oscillator waveforms \u03c8 1 , . . . , \u03c8 K , representing the set of complex basis functions defined by the user, based on psychoacoustics. Both amplitude and frequency of the basis set oscillators get modulated over time based on the learning task. The sonified output is created by the superimposition of all the oscillator signals, i.e., \u03c8 sum(t) = K k=1 \u03c8 k (t).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "shows the polar coordinate evolution plots of the waveforms \u03c8 1 and \u03c8 2 respectively, for two different choices of the parameter \u03b3: (a) \u03b3 = 1 and (b) \u03b3 = 0.01. It can be seen that the oscillators corresponding to \u03c8 1 and \u03c8 2 converge to steady limit cycle oscillations for larger values of \u03b3. However, for very small \u03b3 values, only the oscillator with the maximum amplitude shows sustained oscillations, while the other converges to the fixed point of zero and stops oscillating. The coupled oscillator network can thus be thought of as some form of a frequency tuner for sufficiently small \u03b3.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig 7 demonstratesthe model behavior for a two-dimensional variant of Example 2 (i.e., M = 2). As before, we consider all the four oscillators in this case, namely, \u03c8 11 , \u03c8 12 , \u03c8 21 , \u03c8 22 to have the same value of natural frequency \u03c9 = 600 Hz and relative frequency \u03be ik = \u03be k = 0. Simulations were carried out with Q = I 2 and c = 0.81 2 .Figs 7(a)-(c) show the spectrograms for the waveform \u03a8 sum (t), with a = 0.5, 1 and 2 respectively, and \u03b3 = 1. In all cases, the spectrogram was computed using a 1024-point Short Time Fourier Transform (STFT) with a sliding Kaiser window of size 1024 and steepness parameter 5, with an overlap size of 1023. Figs 7(d)-(f) show the zoomed-in versions of the instantaneous frequency shift computed using Hilbert transform. Figs 7(g)-(l) show similar results for the same set of values for a and a higher value of \u03b3 = 2. The simulation duration is 1.0s, and the optimization process starts 0.1s after the start of the simulation. From the figures, we can conclude the following: (i) the duration of the transient phase as well as the magnitude of frequency shift Time evolution and phase portraits. Time evolution and phase portrait for \u03c8 1 , \u03c8 2 and \u03a8 sum corresponding to a 1D variant of the optimization problem H 2 :(a) Time evolution of the waveforms, with zoomed-in versions of the initial(I), transient(T) and steady-state(S) regions respectively; (b) Phase portrait of \u03c8 1 vs. \u03c8 2 . The oscillators go from an initial state(I) of the same amplitude and no phase difference, through a transient state(T) of varying amplitudes and varying phase difference, to a final steady-state(S) where they have constant amplitudes and a constant phase difference as well.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Quadratic optimization surface. Optimization surface for the 2D variant of the problem H 2 for different values of the parameter a.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Effect of H and \u03b3. Variation of \u03a8 sum (t) with different values of curvature (a) and total energy available (\u03b3) for a 2D variant of the optimization problem H 2 : (a)-(c) show the spectrograms; (d)-(f) show the zoomed-in versions of the instantaneous frequency shift computed using Hilbert transform, for a = 0.5, 1 and 2 respectively, and \u03b3 = 1; (g)-(l) show similar plots for \u03b3 = 2.optimization were 1s and 0.2s, respectively, in all of the following experiments. Case 1 (Constant relative frequency): For the first set of simulations, the relative frequencies are chosen to be \u03be k = \u03be = 500 Hz for all the oscillators, and no amplitude modulations or frequency modulations were added to the relative frequency trajectories. The plot of Re(exp(j\u03be k t)), which is a unit amplitude sinusoid with a constant frequency for all the oscillators in this case, is shown inFig 8(a). Figs 8(b) and (c) show the corresponding spectrogram and instantaneous frequency shift (computed using Hilbert transform) of the superimposed waveform \u03c8 sum (t). Case 2 (Relative frequency modulation): Figs 8(d)-(f)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Effect of frequency perturbations induced by H. Variation of \u03a8 total (t) for a 2D variant of the optimization problem H 2 , for different choices of the relative frequency \u03be k (t): (i) Case 1:(a) \u03be k is constant \u2200k, t, (b) spectrogram of the output waveform, (c) instantaneous frequency shift using Hilbert transform ; (ii) Case 2: (d) value of \u03be k (t) depends on the convergence rate of each variable \u03c8 k (t), (e) spectrogram of the output waveform, (f)instantaneous frequency shift using Hilbert transform; (iii) Case 3: (g) \u03be k (t) exists only during the optimization stage and is an exponentially decaying function which is the same for all \u03c8 k , (h) spectrogram of the output waveform, (i)instantaneous frequency shift ; (iv) Case 4: (g) \u03be k (t) exists only during the optimization stage and is an exponentially decaying function which depends on the convergence rate of each variable \u03c8 k , (h) spectrogram of the output waveform, (i)instantaneous frequency shift.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Musical composition based sonification technique. (a) Illustration of the sonification process in this technique; (b) Spectrogram of an original musical piece; (d) Raw frequency trajectories extracted from the music; (d) Spectrogram of the sonified output; (e) Spectrogram of the distorted music, obtained by superimposing the sonified output signal on the original music signal; and (f) Time evolution of the original musical composition, the sonified output, and the distorted music signal.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Figs 10(e)-(h) show similar results for the musical scale-based technique, while Figs 10(i)-(l) show those corresponding to the musical composition-based sonification.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "b) shows the frequency evolutions, while Figs 11(c) and (d) show the time evolution and spectrogram of the sonified output, respectively. Fig 11(e)-(h) show the corresponding plots for Dataset II, while Fig 11(i)-(l) show those for Dataset III. Fig 12 shows similar results on Datasets I, II, and III, for a basis set of frequencies of size K = 5. Effect of different cluster alignments: Fig 13 shows the results of the experiments on Datasets III and IV, where both have the same number of data points (M = 500) and 5 underlying clusters, but differ in the cluster alignments and geometries (which represents different data complexities).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Different sonification strategies for clustering the 'Iris' dataset. (a)-(d) show the PCA (labeled according to cluster assignments), frequency trajectories, time evolution, and spectrogram of the sonified output for the bark scale-based sonification; (e)-(h) show similar plots for the musical scale based sonification; and (i)-(l) show the results for the musical composition based sonification.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Musical composition based sonification on synthetic datasets with same number of points (M = 500) but different number of clusters, with a basis set size of 3. (a)-(d) show the scatter plot (colored according to the cluster assignments), frequency trajectories, time evolution, and spectrogram of the sonified output respectively for Dataset I; (e)-(h) show similar results for Dataset II; and (i)-(l) show the results for Dataset III.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Musical composition based sonification on synthetic datasets with same number of points (M = 500) but different number of clusters, with a basis set size of 5. (a)-(d) show the scatter plot (colored according to the cluster assignments), frequency trajectories, time evolution, and spectrogram of the sonified output respectively for Dataset I; (e)-(h) show similar results for Dataset II; and (i)-(l) show the results for Dataset III.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "Musical composition based sonification considering synthetic datasets with same number of points (M = 500) and same number of clusters (K = 5), but with different cluster geometries, considering a basis set of size 3. (a)-(c) show the scatter plot (colored according to the cluster assignments), frequency trajectories, and spectrogram of the sonified signal respectively for Dataset III; and (d)-(f) show similar results for Dataset IV. = 1, p k \u2265 0 \u2200k = 1, . . . , n c ,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "Sonification of EEG recordings. (a) 1 hour EEG recording with a single seizure event between 1467-1494s; (b) zoomed-in version of the seizure event; (c) An automatic seizure classification module which involves (i) a supervised learning phase using 2s windows of EEG recordings (represented by the matrix E) along with the corresponding labels, and (ii) a testing phase where the presence of seizure is determined by using the trained model on a previously unseen 2s window; (d) Proposed approach, where features extracted from each 2s EEG window are passed directly through a sonification module and presented to a human listener, who takes the decision; and (e) Sonification module involves computing the pairwise connectivity matrix Q between all the n c EEG channels in the window. Q thus interconnects n c globally coupled growth transform oscillators, and the final sonified output is obtained by the superposition of all the oscillator outputs.Fig 16 shows the results of applying our sonification technique to five different instances of seizure recordings (recording #03, #04, #15, #16, and #18 respectively) for the same patient (Patient #01). Figs 16(a)-(e) show spectrograms for 2s windows corresponding to non-seizure windows in each of the five recordings. Figs 16(f)-(j)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "P7, C3-P3, CZ-PZ, C4-P4, T8-P8, P7-T7, T8-P8 C F7-T7, F3-C3, FZ-CZ, F4-C4, F8-T8, T7-FT9, FT10-T8 E FP1-F7, FP1-F3, FP2-F4, FP2-F8, FT9-FT10 G Illustration of relative frequency assignment to oscillators. Relative frequencies of oscillators were assigned to notes A, C, E, or G of the equally-tempered musical scale around A=440 Hz. Frequencies are assigned based on the electrode placement of the corresponding channel.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Sonification results on arbitrary 2s windows for 5 different recordings of Patient #01 ( #03, #04, #15, #16 and #18 respectively) . (i) Non-seizure events: (a)-(e): Spectrograms for arbitrary non-seizure windows in each of the five recordings; (ii) Seizure events: (f)-(j): similar results for arbitrary seizure windows in each of the five recordings.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "Following are the notational conventions used in this paper:B. Proof of Main ResultTheorem 1: Considering \u03a8 \u2208 D C = {\u03a8 \u2208 C M \u00d7K : K k=1|\u03c8 ik | 2 = 1, \u2200i = 1, . . . , M } \u2282 C M \u00d7K , a time evolution of the form given below converges to limit cycles corresponding to the optimal point of a Lipschitz",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "where \u03c3 ik \u2192 1 \u2200i = 1, . . . , N, k = 1, . . . , M , in steady state. Proof: Consider a constrained optimization problem of the following form : ik = 1, \u2200i = 1, . . . , M, p ik \u2265 0 \u2200i, k",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "ik,n\u22121 | 2 , U n : C M \u00d7K \u2192 C M \u00d7K can be thought of as an instantaneous nonlinear unitary operator[45] that ensures that the signal energy is conserved over time.For the special case of a single global constraint (M = 1), we have U n : C K \u2192 C K , d k,n = [cos(\u03c9 n \u2206t) + j sin(\u03c9 n \u2206t)\u03c3 k,n\u22121 ] exp(j\u03be k,n \u2206t), \u2200k = 1, . . . , K",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "Psychoacoustic Parameters",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Notations zn a discrete-time complex variable at the n th time step Hadamard product of two matrices continuous objective function H(\u03a8):",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "The complex growth transform dynamical system follows a nonlinear unitary transformation. Defining d ik,n\u22121 . . . d M K,n\u22121",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "| to arrive at the following equivalent optimization problem over a probabilistic domain:The above problem, in turn, can be mapped to the complex growth transform-based sonification framework by considering p + i = |\u03c8 i1 | 2 , p \u2212 i = |\u03c8 i2 | 2 , and following the procedure in VIII-B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}
{
    "paper_id": "8ed4b2ab432fd5af4cabe1672e7a5a455a747721",
    "metadata": {
        "title": "Continual Learning in Sensor-based Human Activity Recognition: an Empirical Benchmark Analysis",
        "authors": [
            {
                "first": "Saurav",
                "middle": [],
                "last": "Jha",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of St Andrews",
                    "location": {
                        "country": "UK"
                    }
                },
                "email": ""
            },
            {
                "first": "Martin",
                "middle": [],
                "last": "Schiemer",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of St Andrews",
                    "location": {
                        "country": "UK"
                    }
                },
                "email": ""
            },
            {
                "first": "Franco",
                "middle": [],
                "last": "Zambonelli",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universita' di Modena e Reggio Emilia",
                    "location": {
                        "country": "Italy"
                    }
                },
                "email": ""
            },
            {
                "first": "Juan",
                "middle": [],
                "last": "Ye",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of St Andrews",
                    "location": {
                        "country": "UK"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Sensor-based human activity recognition (HAR), i.e., the ability to discover human daily activity patterns from wearable or embedded sensors, is a key enabler for many real-world applications in smart homes, personal healthcare, and urban planning. However, with an increasing number of applications being deployed, an important question arises: how can a HAR system autonomously learn new activities over a long period of time without being re-engineered from scratch? This problem is known as continual learning and has been particularly popular in the domain of computer vision, where several techniques to attack it have been developed. This paper aims to assess to what extent such continual learning techniques can be applied to the HAR domain. To this end, we propose a general framework to evaluate the performance of such techniques on various types of commonly used HAR datasets. We then present a comprehensive empirical analysis of their computational cost and effectiveness of tackling HAR-specific challenges (i.e., sensor noise and labels' scarcity). The presented results uncover useful insights on their applicability and suggest future research directions for HAR systems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "physical activities such as jogging or climbing stairs. These daily activities can have a significant impact on a wide range of real-world applications [8] , ranging from smart home [82] and adaptive environments [3, 69] to personal healthcare [54, 59, 74] and disease diagnosis [2] .",
            "cite_spans": [
                {
                    "start": 152,
                    "end": 155,
                    "text": "[8]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 182,
                    "end": 186,
                    "text": "[82]",
                    "ref_id": "BIBREF93"
                },
                {
                    "start": 213,
                    "end": 216,
                    "text": "[3,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 217,
                    "end": 220,
                    "text": "69]",
                    "ref_id": "BIBREF79"
                },
                {
                    "start": 244,
                    "end": 248,
                    "text": "[54,",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 249,
                    "end": 252,
                    "text": "59,",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 253,
                    "end": 256,
                    "text": "74]",
                    "ref_id": "BIBREF85"
                },
                {
                    "start": 279,
                    "end": 282,
                    "text": "[2]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We have witnessed an increasing number of HAR applications being deployed and running over longer spans [47, 75] . This drives an important research question: how does a HAR system continuously discover and learn new types of activities? We cannot assume that once a HAR system is trained with an initial set of activities, then the users of the system will only perform the same set of activities all the time. People tend to change their behaviour patterns for internal or external factors [64] . Such change requires a HAR system to adapt their learning and include new types of activities in order to provide desired services. For example, the COVID outbreak has impacted the routine of people across the world, including practising different exercises, cooking new cuisines, and switching work patterns. For a personal healthcare monitoring application, failure to recognise new exercise routines may lead to misdiagnosis and inappropriate medication prescription. Therefore, continual learning is the key enabler for a long-term, sustainable HAR system. Continual learning, or lifelong learning, is evoking increasing attention in the field of machine learning, aiming to retaining old knowledge and accumulating new knowledge by continually learning new tasks over time [24, 35, 37, 42, 43, 57] . In HAR, we are concerned with task-incremental continual learning [52] , where a system learns one task at a time and each task contains training data on a new set of classes. Take a neural network as an example in Figure 1 , where for each new task, the network needs to be extended to include new classes and/or add more parameters, and then trained with the new task's data. Faced with this continual learning setting is often the catastrophic forgetting (CF) problem; that is, the network will be optimised towards the new task's data while forgetting the old knowledge and thus degrade the performance on inferring the old classes. As presented in the example of Figure 2 , after training on each new task, the network can only achieve high accuracy on the current classes, while obtaining low accuracy on all the old classes. The purpose of this paper is to benchmark the state-of-the-art continual learning techniques on sensor-based human activity recognition datasets via extensive empirical evaluation 1 . We exclusively focus on neural network based techniques as they have been widely adopted in HAR tasks [12] . Continual learning techniques have achieved promising performance in computer vision [24, 37, 57] and robotics [43] , and it would benefit to assess to what degree they can tackle the continual learning challenges in HAR, as unlike image data, sensor data exhibits its limitations.",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 108,
                    "text": "[47,",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 109,
                    "end": 112,
                    "text": "75]",
                    "ref_id": "BIBREF86"
                },
                {
                    "start": 492,
                    "end": 496,
                    "text": "[64]",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 1277,
                    "end": 1281,
                    "text": "[24,",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 1282,
                    "end": 1285,
                    "text": "35,",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 1286,
                    "end": 1289,
                    "text": "37,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1290,
                    "end": 1293,
                    "text": "42,",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "text": "43,",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 1298,
                    "end": 1301,
                    "text": "57]",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 1370,
                    "end": 1374,
                    "text": "[52]",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 2422,
                    "end": 2426,
                    "text": "[12]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 2514,
                    "end": 2518,
                    "text": "[24,",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 2519,
                    "end": 2522,
                    "text": "37,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 2523,
                    "end": 2526,
                    "text": "57]",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 2540,
                    "end": 2544,
                    "text": "[43]",
                    "ref_id": "BIBREF53"
                }
            ],
            "ref_spans": [
                {
                    "start": 1519,
                    "end": 1527,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1972,
                    "end": 1980,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": ""
        },
        {
            "text": "\u2022 Scarce labels -In HAR, it is very time-and effort-consuming to label sensor data from real-world deployment. As continual learning is concerned with new, unobserved, unpredicted activities, it relies on users' self-annotation; e.g., a user provides a free-text label for the new activity that he/she is performing. Such labels can be sparse and noisy [50] . Due to the labelling challenge, 1 The code and all the experiments are available at https://github.com/srvCodes/ continual-learning-benchmark. 3 there is a smaller amount of labelled training data in HAR compared to their vision counterparts like CIFAR-100 [41] .",
            "cite_spans": [
                {
                    "start": 353,
                    "end": 357,
                    "text": "[50]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 617,
                    "end": 621,
                    "text": "[41]",
                    "ref_id": "BIBREF51"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 Imbalanced class distribution -Some activity classes can have a high occurrence frequency and dominate the dataset while others being rare. For example, hiking can be relatively infrequent compared to sleeping.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 Sensor noise -Sensors can produce noisy readings due to unintended interactions with the environment or degradation over time [78] . For example, a visitor or a pet can trigger unexpected sensor traces in a single-resident dwelling.",
            "cite_spans": [
                {
                    "start": 128,
                    "end": 132,
                    "text": "[78]",
                    "ref_id": "BIBREF89"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Also, sensor data in HAR shares the characteristics with image data: (1) Intra-class diversity -one activity class can have multiple patterns due to different ways of performing an activity; e.g., different ways of preparing a meal. (2) Inter-class similarity -some activity classes have overlapping decision boundaries, which makes them difficult to separate. For example, reading and writing for a sedentary user can have very similar distributions in sensor feature space.",
            "cite_spans": [
                {
                    "start": 233,
                    "end": 236,
                    "text": "(2)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The above limitations of sensor data will bring extra complication in continual learning. This paper aims to explore to what extent the existing techniques can address continual learning challenges in HAR. More specifically, the questions that we seek to answer are: how computationally expensive are these in terms of memory and training time?, is it affordable to run them on resourceconstrained devices?, or is their performance sensitive to the amount of training data? To answer these questions, we adapt the existing continual learning evaluation methodology to HAR and design a general evaluation framework to assess 10 recent techniques published between 2016 and 2020 on 8 commonly used HAR datasets including both ambient binary sensors and accelerometers. We focus on two types of continual learning techniques: regularisation and rehearsal based methods, which are most appropriate for HAR tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Our evaluation has uncovered the following findings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 The rehearsal techniques significantly outperform regularisation techniques on our selected datasets, and the regularisation terms alone are not able to retain the old knowledge. The regularisation terms that tackle class imbalance and inter-class separation are most effective for HAR.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 Most of the rehearsal techniques do not need to store many samples in memory (e.g., 4 or 6 samples per class), and often random sampling can outperform the other sophisticated, computationexpensive techniques.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 The selected continual learning techniques are not sensitive to training data size, and training with 30% of each dataset can achieve good accuracy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4"
        },
        {
            "text": "\u2022 The computation cost for most of the selected techniques is relatively low; i.e., around 33 seconds for training each incremental task. Therefore, these techniques are affordable on resourceconstrained devices.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4"
        },
        {
            "text": "The paper is outlined as follows. Section 2 defines the continual learning problem and briefly reviews the state of the art techniques in three categories: regularisation, rehearsal, and dynamic architecture. Section 3 presents a detailed description of the selected techniques. Section 4 introduces the experimental setup including datasets, evaluation process, metrics and baseline. Section 5 describes the results and Section 6 discusses our findings to shed light on the future direction of continual learning in sensor-based HAR. Finally, Section 7 concludes the paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4"
        },
        {
            "text": "In this section, we define the setup for our continual learning problem in a task-incremental setting and briefly introduce the mainstream continual learning techniques.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "The task-incremental continual learning setting assumes tasks arriving in a sequential manner,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "where each task comprises one or more classes. Formally, let a task sequence T of N tasks be T = [t 1 , t 2 , t 3 , .., t N ], and a task t i (i \u2208 [1, .., N ]) is coupled with a set of classes C i = {c i 1 , ..., c i K i } and a collection of training data {(x i j , y i j )|y i j \u2208 C i } M i j=1 , where K i is the number of classes and M i is the number of training data in a task t i . The learning objective on the ith task is to optimise a model to classify the current classes in C i and all the previous classes in",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "i.e., each task has a mutually exclusive set of classes and new classes in the current task have not been observed in the previous tasks. The major problem in task-incremental setting resides in catastrophic forgetting (CF), where the new knowledge learnt by the model interferes with the old knowledge so that the performance on classifying old classes degrades over time. CF is caused by the stability-plasticity dilemma [1] .",
            "cite_spans": [
                {
                    "start": 423,
                    "end": 426,
                    "text": "[1]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "Plasticity refers to the ability of a model to accommodate new knowledge while stability refers to its ability to retain old knowledge. High plasticity often causes drastic changes in the model's parameters, thus interfering with the previous knowledge captured by them. Most of the existing continual learning techniques are trying to balance plasticity and stability, and these techniques are often grouped in three categories [56] : (1) regularisation, (2) rehearsal, and (3) dynamic architectures.",
            "cite_spans": [
                {
                    "start": 429,
                    "end": 433,
                    "text": "[56]",
                    "ref_id": "BIBREF66"
                }
            ],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "Regularisation techniques often introduce additional terms to the loss function that constrains the weight updates of the network so as to prevent compromising the performance on old tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "Knowledge Distillation (KD) [31] , a way to transfer knowledge between different networks, has been widely adopted to retain the knowledge of old tasks when learning new ones. It prevents the current model's output deviating from the previous model's prediction that is recorded as logit output as soft labels for old classes. Learning without forgetting (LwF) [45] is the earliest attempt to employ the distillation loss in continual learning, followed by many others such as cross-distilled loss [9] and attention distillation loss [21] .",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 32,
                    "text": "[31]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 361,
                    "end": 365,
                    "text": "[45]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 498,
                    "end": 501,
                    "text": "[9]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 534,
                    "end": 538,
                    "text": "[21]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "Another category of regularisation techniques is to identify important parameters for old tasks and penalises updates on them. For example, Elastic weight consolidation (EWC) applies Fisher Information matrix to measure the importance of the network parameters [39] . Memory Aware Synapses (MAS) estimates the importance of a parameter based on the magnitude of the gradient; that is, how much change the output of the network is caused by a small perturbation to the parameter [4] .",
            "cite_spans": [
                {
                    "start": 261,
                    "end": 265,
                    "text": "[39]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 478,
                    "end": 481,
                    "text": "[4]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "Group sparsity regularisation has been applied to allow for selective training a subset of neurons.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "Adaptive Group Sparsity based Continual learning (AGS-CL) [36] introduces regularisation terms using two node-wise group sparsity based penalties. The first term assigns and learns new important nodes via the ordinary group Lasso penalty when learning a new task, while the second term applies the group-sparsity based deviation penalty to prevents the drift on important node parameters.",
            "cite_spans": [
                {
                    "start": 58,
                    "end": 62,
                    "text": "[36]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "Specific regularisation terms have been designed to tackle specific problems. Weight alignment has been employed to balance the distribution of new tasks' training samples and old tasks' inmemory samples [38, 83] . Customised regularisation terms have been designed to prevent task interference; for example, forcing a large margin between the old and new classes [32] .",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 208,
                    "text": "[38,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 209,
                    "end": 212,
                    "text": "83]",
                    "ref_id": "BIBREF94"
                },
                {
                    "start": 364,
                    "end": 368,
                    "text": "[32]",
                    "ref_id": "BIBREF42"
                }
            ],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "Rehearsal techniques mitigate catastrophic forgetting by mixing data from previous tasks with the current task. The previous task data can be used as inputs for re-training the network or as a constraint to the network updates for penalising the interference with the previous tasks. iCaRL, 6 a class-incremental learning technique, stores a small set of representative samples for each class in memory, and combines these samples with new task data to update the network every time [60] .",
            "cite_spans": [
                {
                    "start": 483,
                    "end": 487,
                    "text": "[60]",
                    "ref_id": "BIBREF70"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Techniques"
        },
        {
            "text": "This type of techniques often requires extra memory space for old task samples and also can be prone to overfitting the stored data, instead of generalising to old tasks [42] . REMIND utilises data compression and augmentation to enable more effective replay [29] .",
            "cite_spans": [
                {
                    "start": 170,
                    "end": 174,
                    "text": "[42]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 259,
                    "end": 263,
                    "text": "[29]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Techniques"
        },
        {
            "text": "In contrast, gradient episodic memory (GEM) uses these old task samples to impose a constraint to allow positive backward transfer; that is, preventing the gradient update from increasing the loss on previous tasks [49] . Similarly, orthogonal gradient descent (OGD) maintains a space consisting of the gradient directions of the network predictions on previous tasks and projects the loss gradients of new samples perpendicular to this gradient before backpropagation [23] . In this way, OGD minimises the interference on old tasks and thus preserves old knowledge.",
            "cite_spans": [
                {
                    "start": 215,
                    "end": 219,
                    "text": "[49]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 469,
                    "end": 473,
                    "text": "[23]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Techniques"
        },
        {
            "text": "In addition to sampling from training data, in-memory samples on previous tasks can also be generated via a generative model that learns the distribution of old tasks [55, 63, 66, 81] . Generative replay model (GRM) [66] employs a Generative Adversarial Network (GAN) for generating samples on previous tasks for pseudo rehearsal. This type of techniques relies on the quality of generated samples; for example, GANs might suffer mode collapse in that the generated samples are clustered in one specific space, rather than diverse across the whole space. Also training GANs can add extra computation cost to training and GANs' performance can degrade over time with more and more classes being learnt [42] .",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 171,
                    "text": "[55,",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 172,
                    "end": 175,
                    "text": "63,",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 176,
                    "end": 179,
                    "text": "66,",
                    "ref_id": "BIBREF76"
                },
                {
                    "start": 180,
                    "end": 183,
                    "text": "81]",
                    "ref_id": "BIBREF92"
                },
                {
                    "start": 216,
                    "end": 220,
                    "text": "[66]",
                    "ref_id": "BIBREF76"
                },
                {
                    "start": 701,
                    "end": 705,
                    "text": "[42]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Techniques"
        },
        {
            "text": "Dynamic architectures tackle catastrophic forgetting by retaining the model on the old tasks while extending it with new parameters to learn new tasks. They are closely tied with ensemble methods which train multiple models for different tasks. ExpertGate [5] consists of a network of experts where each expert is a model trained on a specific task. A gating mechanism decides which expert is required for activation. This bypasses the need for loading all models, which is memory efficient as each model can be computationally intensive [15] . Net2Net [14] is another type of dynamically evolving network which can be widened (adding more neurons) and deepened (adding more layers). Knowledge from the previous network is preserved in the newly constructed network.",
            "cite_spans": [
                {
                    "start": 256,
                    "end": 259,
                    "text": "[5]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 538,
                    "end": 542,
                    "text": "[15]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 553,
                    "end": 557,
                    "text": "[14]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Dynamic Architectures and Ensemble Techniques"
        },
        {
            "text": "Progressive network [65] keeps a pool of models that are pre-trained with previous knowledge and adds lateral connections to them for a new task [15] . To mitigate forgetting, the parameters for previous tasks are never modified while new parameters are learned for the new task. Therefore, it does not deteriorate the performance of previous tasks. One drawback of using this technique is that the network can become complex with an increasing number of tasks learned. Since a new network is learned for each task and it needs to be connected to the previous network, the complexity of the network structure and parameters increases quickly [15] .",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 24,
                    "text": "[65]",
                    "ref_id": null
                },
                {
                    "start": 145,
                    "end": 149,
                    "text": "[15]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 642,
                    "end": 646,
                    "text": "[15]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Dynamic Architectures and Ensemble Techniques"
        },
        {
            "text": "Dynamically Expandable Networks (DEN) [79] allows layer expansion and employs group sparsity regularisation to identify the neurons that are relevant to new tasks and allow selective retraining on these neurons. Compacting, Picking, and Growing (CPG) continual learning [33] adds new neurons for accommodating new tasks and then constantly applies network compression by deleting unnecessary weights.",
            "cite_spans": [
                {
                    "start": 38,
                    "end": 42,
                    "text": "[79]",
                    "ref_id": "BIBREF90"
                },
                {
                    "start": 270,
                    "end": 274,
                    "text": "[33]",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [],
            "section": "Dynamic Architectures and Ensemble Techniques"
        },
        {
            "text": "Dynamic architectures often have been employed in computer vision applications, where there exist hundreds or thousands of classes. As our selected HAR datasets do not have such a high number of activities to learn and a decent size of a neural network often works well, we exclude this category of techniques in our study.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dynamic Architectures and Ensemble Techniques"
        },
        {
            "text": "After presenting the overview of continual learning techniques in the previous section, now we will focus on a small set of techniques. We set three criteria for selecting state-of-the-art continual learning techniques: (1) the techniques are built on neural networks and target classification tasks, as neural networks have been widely adopted in HAR tasks for their effectiveness at feature extraction and recognition [70] , (2) the techniques should be the most representative ones, which have been included in several recent continual learning surveys [37, 42, 56] , and (3) the techniques target HAR limitations including class imbalance and inter-class separation. With these criteria, we select 10 techniques from regularisation and rehearsal categories and in the following, we will provide a brief description of each and illustrate their key characteristics.",
            "cite_spans": [
                {
                    "start": 420,
                    "end": 424,
                    "text": "[70]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 556,
                    "end": 560,
                    "text": "[37,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 561,
                    "end": 564,
                    "text": "42,",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 565,
                    "end": 568,
                    "text": "56]",
                    "ref_id": "BIBREF66"
                }
            ],
            "ref_spans": [],
            "section": "Comparison of Techniques"
        },
        {
            "text": "Learning without Forgetting [44] aims to keep the output of the old tasks from the new network close to the output from the original model. This is achieved by using the knowledge distillation KD loss as a regularisation term. KD is a way to transfer knowledge from a complex teacher model to a simpler student model by minimising the loss on the output class probabilities from the teacher model [31] . KD has been widely applied to various continual learning techniques [9, 21, 44] to distil knowledge learnt from old tasks to the model for new tasks, which is defined as follows:",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 32,
                    "text": "[44]",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 397,
                    "end": 401,
                    "text": "[31]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 472,
                    "end": 475,
                    "text": "[9,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 476,
                    "end": 479,
                    "text": "21,",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 480,
                    "end": 483,
                    "text": "44]",
                    "ref_id": "BIBREF54"
                }
            ],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "where l is the number of class labels, and y",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "o are temperature-scaled recorded and predicted probabilities of the current sample for an old class label i. The temperature is used to tackle the over-confident probability that the teacher model produces on their prediction. That is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "The loss L KD is combined with the cross-entropy loss on a new task's samples L n CE ; that is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "L n CE (y n ,\u0177 n ) = \u2212y n log\u0177 n ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "where y n is the one-hot encoded vector of the ground-truth label,\u0177 n is the predicted logit (i.e., the softmax output of the network) on new class labels, and \u03bb o is a loss balance weight. A larger \u03bb o will favour the old task performance over the new task. During training, a new batch gets first fed through the old network to record its outputs, which then are used for L KD so that the network updates will not deviate from the old network.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "Elastic Weight Consolidation (EWC) [39] draws inspiration from research on mammalian brains, which suggests that catastrophic forgetting can be avoided through the protection of synapses. EWC tries to mimic this by inhibiting changes on parameters that are deemed important for previous tasks. The importance of parameters is modelled as the posterior distribution p(\u03b8|D); that is, optimising the parameters is to find their most probable values with respect to some data D. In the context of continual learning, if the data D is assumed to consist of two independent tasks A and B, then",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 39,
                    "text": "[39]",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "The posterior probability p(\u03b8|D A ) contains information about which parameters are important to task A. However, the true probability p(\u03b8|D A ) is intractable, and thus it is estimated via Laplace approximation [51] as a Gaussian with diagonal precision determined by the Fisher Information Matrix (FIM). The loss function of EWC is defined as:",
            "cite_spans": [
                {
                    "start": 212,
                    "end": 216,
                    "text": "[51]",
                    "ref_id": "BIBREF61"
                }
            ],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "where L B CE is the loss on the new task B only, \u03bb indicates the importance of the old task A with respect to B, and i is the parameter index. Originally, one FIM is required for each task, and later it can be resolved by propagating them into a quadratic penalty term [34] . However, this formulation assumes the FIM to be diagonal, which is not always the case. Rotating EWC (R-EWC) [48] improves upon EWC by reparameterising \u03b8 through rotation in a way that it does not change outputs of the forward pass but the computed FIM is approximately diagonal.",
            "cite_spans": [
                {
                    "start": 269,
                    "end": 273,
                    "text": "[34]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 385,
                    "end": 389,
                    "text": "[48]",
                    "ref_id": "BIBREF58"
                }
            ],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "Memory Aware Synapse (MAS) [4] , inspired by neuroplasticity, also considers the importance of network parameters, which is measured in an online, unsupervised manner. Here, the importance is approximated by the sensitivity of the learned function to a parameter change. Given a data point x k , the network output is defined as F (x k ; \u03b8). A change in the network output caused by a small perturbation \u03b4 = {\u03b4 ij } in the parameters \u03b8 = {\u03b8 ij } can be approximated as:",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 30,
                    "text": "[4]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "where g is the gradient with respect to the parameter \u03b8. By accumulating gradients over all the data points, the importance weight on a parameter \u03b8 ij is computed as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "When learning a new task, the loss function is defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "where L n CE (\u03b8) is the CE loss on the new task, \u03b8 ij and \u03b8 * ij are the new and old network parameters, and \u03bb is a hyperparameter that indicates the regularisation strength. The idea is to retain the old knowledge by penalising large updates on the sensitive network parameters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularisation Techniques"
        },
        {
            "text": "Incremental Classifier and Representation Learning (iCaRL) [60] is an early attempt of rehearsal approaches for class-incremental learning. It leverages memory replay and regularisation. After training on each task, it employs the herding sampling technique [71] to select a small number of exemplars (i.e., representative samples) from the current task's training data and stores them in memory. Herding works by choosing samples that are closest to the centroid of each old class.",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 63,
                    "text": "[60]",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 258,
                    "end": 262,
                    "text": "[71]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "When the next task becomes available, the in-memory exemplars will be combined with the new task's training data to update the network. The loss function of iCaRL is the same as Equation 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "in LwF, which is a combination of CE loss on new classes and the KD loss on old classes to allow knowledge transfer.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "Incremental Learning In Online Scenario (ILOS) [30] , similar to iCaRL, also employs memory replay with KD loss for regularisation. The key difference is that ILOS uses an updated version of the CE loss. It introduces an accommodation ratio 0 \u2264 \u03b2 \u2264 1 to adjust the proportion of logits between the current and previous model:",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 51,
                    "text": "[30]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "where the indices [1, n] refer to old classes, [n + 1, n + m] refer to new classes, y (i) are the one-hot encoded output logits of the current model and\u0177 (i) are the recorded old classes' output logits from the previous model. In this way, the output from the previous model will be retained in the current network and the retaining degree is regulated via the parameter \u03b2. The larger the \u03b2, the more retained the output from the previous model. While the KD loss is still calculated using y (i) , the CE loss is now based on the adjusted output\u1ef9 (i) instead of the recorded output\u0177 (i) :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "The final loss is the combination of the above CE loss and KD loss:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "Following the guideline in the original paper, both \u03b1 and \u03b2 can be set to 0.5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "Gradient Episodic Memory (GEM) [49] alleviates forgetting by controlling the gradient updates to balance the performance on old and new classes. GEM uses a memory space to host examples from previous classes \u222a k\u2208 [1,t\u22121] M k , where M k is the samples stored on a previous task k (k \u2264 t) and t is the current task's index. When training a new task t, M k is used in an inequality constraint to avoid an increase in loss. Hence, the loss of a current task L must be smaller than or equal to the loss of the previous model:",
            "cite_spans": [
                {
                    "start": 31,
                    "end": 35,
                    "text": "[49]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 213,
                    "end": 220,
                    "text": "[1,t\u22121]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "where f \u03b8 represents the model with parameters \u03b8. To achieve this, GEM restricts the angle between gradient vectors g k of previous and current task g t to be no greater than 90 \u2022 . If the angle is greater than 90 \u2022 , the new gradient vector gets projected to the euclidean distance closest vectorg that is inside the allowed range. Thus, the optimisation problem can be defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "Solving this quadratic program problem for all in-memory samples is very computationally expensive. To address the computational cost, A-GEM [11] is proposed to ensure that there is no increase in the average loss over the episodic memory. That is, A-GEM only uses a smaller set of in-memory samples to calculate an average gradient instead of all gradients for a task.",
            "cite_spans": [
                {
                    "start": 141,
                    "end": 145,
                    "text": "[11]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "Learning a Unified Classifier Incrementally via Rebalancing (LUCIR) [32] was proposed to tackle the imbalance between a small number of in-memory samples from old tasks and a larger number of samples from a new task. The imbalance results in the training being biased towards new tasks. LUCIR proposes multiple (loss-) components to mitigate this adverse effect. Firstly, cosine normalisation is used in the last layer to level the difference of the embeddings and biases between all classes since those are significantly higher for new tasks. Secondly, less-forget constraint is introduced to prevent forgetting old classes' geometric configurations by encouraging the extracted features of new classes similarly rotated to those of old ones; that is:",
            "cite_spans": [
                {
                    "start": 68,
                    "end": 72,
                    "text": "[32]",
                    "ref_id": "BIBREF42"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "wheref (x) andf * (x) are normalised features generated by the new and old model, and , measures the cosine similarity between the two normalised vectors. Thirdly, a margin ranking loss is used to enhance inter-class separation. For each in-memory sample x, it aims to separate old classes from all the new classes by a margin. Using x as an anchor, LUCIR finds positive and negative embeddings and aims to maximise their distances. The positive embeddings are from the ground-truth class of",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "x (represented as\u03b8(x)), while the negative embeddings are from the top-K new classes that produce the highest response to x (represented as\u03b8 k ), indicating the classes that x is mostly confused with.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "Then the margin ranking loss is defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "This leads to the combined loss function as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "where N refers to all the training samples and ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "where C b is the total number of classes and C b o is the number of old classes. Their norms are defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "The normalised weights on new classes are W n = \u03b3W n , where \u03b3 = M ean(N ormo) M ean(N ormn) . In this way, the average norm of the weights on the new classes will be the same as that on the old classes. The corrected output logits from the FC layer will be written as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "As a norm-control method; i.e., keeping a check on the norms of class embeddings following each gradient update, WA-MDF clamps the parameters of the FC layer at zero. Assuming that a network usually employs variants of ReLU activation units, the clipping facilitates the projection of weights. This helps eliminate large negative elements of weight vectors thus making its norm more consistent with the non-negative output logits of the ReLU function. While such a restriction seems to interfere with the convergence of the model, several studies have shown that training neural networks with weights projected via such distortions makes them robust to other types of distortions [18, 53] . Here, distortion refers to different ways of weight projection operations; for example, adding Gaussian noise or performing additive, multiplicative, or power operations on weights [53] . Such distortion allows weights to accumulate small gradient updates and thus leads to improved performance.",
            "cite_spans": [
                {
                    "start": 680,
                    "end": 684,
                    "text": "[18,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 685,
                    "end": 688,
                    "text": "53]",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 872,
                    "end": 876,
                    "text": "[53]",
                    "ref_id": "BIBREF63"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "Weight Alignment for Adjusting Decision Boundary (WA-ADB) is also proposed to rescale the weight vectors of majority and minority classes, but in a more general class-imbalance scenario [38] . Different from the above WA-MDF, WA-ADB re-scales the weights based on the sample ratio of old and new classes instead of their weight norms. For a dataset D with K classes, given that n i is the number of samples of class i (n 1 \u2265 ...n i ... \u2265 n K ), the weight vectors of each class are re-scaled by an exponent of the re-scaling factor n 1 /n i ; i.e., w i = ( n 1 n i ) \u03b3 .w i . During training, the weight vectors will be normalised after each gradient update. While a larger \u03b3 scales up the volume of feature space of a model allocated to infrequent classes, weight vector normalisation forces the class conditional probability to have the same variance independent of the sample size.",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 190,
                    "text": "[38]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "To adapt WA-ADB to incremental learning scenario, we make the following adjustment. For an incremental training step i, the dataset D is the combination of in-memory samples on old tasks and new training samples on a new task. We then re-scale the weights of all the classes of old tasks by the same factor, i.e., n 1 /n i , where n 1 is the number of samples on the largest class that has been seen so far and n i is the holdout size. Because n i is fixed due to the memory size, we only compute this number once. While the bias term is dropped in the original work [38] , we take into account that the biases for classes of new tasks also have norms larger in average than that of classes from old tasks. As a result, we re-scale the biases of each class by the corresponding weight re-scaling factor computed in the above step.",
            "cite_spans": [
                {
                    "start": 567,
                    "end": 571,
                    "text": "[38]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "Bias Correction (BiC) [73] introduces a BiC correction layer after the last FC layer to adjust the weights in order to tackle the imbalance problem. There are two stages of training. In the first stage, the network will be trained with new task data and in-memory samples of old tasks using the CE and KD loss similar to iCaRL. In the second stage, the layers of the network are frozen ",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 26,
                    "text": "[73]",
                    "ref_id": "BIBREF83"
                }
            ],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "where \u03b4 y=k is the indicator function to check if the ground-truth label y is the same as a class label k. The intuition is that a balanced small validation set for all seen classes can counter the accumulated bias during the training of the new task. As the non-linearities in the data are already captured by the model's parameters, the linear function proves to be quite effective in estimating the bias in the output logits of the model arising.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "So far we have described all the 10 selected continual learning techniques and in the next section, we will introduce the evaluation framework and methodology for assessing and comparing them on the HAR datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rehearsal Technique"
        },
        {
            "text": "The main objective of this paper is to assess to what degree the state-of-the-art continual learning techniques can enable continual activity recognition. More specifically, we will seek to answer the following questions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup and Evaluation Methodology"
        },
        {
            "text": "1. What is the performance of these continual learning techniques on HAR datasets? 2. Which techniques best balance plasticity and stability when incrementally learning new activities?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup and Evaluation Methodology"
        },
        {
            "text": "3. Which regularisation term works best for what type of data? 4. What is the impact of in-memory sample sizes on the accuracy of these techniques? 5. Are these techniques sensitive to the size of training data? 6. What is the computation cost of these techniques in terms of memory and training time?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup and Evaluation Methodology"
        },
        {
            "text": "To answer these questions, we have selected 8 HAR datasets (in Section 4.1), design the evaluation process (in Section 4.2), select the evaluation metrics (in Section 4.3) that are appropriate for HAR, describe the baseline approach for comparison (in Section 4.4), and illustrate the architecture configuration and hyperparameter tuning for all the techniques (in Section 4.5).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup and Evaluation Methodology"
        },
        {
            "text": "To present a comprehensive profile of selected continual learning techniques, we will assess on a wide variety of most representative, state-of-the-art HAR datasets, ranging from simple datasets (with a single user, a small number of features and activity classes) to more complex datasets (with multiple users, a large number of features and activity classes). Driven by these criteria, we select the following set of publicly available, third-party datasets that are collected on accelerometer and event-driven binary sensor data, as these two are the most common sensor types in HAR. Table 1 and Figure 3 summarise the main characteristics of these datasets. these datasets, we apply state-of-the-art techniques [22] to segment the raw sensor data into a 60-second interval and generate features as an activation ratio of each sensor in the interval. There are 561 features extracted from accelerometer and gyroscope readings.",
            "cite_spans": [
                {
                    "start": 715,
                    "end": 719,
                    "text": "[22]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [
                {
                    "start": 587,
                    "end": 594,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 599,
                    "end": 607,
                    "text": "Figure 3",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "We follow the state-of-the-art task-incremental evaluation methodology [67] , where we assign 2 randomly sampled classes to each task and form a sequence of |C|/2 consecutive tasks, where |C| is the total number of classes in a dataset. In practice, the number of classes in each task can vary, depending on the arrival of new classes and update cycle of the model. Here, we set the number of classes as 2, as a good trade-off to assess the incremental learning capability and the training time.",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[67]",
                    "ref_id": "BIBREF77"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Process"
        },
        {
            "text": "Also, we employ the stratified train-test split [67] to split each class's data into 70% for training and 30% for testing. As accelerometer datasets often have multiple users, to avoid data leakage, we split training and testing data of each class by users; that is, we use 70% of users' data for training and the remaining 30% of users' data for testing.",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 52,
                    "text": "[67]",
                    "ref_id": "BIBREF77"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Process"
        },
        {
            "text": "Given a task sequence, the network will be initialised and trained with the first task's training data. Then for each subsequent task, the network's output layer will be extended to include its new classes and the network will be trained with the task's training data and in-memory data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Process"
        },
        {
            "text": "The in-memory data are sampled from all the previous tasks' training data. The size of in-memory data is determined by the memory constraint of a particular HAR system, and it will contribute to retaining old knowledge and thus affect the accuracy of old class classification. Figure 4 presents an example of task-incremental evaluation on a HAR dataset. As the performance of continual learning can be subject to task sequences, to reduce the bias, we generate several random task sequences (i.e., 30) and report averaged accuracy.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 277,
                    "end": 285,
                    "text": "Figure 4",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Evaluation Process"
        },
        {
            "text": "When training a new task, we compute three types of accuracy. (1) Base accuracy -the accuracy of recognising the activity classes in the first task; (2) Old accuracy -the accuracy of recognising all the old activity classes that have learnt before the current task. Both base and old accuracy will indicate the stability of the model; (3) New accuracy -the accuracy of recognising the new activity classes in the current task, which will indicate the plasticity of the model; and (4) All accuracythe accuracy of recognising all the activity classes that have learned so far, which will indicate the overall performance of the model. The accuracy is measured in F1-scores, which balances precision and recall. As most of the HAR datasets have an imbalanced class distribution, we use F1-macro and F1-micro [25] .",
            "cite_spans": [
                {
                    "start": 805,
                    "end": 809,
                    "text": "[25]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "To understand the retention of knowledge for a given task j at an incremental task k, we employ a commonly used measure -forgetting score (FS) [10] . After a model is trained incrementally till task k > j, the forgetting score f j k is computed as the difference between the maximum accuracy gained for the task j throughout the learning process. However, this score does not concern with the difficulty level of each task. For example, the accuracy decreasing from 60% to 40% on a difficult task suggests more forgetting than the accuracy decreasing from 90% to 70% on an easy task. To address this limitation, we propose an adapted forgetting score by normalising it with the best accuracy that can be achieved on a task. That is,",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 147,
                    "text": "[10]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "where a k,j is the accuracy for a previous task j at the current task k 2 . Finally, the average forgetting F S k at k\u2212th task is normalised against the number of tasks seen previously, i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "A large F S score implies server forgetting. At the extreme, when F S is 1, it suggests that the knowledge on the old task is completely forgotten.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "Besides the selected models, we consider two baselines that serve as an upper and lower bound ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baseline"
        },
        {
            "text": "While the original implementations of our selected techniques use a variety of architectures 3 and classifiers 4 , we try to maintain a fair comparison premise by using the fully connected networks across all our experiments. To retain the best characteristics of these techniques, we follow the state-of-the-art methodology [10, 37] to conduct grid search for the network architecture on each dataset. Table 2 provides a list of optimised configurations. The initial learning rate (LR) is reduced by a factor of 10 after every scheduler step following the first scheduling epoch. For the choice of the 2 We ignore classes whose maximum accuracy is not greater than 0, avoiding the arithmetic error. 3 Most of the vision-based methods use standard CNN architectures. 4 For instance, iCaRL uses nearest-of-mean classifier.",
            "cite_spans": [
                {
                    "start": 325,
                    "end": 329,
                    "text": "[10,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 330,
                    "end": 333,
                    "text": "37]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 603,
                    "end": 604,
                    "text": "2",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 403,
                    "end": 410,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Architecture Configuration"
        },
        {
            "text": "number of neurons in the hidden layers, we rely on fractions of feature dimensions of each dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "19"
        },
        {
            "text": "For the particular case of training GEM, we tune the initial learning rate to 0.01 for TWOR and ARUBA, and to 0.001 for DSADS, as a larger learning rate makes it difficult to converge and leads to low accuracy. The scheduling epochs for TWOR and ARUBA are set to 40 and that for DSADS to 50 while the wight decay rates are set to 1e \u22126 , 1e \u22124 and 2e \u22126 for TWOR, DSADS and ARUBA respectively. For each comparison technique, we run grid search on their own hyperparameters, and select the best value that leads to the highest accuracy on each dataset. We list the setting in the following. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "19"
        },
        {
            "text": "This section presents the results in response to the research questions raised in Section 4. For each question, we summarise the key results in bold, followed by the analysis. Table 3 and 4 report the mean and standard deviation of micro-and macro-F1 scores on 10 selected and 2 baseline models across 8 datasets over 30 runs. The last column of these two tables average the accuracy across all the datasets and demonstrates which technique works best. For all the models that use memory replay, we randomly sample 6 samples from each old class after training each task.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 176,
                    "end": 183,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "The offline baseline provides the reference accuracy which implies the difficulty level of each dataset. As shown in Table 3 and 4, the datasets that have more balanced activity distribution and easier-to-separate classes gain higher micro-and macro-F1 scores; e.g., WS (94.14% and 87.41% in micro-and macro-F1) and PAMAP2 (94.91% and 84.78% in micro-and macro-F1). The imbalanced datasets often result in higher micro-F1 but lower macro-F1; e.g., ARUBA (96.82% and 58.82% in micro-and macro-F1) and TWOR (77.20% and 45.35% in micro-and macro-F1). As lower bound, the finetuning baseline suffers the most catastrophic forgetting, with the overall micro-F1 9.51% and macro-F1 2.11% averaged across all the datasets, which are significantly lower than the accuracy achieved from offline. In comparison with the above baselines, we draw the following observations on the selected continual learning techniques. Rehearsal methods significantly outperform regularisation-alone methods, as the averaged difference in micro-and macro-F1 is around 50%. More specifically, from R-EWC to iCaRL, micro-F1 increases from 11.20% to 63.36% in Table 3 , and macro-F1 from 3% to 52.59% in Table 4 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 117,
                    "end": 124,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 1129,
                    "end": 1136,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 1173,
                    "end": 1180,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Overall Comparison"
        },
        {
            "text": "Regularisation alone does not help retain the knowledge of the original model as LwF, MAS and EWC only produce roughly the lower bound accuracy. For example, the success of LwF depends on the similarity of new tasks to old tasks. Distribution shift between old and new tasks will result in a discrepancy in the KD loss when predicting the class probability using the old model. The errors are accumulated over incremental learning and will significantly impact its performance [15, 30, 42] .",
            "cite_spans": [
                {
                    "start": 477,
                    "end": 481,
                    "text": "[15,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 482,
                    "end": 485,
                    "text": "30,",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 486,
                    "end": 489,
                    "text": "42]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [],
            "section": "Overall Comparison"
        },
        {
            "text": "In HAR, each activity class can have a drastically different sensor feature signature, so in our experiment these regularisation-only methods perform much worse than they do in computer vision experiments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Overall Comparison"
        },
        {
            "text": "Among rehearsal methods, ILOS, WA-MDF, and WA-ADB perform the best, improving on the basic rehearsal method iCaRL over 13% in micro-F1 and 7% in macro-F1. With memory replay, retraining the model is often affected by the imbalance between a large number of new task's data and a small number of in-memory samples. These methods have effectively avoided optimising the model towards the majority classes and thus better retained the knowledge of the old classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Overall Comparison"
        },
        {
            "text": "GEM does not perform better than iCaRL, as it only achieves an increase of 7% in micro-F1 and the same macro-F1 score as iCaRL. We have also considered the improved version of GEM:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Overall Comparison"
        },
        {
            "text": "A-GEM [11] . However, in our experiments, A-GEM often produces worse accuracy than GEM;",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 10,
                    "text": "[11]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Overall Comparison"
        },
        {
            "text": "for example, on PAMAP2 dataset, A-GEM achieves micro-F1 of 37.86% and macro-F1 of 28.35%, which is more than 30% lower than GEM. One possible reason is that activities can have diverse patterns, so when A-GEM down-samples the holdout data, it has an even smaller coverage of space and thus its accuracy is worse than GEM.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Overall Comparison"
        },
        {
            "text": "To look into stability and plasticity over incremental learning, Figure 5 presents task-level micro- To further inspect the catastrophic forgetting, we present the new forgetting score F S in Figure 6 . F S on these three regularisation methods is at the upper bound of 1.0 for the first task and stays much higher than the other techniques for the following tasks, suggesting that these techniques suffer an almost immediate total forgetting effect where they cannot come back from. The high forgetting scores of R-EWC conform to the finding of [37] stating that EWC-based methods are poor at learning new categories incrementally.",
            "cite_spans": [
                {
                    "start": 546,
                    "end": 550,
                    "text": "[37]",
                    "ref_id": "BIBREF47"
                }
            ],
            "ref_spans": [
                {
                    "start": 65,
                    "end": 73,
                    "text": "Figure 5",
                    "ref_id": null
                },
                {
                    "start": 192,
                    "end": 200,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "Balance between Stability and Plasticity"
        },
        {
            "text": "Among rehearsal-based methods, ILOS and WA-ADB exhibit the best knowledge retention as their averaged F S across the tasks on all the datasets is the lowest; e.g., the old classes only lose 20% of their best accuracy throughout continuous learning. In contrast, iCaRL and LUCIR are the worst with their F S being 30%, which is averaged over all the datasets in Figure 6 . GEM shows a relatively greater tendency of increase in F S as the number of incremental tasks grows. This is evident across the plots for DSADS, ARUBA, TWOR, WS and HA.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 361,
                    "end": 369,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "Balance between Stability and Plasticity"
        },
        {
            "text": "We also observe that after a certain number of incremental tasks, the inertia of forgetting on rehearsal methods is relieved. For instance, iCaRL, BiC, LUCIR and ILOS reach the threshold on ARUBA at the 3rd task while GEM and WA-ADB attain this at the 4th task following which their respective F S witnesses either a plateau or start decreasing. There is no clear correlation between the forgetting effect and the number of tasks being learnt, as it can be dataset-specific, especially the interference between the old and new activities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Balance between Stability and Plasticity"
        },
        {
            "text": "There exists a strong effect of the amount and distribution of training data attributes on forgetting. For the rehearsal methods, we can see that the datasets that have a long tail distribution (i.e., many activities have low frequency) have much higher forgetting scores. For example in Figure 3 , TWOR has 19 out of 23 activities whose occurrence is less than 6%, MILAN has 11 out of 15 activities whose occurrence is less than 8%, and ARUBA has 8 out of 11 activities whose occurrence is less than 3%. As shown in Figure 6 , F S of the rehearsal based techniques on these datasets is around 15% higher than those on the other datasets. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 288,
                    "end": 296,
                    "text": "Figure 3",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 517,
                    "end": 525,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "Balance between Stability and Plasticity"
        },
        {
            "text": "From the above results, we can see that regularisation alone does not demonstrate any advantage from a naive finetuning approach, but will their performance be improved when combined with memory replay? If so, which regularisation is more effective in HAR? To answer these two questions,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Effect of Regularisation"
        },
        {
            "text": "we design the following experiment that uses holdout samples with each regularisation term. More specifically, we look into the following settings: (1) replay. We draw the following observations. Firstly, LUCIR-MR and ILOS produce better accuracy than a simple cross-entropy loss. As shown in Figure 7 , ILOS and LUCIR-MR have produced averaged 77% and 61% in micro-F1, which are 13% and 7% higher than the plain CE loss. A possible reason is that LUCIR-MR is dedicated to separating classes and ILOS adjusts the logit output to balance the old and new classes.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 293,
                    "end": 301,
                    "text": "Figure 7",
                    "ref_id": "FIGREF14"
                }
            ],
            "section": "Effect of Regularisation"
        },
        {
            "text": "Secondly, the other regularisation terms do not improve the results from the CE loss. We can only see 5% and 3% improvement in micro-and macro-F1 from the other regularisation terms over CE in Figure 7 . It seems that memory replay and the regularisation terms are dealing with the same problem: interclass discrepancies. Therefore, there might not be a distinct advantage for regularisation. This finding is consistent with the latest theoretical work [40] . The terms that decrease the difference between micro-and macro-F1 the most are inhibiting the learning outcome if used in conjunction with memory replay. For example, ILOS and R-EWC have the lowest F1 scores with memory replay. Looking back at Figure 5 , we can see that ILOS is inhibiting new classes from being learned.",
            "cite_spans": [
                {
                    "start": 453,
                    "end": 457,
                    "text": "[40]",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [
                {
                    "start": 193,
                    "end": 201,
                    "text": "Figure 7",
                    "ref_id": "FIGREF14"
                },
                {
                    "start": 704,
                    "end": 712,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Effect of Regularisation"
        },
        {
            "text": "Since rehearsal methods can improve knowledge retention significantly, now the questions are (1) how many samples are needed to store in memory and (2) what sampling strategy is most effective for selecting these samples that are representative for old classes. To investigate these questions, we experiment different holdout sizes from 2 to 15 with a step size of 2 on widely adopted sampling techniques, including random sampling, herding [71] , exemplar sampling, Frank-Wolfe Sparse Representation (FWSR) sampling [16] , and boundary sampling. Herding is to select the top s samples that are the closest to the mean of each class [71] . FWSR sampling selects a subset of the data that effectively describes the entire data set. Exemplar sampling selects the centroid data points of each class. Boundary sampling [20] , a recent sampling technique for incremental learning, selects exemplars on the class decision boundary and overlapping region based on local geometrical and statistical information. Figure 8 compares the micro-F1 scores of sampling techniques on different sample sizes on 4 selected methods and 4 datasets.",
            "cite_spans": [
                {
                    "start": 441,
                    "end": 445,
                    "text": "[71]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 517,
                    "end": 521,
                    "text": "[16]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 633,
                    "end": 637,
                    "text": "[71]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 815,
                    "end": 819,
                    "text": "[20]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [
                {
                    "start": 1004,
                    "end": 1012,
                    "text": "Figure 8",
                    "ref_id": null
                }
            ],
            "section": "Effect of Sampling"
        },
        {
            "text": "In terms of the sampling techniques, random (in red), exemplar (in dark green), and herding (in light green) work better than the other two more complex techniques, as presented in Figure 8 . Boundary sampling (in orange) works the worst, whose accuracy often stays at the bottom. Sensor data often contains noise and outliers [13] , so it is difficult to characterise geometric shape or precise class boundary of an activity, and thus both boundary and FWSR do not work well. Exemplar and herding try to capture most representative samples, and work well when the holdout size is small; i.e., 2. Random sampling works generally well and is consistent with the results on images [72] , as randomness seems to have better coverage in the entirety of data space with minimal bias for certain data distributions. ",
            "cite_spans": [
                {
                    "start": 327,
                    "end": 331,
                    "text": "[13]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 679,
                    "end": 683,
                    "text": "[72]",
                    "ref_id": "BIBREF82"
                }
            ],
            "ref_spans": [
                {
                    "start": 181,
                    "end": 189,
                    "text": "Figure 8",
                    "ref_id": null
                }
            ],
            "section": "Effect of Sampling"
        },
        {
            "text": "Training data for new classes can be more difficult to acquire in a continual learning setting in HAR, as new classes are discovered when the system is already deployed and running. For example, it often relies on users' voluntary self-annotation when the sensor system is deployed in the real world. These experiments are designed to assess the impact of training data size on the selected techniques. We reduce the training data percentage from 70% to 10% with a step size of 20% and report their micro-and macro-F1 in Figure 9 . The results show that these techniques 28 are not sensitive to training data size, as the accuracy does not change after 30% training data.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 521,
                    "end": 529,
                    "text": "Figure 9",
                    "ref_id": "FIGREF16"
                }
            ],
            "section": "Effect of Training Data Sizes"
        },
        {
            "text": "For datasets with high imbalance (TWOR), the accuracy of the selected methods does not change, staying at 66% in micro-F1 and 36% in macro-F1. The reason is that some of the classes have very little samples; e.g., the activity 'R1_bath' only has 19 samples, and the increase in training percentage still does not lead to many samples. We observe that LUCIR decreases accuracy with the increase in training data. The reason is that TWOR has many difficult-to-separate activities, which makes it more challenging to find good anchors when more training data is available.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Effect of Training Data Sizes"
        },
        {
            "text": "Computation cost is an important consideration in HAR since devices are usually under memory and computational power constraints. Table 5 presents the computation time of all the selected methods on training a single incremental task and as well as the whole task sequence. For each dataset, we highlight the technique with the longest training time. We report the averaged training time in the last column for the overall comparison. All the training is performed on a modest computer with Intel Core i5 8400, 32GB memory, and 2\u00d7500GB SSD. As shown in Table 5 , GEM and LUCIR are the most expensive ones among the others, especially the incremental training time of GEM (i.e., 130s) is 6 times higher than finetuning on average (i.e., 20s). This is because since the quadratic optimisation in GEM is computationally expensive. LUCIR needs to find good anchors, which incurs an extra cost, so it takes significantly longer training time than the other 8 techniques. The rehearsal based methods take longer to compute than the methods purely based on regularisation. For example, LwF, as the simplest technique, is the least computationally expensive; that is, 20s longer in total training time than finetuning on average. Comparing the weight alignment techniques, BiC is more expensive as it faces the largest challenge of inter-class separation; that is, distinguishing meal preparation from one resident to another. A promising future direction in HAR is to introduce regularisation terms that enhance the discriminability of the network; e.g., the contrastive loss [19] that is dedicated to learning the difference between two sets of data.",
            "cite_spans": [
                {
                    "start": 1569,
                    "end": 1573,
                    "text": "[19]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [
                {
                    "start": 130,
                    "end": 137,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 553,
                    "end": 560,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Computation Cost Analysis"
        },
        {
            "text": "Except for GEM and LUCIR, the other selected methods take about twice of training time as finetuning. This is acceptable for most modern devices that run human activity recognition. In terms of memory requirements, as the performance of these methods is not sensitive to the holdout size, so we advise a small number (i.e., 2 or 4 data points per class) will be sufficient. Note that most of the regularisation methods require to store the network parameters in memory to compare before and after an update. For a large neural network that consists of a large number of parameters, this might even be considerably more costly than keeping samples. Also, in our experiments, there is no advantage of sophisticated sampling techniques over random sampling of our datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Computation Cost"
        },
        {
            "text": "The selected techniques are not sensitive to training data size, which is good for incremental learning when labelling new activities is even more difficult. In our experiments, 30% of the training data will be sufficient. However, given a large dataset, 30% still means a large number of samples; e.g., DSADS needs to label 144 samples per class. In the future, we could look into few-shot learning algorithms to further reduce the number of training samples on new classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scarcity of Labelled Data"
        },
        {
            "text": "This paper presents a comprehensive, empirical evaluation of recent continual learning techniques in a task-incremental setting. We seek answers to essential research questions in HAR.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        },
        {
            "text": "We find that rehearsal techniques will lead to the best performance on most of the selected HAR datasets. They can be computationally cheap and do not require much memory space. Sophisticated regularisation terms or gradient updates fall short on their promises. The regularisation terms that help to deal with imbalances and inter-class separation achieve more promising accuracy on HAR datasets. In the following, we will focus on the future developments that HAR techniques have to have.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        },
        {
            "text": "The state-of-the-art continual learning techniques have paved a promising future for continual learning in HAR. However, most of these techniques are either set in a non-realistic continual learning scenario such as permutation MNIST [67] , or in scenarios with distinct task boundaries.",
            "cite_spans": [
                {
                    "start": 234,
                    "end": 238,
                    "text": "[67]",
                    "ref_id": "BIBREF77"
                }
            ],
            "ref_spans": [],
            "section": "31"
        },
        {
            "text": "We envision a continual learning system in a real-world HAR deployment where new activities can occur spontaneously and interweave with the old activities. Therefore, it needs to be able to discover new activities first. This requires to combine the existing continual learning techniques with techniques for anomaly detection; i.e., detecting whether the current sensor data conforms to any existing activity pattern. In HAR, new activity discovery has been extensively studied [27, 75, 77] .",
            "cite_spans": [
                {
                    "start": 479,
                    "end": 483,
                    "text": "[27,",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 484,
                    "end": 487,
                    "text": "75,",
                    "ref_id": "BIBREF86"
                },
                {
                    "start": 488,
                    "end": 491,
                    "text": "77]",
                    "ref_id": "BIBREF88"
                }
            ],
            "ref_spans": [],
            "section": "31"
        },
        {
            "text": "The question is how to combine these two types of techniques and form a feedback loop, where a new activity is discovered and fed into a network for extension in an automated fashion without any human intervention.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "31"
        },
        {
            "text": "Furthermore, acquiring annotations on new tasks can be challenging, as they rely on user input, which can be unavailable or imprecise [28] . Unlike the existing scenario for most of the continual learning techniques, where the training data is abundant (e.g., 1000 or 2000 examples per class) and",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 138,
                    "text": "[28]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "31"
        },
        {
            "text": "well-annotated. In HAR, research interest lies into how to obtain annotations and how to produce robust HAR system in the face of imprecise, insufficient annotation [80] .",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 169,
                    "text": "[80]",
                    "ref_id": "BIBREF91"
                }
            ],
            "ref_spans": [],
            "section": "31"
        },
        {
            "text": "Due to sensor degradation, sensor readings will drift over time, and users' behaviour patterns may change due to their health condition. Both will lead to changes in the distributions of the activities over time. This can be considered as concept drift, a common problem in streaming data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "31"
        },
        {
            "text": "This adds complication when there is a need for not only learning new tasks but also adapting the model on old tasks. A recent approach that provides a self-constructing methodology to extract hidden layers and neurons from streaming data demonstrates its effectiveness in learning nonstationary data [58] .",
            "cite_spans": [
                {
                    "start": 301,
                    "end": 305,
                    "text": "[58]",
                    "ref_id": "BIBREF68"
                }
            ],
            "ref_spans": [],
            "section": "31"
        },
        {
            "text": "In the future, we believe that given the promising results on the existing continual learning techniques, the key challenge for HAR can move on to a system-level approach: how to discover new tasks where there is no clear task boundary, deal with sensor noise and concept drift, and more importantly, tackle noisy and scarce annotations on datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "31"
        }
    ],
    "bib_entries": {
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Memory retention -the synaptic stability versus plasticity dilemma",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wickliffe",
                    "suffix": ""
                },
                {
                    "first": "Anthony",
                    "middle": [],
                    "last": "Abraham",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Robins",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Trends in Neurosciences",
            "volume": "28",
            "issn": "2",
            "pages": "73--78",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A recurrence plot-based approach for parkinson's disease identification",
            "authors": [
                {
                    "first": "Albuquerque",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "Joao",
                    "middle": [
                        "P"
                    ],
                    "last": "Papa",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Future Generation Computer Systems",
            "volume": "94",
            "issn": "",
            "pages": "282--292",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Niklas Fasth, Marko Mihajlovic, and Jacob Norman. A machine learning approach to classify pedestrians' event based on imu and gps",
            "authors": [
                {
                    "first": "Staffan",
                    "middle": [],
                    "last": "Mobyen Uddin Ahmed",
                    "suffix": ""
                },
                {
                    "first": "Alexander",
                    "middle": [],
                    "last": "Brickman",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dengg",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Journal of Artificial Intelligence",
            "volume": "16",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Memory Aware Synapses: Learning What (not) to Forget. ECCV",
            "authors": [
                {
                    "first": "Rahaf",
                    "middle": [],
                    "last": "Aljundi",
                    "suffix": ""
                },
                {
                    "first": "Francesca",
                    "middle": [],
                    "last": "Babiloni",
                    "suffix": ""
                },
                {
                    "first": "Mohamed",
                    "middle": [],
                    "last": "Elhoseiny",
                    "suffix": ""
                },
                {
                    "first": "Marcus",
                    "middle": [],
                    "last": "Rohrbach",
                    "suffix": ""
                },
                {
                    "first": "Tinne",
                    "middle": [],
                    "last": "Tuytelaars",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Expert gate: Lifelong learning with a network of experts",
            "authors": [
                {
                    "first": "Rahaf",
                    "middle": [],
                    "last": "Aljundi",
                    "suffix": ""
                },
                {
                    "first": "Punarjay",
                    "middle": [],
                    "last": "Chakravarty",
                    "suffix": ""
                },
                {
                    "first": "Tinne",
                    "middle": [],
                    "last": "Tuytelaars",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "7120--7129",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Human activity recognition using inertial/magnetic sensor units",
            "authors": [
                {
                    "first": "Kerem",
                    "middle": [],
                    "last": "Altun",
                    "suffix": ""
                },
                {
                    "first": "Billur",
                    "middle": [],
                    "last": "Barshan",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "HBU'10",
            "volume": "",
            "issn": "",
            "pages": "38--51",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Comparative study on classifying human activities with miniature inertial and magnetic sensors",
            "authors": [
                {
                    "first": "Billur",
                    "middle": [],
                    "last": "Kerem Altun",
                    "suffix": ""
                },
                {
                    "first": "Orkun",
                    "middle": [],
                    "last": "Barshan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Tunael",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Pattern Recognition",
            "volume": "43",
            "issn": "10",
            "pages": "3605--3620",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Human activity recognition in indoor environments by means of fusing information extracted from intensity of wifi signal and accelerations",
            "authors": [
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Alvarez-Alvarez",
                    "suffix": ""
                },
                {
                    "first": "Jose",
                    "middle": [
                        "M"
                    ],
                    "last": "Alonso",
                    "suffix": ""
                },
                {
                    "first": "Gracian",
                    "middle": [],
                    "last": "Trivino",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Information Sciences",
            "volume": "233",
            "issn": "",
            "pages": "162--182",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "End-toend incremental learning",
            "authors": [
                {
                    "first": "Francisco",
                    "middle": [
                        "M"
                    ],
                    "last": "Castro",
                    "suffix": ""
                },
                {
                    "first": "Manuel",
                    "middle": [
                        "J"
                    ],
                    "last": "Mar\u00edn-Jim\u00e9nez",
                    "suffix": ""
                },
                {
                    "first": "Nicol\u00e1s",
                    "middle": [],
                    "last": "Guil",
                    "suffix": ""
                },
                {
                    "first": "Cordelia",
                    "middle": [],
                    "last": "Schmid",
                    "suffix": ""
                },
                {
                    "first": "Karteek",
                    "middle": [],
                    "last": "Alahari",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Computer Vision -ECCV 2018",
            "volume": "",
            "issn": "",
            "pages": "241--257",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence",
            "authors": [
                {
                    "first": "Arslan",
                    "middle": [],
                    "last": "Chaudhry",
                    "suffix": ""
                },
                {
                    "first": "Puneet",
                    "middle": [
                        "K"
                    ],
                    "last": "Dokania",
                    "suffix": ""
                },
                {
                    "first": "Thalaiyasingam",
                    "middle": [],
                    "last": "Ajanthan",
                    "suffix": ""
                },
                {
                    "first": "Philip",
                    "middle": [
                        "H S"
                    ],
                    "last": "Torr",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Efficient lifelong learning with A-GEM",
            "authors": [
                {
                    "first": "Arslan",
                    "middle": [],
                    "last": "Chaudhry",
                    "suffix": ""
                },
                {
                    "first": "Ranzato",
                    "middle": [],
                    "last": "Marc&apos;aurelio",
                    "suffix": ""
                },
                {
                    "first": "Marcus",
                    "middle": [],
                    "last": "Rohrbach",
                    "suffix": ""
                },
                {
                    "first": "Mohamed",
                    "middle": [],
                    "last": "Elhoseiny",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "7th International Conference on Learning Representations, ICLR 2019",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Deep learning for sensor-based human activity recognition: Overview, challenges and opportunities. ArXiv, abs",
            "authors": [
                {
                    "first": "Kaixuan",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Dalin",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "Bin",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "A hypergrid based adaptive learning method for detecting data faults in wireless sensor networks",
            "authors": [
                {
                    "first": "Lingqiang",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Guanghui",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Guangyan",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Information Sciences",
            "volume": "553",
            "issn": "",
            "pages": "49--65",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Net2net: Accelerating learning via knowledge transfer. arXiv e-prints",
            "authors": [
                {
                    "first": "Tianqi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Ian",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "Jonathon",
                    "middle": [],
                    "last": "Shlens",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "11",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1511.05641"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Lifelong Machine Learning",
            "authors": [
                {
                    "first": "Zhiyuan",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Bing",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Greedy Frank-Wolfe Algorithm for Exemplar Selection",
            "authors": [
                {
                    "first": "Gary",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "Armin",
                    "middle": [],
                    "last": "Askari",
                    "suffix": ""
                },
                {
                    "first": "Kannan",
                    "middle": [],
                    "last": "Ramchandran",
                    "suffix": ""
                },
                {
                    "first": "Laurent",
                    "middle": [
                        "El"
                    ],
                    "last": "Ghaoui",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Assessing the quality of activities in a smart environment",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cook",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schmitter-Edgecombe",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Methods of Information in Medicine",
            "volume": "48",
            "issn": "",
            "pages": "480--485",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Binaryconnect: Training deep neural networks with binary weights during propagations. ArXiv",
            "authors": [
                {
                    "first": "Matthieu",
                    "middle": [],
                    "last": "Courbariaux",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "Jean-Pierre",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Contrastively smoothed class alignment for unsupervised domain adaptation",
            "authors": [
                {
                    "first": "Shuyang",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Yu",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "Yizhe",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Zhe",
                    "middle": [],
                    "last": "Gan",
                    "suffix": ""
                },
                {
                    "first": "Jingjing",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Lawrence",
                    "middle": [],
                    "last": "Carin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Class boundary exemplar selection based incremental learning for automatic target recognition",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Cui",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Pi",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Transactions on Geoscience and Remote Sensing",
            "volume": "",
            "issn": "",
            "pages": "1--11",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Learning without memorizing",
            "authors": [
                {
                    "first": "Prithviraj",
                    "middle": [],
                    "last": "Dhar",
                    "suffix": ""
                },
                {
                    "first": "Rajat Vikram",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "Kuan-Chuan",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "Ziyan",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Rama",
                    "middle": [],
                    "last": "Chellappa",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR 2019",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Discovery and recognition of emerging human activities using a hierarchical mixture of directional statistical models",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dobson",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Orthogonal gradient descent for continual learning",
            "authors": [
                {
                    "first": "Mehrdad",
                    "middle": [],
                    "last": "Farajtabar",
                    "suffix": ""
                },
                {
                    "first": "Navid",
                    "middle": [],
                    "last": "Azizan",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Mott",
                    "suffix": ""
                },
                {
                    "first": "Ang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "AISTATS 2020",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Towards robust evaluations of continual learning",
            "authors": [
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Farquhar",
                    "suffix": ""
                },
                {
                    "first": "Yarin",
                    "middle": [],
                    "last": "Gal",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "LLARLA 2018",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Modeling skewed class distributions by reshaping the concept space",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kyle",
                    "suffix": ""
                },
                {
                    "first": "Diane",
                    "middle": [
                        "J"
                    ],
                    "last": "Feuz",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Cook",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "AAAI-17",
            "volume": "",
            "issn": "",
            "pages": "1891--1897",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Optimization of type-2 fuzzy weights in backpropagation learning for neural networks using gas and pso",
            "authors": [
                {
                    "first": "Fernando",
                    "middle": [],
                    "last": "Gaxiola",
                    "suffix": ""
                },
                {
                    "first": "Patricia",
                    "middle": [],
                    "last": "Melin",
                    "suffix": ""
                },
                {
                    "first": "Fevrier",
                    "middle": [],
                    "last": "Valdez",
                    "suffix": ""
                },
                {
                    "first": "Juan",
                    "middle": [
                        "R"
                    ],
                    "last": "Castro",
                    "suffix": ""
                },
                {
                    "first": "Oscar",
                    "middle": [],
                    "last": "Castillo",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Appl. Soft Comput",
            "volume": "38",
            "issn": "C",
            "pages": "860--871",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Unsupervised online activity discovery using temporal behaviour assumption",
            "authors": [
                {
                    "first": "Hristijan",
                    "middle": [],
                    "last": "Gjoreski",
                    "suffix": ""
                },
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Roggen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "42--49",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Ubiquitous annotation systems: Technologies and challenges",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Frank Allan Hansen",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Proceedings of the Seventeenth Conference on Hypertext and Hypermedia, HYPERTEXT '06",
            "volume": "",
            "issn": "",
            "pages": "121--132",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Remind your neural network to prevent catastrophic forgetting",
            "authors": [
                {
                    "first": "Tyler",
                    "middle": [
                        "L"
                    ],
                    "last": "Hayes",
                    "suffix": ""
                },
                {
                    "first": "Kushal",
                    "middle": [],
                    "last": "Kafle",
                    "suffix": ""
                },
                {
                    "first": "Robik",
                    "middle": [],
                    "last": "Shrestha",
                    "suffix": ""
                },
                {
                    "first": "Manoj",
                    "middle": [],
                    "last": "Acharya",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Kanan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ECCV 2020",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Incremental learning in online scenario. ArXiv, abs",
            "authors": [
                {
                    "first": "Jiangpeng",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Runyu",
                    "middle": [],
                    "last": "Mao",
                    "suffix": ""
                },
                {
                    "first": "Zeman",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "Fengqing",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Distilling the Knowledge in a Neural Network",
            "authors": [
                {
                    "first": "Geoffrey",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "Oriol",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Learning a unified classifier incrementally via rebalancing",
            "authors": [
                {
                    "first": "Saihui",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "Xinyu",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Chen",
                    "middle": [
                        "Change"
                    ],
                    "last": "Loy",
                    "suffix": ""
                },
                {
                    "first": "Zilei",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Dahua",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR 2019",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Compacting, picking and growing for unforgetting continual learning",
            "authors": [
                {
                    "first": "Ching-Yi",
                    "middle": [],
                    "last": "Hung",
                    "suffix": ""
                },
                {
                    "first": "Cheng-Hao",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                },
                {
                    "first": "Cheng-En",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Chien-Hung",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Yi-Ming",
                    "middle": [],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "Chu-Song",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NeurIPS",
            "volume": "",
            "issn": "",
            "pages": "13647--13657",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Note on the quadratic penalties in elastic weight consolidation",
            "authors": [
                {
                    "first": "Ferenc",
                    "middle": [],
                    "last": "Husz\u00e1r",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "volume": "115",
            "issn": "",
            "pages": "2496--2497",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "A novel lifelong learning model based on cross domain knowledge extraction and transfer to classify underwater images",
            "authors": [
                {
                    "first": "Muhammad",
                    "middle": [],
                    "last": "Irfan",
                    "suffix": ""
                },
                {
                    "first": "Zheng",
                    "middle": [],
                    "last": "Jiangbin",
                    "suffix": ""
                },
                {
                    "first": "Muhammad",
                    "middle": [],
                    "last": "Iqbal",
                    "suffix": ""
                },
                {
                    "first": "Muhammad",
                    "middle": [
                        "Hassan"
                    ],
                    "last": "Arif",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Information Sciences",
            "volume": "552",
            "issn": "",
            "pages": "80--101",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Continual learning with node-importance based adaptive group sparse regularization",
            "authors": [
                {
                    "first": "Sangwon",
                    "middle": [],
                    "last": "Jung",
                    "suffix": ""
                },
                {
                    "first": "Hongjoon",
                    "middle": [],
                    "last": "Ahn",
                    "suffix": ""
                },
                {
                    "first": "Sungmin",
                    "middle": [],
                    "last": "Cha",
                    "suffix": ""
                },
                {
                    "first": "Taesup",
                    "middle": [],
                    "last": "Moon",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "NeurIPS 2020",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Measuring catastrophic forgetting in neural networks",
            "authors": [
                {
                    "first": "Ronald",
                    "middle": [],
                    "last": "Kemker",
                    "suffix": ""
                },
                {
                    "first": "Marc",
                    "middle": [],
                    "last": "Mcclure",
                    "suffix": ""
                },
                {
                    "first": "Angelina",
                    "middle": [],
                    "last": "Abitino",
                    "suffix": ""
                },
                {
                    "first": "Tyler",
                    "middle": [
                        "L"
                    ],
                    "last": "Hayes",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Kanan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "AAAI-18",
            "volume": "",
            "issn": "",
            "pages": "3390--3398",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Adjusting decision boundary for class imbalanced learning",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Access",
            "volume": "8",
            "issn": "",
            "pages": "81674--81685",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks",
            "authors": [
                {
                    "first": "James",
                    "middle": [],
                    "last": "Kirkpatrick",
                    "suffix": ""
                },
                {
                    "first": "Razvan",
                    "middle": [],
                    "last": "Pascanu",
                    "suffix": ""
                },
                {
                    "first": "Neil",
                    "middle": [],
                    "last": "Rabinowitz",
                    "suffix": ""
                },
                {
                    "first": "Joel",
                    "middle": [],
                    "last": "Veness",
                    "suffix": ""
                },
                {
                    "first": "Guillaume",
                    "middle": [],
                    "last": "Desjardins",
                    "suffix": ""
                },
                {
                    "first": "Andrei",
                    "middle": [
                        "A"
                    ],
                    "last": "Rusu",
                    "suffix": ""
                },
                {
                    "first": "Kieran",
                    "middle": [],
                    "last": "Milan",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Quan",
                    "suffix": ""
                },
                {
                    "first": "Tiago",
                    "middle": [],
                    "last": "Ramalho",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Optimal continual learning has perfect memory and is NP-hard",
            "authors": [
                {
                    "first": "Jeremias",
                    "middle": [],
                    "last": "Knoblauch",
                    "suffix": ""
                },
                {
                    "first": "Hisham",
                    "middle": [],
                    "last": "Husain",
                    "suffix": ""
                },
                {
                    "first": "Tom",
                    "middle": [],
                    "last": "Diethe",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 37th International Conference on Machine Learning",
            "volume": "119",
            "issn": "",
            "pages": "13--18",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Learning multiple layers of features from tiny images. Handbook of Systemic Autoimmune Diseases",
            "authors": [
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "1",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "A continual learning survey: Defying forgetting in classification tasks",
            "authors": [
                {
                    "first": "Matthias",
                    "middle": [],
                    "last": "De Lange",
                    "suffix": ""
                },
                {
                    "first": "Rahaf",
                    "middle": [],
                    "last": "Aljundi",
                    "suffix": ""
                },
                {
                    "first": "Marc",
                    "middle": [],
                    "last": "Masana",
                    "suffix": ""
                },
                {
                    "first": "Sarah",
                    "middle": [],
                    "last": "Parisot",
                    "suffix": ""
                },
                {
                    "first": "Xu",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "Ales",
                    "middle": [],
                    "last": "Leonardis",
                    "suffix": ""
                },
                {
                    "first": "Gregory",
                    "middle": [],
                    "last": "Slabaugh",
                    "suffix": ""
                },
                {
                    "first": "Tinne",
                    "middle": [],
                    "last": "Tuytelaars",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1909.08383"
                ]
            }
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges",
            "authors": [
                {
                    "first": "Timoth\u00e9e",
                    "middle": [],
                    "last": "Lesort",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lomonaco",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Stoian",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Maltoni",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Filliat",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Rodr\u00edguez",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Inf. Fusion",
            "volume": "58",
            "issn": "",
            "pages": "52--68",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Learning without forgetting",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hoiem",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "40",
            "issn": "12",
            "pages": "2935--2947",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Learning Without Forgetting",
            "authors": [
                {
                    "first": "Zhizhong",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Derek",
                    "middle": [],
                    "last": "Hoiem",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ECVC 2016",
            "volume": "",
            "issn": "",
            "pages": "614--629",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Complex activity recognition using time series pattern dictionary learned from ubiquitous sensors",
            "authors": [
                {
                    "first": "Li",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Yuxin",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "Shu",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Ming",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Zigang",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Information Sciences",
            "volume": "",
            "issn": "",
            "pages": "41--57",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "A framework of mining semantic-based probabilistic event relations for complex activity recognition",
            "authors": [
                {
                    "first": "Li",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Shu",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Guoxin",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "Bin",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Yuxin",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "Qingyu",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "Junhao",
                    "middle": [],
                    "last": "Wen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Information Sciences",
            "volume": "",
            "issn": "",
            "pages": "13--33",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Rotate your networks: Better weight consolidation and less catastrophic forgetting",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Masana",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Herranz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Van De Weijer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "L\u00f3pez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "D"
                    ],
                    "last": "Bagdanov",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "24th International Conference on Pattern Recognition (ICPR)",
            "volume": "",
            "issn": "",
            "pages": "2262--2268",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Gradient episodic memory for continual learning",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Lopez",
                    "suffix": ""
                },
                {
                    "first": "-",
                    "middle": [],
                    "last": "Paz",
                    "suffix": ""
                },
                {
                    "first": "Marc&apos;aurelio",
                    "middle": [],
                    "last": "Ranzato",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "6470--6479",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Noisy label tolerance: A new perspective of partial multi-label learning",
            "authors": [
                {
                    "first": "Gengyu",
                    "middle": [],
                    "last": "Lyu",
                    "suffix": ""
                },
                {
                    "first": "Songhe",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "Yidong",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Information Sciences",
            "volume": "543",
            "issn": "",
            "pages": "454--466",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "A practical bayesian framework for backpropagation networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mackay",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Neural Comput",
            "volume": "4",
            "issn": "3",
            "pages": "448--472",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Self-net: Lifelong learning via continual selfmodeling",
            "authors": [
                {
                    "first": "Krishna",
                    "middle": [],
                    "last": "Jaya",
                    "suffix": ""
                },
                {
                    "first": "Blake",
                    "middle": [],
                    "last": "Mandivarapu",
                    "suffix": ""
                },
                {
                    "first": "Rolando",
                    "middle": [],
                    "last": "Camp",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Estrada",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Frontiers in Artificial Intelligence",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
            "authors": [
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Merolla",
                    "suffix": ""
                },
                {
                    "first": "Rathinakumar",
                    "middle": [],
                    "last": "Appuswamy",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [
                        "V"
                    ],
                    "last": "Arthur",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [
                        "K"
                    ],
                    "last": "Esser",
                    "suffix": ""
                },
                {
                    "first": "Dharmendra",
                    "middle": [
                        "S"
                    ],
                    "last": "Modha",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "Fall detection in older adults with mobile iot devices and machine learning in the cloud and on the edge",
            "authors": [
                {
                    "first": "Dariusz",
                    "middle": [],
                    "last": "Mrozek",
                    "suffix": ""
                },
                {
                    "first": "Anna",
                    "middle": [],
                    "last": "Koczur",
                    "suffix": ""
                },
                {
                    "first": "Bo\u017cena",
                    "middle": [],
                    "last": "Ma\u0142ysiak-Mrozek",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Information Sciences",
            "volume": "537",
            "issn": "",
            "pages": "132--147",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "Learning to remember: A synaptic plasticity driven framework for continual learning",
            "authors": [
                {
                    "first": "Oleksiy",
                    "middle": [],
                    "last": "Ostapenko",
                    "suffix": ""
                },
                {
                    "first": "Mihai",
                    "middle": [],
                    "last": "Puscas",
                    "suffix": ""
                },
                {
                    "first": "Tassilo",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "Patrick",
                    "middle": [],
                    "last": "J\u00e4hnichen",
                    "suffix": ""
                },
                {
                    "first": "Moin",
                    "middle": [],
                    "last": "Nabi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR '19",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "Continual lifelong learning with neural networks: A review",
            "authors": [
                {
                    "first": "German",
                    "middle": [
                        "I"
                    ],
                    "last": "Parisi",
                    "suffix": ""
                },
                {
                    "first": "Ronald",
                    "middle": [],
                    "last": "Kemker",
                    "suffix": ""
                },
                {
                    "first": "Jose",
                    "middle": [
                        "L"
                    ],
                    "last": "Part",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Kanan",
                    "suffix": ""
                },
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Wermter",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Neural Networks",
            "volume": "113",
            "issn": "",
            "pages": "54--71",
            "other_ids": {}
        },
        "BIBREF67": {
            "ref_id": "b67",
            "title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pf\u00fclb",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gepperth",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ICLR 2019",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF68": {
            "ref_id": "b68",
            "title": "Deep stacked stochastic configuration networks for lifelong learning of non-stationary data streams",
            "authors": [
                {
                    "first": "Mahardhika",
                    "middle": [],
                    "last": "Pratama",
                    "suffix": ""
                },
                {
                    "first": "Dianhui",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Information Sciences",
            "volume": "495",
            "issn": "",
            "pages": "150--174",
            "other_ids": {}
        },
        "BIBREF69": {
            "ref_id": "b69",
            "title": "Evolving fuzzy models for myoelectric-based control of a prosthetic hand",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Precup",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Teban",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "E A D"
                    ],
                    "last": "Oliveira",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "M"
                    ],
                    "last": "Petriu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)",
            "volume": "",
            "issn": "",
            "pages": "72--77",
            "other_ids": {}
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "iCaRL: Incremental classifier and representation learning",
            "authors": [
                {
                    "first": "Alexander",
                    "middle": [],
                    "last": "Sylvestre Alvise Rebuffi",
                    "suffix": ""
                },
                {
                    "first": "Georg",
                    "middle": [],
                    "last": "Kolesnikov",
                    "suffix": ""
                },
                {
                    "first": "Christoph",
                    "middle": [
                        "H"
                    ],
                    "last": "Sperl",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lampert",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF71": {
            "ref_id": "b71",
            "title": "Introducing a new benchmarked dataset for activity monitoring",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Reiss",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Stricker",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ISWC 2012",
            "volume": "",
            "issn": "",
            "pages": "108--109",
            "other_ids": {}
        },
        "BIBREF72": {
            "ref_id": "b72",
            "title": "Transition-aware human activity recognition using smartphones",
            "authors": [
                {
                    "first": "Jorge-L",
                    "middle": [],
                    "last": "Reyes-Ortiz",
                    "suffix": ""
                },
                {
                    "first": "Luca",
                    "middle": [],
                    "last": "Oneto",
                    "suffix": ""
                },
                {
                    "first": "Albert",
                    "middle": [],
                    "last": "Sam\u00e3",
                    "suffix": ""
                },
                {
                    "first": "Xavier",
                    "middle": [],
                    "last": "Parra",
                    "suffix": ""
                },
                {
                    "first": "Davide",
                    "middle": [],
                    "last": "Anguita",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Neurocomputing",
            "volume": "171",
            "issn": "",
            "pages": "754--767",
            "other_ids": {}
        },
        "BIBREF73": {
            "ref_id": "b73",
            "title": "Closed-loop gan for continual learning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rios",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Itti",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 NIPS workshop on Continual Learning",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF74": {
            "ref_id": "b74",
            "title": "Online recognition of human activities and adaptation to habit changes by means of learning automata and fuzzy temporal windows",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ros",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "P"
                    ],
                    "last": "Cu\u00e9llar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Delgado",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vila",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Online Fuzzy Machine Learning and Data Mining",
            "volume": "220",
            "issn": "",
            "pages": "86--101",
            "other_ids": {}
        },
        "BIBREF76": {
            "ref_id": "b76",
            "title": "Continual learning with deep generative replay",
            "authors": [
                {
                    "first": "Hanul",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "Jaehong",
                    "middle": [],
                    "last": "Jung Kwon Lee",
                    "suffix": ""
                },
                {
                    "first": "Jiwon",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2990--2999",
            "other_ids": {}
        },
        "BIBREF77": {
            "ref_id": "b77",
            "title": "Three scenarios for continual learning",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Van De Ven",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gido",
                    "suffix": ""
                },
                {
                    "first": "Andreas",
                    "middle": [
                        "S"
                    ],
                    "last": "Tolias",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1904.07734"
                ]
            }
        },
        "BIBREF78": {
            "ref_id": "b78",
            "title": "Human Activity Recognition from Wireless Sensor Network Data: Benchmark and Software",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "L M"
                    ],
                    "last": "Van Kasteren",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Englebienne",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "J A"
                    ],
                    "last": "Kr\u00f6se",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "165--186",
            "other_ids": {}
        },
        "BIBREF79": {
            "ref_id": "b79",
            "title": "A biochemical approach to adaptive service ecosystems",
            "authors": [
                {
                    "first": "Mirko",
                    "middle": [],
                    "last": "Viroli",
                    "suffix": ""
                },
                {
                    "first": "Franco",
                    "middle": [],
                    "last": "Zambonelli",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Information Sciences",
            "volume": "180",
            "issn": "10",
            "pages": "1876--1892",
            "other_ids": {}
        },
        "BIBREF80": {
            "ref_id": "b80",
            "title": "Deep learning for sensor-based activity recognition: A survey",
            "authors": [
                {
                    "first": "Jindong",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Yiqiang",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Shuji",
                    "middle": [],
                    "last": "Hao",
                    "suffix": ""
                },
                {
                    "first": "Xiaohui",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "Lisha",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Pattern Recognition Letters",
            "volume": "119",
            "issn": "",
            "pages": "3--11",
            "other_ids": {}
        },
        "BIBREF81": {
            "ref_id": "b81",
            "title": "Herding dynamical weights to learn",
            "authors": [
                {
                    "first": "Max",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the 26th Annual International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1121--1128",
            "other_ids": {}
        },
        "BIBREF82": {
            "ref_id": "b82",
            "title": "Few-shot self reminder to overcome catastrophic forgetting",
            "authors": [
                {
                    "first": "Junfeng",
                    "middle": [],
                    "last": "Wen",
                    "suffix": ""
                },
                {
                    "first": "Yanshuai",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Ruitong",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF83": {
            "ref_id": "b83",
            "title": "Large scale incremental learning",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF84": {
            "ref_id": "b84",
            "title": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "authors": [],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "374--382",
            "other_ids": {}
        },
        "BIBREF85": {
            "ref_id": "b85",
            "title": "Activity recognition using binary sensors for elderly people living alone: Scanpath trend analysis approach",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "Y"
                    ],
                    "last": "Yatbaz",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Eraslan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yesilada",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ever",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Sensors Journal",
            "volume": "19",
            "issn": "17",
            "pages": "7575--7582",
            "other_ids": {}
        },
        "BIBREF86": {
            "ref_id": "b86",
            "title": "Lifelong learning in sensor-based human activity recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dobson",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zambonelli",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Pervasive Computing",
            "volume": "18",
            "issn": "3",
            "pages": "49--58",
            "other_ids": {}
        },
        "BIBREF87": {
            "ref_id": "b87",
            "title": "Evolving models for incrementally learning emerging activities",
            "authors": [
                {
                    "first": "Juan",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "Elise",
                    "middle": [],
                    "last": "Callus",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Ambient Intelligence and Smart Environments",
            "volume": "12",
            "issn": "",
            "pages": "313--325",
            "other_ids": {}
        },
        "BIBREF88": {
            "ref_id": "b88",
            "title": "Discovery and recognition of unknown activities",
            "authors": [
                {
                    "first": "Juan",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "Lei",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Simon",
                    "middle": [],
                    "last": "Dobson",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of Ubicomp '16 Adjunct",
            "volume": "",
            "issn": "",
            "pages": "783--792",
            "other_ids": {}
        },
        "BIBREF89": {
            "ref_id": "b89",
            "title": "Detecting abnormal events on binary sensors in smart home environments",
            "authors": [
                {
                    "first": "Juan",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "Graeme",
                    "middle": [],
                    "last": "Stevenson",
                    "suffix": ""
                },
                {
                    "first": "Simon",
                    "middle": [],
                    "last": "Dobson",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Pervasive and Mobile Computing",
            "volume": "33",
            "issn": "",
            "pages": "32--49",
            "other_ids": {}
        },
        "BIBREF90": {
            "ref_id": "b90",
            "title": "Lifelong learning with dynamically expandable networks. ICLR",
            "authors": [
                {
                    "first": "Jaehong",
                    "middle": [],
                    "last": "Yoon",
                    "suffix": ""
                },
                {
                    "first": "Eunho",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Jeongtae",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Sung",
                    "middle": [
                        "Ju"
                    ],
                    "last": "Hwang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF91": {
            "ref_id": "b91",
            "title": "Challenges providing ground truth for pervasive healthcare systems",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yordanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Pervasive Computing",
            "volume": "18",
            "issn": "2",
            "pages": "100--104",
            "other_ids": {}
        },
        "BIBREF92": {
            "ref_id": "b92",
            "title": "Lifelong gan: Continual learning for conditional image generation",
            "authors": [
                {
                    "first": "Mengyao",
                    "middle": [],
                    "last": "Zhai",
                    "suffix": ""
                },
                {
                    "first": "Lei",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Frederick",
                    "middle": [],
                    "last": "Tung",
                    "suffix": ""
                },
                {
                    "first": "Jiawei",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Megha",
                    "middle": [],
                    "last": "Nawhal",
                    "suffix": ""
                },
                {
                    "first": "Greg",
                    "middle": [],
                    "last": "Mori",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "2759--2768",
            "other_ids": {}
        },
        "BIBREF93": {
            "ref_id": "b93",
            "title": "A knowledge-based approach for multiagent collaboration in smart home: From activity recognition to guidance service",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Transactions on Instrumentation and Measurement",
            "volume": "69",
            "issn": "2",
            "pages": "317--329",
            "other_ids": {}
        },
        "BIBREF94": {
            "ref_id": "b94",
            "title": "Maintaining discrimination and fairness in class incremental learning",
            "authors": [
                {
                    "first": "Bowen",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Xi",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "Guojun",
                    "middle": [],
                    "last": "Gan",
                    "suffix": ""
                },
                {
                    "first": "Bin",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shutao",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1911.07053"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF1": {
            "text": "An example of task-incremental continual learning with neural network",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "An example of catastrophic forgetting effect[76] where the accuracy on new classes is high and the accuracy on old classes remains low as the network keeps forgetting the old knowledge A large number of continual learning techniques with neural networks have devoted to tackling this CF challenge[42,56]: (1) regularisation -constraining the network parameter updates such that old knowledge is retained; (2) rehearsal -storing a small number of old classes' data to replay with new classes' data when training a network; and (3) dynamic architectures -extending the network architecture for new knowledge while keeping the parameters important for the old classes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "N o (\u2282 N ) refers to the reserved old samples. \u03bb is a dynamic weight to adjust how much knowledge of the previous model needs to be preserved depending on how many new classes are introduced. It is calculated by multiplying the base \u03bb with the squared root of the ratio between new and old classes; that is, \u03bb = \u03bb base |C N |/|C o |. It regulates the degree of preserving the old knowledge by taking into account the number of new classes being added. For example, when there are many new classes are introduced, the model would preserve less old knowledge to allow the model to adapt to the new knowledge, and vice versa. In general, \u03bb base is set as 5.0 and the margin value m in Eq 11 as 0.5 [32]. Weight Alignment for Maintaining Discrimination and Fairness (WA-MDF) is another approach that tackles the mentioned imbalance problem by correcting the biased weights in the last fully connected (FC) layer after training each task. It aligns the norms of the weight vectors of new classes to those of old classes. The FC layer output of a model can be expressed in a general form: o(x) = W T \u03c6(x), where \u03c6(x) is the feature extraction function of an input x and W is the weight vector of the FC layer. W can be separated into W = (W o , W n ), where W o and W n refers to the weights corresponding to the old and new classes; that is,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "and a linear BiC layer is added to the end of the network and trained with a small validation set consisting of samples from both old and new tasks. The linear model of the BiC layer corrects the bias on the output logits for the new classes:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "where\u0177 k is the output logits on the kth class, the old classes are [1, ..., n] and the new classes are [n + 1, ..., n + m]. \u03b1 and \u03b2 are the bias parameters of the linear model, which are optimised in the following loss function:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Activity histograms on all the selected datasets.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Accelerometer sensor data Physical Activity Monitoring Dataset (PAMAP2 )[61] contains 12 activities such as sitting, lying, and house cleaning. The data is collected on 9 users with 3 accelerometer units on each user's chest, dominant arm and side ankle. Daily and Sports Activities Dataset (DSADS )[6,7] contains 19 activities such as running, rowing, and sitting. The data is collected on 8 users with 5 accelerometer units on each user's torso, arms, and legs. On these datasets, we use the sensor features that have been extracted from raw accelerometer data; that is, on each accelerometer unit, 27 features are generated, including mean, standard deviation, correlations, and spectrum peak position. Human Activity Recognition Dataset (HAPT )[62] contains 12 daily activities collected on 30 subjects with a smartphone (Samsung Galaxy S II) on their waist.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "st task t 1 = {Sleep, Toilet} 2 nd task t 2 = {Breakfast, Shower} Normalised sensor features Class Train the network with t1's training data Sample a small number of samples from t1's training data and store them in memory for replay techniques Extend the network's output layer to include t2's classes Train the network with t2's training data, or together with in-memory data on previous tasks Sample a small number of samples from t2's training data and store them in memory t1 's tr ai ni ng da ta t2 's tr ai n in g d at a \u2026 An example of task-incremental evaluation",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "of the performance of continual learning. (1) Offline batch learning: we train a network with the training data on all the classes simultaneously, and (2) finetuning: we do not use any holdout samples for replay and only use training data on each new class to update the model with the plain cross-entropy loss.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "LwF, the loss balance weight \u03bb o is set as 1.6 for all the datasets. For R-EWC, the regularisation coefficient \u03bb in Eq 4 is set as 5 for ARUBA and TWOR and 3 for all the other datasets. For MAS, the regularisation coefficient \u03bb in Eq 5 is set as 0.1 for TWOR, ARUBA, and MILAN datasets and 0.25 for all the other datasets.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "of base, old, new, and all classes on each technique. As we can see, regularisation methods demonstrate better plasticity as with the increase of tasks, the new accuracy of LwF and EWC remains high. These methods also exhibit poor stability as their base and old accuracy stays at the bottom of the plots, even from the second task on. Overall, the weight alignment methods in the rehearsal category achieve a better balance between stability and plasticity, as the overall accuracy of ILOS, WA-MDF, and WA-ADB suffers less steep drop. Besides, we can observe that each technique has a different performance profile on these 8 datasets, implying the characteristics of the datasets might impact the effectiveness of each technique.Rehearsal methods that tackle imbalance are less plastic to new classes. With a direct tuning of the new model's logits based on the previous model, ILOS can help surpass complex regularisation operations in retaining the knowledge but it is not able to quickly adapt to new 22 classes. We also observe that LUCIR performs poorly on new tasks but is robust at preserving old knowledge. An intuitive explanation could be that the design of marginal ranking reinforces the model's confidence at recognising ground truth embedding for old class samples after multiple incremental training steps.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Accuracy comparison of task-level performance. It records micro-F1 scores on Base, Old, New and All classes. Comparison of forgetting scores on selected techniques",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "cross-entropy (CE) only (as a baseline without any regularisation) in Eq 4, (2) KD in Eq 1, (3) EWC in Eq 4, (4) MAS in Eq 5, (5) LUCIR discrimination loss (LUCIR-DIS) L G dis in Eq 10, (6) LUCIR marginal ranking loss (LUCIR-MR) L mr in Eq 11, (7) LUCIR combination loss (LUCIR) in Eq 12, (8) ILOS cross-entropy loss (ILOS-CE) in Eq 7 and (9) ILOS combination loss (ILOS) in Eq 8. To make a fair comparison, we set up the same setting for each technique, including randomly sampling holdout data from old classes' training data and employing the same training procedure.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "compares micro-and macro-F1 scores of different regularisation terms with memory",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "Performance comparison of different regularisation terms with memory replay",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Comparison of accuracy on different training data percentage. The results demonstrate that the selected lifelong learning techniques are insensitive to training data size.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Main characteristics of selected datasets",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Hyperparameter configuration for training",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Comparison of micro-F1 scores on comparison techniques (The best accuracy is highlighted in bold.)",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Comparison of macro-F1 scores on comparison techniques (The best accuracy is highlighted in bold.)",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Comparison of computation time (in seconds) of training each incremental (Incre.) task and all the tasks (Total ). (The longest training time is highlighted in bold.) .42 15.31 43.92 322.51 53.93 283.09 38.81 439.46 20.77 114.23 81.80 451.57 28.56 28.56 34.23 207.93 GEM 2.55 8.61 3.34 15.14 67.49 488.94 73.64 383.29 59.97 673.61 121.08 623.63 596.56 3050.39 116.55 116.55 130.15 670.02 LUCIR 3.90 12.90 5.36 23.73 54.97 406.45 73.90 388.13 40.54 464.18 35.39 191.95 148.54 818.96 34.53 34.53 49.64 292.60 iCaRL 2.05 7.14 2.61 12.20 28.50 216.18 40.91 219.41 21.36 249.13 30.16 168.22 79.25 445.69 21.14 21.14 28.25 167.39 R-EWC 1.45 5.34 2.00 9.79 29.96 212.40 42.86 229.51 20.41 238.66 23.48 132.11 144.10 774.97 25.38 25.38 36.20 203.52 MAS 1.45 5.32 1.98 9.75 27.78 211.09 42.25 226.46 20.30 237.29 27.38 149.20 133.76 719.23 25.37 25.37 35.03 197.96 LwF 1.27 4.76 1.73 8.67 23.41 179.64 38.00 204.65 17.38 204.53 18.61 108.49 72.82 413.09 18.44 18.44 23.96 142.79 Finetuning 0.93 3.74 1.25 6.78 17.08 135.85 28.61 157.91 12.54 152.03 15.62 92.82 69.19 406.77 11.77 11.77 19.62 120.96",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Holdout size does not impact much on the accuracy in that the accuracy on ILOS, WA-MDF, and LUCIR in Figure 8 does not vary much with the increase of the holdout size. For most of the datasets, the accuracy converges when the holdout size is around 4 or 6.requires to tune the bias correction layer, which happens after a new task has converged. This is slightly more expensive (i.e., 30s more in total training time) than the relatively simple correction methods of WA-MDF/ADB. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 101,
                    "end": 109,
                    "text": "Figure 8",
                    "ref_id": null
                }
            ],
            "section": "annex"
        },
        {
            "text": "to use under what conditions for HAR systems. Table 6 lists the key ideas of each technique and how well they tackle HAR limitations.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 46,
                    "end": 53,
                    "text": "Table 6",
                    "ref_id": null
                }
            ],
            "section": "This section summarises the above results and provides guidelines on what type of techniques"
        },
        {
            "text": "Among the selected techniques, ILOS and WA-ADB produce higher macro-F1 scores than the others on more skewed datasets such as WS, MILAN and ARUBA. WA-ADB accounts for the ratio of training samples in each class at each incremental training step. For ILOS, the averaging of output logits reduces the magnitude of the logits of new dominant classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Imbalanced Activity Distribution"
        },
        {
            "text": "Inter-class similarity is the key characteristic of HAR. Some activities can have very similar sensor signatures. For example in the DSADS dataset, the activity classes between 'lying on right'and 'lying on back' exhibit high correlation, which can result in overlapping decision boundaries.LUCIR and GEM have attempted to tackle this problem, and LUCIR outperforms GEM with 6% increase in macro-F1. In addition, LUCIR has achieved the highest accuracy on TWOR dataset that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inter-class Similarity"
        }
    ]
}
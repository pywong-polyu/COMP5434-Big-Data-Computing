{
    "paper_id": "46352f79c695e00340fd3ca83827345362cc1e54",
    "metadata": {
        "title": "Deep drug-target binding affinity prediction with multiple attention blocks",
        "authors": [
            {
                "first": "Yuni",
                "middle": [],
                "last": "Zeng",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Laboratory",
                    "institution": "Sichuan University",
                    "location": {
                        "postCode": "610065, 518052",
                        "settlement": "Chengdu, Shenzhen",
                        "region": "Sichuan",
                        "country": "China;, China; Chengdu"
                    }
                },
                "email": ""
            },
            {
                "first": "Xiangru",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Laboratory",
                    "institution": "Sichuan University",
                    "location": {
                        "postCode": "610065, 518052",
                        "settlement": "Chengdu, Shenzhen",
                        "region": "Sichuan",
                        "country": "China;, China; Chengdu"
                    }
                },
                "email": ""
            },
            {
                "first": "Yujie",
                "middle": [],
                "last": "Luo",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Laboratory",
                    "institution": "Sichuan University",
                    "location": {
                        "postCode": "610065, 518052",
                        "settlement": "Chengdu, Shenzhen",
                        "region": "Sichuan",
                        "country": "China;, China; Chengdu"
                    }
                },
                "email": ""
            },
            {
                "first": "Xuedong",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Laboratory",
                    "institution": "Sichuan University",
                    "location": {
                        "postCode": "610065, 518052",
                        "settlement": "Chengdu, Shenzhen",
                        "region": "Sichuan",
                        "country": "China;, China; Chengdu"
                    }
                },
                "email": ""
            },
            {
                "first": "Dezhong",
                "middle": [],
                "last": "Peng",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Laboratory",
                    "institution": "Sichuan University",
                    "location": {
                        "postCode": "610065, 518052",
                        "settlement": "Chengdu, Shenzhen",
                        "region": "Sichuan",
                        "country": "China;, China; Chengdu"
                    }
                },
                "email": "pengdz@scu.edu.cn"
            },
            {
                "first": "Peng",
                "middle": [],
                "last": "Dezhong",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Laboratory",
                    "institution": "Sichuan University",
                    "location": {
                        "postCode": "610065, 518052",
                        "settlement": "Chengdu, Shenzhen",
                        "region": "Sichuan",
                        "country": "China;, China; Chengdu"
                    }
                },
                "email": ""
            },
            {
                "first": "Shenzhen",
                "middle": [
                    "Peng"
                ],
                "last": "Cheng",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sobey Digital Technology Co., Ltd",
                    "location": {
                        "postCode": "610041",
                        "settlement": "Chengdu",
                        "country": "China"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Drug-target interaction (DTI) prediction has drawn increasing interest due to its substantial position in the drug discovery process. Many studies have introduced computational models to treat DTI prediction as a regression task, which directly predict the binding affinity of drug-target pairs. However, existing studies (i) ignore the essential correlations between atoms when encoding drug compounds and (ii) model the interaction of drug-target pairs simply by concatenation. Based on those observations, in this study, we propose an end-to-end model with multiple attention blocks to predict the binding affinity scores of drug-target pairs. Our proposed model offers the abilities to (i) encode the correlations between atoms by a relation-aware self-attention block and (ii) model the interaction of drug representations and target representations by the multi-head attention block. Experimental results of DTI prediction on two benchmark datasets show our approach outperforms existing methods, which are benefit from the correlation information encoded by the relation-aware self-attention block and the interaction information extracted by the multi-head attention block. Moreover, we conduct the experiments on the effects of max relative position length and find out the best max relative position length value k \u2208 {3, 5}. Furthermore, we apply our model to predict the binding affinity of Corona Virus Disease 2019 (COVID-19)-related genome sequences and 3137 FDA-approved drugs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Drugs work by interacting with target proteins to activate or inhibit the biological process of the targets. Thus, identifing novel drug-target interactions (DTIs) is an essential step in the drug discovery field, like drug repurposing [12, 18, 26] . However, transitional costly experiments limit the process to identify new DTIs [26, 28, 31] . Thus, the computational approach for DTI prediction is urgent [37] . Recently, a large of studies proposed Recently, deep learning methods are utilized for DTI prediction. DeepDTA [18] proposed a convolutional neural networks (CNNs)-based model for drug representation learning, target representation learning and predicting interaction between them. As one of the widely used deep learning-based models for predicting the binding affinity values, it has achieved an acceptable result. However, it is limited due to that CNN cannot capture the long-distance relationship among atoms in drugs. Based on this, the study [24] introduced a selfattention mechanism-based model with position embedding to encode the relationship among all atoms in compounds. Nevertheless, firstly, it is far from enough to model the compounds since these existing methods just label each atom a corresponding integer according to a dictionary. During modeling the compounds, what is learned is an atom at a specific position. It ignores the correlation between atoms and separates each atom. For example, the compounds 'COC1=C (C=C2C(=C1)CCN=C2C3=CC(=C(C=C3)Cl)Cl)Cl'.",
            "cite_spans": [
                {
                    "start": 236,
                    "end": 240,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 241,
                    "end": 244,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 245,
                    "end": 248,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 331,
                    "end": 335,
                    "text": "[26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 336,
                    "end": 339,
                    "text": "28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 340,
                    "end": 343,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 408,
                    "end": 412,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 526,
                    "end": 530,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 964,
                    "end": 968,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In existing methods, given a dictionary { C : 1, l : 2, O : 3, etc.}, the character 'C' would be coded as '1' and 'l' is labeled as '2'. The existing methods separated the chloride atom 'Cl' as two fake atoms since they cannot further learn the relative position information between character 'C' and character 'l'. Moreover, the correlation between atoms not only depicts relative position information but also enhances the diversity of atoms. As the 'C' in that example, 'C' atoms would be in any position, but each one includes unique information since their connected atoms are different. Secondly, most existing methods always simply modeled the interaction between drugs and targets by concatenating their representations which is not sufficient to describe the interactions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Based on these observations, we propose an end-to-end model with multiple attention blocks, named MATT_DTI, to predict the binding affinity scores of drug-target pairs. The protein sequences and SMILES (Simplified Molecular Input Line Entry System) of drugs are the inputs of our proposed model. Firstly, we propose a relation-aware self-attention block to model the drugs from SMILES data, considering the correlation between atoms. The relative self-attention block makes it possible to enhance the relative position information between atoms in compounds while considering the relationship of all elements at the same time. Secondly, two CNN models are utilized to learn the representations of drugs and targets, respectively. Finally, a multi-head attention block is built to model the similarity of drug-target pairs as the interaction information and fully connected networks (FNNs) are used to extract interaction features. Compared with the baseline DeepDTA [18] , both of us are sequence representation methods and the protein representation learning part uses the same CNN model. The difference is that we employ a relation-aware self-attention block in drug representation learning to encode correlations of atoms, and a multi-head attention block to model the interaction information of DTIs.",
            "cite_spans": [
                {
                    "start": 966,
                    "end": 970,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the experiments, we evaluate our proposed model on two public benchmark datasets, Davis [4] and KIBA [25] datasets, and compare our model with regression-based baselines, KronRLS [19] , SimBoost [9] , DeepDTA [18] and other recent sequence representation learning methods for DTI prediction. Our MATT_DTI model outperforms these baseline models on Concordance index (CI) and r 2 m index metrics. Moreover, in order to further investigate the potential of our model, we apply our proposed model to Corona Virus Disease 2019 (COVID-19)related proteins and list the FDA-approved drugs with high binding affinity scores predicted by our model.",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 94,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 104,
                    "end": 108,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 182,
                    "end": 186,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 198,
                    "end": 201,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 212,
                    "end": 216,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The main contributions of this paper can be summarized as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(i) In order to model the drug compounds, a relation-aware self-attention block is built to enhance the relative position information between atoms in drugs and capture the longdistance relationship among all the atoms at the same time (section 3 Methods). (ii) In order to further extract the interaction information of drug-target pairs, a multi-head attention block is proposed to model the similarity between drugs and target (section 3 Methods). (iii) To the best our knowledge, our results are the state-ofthe-art on the two datasets in sequence presentation learning methods for drug-target binding affinity prediction (section 4 Experiments). (iv) We apply our model to COVID-19-related proteins and provide a reference to medical experts to find related drugs (section 5 Discussion).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we introduce existing approaches for DTI prediction, the background knowledge of attention mechanism and the motivation of this work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "Many studies [6, 16, 29, 32] regarded DTI prediction as a binary classification problem. The proposed models to determine whether the interactions exist between drugs and targets. However, those methods simplified the DTI problem as with chosen binding affinity threshold values [18] . They overlooked the information for the binding affinity value, which describes the strength of the interaction between a drug-target pair. Therefore, the exact way for DTI prediction is directly to predict the binding affinity value based on a regression model. In recent years, many efforts have been conducted on regression-based models for DTI prediction. The approaches based on random forest algorithm [11, 22] have been successful to predict the binding affinities of drugs and targets. Moreover, similarity-based methods were one option for regression-based DTI prediction, which utilized the similarity information of drugs and targets, such as SimBoost [9] and KronRLS [19] .",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 16,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 17,
                    "end": 20,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 21,
                    "end": 24,
                    "text": "29,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 25,
                    "end": 28,
                    "text": "32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 279,
                    "end": 283,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 694,
                    "end": 698,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 699,
                    "end": 702,
                    "text": "22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 949,
                    "end": 952,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 965,
                    "end": 969,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "The related studies on DTI prediction"
        },
        {
            "text": "With the significant success of deep neural networks in the computer version, speech recognition and natural language processing (NLP), many deep learning-based models were proposed to predict DTIs based on regression motivation. In recent works, deep models for DTI prediction mainly include two branches, graph representation method-based approaches with structure information as inputs [15, 30] and sequence representationbased approaches considering sequence information of DTI [18, 34, 35] . In this work, we focus on sequence representation learning approaches DeepDTA [18] and OnionNet [36] proposed CNNbased models for DTI predicting. Especially, DeepDTA focused on the sequence information of both drugs and targets and then used two CNN models for drugs (the SMILES input) and targets (the protein sequence input) as representation learning parts. Then, an information fusion part was connected to predict the binding affinities of drugs and targets. There, three FNN layers were regarded as the information fusion part. Since it could not capture the long-distance relationship between atoms, the study [24] applied a self-attention network (SAN) with position embedding to extract drug representation for DTI prediction.",
            "cite_spans": [
                {
                    "start": 389,
                    "end": 393,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 394,
                    "end": 397,
                    "text": "30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 482,
                    "end": 486,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 487,
                    "end": 490,
                    "text": "34,",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 491,
                    "end": 494,
                    "text": "35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 575,
                    "end": 579,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 593,
                    "end": 597,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "The related studies on DTI prediction"
        },
        {
            "text": "Most neural sequence transaction models have an encoderdecoder structure. The transformer [27] is a typical encoderdecoder model based on an attention mechanism. It is widely used in the field of NLP, which has proven the strong ability of transformer in processing text data. The main block of the transformer is the attention function. The attention function can be described as mapping a query (Q) and a set of key-value (K-V) pairs to an output.",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 94,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "The background of attention mechanism"
        },
        {
            "text": "Scaled dot-product attention is defined as the generalized attention with Q, K and V. Let the dimension of Q and K be d k and the dimension of V be d v",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The background of attention mechanism"
        },
        {
            "text": "where Q \u2208 R n\u00d7d k , K \u2208 R m\u00d7d k and V \u2208 R m\u00d7dv . Attention describes the similarity between the query and each value. The similarity can be measured by inner product of the softmax results and the value. The factor d k plays a regulatory role so that the inner product is not too large. When Q, K and V are projections from the same inputs, the attention function is the self-attention. Multi-head Attention is an improved attention mechanism. Firstly, before scaled dot-product attention, the d model -dimensional Q, K and V should be linearly projected h times with learned linear projections to d k , d k and d v , respectively. Specifically,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The background of attention mechanism"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The background of attention mechanism"
        },
        {
            "text": "Then, concatenate the results of attention",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The background of attention mechanism"
        },
        {
            "text": "where W o \u2208 R hdv\u00d7d model . Here, for each of those Q, K and V, a number of different 'heads' are obtained through linear projections and attention with different weights. Thus, multi-head can be regarded as multiple the same operation in parallel, while parameters are not shared.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The background of attention mechanism"
        },
        {
            "text": "As seen, when SANs are used on DTI prediction, an atom will conduct an attention operation with all atoms. It leads to SANs disperse the attention distribution to all elements and then overlook the essential correlation between atoms. In addition, most deep models for DTI prediction simply concatenate drug and protein representations to model the interaction between them. The way ignores the interaction features between drug and protein representations. In this study, we propose a deep model on DTI binding affinity prediction, in which the correlation between atoms and the interaction information between drugs and targets are considered.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation"
        },
        {
            "text": "In this work, we introduce a multiple attention blocks-based model-MATT_DTI, to predict the binding affinity scores of drugtarget pairs, as shown in Figure 1 . Like most deep learning-based DTI models, our model consists of three parts: drug representation learning, protein representation learning and interaction learning. Specifically, we propose a relation-aware self-attention block in the drug representation learning process. The relationaware self-attention block is to encode correlations by enhancing the relative position information between atoms. Then, two CNN models are utilized to extract features from drugs and proteins in drug representation learning and protein representation learning processes, respectively. Finally, the interaction learning model is exploited to combine and extract interaction features from both drug representations and protein representations by multi-head attention.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 149,
                    "end": 157,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Methods"
        },
        {
            "text": "The inputs of our model are SMILES sequences for drugs and FASTA sequences for proteins. According to the work [18] , the SMILES sequence is comprised of characters representing atoms and structure indicators. Mathematically, a drug is",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 115,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Input embedding"
        },
        {
            "text": "where d i \u2208 N * and the sequence length is varied, which depends on a compound. In this study, we define a hyperparameter l d to restrict the max input length for drugs. Inspired by the token embedding and position embedding in transformer [27] , the input of drug representation learning is the sum of token embedding and position embedding of SMILE sequences. The token embedding",
            "cite_spans": [
                {
                    "start": 240,
                    "end": 244,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Input embedding"
        },
        {
            "text": "where v d is the vocabulary size of drugs and e d is the embedding size of drugs. The position embedding E d p \u2208 R l d \u00d7e d has a trainable weight W p \u2208 R l d \u00d7e d . The output of the embedding operations is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input embedding"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input embedding"
        },
        {
            "text": "The same as the mathematical expression of drugs, a protein sequence is mathematically expressed as,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input embedding"
        },
        {
            "text": "where p i \u2208 N * and the length of P depends on proteins. We also define a hyperparameter l p as the fixed protein input length to ensure the same size of inputs. Different from drug sequence, the trainable embedding layer of protein sequence is similar to DeepDTA [18] as",
            "cite_spans": [
                {
                    "start": 264,
                    "end": 268,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Input embedding"
        },
        {
            "text": "where e p is the embedding size of protein sequences and X p \u2208 R lp\u00d7ep .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input embedding"
        },
        {
            "text": "As for protein representation leaning process, our proposed model is developed from DeepDTA [18] and the learning model for protein sequences also utilizes three convolutional layers as the feature extractor, followed by a max pooling layer. As for the CNN model in presentation learning model, suppose there exist L c convolutional layer where C Lc is the output of L c th convolutional layer and the lth (l \u2208 (0, L c ]) layer can be formally expressed as",
            "cite_spans": [
                {
                    "start": 92,
                    "end": 96,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Protein representation learning model"
        },
        {
            "text": "where k l indicates the trainable filters in lth convolutional layer, the size of it is 1 \u00d7 V, C l is the output of lth layer and f (\u00b7) is the activation function. Based on this, when L c = 3 in protein representation learning, the output of protein representation learning could be calculated as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Protein representation learning model"
        },
        {
            "text": "where Pooling(\u00b7) is the max pooling function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Protein representation learning model"
        },
        {
            "text": "SANs have drawn increasing interest, especially in the NLP field. SANs have the ability to capture long-distance dependencies by explicitly attending to all the elements, regardless of distance [33] . It contributes to representing a drug because SANs capture the long-distance relation between all atoms in a compound. However, SANs have a major limitation that is it disperses the attention distribution and thus overlooks relative information of elements [7, 23] . The relative information of atoms in drugs describes the essential correlation between atoms. This leads to that SAN is not sufficient to model SMILES data. Thus, a relationaware self-attention block is proposed in drug representation learning.",
            "cite_spans": [
                {
                    "start": 194,
                    "end": 198,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 458,
                    "end": 461,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 462,
                    "end": 465,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Drug representation learning model"
        },
        {
            "text": "In the field of NLP, self-attention with relative position representations [23] has already considered the pairwise relationship between words. It considers the relative distance information to model the distance between the words during conducting the self-attention. According to work [23] , it can be simplified as",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 79,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 287,
                    "end": 291,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "The relation-aware self-attention block"
        },
        {
            "text": "where w r \u2208 N * is the relative distance length between words. Inspired by this, we developed it to encode the correlation between atoms. We first define k kinds of relative relationships between atoms, which are embedded into learnable parameters W R \u2208 R k\u00d7e d . Taken X d as the input, the output of a self-attention with relative position representations [23] layer can be formally expressed as",
            "cite_spans": [
                {
                    "start": 358,
                    "end": 362,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "The relation-aware self-attention block"
        },
        {
            "text": "where W Q , W V , W K \u2208 R e d \u00d7e d denote parameter matrices of attention layer. A R \u2208 R l d \u00d7l d indicates the relationship matrix, of which, A R i,j represents the correlation between the ith and the jth elements",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The relation-aware self-attention block"
        },
        {
            "text": "Here, X d i is the ith vector in X d . clip( * ) is employed to select corresponding embedding in W R . Thus, A R can be served as inductive biases to revise the attention distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The relation-aware self-attention block"
        },
        {
            "text": "Then, a residual connection [8] and an FNN layer are following. As for the FNN layers in this study, L fully connected layers can be calculated by",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 31,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "The relation-aware self-attention block"
        },
        {
            "text": "where a 0 is the input of FNN layers, the a L is the output of Lth FNN layer and the lth \u2208 (0, L] fully connected layer can be described as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The relation-aware self-attention block"
        },
        {
            "text": "where W f is the trainable wight and a l is the output of lth fully connected layer. Therefore, the output of the residual connection and the FNN layer in the relation-aware self-attention block is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The relation-aware self-attention block"
        },
        {
            "text": "Since the protein representation is learned by the CNN model, the three convolutional layers are also used to exploited drug information. We insist the two CNN models could ensure that the drug and protein representations are projected to the same space. Thus, the output is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The relation-aware self-attention block"
        },
        {
            "text": "while L c = 3 in drug representation learning. In this process, layer normalization [2] and dropout [10] are used.",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 87,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 100,
                    "end": 104,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "The relation-aware self-attention block"
        },
        {
            "text": "The existing way of interaction learning is to concatenate the representations of drugs and proteins. It overlooks the interaction information of drugs and proteins. In similarity-based DTI prediction models [9, 19] , the similarity information of drugprotein pairs was used as the interaction information in them. Inspired by this, a multi-head attention block is exploited to model the similarity of drug-protein pairs as the interaction information of them. Here, the drug representations are regarded as the query, while the protein representations are the key and value in the attention mechanism. Mathematically, the output is",
            "cite_spans": [
                {
                    "start": 208,
                    "end": 211,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 212,
                    "end": 215,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Interaction learning model"
        },
        {
            "text": "with three heads in this study. Then, a residual connection [8] is used as",
            "cite_spans": [
                {
                    "start": 60,
                    "end": 63,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Interaction learning model"
        },
        {
            "text": "where conc(\u00b7) is a concatenation function and g(\u00b7) is global average pooling operation. Then, a 3-layered FCN (L=3) is employed to learn the interaction information from I in dp and the last layer of the network has only one neuron as the output of our model y * = FNN(I dp ), Proteins  442  229  Compounds  68  2111  Interactions  30056  118254  Training data  25046  98545  Test data  5010  19709 where y * is the predicted binding affinity value of the drugtarget pair. The weights of our proposed MATT_DTI model are optimized by the mean square error (MSE) between the network output y * and the actual binding affinity value y",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 277,
                    "end": 398,
                    "text": "Proteins  442  229  Compounds  68  2111  Interactions  30056  118254  Training data  25046  98545  Test data  5010  19709",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Interaction learning model"
        },
        {
            "text": "We proposed a novel drug-target binding affinity prediction method based on multiple attention blocks with sequence information of drugs (compounds) and proteins as inputs. In this section, we conducted experiments with our proposed model (MATT_DTI) on two benchmark datasets: Davis [4] and KIBA [25] datasets. The CI and r 2 m metrics were used to measure the performance of the proposed model and the baseline models.",
            "cite_spans": [
                {
                    "start": 283,
                    "end": 286,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 296,
                    "end": 300,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "We evaluated our proposed model on two benchmark datasets, Davis [4] and KIBA [25] datasets. The Davis dataset contains the 442 kinase proteins and their relevant inhibitors (68 ligands) with respective dissociation constant (K d ) value. The K d values were transformed into log space, as [9, 18] , pK d , as the binding affinity values, which is explained in 20,",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 68,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 78,
                    "end": 82,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 290,
                    "end": 293,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 294,
                    "end": 297,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Benchmark datasets"
        },
        {
            "text": "The KIBA dataset was developed from the KIBA approach, which comprised 467 proteins, 52 498 drugs and their binding affinity scores originally. Here, the KIAB scores measure the kinase inhibitor bioactivities and are regarded as the binding affinity values. SimBoost [9] filtered it to contain 229 unique proteins and 2111 unique drugs for a fair comparison. As for the input of proteins and drugs in the Davis and KIBA dataset, we followed the DeepDTA method [18] in which the SMILES of drugs and protein sequences were digitized to a fixed maximum length by a dictionary. Table 1 summarizes the details of the Davis and KIBA dataset.",
            "cite_spans": [
                {
                    "start": 267,
                    "end": 270,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 460,
                    "end": 464,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 574,
                    "end": 581,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Benchmark datasets"
        },
        {
            "text": "We evaluated the performance of our MATT_DTI on the benchmark datasets. Like the study DeepDTA [18] , we firstly clipped the training data as training set and validation set to find the optimal settings of our model, like number of filters, filter length, hidden size, dropout rate and number of epochs. The final results given in this section were the average results on the test set with 5 times training. Table 2 gives the parameter settings in experiments depending on datasets. All models were trained on 1 NVIDIA 2080Ti GPU. ",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 99,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 408,
                    "end": 415,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experiments setup"
        },
        {
            "text": "To evaluate the performance of our model, firstly, the CI was used as the evaluation metrics",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "where b i is the prediction value with larger affinity \u03b4 i , b j is the prediction value for smaller affinity \u03b4 j and Z is a normalization constant. Moreover, the b(x) is the step function [19] b",
            "cite_spans": [
                {
                    "start": 189,
                    "end": 193,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "Then, in order to better evaluate our model, r 2 m [20, 21] , which is widely used in this filed, is the another metric in this work. Mathematically,",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 55,
                    "text": "[20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 56,
                    "end": 59,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "where r 2 and r 2 0 are the squared correlation coefficient values between the observed and predicted values with and without intercept, respectively. Only r 2 m value of a model on test set is larger than 0.5, the model is an acceptable model. Table 3 lists the average results on the drug-target binding affinity prediction tasks. As seen, MATT_DTIs improve the prediction quality in both two datasets, reconfirming the necessity of modeling the long-distance relationship and the relative position information of compounds. Besides, our models outperform all the baseline works in all metrics, indicating the superiority of the proposed approaches. In particular, The MATT_DTI with the self-attention block achieves better performance than Deep-DTA, revealing the contribution of self-attention that modeling the long-distance relation of all elements in drugs. Moreover, the MATT_DTI with a relation-ware self-attention block (Rel_sa:CNN) outperforms the MATT_DTI model with a selfattention layer (sa:CNN), indicating that modeling the relative information can raise the ability of the self-attention model on capturing the atoms' information.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 245,
                    "end": 252,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Metrics"
        },
        {
            "text": "In this section, we conducted the experiment about the interaction learning part based on multi-head attention and compared it with the existing way based on concatenation way. Table 4 gives the average test results on both KIBA and Davis datasets. One intuition of our approach is to capture interaction features via modeling the similarity between drugprotein pairs by a multi-head attention block. To evaluate it, we implemented models with a multi-head attention block in the interaction learning process. As shown in Table 4 , the DeepDTA model and MATT_DTI model with a multi-head attention block (MulH_attention + FNN) achieve higher results than the models without it (Concatenation + FNN), revealing that extracting interaction features with multi-head attention is superior to concatenation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 177,
                    "end": 184,
                    "text": "Table 4",
                    "ref_id": null
                },
                {
                    "start": 522,
                    "end": 529,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Experiment 2: interaction learning with multi-head attention"
        },
        {
            "text": "We finally investigated the effects of relation position length in the relation-aware self-attention block on the drug-target binding affinity prediction task. As shown in Figures 2 and 3 , MATT_DTI with max relative position length k = 5 has the best performance in the KIBA dataset. As plotted in Figures 4 and 5 , we believe the max relative position length with k = 3 is superior to other setting for Davis dataset. The different distribution of KIBA and Davis dataset may lead to the slight difference of best max relative position length. As seen, MATT_DTI with max relative position length k \u2208 {3, 5} improves the prediction performance, indicating that the correlation between atoms is better modeled with max relative position length k \u2208 {3, 5}.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 172,
                    "end": 187,
                    "text": "Figures 2 and 3",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 299,
                    "end": 314,
                    "text": "Figures 4 and 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Experiment 3: effects of max relative position length"
        },
        {
            "text": "Moreover, we compare our proposed model with other sequence representation-based approaches in Table 5 . As seen, our final results are higher 0.026 than DeepDTA on KIBA and higher 0.012 on Davis for CI metric. The r 2 m also higher 0.083 than DeepDTA on KIBA and higher 0.052 on Davis. In general, our model performs better than all other sequence representation learning approaches.",
            "cite_spans": [
                {
                    "start": 280,
                    "end": 286,
                    "text": "Davis.",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 95,
                    "end": 102,
                    "text": "Table 5",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Experiment 3: effects of max relative position length"
        },
        {
            "text": "Recently, the new coronavirus (SARS-CoV-2) infection is spreading rapidly, and the daily incidence rate is increasing worldwide. It is urgent to find out a valid drug for the patient. Drug repurposing is one of the computational efforts that re-utilize FDA approved drugs, or compound succeeded in phase one clinical trials, for a new indication, to take advantage of the proved toxicity [5, 13] . Drug repurposing is regarded as one potential way for finding new coronavirus treatment [3] . Therefore, in this section, we apply our trained model to predict the binding affinity scores between existing drugs and the genome sequences of COVID-19related severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). We believe that the discussion could provide an example to apply our model in real-life situations and hope our results can provide scientists with an assistant to learn the coronavirus.",
            "cite_spans": [
                {
                    "start": 388,
                    "end": 391,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 392,
                    "end": 395,
                    "text": "13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 486,
                    "end": 489,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Based on studies [1, 3] , we extract the genome sequences, 3Clike proteinase, RNA-dependent RNA polymerase, helicase, 3'-to-5' exonuclease, endoRNAse and 2'-O-ribose methyltransferase of SARS-CoV-2 from the National Center for Biotechnology Information database; 3137 FDA-approved drugs are included in this section. Table 6 lists parts of the FDA-approval antiviral drugs with top binding affinity values predicted by our MATT_DTI with weights trained by KIBA dataset and existing approaches [1] . The full lists of the 6 genome sequences could be found at supplementary data, see Supplementary Data available online at http://bib.oxfordjournals.org/.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 20,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 21,
                    "end": 23,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 493,
                    "end": 496,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 317,
                    "end": 324,
                    "text": "Table 6",
                    "ref_id": null
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "Nowadays, the effective drugs to cure COVID-19 have not been found; thus, we cannot verify our results. It is only a theoretical result on drug repurposing task. We just hope that the experiment will reflect the way to use our model in practical applications. Moreover, like studies [1, 3] , we hope our work can provide scientists with some ideas for new drugs. ",
            "cite_spans": [
                {
                    "start": 283,
                    "end": 286,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 287,
                    "end": 289,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "In this work, we propose a multiple attention blocks-based model to (i) enhance relative position information between atoms when encoding drugs and (ii) model the interaction between drug representations and target representations. Empirical results of the drug-target binding affinity prediction task on two benchmark datasets demonstrate the effectiveness of our proposed methods. The extensive analyses suggest that (i) encoding the relative position information is beneficial to drug representations, (ii) modeling the interaction can further improve the performance of predicting the binding affinity of DITs and (iii) the best max relative position length to encode drugs is in 3-5 for the KIBA and Davis dataset. Furthermore, we apply our trained model to predict the binding affinity scores of SARS-CoV-2-related genome sequences and 3137 FDAapproved drugs to provide some reference for COVID-19-related scientists.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "\u2022 MATT_DTI is a deep learning-based model for drugtarget binding affinity score prediction.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Key Points"
        },
        {
            "text": "\u2022 In order to encode the correlation between atoms of drugs, MATT_DTI employs a relation-aware selfattention block to enhance the relative information between atoms when encoding drug compounds. \u2022 In order to extract interaction feature of drug-target pairs, a multi-head attention block is proposed to model the similarity between drugs and target in MATT_DTI.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Key Points"
        },
        {
            "text": "\u2022 Experimental results of DTI prediction on two benchmark datasets show our MATT_DTI outperforms existing models, which is benefit from the correlation and interaction information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Key Points"
        },
        {
            "text": "\u2022 We further apply our model to FAD-approved drugs and COVID-19-related proteins, which could provide a reference to medical expert.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Key Points"
        },
        {
            "text": "Supplementary data, including code, weight and results, are available online at https://github.com/ZengYuni/MATT_ DTI/.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supplementary data"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Deep learning for predicting drug-target interactions: A case study of COVID-19 drug repurposing",
            "authors": [
                {
                    "first": "Abdel-Basset M",
                    "middle": [],
                    "last": "Hawash",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Elhoseny",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Access",
            "volume": "8",
            "issn": "",
            "pages": "170433--51",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Layer normalization. CoRR, abs",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Ba",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Kiros",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Predicting commercially available antiviral drugs that may act on the novel coronavirus (sars-cov-2) through a drug-target interaction deep learning model",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "B"
                    ],
                    "last": "Bo",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Comput Struct Biotechnol J",
            "volume": "18",
            "issn": "",
            "pages": "784--90",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Comprehensive analysis of kinase inhibitor selectivity",
            "authors": [
                {
                    "first": "Dmi",
                    "middle": [],
                    "last": "Davis",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Hunt",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Herrgard",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Nat Biotechnol",
            "volume": "29",
            "issn": "",
            "pages": "1046--51",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Exploiting drug-disease relationships for computational drug repositioning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dudley",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Deshpande",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Butte",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Briefings Bioinform",
            "volume": "12",
            "issn": "4",
            "pages": "303--314",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Interpretable drug target prediction using deep neural representation",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "Y"
                    ],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fokoue",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI",
            "volume": "",
            "issn": "",
            "pages": "3371--3378",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A lightweight approach for natural language inference",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Tlg",
                    "middle": [],
                    "last": "Transformer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019",
            "volume": "",
            "issn": "",
            "pages": "6489--96",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Simboost: a read-across approach for predicting drug-target binding affinities using gradient boosting machines",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Heidemeyer",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ban",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "J. Cheminformatics",
            "volume": "9",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Low-quality structural and interaction data improves binding affinity prediction via random forest",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K-S",
                    "middle": [],
                    "last": "Leung",
                    "suffix": ""
                },
                {
                    "first": "M-H",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Molecules",
            "volume": "20",
            "issn": "6",
            "pages": "10947--62",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Machine learning approaches and databases for prediction of drug-target interaction: a survey paper",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Maryam",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Elyas",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Kai",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Brief Bioinform",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Identify drug repurposing candidates by mining the protein data bank",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Moriaud",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "B"
                    ],
                    "last": "Richard",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Adcock",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Briefings Bioinform",
            "volume": "12",
            "issn": "4",
            "pages": "336--376",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Rectified linear units improve restricted boltzmann machines",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nair",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10)",
            "volume": "",
            "issn": "",
            "pages": "807--821",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Graphdta: prediction of drugtarget binding affinity using graph convolutional networks",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Venkatesh",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "BioRxiv",
            "volume": "684662",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "A comparative study of smiles-based compound similarity functions for drugtarget interaction prediction",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "\u00d6zt\u00fcrk",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "O"
                    ],
                    "last": "Olmez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "\u00d6zg\u00fcr",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "BMC Bioinform",
            "volume": "17",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Widedta: prediction of drugtarget binding affinity. CoRR, abs",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "\u00d6zt\u00fcrk",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "O"
                    ],
                    "last": "Olmez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "\u00d6zg\u00fcr",
                    "suffix": ""
                }
            ],
            "year": 1902,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Deepdta: deep drug-target binding affinity prediction",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "\u00d6zt\u00fcrk",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "\u00d6zg\u00fcr",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "O"
                    ],
                    "last": "Olmez",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Bioinformatics",
            "volume": "34",
            "issn": "17",
            "pages": "821--830",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Toward more realistic drug-target interaction predictions",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pahikkala",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Airola",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pietil\u00e4",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Briefings Bioinform",
            "volume": "16",
            "issn": "2",
            "pages": "325--362",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Some case studies on application of r_m 2 metrics for judging quality of quantitative structure-activity relationship predictions: Emphasis on scaling of response data",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Chakraborty",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Mitra",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "J. Comput. Chem",
            "volume": "34",
            "issn": "12",
            "pages": "1071--82",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "On two novel parameters for validation of predictive qsar models",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "P"
                    ],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Paul",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Mitra",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Molecules",
            "volume": "14",
            "issn": "5",
            "pages": "1660--701",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Pred-binding: large-scale proteinligand binding affinity prediction",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "A"
                    ],
                    "last": "Shar",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Journal of Enzyme Inhibition & Medicinal Chemistry",
            "volume": "31",
            "issn": "6",
            "pages": "1443--50",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Self-attention with relative position representations",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Shaw",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT",
            "volume": "",
            "issn": "",
            "pages": "464--472",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Self-attention based molecule representation for predicting drug-target interaction",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the Machine Learning for Healthcare Conference",
            "volume": "106",
            "issn": "",
            "pages": "230--278",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Making sense of largescale kinase inhibitor bioactivity data sets: A comparative and integrative analysis",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Szwajda",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shakyawar",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "J Chem Inf Model",
            "volume": "54",
            "issn": "3",
            "pages": "735--778",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Dtigems+: drugtarget interaction prediction using graph embedding, graph mining, and similarity-based techniques",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Thafar",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "S"
                    ],
                    "last": "Olayan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ashoor",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "J Chem",
            "volume": "12",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Neodti: neural integration of neighbor information from a heterogeneous network for discovering new drug-target interactions",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "An",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Bioinformatics",
            "volume": "35",
            "issn": "1",
            "pages": "104--115",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "A computational-based method for predicting drug-target interactions by using stacked autoencoder deep neural network",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z-H",
                    "middle": [],
                    "last": "You",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "J Comput Biol",
            "volume": "25",
            "issn": "3",
            "pages": "361--73",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Dipeptide frequency of word frequency and graph convolutional networks for dta prediction",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Front Bioeng Biotechnol",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Predicting drug-target interactions using restricted boltzmann machines",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Bioinformatics",
            "volume": "29",
            "issn": "13",
            "pages": "126--160",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Deeplearning-based drug-target interaction prediction",
            "authors": [
                {
                    "first": "Ming",
                    "middle": [],
                    "last": "Wen",
                    "suffix": ""
                },
                {
                    "first": "Zhimin",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shaoyu",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "Haozhi",
                    "middle": [],
                    "last": "Sha",
                    "suffix": ""
                },
                {
                    "first": "Ruihan",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Yonghuan",
                    "middle": [],
                    "last": "Yun",
                    "suffix": ""
                },
                {
                    "first": "&quot;",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "Hongmei",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "J Proteome Res",
            "volume": "16",
            "issn": "4",
            "pages": "1401--1410",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Convolutional self-attention networks",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "F"
                    ],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis",
            "volume": "",
            "issn": "",
            "pages": "4040--4045",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Gansdta: Predicting drug-target binding affinity using gans",
            "authors": [
                {
                    "first": "Lingling",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Junjie",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Long",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "Yang",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Frontiers in Genetics",
            "volume": "10",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Attentiondta: prediction of drug-target binding affinity using attention model",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2019",
            "volume": "",
            "issn": "",
            "pages": "64--73",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Onionnet: a multiple-layer intermolecular-contact-based convolutional neural network for protein-ligand binding affinity prediction",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Mu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACS Omega",
            "volume": "4",
            "issn": "14",
            "pages": "15956--65",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Combining phenome-driven drug-target interaction prediction with patients' electronic health records-based clinical corroboration toward drug discovery",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Rong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Bioinformatics",
            "volume": "36",
            "issn": "",
            "pages": "436--480",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Illustration of our proposed MATT_DTI which considers drug SMILES and protein sequence as input to predict the binding affinity of drug-target pairs, which use a relation-aware self-attention block to strengthen the relative position information when encoding drug compounds and employ multi-head attention to model the interaction of drug representations and protein representations.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "CI results on KIBA dataset. Effects of relative position length in proposed MATT_DTI with a relation-aware self-attention block for drug representation and a multi-head self-attention in interaction learning.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "r 2 m results on KIBA dataset. Effects of relative position length in proposed MATT_DTI with relative self-attention for drug representation and a multi-head self-attention in interaction learning.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "CI results on Davis dataset. Effects of relative position length in proposed MATT_DTI with a relation-aware self-attention block for drug representation and a multi-head self-attention in interaction learning.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "r 2 m results on Davis dataset. Effects of relative position length in proposed MATT_DTI with a relation-aware self-attention block for drug representation and a multi-head self-attention in interaction learning.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Summary of the benchmark datasets",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Summary of parameter settings for MATT_DTI",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Test results on KIBA and Davis dataset. The proposed MATT_DTI model includes a relative self-attention block in drug representation learning. The max relative position length k in 'Rel_sa:CNN' is set to 5. In this table, 'sa:CNN' denotes that the model has a self-attention block before CNN layers, while 'Rel_sa:CNN' has a relative self-attention block in front of CNN layers",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Results on KIBA and Davis dataset of our proposed model and the existing baseline methodsTable 6. Parts of the FDA-approval antiviral drugs with top affinity scores of 3 genome sequences of SARS-CoV-2 predicted by our model",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "National Natural Science Foundation of China (Grants No. 61971296, U19A2078); Sichuan Science and Technology Planning Project (Grants No. 2020YFG0319, 2020YFH0186).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Funding"
        }
    ]
}
{
    "paper_id": "f9e7369d7d36a79ad720b8ccd3ff0eac5c322a43",
    "metadata": {
        "title": "Learning Functions Using Data-Dependent Regularization: Representer Theorem Revisited",
        "authors": [
            {
                "first": "Qing",
                "middle": [],
                "last": "Zou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Iowa",
                    "location": {
                        "postCode": "52242",
                        "settlement": "Iowa City",
                        "region": "IA",
                        "country": "USA"
                    }
                },
                "email": "zou-qing@uiowa.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "We introduce a data-dependent regularization problem which uses the geometry structure of the data to learn functions from incomplete data. We show another proof of the standard representer theorem when introducing the problem. At the end of the paper, two applications in image processing are used to illustrate the function learning framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Many machine learning problems involve the learning of multidimensional functions from incomplete training data. For example, the classification problem can be viewed as learning a function whose function values give the classes that the inputs belong to. The direct representation of the function in high-dimensional spaces often suffers from the issue of dimensionality. The large number of parameters in the function representation would translate to the need of extensive training data, which is expensive to obtain. However, researchers found that many natural datasets have extensive structure presented in them, which is usually known as manifold structure. The intrinsic structure of the data can then be used to improve the learning results. Nowadays, assuming data lying on or close to a manifold becomes more and more common in machine learning. It is called manifold assumption in machine learning. Though researchers are not clear about the theoretical reason why the datasets have manifold structure, it is useful for supervised learning and it gives excellent performance. In this work, we will exploit the manifold structure to learn functions from incomplete training data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Background"
        },
        {
            "text": "One of the main problems in numerical analysis is function approximation. During the last several decades, researchers usually considered the following problem to apply the theory of function approximation to real-world problems:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Motivated Example"
        },
        {
            "text": "where L is some linear operator, {(x i , y i )} n i=1 \u2282 X \u00d7 R are n accessible observations and X \u2282 R d , d \u2265 1 is the input space. We can use the method of Lagrange multiplier to solve Problem (1) . Assume that the searching space for the function f is large enough (for example L 2 space). Then the Lagrangian function C(f ) is given by",
            "cite_spans": [
                {
                    "start": 194,
                    "end": 197,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "A Motivated Example"
        },
        {
            "text": "Taking the gradient of the Lagrangian function w.r.t. the function f gives us",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Motivated Example"
        },
        {
            "text": "Setting C (f ) = 0, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Motivated Example"
        },
        {
            "text": "where \u03b4(\u00b7 \u2212 x) is the delta function. Suppose L * is the adjoint operator of L.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Motivated Example"
        },
        {
            "text": "Then we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Motivated Example"
        },
        {
            "text": "for some a i and (\u00b7, \u00b7).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Motivated Example"
        },
        {
            "text": "As machine learning develops fast these years, kernel methods [1] have received much attentions. Researchers found that working in the original data space is somehow not well-performed. So, we would like to map the data to a high dimensional space (feature space) using some non-linear mapping (feature map). Then we can do a better job (e.g. classification) in the feature space. When we talk about feature map, one concept that is unavoidable to mention is the kernel, which easily speaking is the inner product of the features. With a kernel (positive definite), we can then have a corresponding reproducing kernel Hilbert space (RKHS) [2] H K . We can now solve the problem that is similar to (1) in the RKHS: min",
            "cite_spans": [
                {
                    "start": 62,
                    "end": 65,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 639,
                    "end": 642,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "A more feasible way is to consider a regularization problem in the RKHS:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "Then the searching space of f becomes H K , which is a Hilbert space. Before solving Problem (2), we would like to recall some basic concepts about the RKHS. Suppose we have a positive definite kernel K :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "x n \u2208 X, a 1 , \u00b7 \u00b7 \u00b7 , a n \u2208 R, then H K is the Hilbert space corresponding to the kernel K(\u00b7, \u00b7). It is defined by all the possible linear combination of the kernel K(\u00b7, \u00b7), i.e., H K = span{K(\u00b7, \u00b7)}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "Thus, for any f (\u00b7) \u2208 H K , there exists x i and \u03b1 i such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "Since H K is a Hilbert space, it is equipped with an inner product. The principle to define the inner product is to let H K have representer K(\u00b7, x) and the representer performs like the delta function for functions in L 2 (note that delta function is not in L 2 ). In other word, we want to have a similar result to the following formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "This is called reproducing relation or reproducing property. In H K , we want to define the inner product so that we have the reproducing relation in H K :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "To achieve this goal, we can define",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "Then we have K(\u00b7, x), K(\u00b7, y) HK = K(x, y)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "With the kernel, the feature map \u03a6 \u00b7 (x) can be defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "Having these knowledge about the RKHS, we can now look at the solution of Problem (2) . It can be characterized by the famous conclusion named representer theorem, which states that the solution of Problem (2) is",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 85,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "The standard proof of the representer theorem is well-known and can be found in many literatures, see for example [3, 4] . While the drawback of the standard proof is that the proof did not provide the expression of the coefficients \u03b1 i . In the first part of this work, we will provide another proof of the representer theorem. As a by-product, we can also build the relation between Problem (1) and Problem (2).",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 117,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 118,
                    "end": 120,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Kernels and Representer Theorem"
        },
        {
            "text": "To give another proof of the representer theorem, we first build some relations between \u00b7, \u00b7 HK and \u00b7, \u00b7 L2 . We endow the dataset X with a measure \u03bc. Then the corresponding L 2 (X) inner product is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Consider an operator L on f with respect to the kernel K:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "which is the Hilbert-Schmidt integral operator [5] . This operator is self-adjoint, bounded and compact. By the spectral theorem [6] , we can obtain that the eigenfunctions e 1 (x), e 2 (x), \u00b7 \u00b7 \u00b7 of the operator will form an orthonormal basis of L 2 (X), i.e.,",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 50,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 129,
                    "end": 132,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "With the operator L defined as (3), we can look at the relations between \u00b7, \u00b7 L2(X) and \u00b7, \u00b7 HK . Suppose e i (x) are the eigenfunctions of the operator L and \u03bb i are the corresponding eigenvalues, then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "But by the reproducing relation, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Now, let us look at how to represent K(x, y) by the eigenfunctions. We have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "and \u03bb i can be computed by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "To see K(x, y) = i \u03bb i e i (x)e i (y), we can just plug it into (4) to verify it:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Since the eigenfunctions of L form an orthogonal basis of L 2 (X), then for any f \u2208 L 2 , it can be written as f = i a i e i (x). So we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "While for the L 2 norm, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Next we show that the orthonormal basis e i (x) are within H K . Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "So we can get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "We now need to investigate that for any",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "This means that to let f = i a i e i (x) \u2208 H K , we need to have i",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Combining all these analysis, we can then get the following relation between \u00b7, \u00b7 L2(X) and \u00b7, \u00b7 HK :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "According to which, we can have another proof of the representer theorem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Proof. Suppose e 1 , e 2 , \u00b7 \u00b7 \u00b7 are eigenfunctions of the operator L. Then we can write the solution as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "We consider here a more general form of Problem (2):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "where E(\u00b7, \u00b7) is the error function which is differentiable with respect to each a i . We would use the tools in L 2 (X) space to get the solution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "The cost function of the regularization problem is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "By substituting f * into the cost function, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "differentiating C(f * ) w.r.t. each a i and setting it equal to zero gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Solving a k , we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Since f * = k a k e k (x), we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "This proves the representer theorem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Note that this result not only proves the representer theorem, but also gives the expression of the coefficients \u03b1 i .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "With the operator L, we can also build a relation between Problem (1) and Problem (2) . Define the operator in Problem (1) to be the inverse of the Hilbert-Schmidt Integral operator. The discussion on the inverse of the Hilbert-Schmidt Integral operator can be found in [8] . Note that for the delta function, we have",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 85,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 270,
                    "end": 273,
                    "text": "[8]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "Then the solution of Problem (1) becomes 2(",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "By which we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Another Proof of Representer Theorem"
        },
        {
            "text": "So far, we have introduced the standard representer theorem. While as we discussed at the very beginning, many natural datasets have the manifold structure presented in them. So based on the classical Problem (2), we would like to introduce a new learning problem which exploits the manifold structure of the data. We call it the data-dependent regularization problem. Regularization problem has a long history going back to Tikhonov [9] . He proposed the Tikhonov regularization to solve the ill-posed inverse problem.",
            "cite_spans": [
                {
                    "start": 434,
                    "end": 437,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Data-Dependent Regularization"
        },
        {
            "text": "To exploit the manifold structure of the data, we can then divide a function into two parts: the function restricted on the manifold and the function restricted outside the manifold. So the problem can be formulated as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Dependent Regularization"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Dependent Regularization"
        },
        {
            "text": "The norms || \u00b7 || M and || \u00b7 || M c will be explained later in details. \u03b1 and \u03b2 are two parameters which control the degree for penalizing the energy of the function on the manifold and outside the manifold. We will show later that by controlling the two balancing parameters (set \u03b1 = \u03b2), the standard representer theorem is a special case of Problem (5).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data-Dependent Regularization"
        },
        {
            "text": "We now discuss something about the functions f 1 and f 2 . Consider the ambient space X \u2282 R n (or R n ) and a positive definite kernel K. Let us first look at the restriction of K to the manifold M \u2282 X. The restriction is again a positive definite kernel [2] and it will then have a corresponding Hilbert space. We consider the relation between the RKHS H K and the restricted RKHS to explain the norms || \u00b7 || M and || \u00b7 || M c .",
            "cite_spans": [
                {
                    "start": 255,
                    "end": 258,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Data-Dependent Regularization"
        },
        {
            "text": "K : X \u00d7 X \u2192 R (or R n \u00d7 R n \u2192 R) is a positive definite kernel. Let M be a subset of X (or R n )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 1 ([10]). Suppose"
        },
        {
            "text": "with the norm defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "Proof. Define the set",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "We first show that the set A = {||f || HK : f \u2208 H K , f| M = f r } has a minuma for any f r \u2208 S(M). Choose a sequence {f i } \u221e i=1 \u2282 H K . Then the sequence is bounded because the space H K is a Hilbert space. It is reasonable to assume that",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "is weakly convergent because of the Banach-Alaoglu theorem [11] . By the weakly convergence, we can obtain pointwise convergence according to the reproducing property. So the limit of the sequence {f i } \u221e i=1 attains the minima. We further define ||f r || S(M) = min A. We show that S(M), || \u00b7 || S(M) is a Hilbert space by the parallelogram law. In other word, we are going to show that",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 63,
                    "text": "[11]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "Since we defined ||f r || S(M) = min A. Then for all f 1 , g 1 \u2208 S(M), there exists f, g \u2208 H K such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "By the definition of S(M), we can choose f 1 , g 1 such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "For the reverse inequality, we first choose f 1 , g 1 such that ||f 1 || 2 S(M) = ||f || 2 HK and ||g 1 || 2 S(M) = ||g|| 2 HK . Then",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "Therefore, we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "Next, we show (6) by showing that for all f r \u2208 S(M) and x \u2208 M,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "Choose f \u2208 H K such that f r = f | M and ||f r || S(M) = ||f || HK . This is possible because of the analysis above. Specially, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "Now, for any function f \u2208 H K such that f | M = 0, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "Thus,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "This completes the proof of the lemma.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "With this lemma, the solution of Problem (5) then becomes easy to obtain. By the representer theorem we mentioned before, we know that the function satisfies min",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "Thus, we can conclude that is solution of (5) is exactly",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "where the coefficients a i are controlled by the parameters \u03b1 and \u03b2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "With the norms || \u00b7 || M and || \u00b7 || M c being well-defined, we would like to seek the relation between || \u00b7 || M , || \u00b7 || M c and || \u00b7 || HK . Before stating the relation, we would like to restate some of the notations to make the statement more clear. Let",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "To find the relation between || \u00b7 || M , || \u00b7 || M c and || \u00b7 || HK , we need to pullback the restricted kernel K 1 and K 2 to the original space. To do so, define",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "Then we have K = K p 1 + K p 2 . The corresponding Hilbert spaces for K p 1 and K p",
            "cite_spans": [],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "The following lemma shows the relation between || \u00b7 || H K p 1 , || \u00b7 || H K p 2 and || \u00b7 || HK , which also reveals the relation between || \u00b7 || M , || \u00b7 || M c and || \u00b7 || HK by Moore-Aronszajn theorem [12] . Note that we can also use other kernels, for example, polynomial kernel and Gaussian kernel to proceed image interpolation. Choosing the right kernel is an interesting problem and we do not have enough space to compare different kernels in this paper.",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 208,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "In Fig. 2(b) , we downsampled the original image by a factor of 3 in each direction. The zoomed image is shown in Fig. 2(e) . The interpolation result with the zoomed image are shown in Fig. 2(c) and Fig. 2(f) . ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "text": "Fig. 2(b)",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 114,
                    "end": 123,
                    "text": "Fig. 2(e)",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 186,
                    "end": 195,
                    "text": "Fig. 2(c)",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 200,
                    "end": 209,
                    "text": "Fig. 2(f)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": ". F(M) denote all the functions defined on M. Then the RKHS given by the restricted kernel"
        },
        {
            "text": "From the function learning point of view, the patch-based image denoising problem can be viewed as learning a function from noisy patches to their \"noise-free\" centered pixels. See Fig. 3 for illustration.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 181,
                    "end": 187,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Patch-Based Image Denoising"
        },
        {
            "text": "In the patch-based image denoising application, we use the Laplacian kernel as well. We assume that the noisy patches are lying close to some manifold so we set the balancing parameter which controls the energy outside the manifold to be large enough. We use the images in Fig. 4 as known data to learn the function. Then for a given noisy image, we can use the learned function to do image denoising. To speed up the learning process, we randomly choose only 10% of the known data to learn the function. We use the image Baboon to test the learned denoising function. The denoising results are shown in Fig. 5 . Each column shows the result corresponding to one noise level. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 273,
                    "end": 279,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 604,
                    "end": 610,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Patch-Based Image Denoising"
        },
        {
            "text": "In this paper, we introduced a framework of learning functions from part of the data. We gave a data-dependent regularization problem which helps us learn a function using the manifold structure of the data. We used two applications to illustrate the learning framework. While these two applications are just part of the learning framework. They are special cases of the data-dependent regularization problem. However, for the general application, we need to calculate ||f 1 || 2 M and ||f 2 || 2 M c , which is hard to do so since we only have partial data. So we need to approximate ||f 1 || 2 M and ||f 2 || 2 M c from incomplete data and to propose a new learning algorithm so that our framework can be used in a general application. This is part of our future work. Another line for the future work is from the theoretical aspect. We showed that the solution of the data-dependent regularization problem is the linear combination of the kernel. It then can be viewed as a function approximation result. If it is an approximated function, then we can consider the error analysis of the approximated function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Bach",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Theory of reproducing kernels",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Aronszajn",
                    "suffix": ""
                }
            ],
            "year": 1950,
            "venue": "Trans. Am. Math. Soc",
            "volume": "68",
            "issn": "3",
            "pages": "337--404",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A generalized representer theorem",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Herbrich",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "COLT 2001. LNCS (LNAI)",
            "volume": "2111",
            "issn": "",
            "pages": "416--426",
            "other_ids": {
                "DOI": [
                    "10.1007/3-540-44581-1_27"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "When is there a representer theorem? vector versus matrix regularizers",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Argyriou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "A"
                    ],
                    "last": "Micchelli",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pontil",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "J. Mach. Learn. Res",
            "volume": "10",
            "issn": "",
            "pages": "2507--2529",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Hilbert-Schmidt operators",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Gohberg",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Goldberg",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Kaashoek",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Classes of Linear Operators",
            "volume": "I",
            "issn": "",
            "pages": "138--147",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Introduction to Spectral Theory in Hilbert Space",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Helmberg",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Courier",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Manifold regularization: a geometric framework for learning from labeled and unlabeled examples",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Mikhail",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Partha",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vikas",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "J. Mach. Learn. Res",
            "volume": "7",
            "issn": "",
            "pages": "2507--2529",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Regularization of incorrectly posed problems",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Tikhonov",
                    "suffix": ""
                }
            ],
            "year": 1963,
            "venue": "Soviet Math. Doklady",
            "volume": "4",
            "issn": "6",
            "pages": "1624--1627",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Theory of Reproducing Kernels and Applications",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Saitoh",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sawano",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1007/978-981-10-0530-5"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Adaptive learning for symbol detection: a reproducing kernel hilbert space approach",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "D"
                    ],
                    "last": "Amir",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "C R"
                    ],
                    "last": "Luis",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yukawa",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Stanczak",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Mach. Learn. Fut. Wirel. Commun",
            "volume": "",
            "issn": "",
            "pages": "197--211",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Kernel Functions for Machine Learning Applications",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Illustration of image interpolation. The size of the original image is 6 \u00d7 6. We want to enlarge it as an 11 \u00d7 11 image. Then the blue shaded positions are unknown. Using image interpolation, we can find the values of these positions. (Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Illustration of image interpolation. The original image is downsampled by a factor of 3 in each direction. We use the proposed function learning framework to obtain the interpolation function from downsampled image. From the results, we can see that the proposed framework works for image interpolation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Illustration of patch-based image denoising. It can be viewed as learning a function from the m \u00d7 m noisy patches to the centered clean pixels.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Four training images. We use noisy images and clean pixels to learn the denoising function.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Illustration of the denoising results.",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "Lemma 2. Suppose K 1 , K 2 : Y \u00d7 Y \u2192 R (or R n \u00d7 R n \u2192 R) are two positive definie kernels. If K = K 1 + K 2 , thenThe idea of the proof of this lemma is exactly the same as the one for Lemma 1. Thus we omit it here.A direct corollary of this lemma is: If we go back to our scenario, we can get the following result by Corollary 1:This means that if we set \u03b1 = \u03b2 in Problem (5), it will reduce to Problem (2) . Therefore, the standard representer theorem is a special case of our datadependent regularization problem (5).",
            "cite_spans": [
                {
                    "start": 405,
                    "end": 408,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "annex"
        },
        {
            "text": "As we said in the introduction part, many engineering problems can be viewed as learning multidimensional functions from incomplete data. In this section, we would like to show two applications of functions learning: image interpolation and patch-based iamge denoising.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Applications"
        },
        {
            "text": "Image interpolation tries to best approximate the color and intensity of a pixel based on the values at surrounding pixels. See Fig. 1 for illustration. From function learning perspective, image interpolation is to learn a function from the known pixels and their corresponding positions. We would like to use the Lena image as shown in Fig. 2(a) to give an example of image interpolation utilizing the proposed framework. The zoomed image is shown in Fig. 2(d) . In the image interpolation example, the two balancing parameters are set to be the same and the Laplacian kernel [13] is used:",
            "cite_spans": [
                {
                    "start": 577,
                    "end": 581,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [
                {
                    "start": 128,
                    "end": 134,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 337,
                    "end": 346,
                    "text": "Fig. 2(a)",
                    "ref_id": null
                },
                {
                    "start": 452,
                    "end": 461,
                    "text": "Fig. 2(d)",
                    "ref_id": null
                }
            ],
            "section": "Image Interpolation"
        }
    ]
}
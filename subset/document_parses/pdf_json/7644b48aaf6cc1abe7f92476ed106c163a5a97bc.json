{
    "paper_id": "7644b48aaf6cc1abe7f92476ed106c163a5a97bc",
    "metadata": {
        "title": "A Clustering-Based Method for Automatic Educational Video Recommendation Using Deep Face-Features of Lecturers",
        "authors": [
            {
                "first": "Paulo",
                "middle": [
                    "R C"
                ],
                "last": "Mendes",
                "suffix": "",
                "affiliation": {
                    "laboratory": "TeleMidia Lab",
                    "institution": "Pontifical Catholic University of Rio de Janeiro Rio de Janeiro",
                    "location": {
                        "country": "Brazil"
                    }
                },
                "email": ""
            },
            {
                "first": "Eduardo",
                "middle": [
                    "S"
                ],
                "last": "Vieira",
                "suffix": "",
                "affiliation": {
                    "laboratory": "TeleMidia Lab",
                    "institution": "Pontifical Catholic University of Rio de Janeiro Rio de Janeiro",
                    "location": {
                        "country": "Brazil"
                    }
                },
                "email": ""
            },
            {
                "first": "\u00c1lan",
                "middle": [
                    "L V"
                ],
                "last": "Guedes",
                "suffix": "",
                "affiliation": {
                    "laboratory": "TeleMidia Lab",
                    "institution": "Pontifical Catholic University of Rio de Janeiro Rio de Janeiro",
                    "location": {
                        "country": "Brazil"
                    }
                },
                "email": ""
            },
            {
                "first": "Antonio",
                "middle": [
                    "J G"
                ],
                "last": "Busson",
                "suffix": "",
                "affiliation": {
                    "laboratory": "TeleMidia Lab",
                    "institution": "Pontifical Catholic University of Rio de Janeiro Rio de Janeiro",
                    "location": {
                        "country": "Brazil"
                    }
                },
                "email": "busson@telemidia.puc-rio.br"
            },
            {
                "first": "S\u00e9rgio",
                "middle": [],
                "last": "Colcher",
                "suffix": "",
                "affiliation": {
                    "laboratory": "TeleMidia Lab",
                    "institution": "Pontifical Catholic University of Rio de Janeiro Rio de Janeiro",
                    "location": {
                        "country": "Brazil"
                    }
                },
                "email": "colcher@inf.puc-rio.br"
            }
        ]
    },
    "abstract": [
        {
            "text": "Discovering and accessing specific content within educational video bases is a challenging task, mainly because of the abundance of video content and its diversity. Recommender systems are often used to enhance the ability to find and select content. But, recommendation mechanisms, especially those based on textual information, exhibit some limitations, such as being error-prone to manually created keywords or due to imprecise speech recognition. This paper presents a method for generating educational video recommendation using deep facefeatures of lecturers without identifying them. More precisely, we use an unsupervised face clustering mechanism to create relations among the videos based on the lecturer's presence. Then, for a selected educational video taken as a reference, we recommend the ones where the presence of the same lecturers is detected. Moreover, we rank these recommended videos based on the amount of time the referenced lecturers were present. For this task, we achieved a mAP value of 99.165%.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The traditional paradigm of classroom courses centered on the physical presence of a teacher has been gradually giving space to online and hybrid courses, which enables the emergence of VTEs (Virtual Teaching Environment) and MOOCs (Massive Open Online Courses, such as Udacity, 1 Coursera, 2 and EdX). 3 For example, in 2018, a study [1] has shown that almost 59% of people aged 14 to 23 prefer YouTube as a learning tool rather than printed books, with 55% of them also saying that YouTube has contributed to their education. Recently, due to the covid-19 outbreak, the world has experienced an unprecedented usage of virtual education [2] , and some say that this model of education came to stay. 4 If, on the one hand, the abundance of educational videos can contribute to and facilitate learning, on the other hand, it also makes it challenging to discover and access the content of interest [3] . This issue is usually addressed by a proactive user search (using queries, for example), or by automatic recommendations made by specialized systems.",
            "cite_spans": [
                {
                    "start": 279,
                    "end": 280,
                    "text": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 303,
                    "end": 304,
                    "text": "3",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 335,
                    "end": 338,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 638,
                    "end": 641,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 700,
                    "end": 701,
                    "text": "4",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 897,
                    "end": 900,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Recommendation mechanisms are usually based on two methods: collaborative filtering and content-based filtering. In collaborative filtering, the system groups users based on their common interest on items, using users' preferences, rates, purchases or accesses to those items. With this approach, knowledge about the item's content is not needed; the recommendation is purely based on the relationship between users and items. The content-based filtering, differently, requires items' description; similar items are the ones recommended to the user.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "In general, the current video recommendation methods are heavily dependent on textual information from the video, such as labels (i.e. keywords) [4] , [5] , or automatically generated captions [6] from the lecturer speech. These systems face problems such as errors introduced by manually inserted labels and by imprecise speech recognition. In our research, we aim to investigate methods that are able to perform video recommendations that are not based on content nor on any error-prone textual descriptions, but solely on lecturers' presence. Notice that this approach does not necessarily have to completely replace textual-based recommendations; in fact, it can be easily used as an additional aid to enhance the ability to find content in any system. Face detection methods have been attracting the attention of researchers for more than two decades [7] . Nowadays, it is used for surveillance, video analytics systems, smart shopping, automatic face tagging in photo collections, investigative tools that search for identities in social networks based on face images, and thousands of other applications in our daily lives. For instance, Facer [8] is the Facebook's face detection and recognition framework; given a photograph, it first detects all the faces, and then runs a deep model to determine the likelihood of that face belonging to one of the top-N user friends. This allows Facebook to suggest which friends the user might want to tag within the uploaded photographs.",
            "cite_spans": [
                {
                    "start": 145,
                    "end": 148,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 151,
                    "end": 154,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 193,
                    "end": 196,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 856,
                    "end": 859,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1151,
                    "end": 1154,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "This work aims at recommending educational video content based on lecturers' presence. To do that, we take advantage of face detection methods. More precisely, we detect lecturers in a video taken as a reference and perform a clustering based on the face of these lecturers in different videos. Given these clusters, we extract their centroids (explained in Section III), and perform another clustering step for creating a relationship between videos that share the presence of the same lecturers. Finally, we rank the recommended videos based on the amount of time the referenced lecturers were present. A particular feature of this approach is that it can be done without supervision, allowing for new videos to be automatically analyzed. Moreover, our approach permits the creation of timelines based on lecturers' presence that can be used in the search for specific parts of a content where only specific lecturers' are present. To evaluate our recommendation ranking, we use the mAP (Mean Average Precision) metric, which is commonly used in information retrieval evaluation tasks [9] .",
            "cite_spans": [
                {
                    "start": 1087,
                    "end": 1090,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The remainder of this paper is structured as follows. Section II discusses some related work. Then, we present our method in Section III. Section IV presents the used dataset, followed by Section V, that shows the experiments to validate the face clustering and the video recommendation ranking mechanisms. Finally, Section VI brings our final remarks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "We have organized the related work into two groups. In the first, we grouped works that share our goal of educational video recommendation but do not necessarily use faceembeddings (deep face-features). The second group is the one in which every work addresses the task of face recognition in videos.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "Regarding Educational Video Recommendation, we cite works based on content-filtering. These works perform analyses and comparisons using the video textual description or speech recognition performed on them. Omisore et. al. [5] , for example, propose combining fuzzy techniques to recommend books with content suitable for students based on their reading histories in a digital library, while Mahajan et. al. [4] propose, given a reference video, mining social media, and web for suggesting links for a student to visit. Moreover, Barr\u00e9re et. al. [6] use texts from speech recognition to create recommendations. These works are only based on textual characteristics (or content converted to it) for performing recommendations. Our work focuses on using a visual part of the video, more precisely the presence of lecturers.",
            "cite_spans": [
                {
                    "start": 224,
                    "end": 227,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 409,
                    "end": 412,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 547,
                    "end": 550,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "Works based on Video Face Recognition usually apply deep learning models for the task. DeepFace [10] and DeepID [11] , for example, use a CNN (Convolutional Neural Network) with a fully-connected layer output to produce a representation of high-level features (face embeddings) from an input image, followed by a softmax layer to indicate the identity of classes. Other approaches, such as FaceNet [12] , can directly measure the similarity among faces using euclidean space. Yang et al. [13] proposed a deep network for video face recognition called NAN (Neural Aggregation Network). They use a CNN to generate the embeddings, followed by an aggregation module that consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them. Rao et al. [14] proposed a method for video face recognition based on attention-aware deep reinforcement learning. They formulated the process of finding the attention of videos as a Markov decision process and training the attention model without using extra labels. Unlike existing attention models, their method takes information from both the image space and the feature space as the input to make use of face information that is discarded in the feature learning process. Sohn et al. [15] proposed an adaptative deep learning framework for image-based face recognition and video-based face recognition. Given an embedding generated by a CNN, their framework adaptation is achieved by (1) distilling knowledge from the network to a video adaptation network through feature matching, (2) performing feature restoration through synthetic data augmentation, and (3) learning a domain-invariant feature through an adversarial domain discriminator.",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 100,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 112,
                    "end": 116,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 398,
                    "end": 402,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 488,
                    "end": 492,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 823,
                    "end": 827,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1301,
                    "end": 1305,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "Like [13] , [14] , [15] , our method uses a CNN to generate face embeddings from face images, with the difference that we use an unsupervised cluster-based method to compare the similarity among faces extracted from videos.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 12,
                    "end": 16,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 19,
                    "end": 23,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "Our method intends to recommend educational videos based on the lecturers that appear in each video, so that, when a person watches a video, other videos containing the same lecturers are recommended. For didactic purposes we decided to divide our exposition in two phases: (i) video representation and (ii) video recommendation, which are described in Sections III-A and III-B respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. METHOD"
        },
        {
            "text": "The objective of this phase is to represent each video with vectors (centroids) of the lecturers that appear on it. Fig. 1 shows the pipeline we propose for this phase, described in the remainder of this subsection. It is divided into four steps: Frames Extraction, Face Detection, Embeddings Generation and Clustering Representation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 116,
                    "end": 122,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "A. Video Representation"
        },
        {
            "text": "First, we perform the Frames Extraction by receiving a video file as input and extracting its frames according to a given frame rate. Next, for each of the frames, the Face Detection step uses an object detection model for detecting faces in each of them. The face detection model is responsible for returning the bounding boxes of the faces present in the image giving the x and y axes coordinates of the upper-left corner and of the lower-right corner of the rectangle that establishes the visual limits encapsulating each face. With these bounding boxes, we can isolate and extract the bounded images, obtaining a dataset composed of images of faces only.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "The objective of the Embeddings Generation step is to represent each face image as a vector in R n . To achieve that, it processes each of the faces generated in the previous step through a CNN that generates their embeddings. An embedding is a representation of the input in a lower dimensionality space. Ideally, an embedding captures some semantics of the input, e.g. by placing semantically similar inputs close together in an embedding space. Therefore, at the end of this step, we have all faces represented as embeddings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "In the Clustering Representation step, we group embeddings (e), and, consequently, faces that are close enough in the embedding space using a clustering algorithm. Clustering is the task of dividing a set of data points, embeddings in this case, into a number of groups (called clusters) such that data points in a given group are similar to other data points in the same group and dissimilar to the data points in other groups. The clustering process should produce a partition of the faces present in the frames, hopefully with each generated cluster representing a specific person; moreover, the union of all clusters covers the whole set of faces found in the video.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "As most of the clustering algorithms require the number of clusters as parameter, we use a strategy (defined in Algorithm 1) based on the Silhouette Score (s) [16] , that corresponds to the mean of the Silhouette Coefficient (\u03c3) of all samples. This coefficient for each sample is",
            "cite_spans": [
                {
                    "start": 159,
                    "end": 163,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "where a is the mean distance from a sample to all other samples in the same cluster, and b is the mean distance from a sample to all other samples in the closest cluster to that sample. In this way, the best value is 1 and the worst is -1. Values close to 0 indicate overlapping clusters, whereas negative values usually indicate that a sample has been assigned to the wrong cluster since a different cluster is more similar.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "With the strategy defined in Algorithm 1, we increase the number of clusters until the maximum Silhouette Score decreases to more than t times in a row or until it reaches the maximum number of clusters (lines 5-18), which is the number of embeddings (|e|). The Clustering procedure (line 7) can be substituted by any clustering algorithm that requires the number of clusters in advance. When the iteration stops, we return the clustering configuration with the highest Silhouette Score. Since the Silhouette Coefficient requires at least two clusters, it would not be possible to compute the Silhouette Score for a clustering configuration with only one cluster (there are only faces of a single person). To overcome this problem, we start with 2 clusters consecutively increasing it as described above. Then, if the returned clustering configuration has a Silhouette Score smaller than a threshold \u03c9, that Algorithm 1 Iteratively finding the best clustering configuration for unknown number of clusters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "1: procedure BLINDCLUSTERING(e, t, \u03c9) 2:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "n K \u2190 n K + 1 return K 23: end procedure probably indicates overlapping, we say that all faces belong to one single cluster (lines [19] [20] .",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 135,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 136,
                    "end": 140,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "Next, we compute the clusters' centroids for each of the clusters k \u2208 K where K is the best clustering configuration found with the Silhouette Score. A centroid c k for each cluster is the mean of the elements present in the cluster, and can be defined as follows",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "where a represent each element of a cluster k.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "By the end of this phase, we have each video in the dataset represented by its centroids where, ideally, each centroid represents a lecturer present in the video. We also record the frames where each lecturer is present. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Video Representation"
        },
        {
            "text": "This phase aims at recommending videos by the lecturers present in it and in the other videos. It is divided in two steps: Centroids Clustering and Ranking, as depicted in Fig. 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 172,
                    "end": 178,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "B. Video Recommendation"
        },
        {
            "text": "First, we gather the centroids from the videos of the dataset as one single set and perform the Centroids Clustering. For performing this clustering, we also use the strategy for an unknown number of clusters described in Algorithm 1. By doing that, we group centroids from the same lecturer that are in different videos. For instance, in Fig. 2 , one can see that the purple lecturer is present in both Videos 1 and 2, while the orange lecturer is present in both Videos 2 and n. By the end of this step, we have the group L of lecturers present in the dataset of videos V , and we can also denote L v as the group of lecturers present in video v.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 339,
                    "end": 345,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "B. Video Recommendation"
        },
        {
            "text": "Next, based on these relationships among different videos, we perform Ranking, by recommending videos in which lecturers of the current video are present. For doing that, we compute a similarity score using the presence of the lecturers in the current video and the presence of these same lecturers in the other video. Let p l,v denote the percentage of frames in which the lecturer l \u2208 L v is present in video v \u2208 V . For each video v \u2208 V and u \u2208 V \u2212 v we compute a score of similarity S v,u .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Video Recommendation"
        },
        {
            "text": "Finally, using this score, for each video v we compute a ranking R v where R v,i denotes the i-greatest S v and R v,i \u2265 R v,i+1 for all i \u2208 1...n v , where n v is the number of videos u in which S v,u > 0. In this way, the more lecturers a video have in common with the reference video, and the more time these lecturers are present in both videos, the higher the video is positioned in the ranking of the reference video.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Video Recommendation"
        },
        {
            "text": "By the end of this phase, we have a ranking of recommended videos for each video in the dataset. It is important to notice that our method is unsupervised and does not require the information of the lecturers in advance. Consequently, we do not store any information regarding the identity of the lecturers, respecting their privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Video Recommendation"
        },
        {
            "text": "The experiments were conducted using a dataset created in the context of this work. It is composed of 98 educational videos publicly available on YouTube. 5, 6 Each video contains at least one lecturer; moreover, some videos could have some special participation or collaboration. Thus, each video is annotated to contain between 1 to 5 people. In total, 16 people are present in the dataset. Each person has an average presence of 6.67% in the videos, and their identities are known and ready to be used to assist in the nominal dataset organization.",
            "cite_spans": [
                {
                    "start": 155,
                    "end": 157,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 158,
                    "end": 159,
                    "text": "6",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "IV. DATASET"
        },
        {
            "text": "Regarding the duration, the videos vary between 00m:30s and 1h:49m:01s. The average duration of the videos is 23m:34s, with a standard deviation of 23m:05s. The high value of the standard deviation for the time estimates indicates that the videos are not in the same time range, and therefore have a wide duration variety.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. DATASET"
        },
        {
            "text": "First we compute the centroids that represent each lecturer in each video using the process described in Section III-A. Next we perform the video recommendation using the process described in Section III-B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS"
        },
        {
            "text": "For representing the video files in the dataset, we start by performing Frames Extraction for each video file using a frame rate of 1 frame per second (fps). Next, in the Face Detection step, we use MTCNN [17] (Multitask Cascaded Convolutional Networks), which is widely used for the face detection task [18] , [19] , [20] . Once we have detected the faces of lecturers in the video frames, we perform Embeddings Generation using SE-ResNet-50 [21] (SeNet-50 for short) that generates embeddings on the R 2048 feature space. We used the architecture and weights pre-trained on the VGGFace2 dataset [22] , available on the keras-vggface library. 7 The VGGFace2 dataset contains 3.31 million images of 9,131 subjects and has large variations in pose, age, illumination, ethnicity, and profession. Finally, we use Algorithm 1 in the Clustering Representation step with the parameters t = 5, \u03c9 = 0.2, and the Ward Agglomerative Clustering [23] as the Clustering procedure using its implementation in the scikit-learn [24] library. The Ward Agglomerative Clustering algorithm merges pairs of clusters that minimize the ward criterion, which is the variance of the clustering being merged. By using this method, at each step, the algorithm finds the pair of clusters that lead to a minimum increase in total within-cluster variance after merging. Finally, we compute the centroids of each of the clusters generated.",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 209,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 304,
                    "end": 308,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 311,
                    "end": 315,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 318,
                    "end": 322,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 443,
                    "end": 447,
                    "text": "[21]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 597,
                    "end": 601,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 644,
                    "end": 645,
                    "text": "7",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 934,
                    "end": 938,
                    "text": "[23]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1012,
                    "end": 1016,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "V. EXPERIMENTS"
        },
        {
            "text": "For performing the video recommendation task, we gather the centroids (that represent each lecturer in the video) from all videos in the dataset. Next, we perform the process described in Section III-B. For Centroids Clustering, we also use Algorithm 1 with the same parameters t = 5, \u03c9 = 0.2, and the Ward Aglommerative Clustering as Clustering procedure. Finally, based on the clusters generated, we perform the Ranking step.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS"
        },
        {
            "text": "The remainder of this Section describes the evaluation of the centroids clustering (Section V-A) and the evaluation of the video recommendation (Section V-B).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. EXPERIMENTS"
        },
        {
            "text": "Our evaluation aims at discovering the precision achieved by the clustering over the centroids. More precisely, we want to evaluate how well our approach identified that the same lecturer is present in different video files. For this task, we require some human feedback. To receive that feedback, we developed an application, called VideoFacesTool consisting of a graphical web interface. The tool allows participants to import a file, which contains information after the Centroids Clustering step, so that we have the set of centroids and a sample face image of it. Inside the tool, a cluster is called a group. After being imported, faces are visually organized according to the group to which they belong and when a group is selected, all face centroids from that group are displayed, as shown in Fig. 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 802,
                    "end": 808,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "A. Centroids Clustering Evaluation"
        },
        {
            "text": "Each face centroid has a Boolean property, which indicates whether it is correctly grouped (it belongs to a group in which all the face centroids are from the same lecturer) or not. If there is an error, the participant can indicate it.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Centroids Clustering Evaluation"
        },
        {
            "text": "A total of 5 participants collaborated in the evaluation session. They were advised to mark a face centroid as wrong if it represents (a) an object, or (b) a part of the human body, or (c) a lecturer other than the lecturer in the group. Fig. 4 shows examples of these types of errors, and Table I provides an overview of the evaluations obtained from each participant. It is important to notice that these results do not reflect the recommendation of educational videos, they only evaluate the Centroids Clustering step. For instance, we could have a group of face centroids of people that appear for a few amount of time in the videos and are not lecturers. This case, of course, would reduce the precision of the Centroids Clustering step. However, our method for video recommendation and ranking is robust to these cases, as it considers the amount of time that a person appears for scoring the recommended videos. With the evaluation completed, it is possible to export the analysis information with the number of right and wrong face centroids. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 238,
                    "end": 244,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 290,
                    "end": 297,
                    "text": "Table I",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "A. Centroids Clustering Evaluation"
        },
        {
            "text": "We evaluate our approach based on the relevance of the videos recommended. A video is considered relevant to another if they have at least one lecturer in common. To verify that, we use the information of the lecturers' presence available on our dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Recommendation Evaluation"
        },
        {
            "text": "To evaluate our ranking, for each video we compute the Average Precision (AP), that evaluates how well a ranking of recommendations is based on each element's relevancy. This metric penalizes more a ranking if a non-relevant element is recommended in the first positions than if it was in the last ones. Let P k be the precision of the first k elements of a ranking, which is the percentage of videos that are relevant in the sub-ranking that starts at position 1 and ends at position k. Let \u03b1 k denote the relevancy of the video in position k, where \u03b1 k = 1 if the video is relevant, and 0 otherwise. The AP of a given ranking is defined as follows",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Recommendation Evaluation"
        },
        {
            "text": "where GTP refers to the total number of ground truth positives in the ranking, which is the total number of videos that are considered relevant in a ranking. Fig. 5 shows an example of how the AP is computed for a given ranking. In this case, the GT P = 3 because the total number of relevant videos in the ranking is 3 (videos A, B and D).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 158,
                    "end": 164,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "B. Recommendation Evaluation"
        },
        {
            "text": "In order to prevent outliers from having much influence in the recommendation (e.g. a person that is not a lecturer -and not relevant to the video -and appears for a short amount of time), we experimented different thresholds of presence intervals in a video for a person to be considered as \"present\" when computing the score for the ranking. In this way, a p l,v lesser than the threshold is considered as 0. Besides the Average Precision, we also compute the mean and minimum values of the recall (MeanR and MinR), precision (MeanP and MinP), and F1-Score (MeanF1 and Min F1) for the recommen- dation generated for each of the videos, without considering the positioning of these videos in the rankings. The recall refers to the percentage of relevant videos that are present in the ranking. The F1-score represents an overall performance metric based on the harmonic mean of the precision and recall and is defined as follows. Table II shows the thresholds used, the values of recall, precision, F1-score, and the mean and minimum Average Precision (mAP and MinAP), One can observe from Table II that the precision clearly increased with the use of the threshold. Different from the precision, the recall decreased with the increase of the threshold. It means that with a greater threshold more videos that should be recommended were not chosen by our method. It is important to notice that these two metrics (precision and recall) do not consider the ordering of the recommendations. Different from them, the Mean Average Precision (mAP) has high values for all thresholds, specially because the score for computing the ranking takes into consideration the percentage of time that a person appears in the reference and recommended videos. Then, we can conclude that our proposed approach for ordering the recommended videos tends to recommend more suitable videos first with a high mAP\u2248 0.99. Moreover, despite the precision of the Clustering Step shown in Table I , our method for ranking was robust to outliers and was able to correctly recommend and rank relevant videos.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 931,
                    "end": 939,
                    "text": "Table II",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1091,
                    "end": 1099,
                    "text": "Table II",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1962,
                    "end": 1969,
                    "text": "Table I",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "B. Recommendation Evaluation"
        },
        {
            "text": "In this paper, we present a method for educational video recommendation using deep-face-features of lecturers. More precisely, we use an unsupervised clustering-based method and an heuristic for ranking. It takes advantage of face detection mechanisms to perform educational video recommendation based on the lecturers' presence. Besides the face detection, we also perform face clustering of the lecturers in each video, and, given these clusters, we extract their centroids to perform another clustering step that creates a relationship of videos that share the presence of the same lecturers. Finally, we rank the recommended videos based on the amount of time each lecturer is present. It is worth mentioning that our method is completely automatic and does not require any information of the video files in advance. Moreover, our approach does not need to know or store the identity of the lecturers for performing recommendation, preserving their privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. FINAL REMARKS"
        },
        {
            "text": "A collateral contribution of our paper is video segmentation the by lecturer. As illustrated in Fig. 6 , we can create a timeline based on lecturers' presence, which can be used to help students in finding moments where specific lecturers are present. With this segmentation, we could recommend specific parts of the video to the student. Fig. 6 : Educational video timeline tagged by the lecturers presence. Notice that the frames with the lecturer on the left are tagged in red, while the frames with the lecturer on the right are tagged in yellow. This timeline is resulted from the Video Representation phase, described in Section III-A.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 96,
                    "end": 102,
                    "text": "Fig. 6",
                    "ref_id": null
                },
                {
                    "start": 339,
                    "end": 345,
                    "text": "Fig. 6",
                    "ref_id": null
                }
            ],
            "section": "VI. FINAL REMARKS"
        },
        {
            "text": "The main limitation of our work is that we can only recommend videos in which the lecturers are visually present.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. FINAL REMARKS"
        },
        {
            "text": "As future work, we intend to use a hybrid recommendation approach, that combines both textual and audiovisual information from the video to create clusters. Video summarization is also a technique that can be explored to enhance video content searching and selection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. FINAL REMARKS"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Beyond millennials: The next generation of learners",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "R"
                    ],
                    "last": "Insights",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Coronavirus pushes education online",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zuo",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature Materials",
            "volume": "19",
            "issn": "6",
            "pages": "687--687",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "An approach to identify similarity among educational resources using external knowledge bases",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "L"
                    ],
                    "last": "Dias",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Barbosa",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Barr\u00e9re",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "De Souza",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Brazilian Journal of Computers in Education",
            "volume": "25",
            "issn": "02",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Optimising web usage mining for building adaptive e-learning site: a case study",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mahajan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sodhi",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Mahajan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Journal of Innovation and Learning",
            "volume": "18",
            "issn": "4",
            "pages": "471--486",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Personalized recommender system for digital libraries",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Omisore",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Samuel",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International Journal of Web-Based Learning and Teaching Technologies (IJWLTT)",
            "volume": "9",
            "issn": "1",
            "pages": "18--32",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Utiliza\u00e7\u00e3o de enriquecimento sem\u00e2ntico para a recomenda\u00e7\u00e3o autom\u00e1tica de videoaulas no moodle",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Barr\u00e9re",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Souza",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Vitor",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "De Almeida",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Revista Brasileira de Inform\u00e1tica na Educa\u00e7\u00e3o",
            "volume": "28",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Deep face recognition: A survey",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Masi",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hassner",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Natarajan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)",
            "volume": "",
            "issn": "",
            "pages": "471--478",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Applied machine learning at facebook: A datacenter infrastructure perspective",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hazelwood",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bird",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Brooks",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chintala",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Diril",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dzhulgakov",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Fawzy",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kalro",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)",
            "volume": "",
            "issn": "",
            "pages": "620--629",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Introduction to information retrieval-cs 276 lecture slides",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Deepface: Closing the gap to human-level performance in face verification",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Taigman",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ranzato",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "1701--1708",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Deep learning face representation from predicting 10,000 classes",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "1891--1898",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Schroff",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kalenichenko",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Philbin",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "815--823",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Neural aggregation network for video face recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hua",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "4362--4371",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Attention-aware deep reinforcement learning for video face recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Rao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE international conference on computer vision",
            "volume": "",
            "issn": "",
            "pages": "3931--3940",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Unsupervised domain adaptation for face recognition in unlabeled videos",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sohn",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhong",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "M.-H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chandraker",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "3210--3218",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Rousseeuw",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "Journal of computational and applied mathematics",
            "volume": "20",
            "issn": "",
            "pages": "53--65",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Qiao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Signal Processing Letters",
            "volume": "23",
            "issn": "10",
            "pages": "1499--1503",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Face recognition based surveillance system using facenet and mtcnn on jetson tx2",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Jose",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Greeshma",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Tp",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Supriya",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 5th International Conference on Advanced Computing & Communication Systems (ICACCS)",
            "volume": "",
            "issn": "",
            "pages": "608--613",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Realtime face-detection and emotion recognition using mtcnn and minishufflenet v2",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ghofrani",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "Toroghi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ghanbari",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "2019 5th Conference on Knowledge Based Engineering and Innovation (KBEI)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Recognition of occluded and lateral faces using mtcnn, dlib and homographies",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Bezerra",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Gomes",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Squeeze-and-excitation networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "7132--7141",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Vggface2: A dataset for recognising faces across pose and age",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [
                        "M"
                    ],
                    "last": "Parkhi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG",
            "volume": "",
            "issn": "",
            "pages": "67--74",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Hierarchical grouping to optimize an objective function",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Ward",
                    "suffix": ""
                }
            ],
            "year": 1963,
            "venue": "Journal of the American statistical association",
            "volume": "58",
            "issn": "301",
            "pages": "236--244",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Scikit-learn: Machine learning in Python",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pedregosa",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Varoquaux",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gramfort",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Michel",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Thirion",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Grisel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Blondel",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Prettenhofer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Weiss",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Dubourg",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Vanderplas",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Passos",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cournapeau",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Brucher",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Perrot",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Duchesnay",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Journal of Machine Learning Research",
            "volume": "12",
            "issn": "",
            "pages": "2825--2830",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Lecturers representation process in video. This process receives a video file and returns the centroids of the clusters that ideally represent each of the lecturers present in the video file.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Video Recommendation based on Lecturers Centroids Clustering. This pipeline receives the centroids of lecturers from all the videos in the dataset, then it creates relationships among videos that share the presence of the same lecturers. Finally, it performs ranking of recommended videos for each of the videos in the dataset. This ranking is based on the number of lecturers shared and their time presence.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Centroids images correction in VideoFacesTool. On the top, each image represents one lecturer. When a lecturer is selected, the tool displays all appearances (centroids) of that lecturer in different videos. The user can then mark each of these appearances as correct or wrong.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Examples of wrong faces centroids. (a) a part of an icon (b) a hand and (c) face centroids that are not from the same lecturer .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Example of how the Average Precision (AP) is computed for a reference video and its recommended videos.",
            "latex": null,
            "type": "figure"
        },
        "TABREF3": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Results obtained with our approach with different thresholds of time presence for a lecturer to be considered as present in a video.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
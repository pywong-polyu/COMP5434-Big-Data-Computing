{
    "paper_id": "3939685eb69dfdffefb70618bb4444457a7ab450",
    "metadata": {
        "title": "Automatic Assistance to Cognitive Disabled Web Users via Reinforcement Learning on the Browser",
        "authors": [
            {
                "first": "Tomas",
                "middle": [],
                "last": "Murillo-Morales",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Johannes Kepler University",
                    "location": {
                        "addrLine": "Altenbergerstra\u00dfe 69, Peter.Heumader",
                        "postCode": "4040",
                        "settlement": "Linz",
                        "region": "Klaus",
                        "country": "Austria"
                    }
                },
                "email": ""
            },
            {
                "first": "Peter",
                "middle": [],
                "last": "Heumader",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Johannes Kepler University",
                    "location": {
                        "addrLine": "Altenbergerstra\u00dfe 69, Peter.Heumader",
                        "postCode": "4040",
                        "settlement": "Linz",
                        "region": "Klaus",
                        "country": "Austria"
                    }
                },
                "email": ""
            },
            {
                "first": "Klaus",
                "middle": [],
                "last": "Miesenberger",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Johannes Kepler University",
                    "location": {
                        "addrLine": "Altenbergerstra\u00dfe 69, Peter.Heumader",
                        "postCode": "4040",
                        "settlement": "Linz",
                        "region": "Klaus",
                        "country": "Austria"
                    }
                },
                "email": "miesenberger@jku.at"
            }
        ]
    },
    "abstract": [
        {
            "text": "This paper introduces a proof of concept software reasoner that aims to detect whether an individual user is in need of cognitive assistance during a typical Web browsing session. The implemented reasoner is part of the Easy Reading browser extension for Firefox. It aims to infer the user's current cognitive state by collecting and analyzing user's physiological data in real time, such as eye tracking, heart beat rate and variability, and blink rate. In addition, when the reasoner determines that the user is in need of help it automatically triggers a support tool appropriate for the individual user and Web content being consumed. By framing the problem as a Markov Decision Process, typical policy control methods found in the Reinforcement Learning literature, such as Q-learning, can be employed to tackle the learning problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Accessibility to the digital world, including the Web, is increasingly important to enable people with disabilities to carry out normal lives in the information society, something that has been acknowledged by the United Nations and many individual governments to be a right for people with disabilities. This is as true for people with cognitive, language, and learning differences and limitations as it is for anyone else [6] . Nowadays, many Web users suffering from a cognitive or learning disability struggle to understand and navigate Web content in its original form because of the design choices of content providers [6] . Therefore, Web content often ought to be adapted to the individual needs of the reader.",
            "cite_spans": [
                {
                    "start": 424,
                    "end": 427,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 625,
                    "end": 628,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Cognitive Accessibility on the Web"
        },
        {
            "text": "Currently available software tools for cognitive accessibility of Web content include Immersive Reader [4] , the Read&Write browser extension [13] , and Easy Reading [3] . These tools embed alternative easy-to-read or clarified content directly into the original Web document being visited when the user requests it, thereby enabling persons with a cognitive disability to independently browse the Web. Access methods may be tailored to the specific users based on personal data, generally created by supporting staff or educators [8] . Besides these semi-automatic tools, current approaches to making websites accessible to people with cognitive and learning impairments still mostly rely on manual adaptations performed by human experts [8] .",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 106,
                    "text": "[4]",
                    "ref_id": null
                },
                {
                    "start": 142,
                    "end": 146,
                    "text": "[13]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 166,
                    "end": 169,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 531,
                    "end": 534,
                    "text": "[8]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 739,
                    "end": 742,
                    "text": "[8]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Cognitive Accessibility on the Web"
        },
        {
            "text": "The Easy Reading framework 1 improves cognitive accessibility of original websites by providing real time personalization through annotation (using e.g. symbol, pictures, videos), adaptation (e.g. by altering the layout or structure of a website) and translation (using e.g. Easy-to-Read, plain language, or symbol writing systems) [3] .",
            "cite_spans": [
                {
                    "start": 332,
                    "end": 335,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "The \"Easy Reading\" Framework"
        },
        {
            "text": "The main advantage of the Easy Reading framework over existing cognitive support methods is that the personalized support tools are provided at the original websites in an automatic fashion instead of depending on separate user experiences which are commonly provided to users in a static, content-dependent manner and that must be manually authored by experts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The \"Easy Reading\" Framework"
        },
        {
            "text": "Easy Reading software clients have been designed as Web browser extensions 2 (for Mozilla Firefox and Google Chrome) and mobile OS apps (Android and iOS). The main interaction mechanism between the user and the client consist on a graphical user interface (GUI) that the user may choose to overlay on top of any Website being currently visited. A number of tools, personalized to the specific user, are available to the user in Easy Reading's GUI (see Fig. 1 ). The user may choose at any time to use some of the available framework functions by triggering their corresponding tool by clicking on the available buttons of the GUI.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 452,
                    "end": 458,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "The \"Easy Reading\" Framework"
        },
        {
            "text": "Given the special needs of Easy Reading's user base, having a traditional GUI as the only interaction mechanism between the user and the browser extension may not suit the specific needs of all users. Some users, especially those suffering from a profound cognitive disability, may not possess the necessary expertise and/or understanding to interact with Easy Reading's GUI. This is particularly the case if there are many tools being overlaid on the GUI, as this may overwhelm the user given the considerable amount of personalization mechanisms to choose from. The use of Easy Reading is also restricted for those suffering from additional physical disabilities making interaction slow or impossible when no easy to use AT solutions are at hand.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "We therefore aim to assist the user in choosing and using the right cognitive support tool when he or she is in need of help while navigating Web content which appears to be confusing or unclear. We have expanded the Easy Reading framework so that it supports the automatic triggering of any support tool with the addition of two components; namely, (1) a user data collection module and (2) a client-based reasoner that learns about the mental state of the user based on the gathered data and previous experiences, and reacts accordingly by triggering support tools when necessary. Figure 2 displays the interaction between these two components within the Easy Reading framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "The next section gives a short overview on current methods for automatically detecting the cognitive load/affect of a person from collected user data. Based on some of these results, the design of the Easy Reading User Tracking and Reasoning framework is outlined in the remaining of this document.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "Affect recognition is the signal and pattern recognition problem that aims to detect the affective state of a person based on observables, with the goal of, for example, providing reasoning for decision making or supporting mental well-being [14] . Terms such as affect and mood elude a precise definition in the literature, but some working definitions may be characterized. Namely, affect is a neurophysiological state that is consciously accessible as the simplest raw, nonreflective, primitive feeling evident in mood and emotions e.g. the feeling of being scared while watching a scary movie [7] . The Easy Reading graphical user interface (GUI) overlaid on a website. The symbol support tool has been automatically triggered on a text paragraph by the Easy Reading reasoner, adapting its content automatically with symbol annotations over the original text. The user may reject automatically given help by means of an onscreen dialogue (top right). Any of the available tools on the Easy Reading GUI may be also manually triggered by the user at any given time by clicking on its corresponding button.",
            "cite_spans": [
                {
                    "start": 242,
                    "end": 246,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 597,
                    "end": 600,
                    "text": "[7]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Affect Detection: State of the Art"
        },
        {
            "text": "On the other hand, emotions are intense and directed indicators of affect e.g. shock and scream are emotions that indicate the affect of being scared [7] . As opposed to emotions, moods are less intense, more diffuse, and last for a longer period of time than emotions [14] . For example, the emotion of anger, which does not last long by itself, can lead to an irritable mood [14] . On this paper we focus on the binary classification problem of the user's affect, namely, whether the user is in a confused mental state during a Web browsing activity. This problem is closely related to that of stress detection, in which data is analyzed to predict the stress level of a person, generally as a binary variable (stressed/unstressed). Stress detection is a well-researched problem that can be reliably undertaken by analyzing user physiological signals provided by e.g. a wrist-worn device such as a smartwatch [16] .",
            "cite_spans": [
                {
                    "start": 150,
                    "end": 153,
                    "text": "[7]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 269,
                    "end": 273,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 377,
                    "end": 381,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 911,
                    "end": 915,
                    "text": "[16]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Affect Detection: State of the Art"
        },
        {
            "text": "Affect is a subjective experience of a person which is generally detected through self-reporting. Nevertheless, numerous approaches that aim to infer a person's affective state in an automatic fashion can be found in the literature. These approaches can be divided into four categories depending on the kind of data they process:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affect Detection: State of the Art"
        },
        {
            "text": "\u2022 Contextual approaches learn from the interaction between the user and a software system by e.g. analyzing mouse gestures or page visit times. \u2022 Physiological approaches collect and analyze physiological data from the user, such as heart beat rate or skin temperature. \u2022 Text-based approaches process and interpret the textual contents of speech spoken or written by the user using natural language processing (NLP) techniques for sentiment analysis generally based on supervised machine learning methods. \u2022 Audio-visual approaches study recorded audio (generally speech) or video (of e.g. the user's face or full body) while the user interacts with the system.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affect Detection: State of the Art"
        },
        {
            "text": "Preferably, affect recognition systems should employ multimodal data i.e. fusion analysis of more than one input modality, since multimodal affect recognition system are consistently more accurate than unimodal methods [1] . A collection of state-of-theart methods for affect recognition can be found in [1, 12, 14] . The vast majority of these methods rely on supervised machine learning models such as Deep Neural Networks (DNN) for image analysis e.g. for analyzing the user's facial expressions; or Random Forests (RF) and Support Vector Machines (SVM) for analysis of physiological signals e.g. heart rate variability. What these methods have in common is that they require of big amounts of training data that the learning model must be trained on. Available datasets such as the well-known DEAP dataset [5] aim to simplify this process by providing a large amount of pre-labelled training data for affect recognition tasks. For a list of available datasets the reader is directed to [12] and [14] . However, these approaches, especially those relying on physiological signals, suffer from a number of drawbacks that hinder their application in practice:",
            "cite_spans": [
                {
                    "start": 219,
                    "end": 222,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 304,
                    "end": 307,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 308,
                    "end": 311,
                    "text": "12,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 312,
                    "end": 315,
                    "text": "14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 810,
                    "end": 813,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 990,
                    "end": 994,
                    "text": "[12]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Affect Detection: State of the Art"
        },
        {
            "text": "\u2022 Even if supervised models perform well when tested on known users, they exhibit high generalization errors when tested on unknown users, and thus models must be fine-tuned to the specific user [9] . \u2022 Available datasets have been collected using a specific combination of devices and sensors e.g. a specific wristband. Therefore, end users are forced to acquire a very similar combination of devices to make use of models trained on such datasets.",
            "cite_spans": [
                {
                    "start": 195,
                    "end": 198,
                    "text": "[9]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Affect Detection: State of the Art"
        },
        {
            "text": "Preferably, the reasoner model should adapt to the available hardware, not the other way around. \u2022 Many tracking devices employed to collect data for these datasets are too expensive or obtrusive to be used in an informal home/office setting by end users, such as EEG headsets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affect Detection: State of the Art"
        },
        {
            "text": "This section introduces our approach to automatic affect detection tailored to the specific user and available devices that aims to overcome some of the issues described in the previous section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Easy Reading User Tracking and Reasoning"
        },
        {
            "text": "In order to detect whether the user is confused or mentally overwhelmed by the content he or she is visiting during an ordinary Web browsing activity, user data needs to be collected in a transparent, unobtrusive manner to the user. In addition, specific tracking devices whose presence in a common household or office environment would normally be unwarranted (due to e.g. high cost) ought to be avoided. Therefore, after a study of the relevant literature and filtering out those tracking devices which did not satisfy these requirements, the following signals were considered:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "User Tracking Data Collection"
        },
        {
            "text": "\u2022 Eye movement and position. The current position of the user's gaze on the screen and the voluntary or involuntary movement of the eyes can be collected with the use of inexpensive eye trackers, commonly used in gaming, that can be mounted near a computer's screen in close distance to the user. Eye movement data is of great utility to ascertain cognitive load. Some authors even argue that eye movement data suffices to infer the cognitive demand of tasks being carried out by a person [15] . \u2022 Blink rate. The time period between two or more consecutive eye blinks can be a good indicator of task difficulty as well. For example, a poor understanding of the subject matter in a lecture on mathematics resulted, for some persons, on an increased number of rapid serial blinks [10] . Blink frequency can be easily measured by, for example, analysing a video of the user's face recorded with a typical laptop webcam. \u2022 Heart Rate. The current heart rate (HR) of the user, measured in beats per minute (BPM), and especially heart rate variability (HRV), which describes the variation of the time between heartbeats, is a rather simple but effective measure of the current affective state and stress level of the user [14] . These dimensions can be easily determined with the use of commercial smartwatches and fitness trackers, which are the most popular wearable devices being sold nowadays. \u2022 Implicit behavioural information. Several measures of user behaviour on websites that aim to predict disorientation, task difficulty and user preferences can be found in the information retrieval literature. For example, time spent on site and clickthrough rates have been used to measure the cognitive load of users visiting a web search engine [2] . It is however important to note that other studies have concluded that user disorientation on websites is only weakly related to user behaviour [2] . Therefore, physiological signals are employed as the main data source employed by Easy Reading's reasoner module.",
            "cite_spans": [
                {
                    "start": 489,
                    "end": 493,
                    "text": "[15]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 779,
                    "end": 783,
                    "text": "[10]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1741,
                    "end": 1744,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1891,
                    "end": 1894,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "User Tracking Data Collection"
        },
        {
            "text": "User data are processed and gathered by the Easy Reading framework as follows. Eye fixation duration (in milliseconds) and current x and y coordinates of the user gaze on the screen is measured by a Tobii 4C Eye Tracker 3 . To measure HR and HRV, the Vivoactive 3 smartwatch 4 by Garmin was selected as a good compromise between accuracy and affordability. Blink rate can be measured from video data recorded from any standard webcam, whether integrated in a laptop or an external one. Input signals are gathered and processed in an easy, flexible, and tailorable manner by means of an AsTeRICS model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "User Tracking Data Collection"
        },
        {
            "text": "AsTeRICS [11] is an accessible technology (AT) construction set that provides plug-ins for many common input devices and signal processing operations. By combining already existing and newly developed plug-ins into an AsTeRICS model, raw input physiological signals are pre-processed before being sent to the reasoning module. Pre-processing includes methods for synchronization of data streams, handling of missing values (e.g. if the user does not possess some of the input devices), noise removal, and segmentation of the collected data into batches. Several pre-processing parameters can be adjusted directly in the AsTeRICS model by end-users or carers without the need of possessing technical skills. For example, batch (temporal window) size is by default set to 120 s (e.g. 12 samples aggregated after 10 s each) following state-of-the-art recommendations [16] , but can be easily adjusted by modifying the relevant parameters of the Easy Reading AsTeRICS Data Collector plug-in. Collected batches are next converted to JSON objects and sent to the Easy Reading browser extension via a secure WebSocket connection maintained by the AsTeRICS Runtime Environment (ARE) Web server.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 13,
                    "text": "[11]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 864,
                    "end": 868,
                    "text": "[16]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "User Tracking Data Collection"
        },
        {
            "text": "The Easy Reading Reasoner is the client-based module in charge of solving the problem of inferring the affective state of the user from the current readings of physiological signals collected by a running AsTeRICS model. The reasoner is hosted on the client in order to minimize the amount of messaging needed between the distributed components of the user tracking and reasoning framework, which in turn results in more responsive reasoner actions. This however comes at the cost of a more limited computational capacity, as the whole learning model has to run on the user's browser.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "We have adopted a Markov decision process (MDP) as the framework for the problem, which allows it to be theoretically solved using a number of well-established control learning methods in the reinforcement learning (RL) literature. As previously stated, research shows that affection/stress recognition methods must be tailored to the individual differences of each person. Given that RL is specially well suited to problems in which the only way to learn about an environment is to interact with it, we model the environment shown in Fig. 2 as a MDP to be solved for a specific user. For a detailed characterization of MDPs and RL, the reader is directed to [17] , Chapters 1 and 3. Like every MDP, our problem consists of an agent, (intractable) environment, state set (S), action set (A), and policy (p), characterized as follows.",
            "cite_spans": [
                {
                    "start": 659,
                    "end": 663,
                    "text": "[17]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [
                {
                    "start": 535,
                    "end": 541,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "The agent in a MDP is the learner and decision maker. It corresponds to the reasoner module being executed in the background script of the browser extension, as shown in Fig. 2 . At any given time step, t, the reasoner observes the current state of the user, s t , as specified by each sample being delivered by the AsTeRICS model, and decides on an action, a t , to be taken with probability p i.e. p a t js t \u00f0 \u00de \u00bc p. The current status, s t , is the JSON object produced by the data collector model, which consists on a number of features e.g. HRV, and the current value of the user readings for that feature. Note that t does not correspond to an exact moment in time, but rather to the latest time window that has been aggregated by the data collector. Depending on the feature, the data collector sets its value to the latest received input or an aggregation thereof, such as the average or most common value (mode) during the time window.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 170,
                    "end": 176,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "The reasoner may next take one of three actions (a t ), namely:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "1. No action (NOP). The reasoner has inferred that the user is not in need of help at the moment, and thus no further action is necessary. 2. Help user (Help). The reasoner has inferred that the user is in need of help with some content of the website being currently visited, and a suitable framework tool needs to be triggered. Figure 1 displays an example of this action being triggered on a website. 3. Ask user explicitly for the next action to take (Ask). The reasoner is unsure about the next action to take, as it expects both NOP and help actions to yield a low reward. In this case, it asks the user, via an onscreen dialogue, about which of these two actions to take next.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 330,
                    "end": 338,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "The user gives feedback, which may be implicit or explicit, on the action just taken by the reasoner and a numeric reward, r t \u00fe 1 , is computed as a function of a t and the user feedback, as shown in Table 1 . This reward function heavily penalizes the case in which the agents fails to help a user in need. However, to prevent the agent from persistently asking the user for explicit feedback on the best action to take, asking the user is always given a (low) negative reward as well. Moreover, since the correct s t ; a t \u00f0 \u00depair is known after the user has answered a feedback dialogue, this combination is accordingly rewarded in order to speed learning up. The agent is only positively rewarded in the case that it manages to predict that the user is currently in need of help and automatically triggers the correct tool for his or her needs.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 201,
                    "end": 208,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "Currently, the support tool triggered by the reasoner is the most frequently used tool by the user for the content type (text or image) that has been stared at the longest during the last time window t. Use frequency for each tool is computed and stored in each user's profile on Easy Reading's cloud backend.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "In the subsequent time step, t \u00fe 1, the agent receives, along with the reward signal r t \u00fe 1 , a new user state, s t \u00fe 1 . This state is an aggregation of user data representing the user's reaction to a t . Therefore, the Easy Reading extension does not start collecting a new state until the user's feedback has been gathered. Consequently, any user data incoming during the processing of s t , and after a t has been yielded but before the user feedback has been inferred, is discarded. This whole process is summarized in the workflow diagram shown in Fig. 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 555,
                    "end": 561,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "The goal of the agent (reasoner) is to learn which sequence of actions leads to a maximum reward in the long run. This behavior is encoded in a so-called policy, which the reasoner has to learn. The policy, p ajs \u00f0 \u00de, specifies which action to take on a given state, or, in the nondeterministic case, the probability of each action of the action space for a given state.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "The Easy Reading reasoner keeps an estimation of the value, in terms of future expected returns, of each action a performed on each state s it has seen so far, q p s; a \u00f0 \u00de, known as the action-value function of p. This function can be stored as a table in memory if the set state is small. In this case, input observables are further pre-processed in the browser extension via data binning to obtain a manageable state set. Otherwise, the state-value function can be approximated via function approximation methods e.g. by modelling it with a neural network (NN). The latter approach is possible by defining actor and/or critic NNs with Tensorflow.js directly on the user's browser 5 . Note however that due to the strict security policy of the Firefox add-on store this possibility cannot be included in officially listed extensions, and therefore our extension currently only implements a tabular q-function. The policy that maximizes q p \u00c3 s; a \u00f0 \u00de, for all states and actions is known as the optimal policy, p \u00c3 . Some RL control methods, such as Temporal-Difference (TD) learning methods, converge to the optimal policy given enough training time. The Easy Reading reasoner implements a number of value-based RL methods that aim to find p \u00c3 ; namely, Q-learning and double-Q-learning. Describing these methods is out of scope of this paper, for further information the reader is directed to [17] . The basic q-learning update rule is shown in context in Fig. 3 .",
            "cite_spans": [
                {
                    "start": 683,
                    "end": 684,
                    "text": "5",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1397,
                    "end": 1401,
                    "text": "[17]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [
                {
                    "start": 1460,
                    "end": 1466,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "When interacting with a new user, the agent does not know anything about the environment (the user), and therefore it has to be explored. Once enough knowledge is acquired, this knowledge can be exploited to maximize the rewards obtained from this point onwards. The agent follows a e-greedy behavioral policy with respect to q p s; a \u00f0 \u00de, whose values are initialized to zero. However, instead of choosing a random action with e probability, it chooses the \"no action\" action both at exploration time and when state-action values are tied at exploitation time. This way, since most of the time the user will not be in a confused state, help tools and dialogues are less likely to come up at unsought times, which aims to reduce user frustration overall. The system can then slowly learn to identify the states in which the user needs help as tools are manually triggered during training sessions with a caregiver where negative feedback is acquired. The implemented policy's pseudocode is shown in context in Fig. 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1010,
                    "end": 1016,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Easy Reading Reasoner"
        },
        {
            "text": "This article has introduced an innovative approach to automatically detecting the affective state of a web user in real time based on the analysis of physiological signals. The main goal of the Easy Reading reasoner is to infer the current cognitive load of the user in order to automatically trigger the corresponding assistance mechanism of the Easy Reading framework that would help the user in getting a better understanding of a difficult piece of Web content (text or image). It is included in the latest version of the Easy Reading extension as a proof of concept and is ready to be tested with voluntary participants.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        },
        {
            "text": "Training sessions with synthetic data have been carried out, yielding a very good accuracy of around 90% after 150 time steps i.e. around 5 h of real training time. However, detecting user confusion on actual users may prove much more challenging, since changes in physiological signals may be too subtle or complex to be properly modelled by the Easy Reading reasoner and the inexpensive consumer devices employed. It must be noted that initial informal evaluation with end users has shown that triggering support tools when the user does not need them should be avoided altogether, since it frustrates and confuses them to the point where they refuse to keep using the Easy Reading extension with reasoner support enabled. After this observation, we modified the agent's policy from a traditional e-greedy policy to the modified policy shown in Fig. 3 . The next step is to test our approach with end users in a laboratory setting.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 847,
                    "end": 853,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Conclusions and Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A review and meta-analysis of multimodal affect detection systems",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "K"
                    ],
                    "last": "D&apos;mello",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kory",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ACM Comput. Surv. (CSUR)",
            "volume": "47",
            "issn": "3",
            "pages": "1--36",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Implicit measures of lostness and success in web navigation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gwizdka",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Spence",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Interact. Comput",
            "volume": "19",
            "issn": "3",
            "pages": "357--369",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "The EasyReading framework -keep the user at the digital original",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Heumader",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Miesenberger",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Reinhard",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "J. Technol. Pers. Disabil",
            "volume": "6",
            "issn": "",
            "pages": "33--42",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "DEAP: a database for emotion analysis using physiological signals",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Koelstra",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Trans. Affect. Comput",
            "volume": "3",
            "issn": "1",
            "pages": "18--31",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Policy and standards on web accessibility for cognitive and learning disabilities",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lewis",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Seeman",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Web Accessibility. HIS",
            "volume": "",
            "issn": "",
            "pages": "281--299",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Many facets of sentiment analysis",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Cambria",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bandyopadhyay",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "A Practical Guide to Sentiment Analysis. Socio-Affective Computing",
            "volume": "5",
            "issn": "",
            "pages": "11--39",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-55394-8_2"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Tools and applications for cognitive accessibility",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Miesenberger",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Edler",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Heumader",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Petz",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Web Accessibility. HIS",
            "volume": "",
            "issn": "",
            "pages": "523--546",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Importance of individual differences in physiological-based stress recognition models",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nkurikiyeyezu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Yokokubo",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lopez",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "15th International Conference on Intelligent Environments",
            "volume": "2019",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Rapid serial blinks: an index of temporally increased cognitive load",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nomura",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Maruno",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "PLoS One",
            "volume": "14",
            "issn": "12",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "AsTeRICS, a flexible assistive technology construction set",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ossmann",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Procedia Comput. Sci",
            "volume": "14",
            "issn": "",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "A review of affective computing: from unimodal analysis to multimodal fusion",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Poria",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Cambria",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bajpai",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hussain",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Inf. Fusion",
            "volume": "37",
            "issn": "",
            "pages": "98--125",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Read&Write Literacy Support Software. Texthelp",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "7",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Wearable affect and stress recognition: a review",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Schmidt",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Reiss",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Duerichen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Van Laerhoven",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Detecting task demand via an eye tracking machine learning system",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shojaeizadeh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Djamasbi",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "C"
                    ],
                    "last": "Paffenroth",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Trapp",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "116",
            "issn": "",
            "pages": "91--101",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Continuous stress detection using the sensors of commercial smartwatch",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Siirtola",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers",
            "volume": "",
            "issn": "",
            "pages": "1198--1201",
            "other_ids": {
                "DOI": [
                    "10.1145/3341162.3344831"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Reinforcement Learning: An Introduction, 2nd edn. A Bradford Book",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "S"
                    ],
                    "last": "Sutton",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Barto",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence and indicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Fig. 1. The Easy Reading graphical user interface (GUI) overlaid on a website. The symbol support tool has been automatically triggered on a text paragraph by the Easy Reading reasoner, adapting its content automatically with symbol annotations over the original text. The user may reject automatically given help by means of an onscreen dialogue (top right). Any of the available tools on the Easy Reading GUI may be also manually triggered by the user at any given time by clicking on its corresponding button.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Main components of the Easy Reading User Tracking and Reasoning Framework",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Reward function of the Easy Reading reasoner Fig. 3. Simplified Easy Reading Reasoner Workflow Automatic Assistance to Cognitive Disabled Web Users",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgments. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 780529).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}
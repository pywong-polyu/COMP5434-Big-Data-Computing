{
    "paper_id": "0bb2e7a5ca4d03edbb81ac1a0e73d1a07ed3b01a",
    "metadata": {
        "title": "NUCLEIC TRANSFORMER: DEEP LEARNING ON NUCLEIC ACIDS WITH SELF-ATTENTION AND CONVOLUTIONS",
        "authors": [
            {
                "first": "Shujun",
                "middle": [],
                "last": "He",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Texas A&M University College Station",
                    "location": {
                        "region": "TX, TX, TX, TX"
                    }
                },
                "email": "shujun@tamu.edu"
            },
            {
                "first": "Baizhen",
                "middle": [],
                "last": "Gao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Texas A&M University College Station",
                    "location": {
                        "region": "TX, TX, TX, TX"
                    }
                },
                "email": "baizhen@tamu.edu"
            },
            {
                "first": "Rushant",
                "middle": [],
                "last": "Sabnis",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Texas A&M University College Station",
                    "location": {
                        "region": "TX, TX, TX, TX"
                    }
                },
                "email": "rushantsabnis@tamu.edu"
            },
            {
                "first": "Qing",
                "middle": [],
                "last": "Sun",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Texas A&M University College Station",
                    "location": {
                        "region": "TX, TX, TX, TX"
                    }
                },
                "email": "sunqing@tamu.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Much work has been done to apply machine learning and deep learning to genomics tasks, but 1 these applications usually require extensive domain knowledge and the resulting models provide 2 very limited interpretability. Here we present the Nucleic Transformer, a conceptually simple but 3 effective and interpretable model architecture that excels in a variety of DNA/RNA tasks. The Nucleic 4 Transformer processes nucleic acid sequences with self-attention and convolutions, two deep learning 5 techniques that have proved dominant in the fields of computer vision and natural language processing. 6 We demonstrate that the Nucleic Transformer can be trained in both supervised and unsupervised 7 fashion without much domain knowledge to achieve high performance with limited amounts of data in 8 E. coli promoter classification, viral genome identification, and degradation properties of COVID-19 9 mRNA vaccine candidates. Additionally, we showcase extraction of promoter motifs from learned 10 attention and how direct visualization of self-attention maps assists informed decision making using 11 deep learning models. 12 1 Introduction 15 DNA and RNA are essential components of life. Information stored in DNA and RNA provides instructions on the 16 complex functions should in biological organisms [1]. However, understanding DNA and RNA, including the stored 17 information and degradation properties, has always been challenging because of the complex and large potential 18 sequence space of DNA and RNA. Especially, the functions and properties of many coding and non-coding DNA/RNA 19 sequences still remain poorly understood [2, 3].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "20",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Deep learning is a class of data-driven modeling approaches that have found much success in many fields including 21 image recognition [4], natural language processing [5, 6], and computation biology [7]. It has allowed researchers to 22 efficiently predict the function, origin, and properties of DNA/RNA sequences by training neural networks on large 23 datasets [8, 9, 10, 11, 12, 13]. The sequential nature of DNA/RNA has made recurrence (RNN/LSTM/GRU) appealing 24 for models dealing with such data [14, 15, 16, 17, 18] . However, the sequential computation is difficult to parallelize 25 and objects at long distances suffer from vanishing gradients. Transformers, on the other hand, is a recently proposed 26 architecture that solely relies on attention mechanisms that can model dependencies regardless of the distance in the 27 input or output sequences [5] . Although transformer has been adopted in many natural language processing tasks, we 28 have found few studies using transformer in biological systems such as DNA/RNA sequences.",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 171,
                    "text": "[5,",
                    "ref_id": null
                },
                {
                    "start": 372,
                    "end": 375,
                    "text": "10,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 521,
                    "end": 524,
                    "text": "18]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 863,
                    "end": 866,
                    "text": "[5]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "In this study, we propose a model architecture Nucleic Transformer that utilizes convolution and self-attention to capture 30 both local and global dependencies, which enable the model to achieve high accuracy and provide interpretability for 31 three DNA/RNA tasks. The Nucleic Transrformer formulates DNA understanding as natural language processing tasks, 32 and RNA degradation as molecular prediction tasks. First, Nucleic Transformer is trained to classify short pieces of 33 DNA sequence (81 bp) as either an E. coli (Escherichia coli) promoter sequence or non-promoter sequence. Nucleic",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Transformer learns from labeled promoter/non-promoter sequences, and the classification accuracy is compared with 35 other state-of-the-art promoter identification models [19, 20, 21]. Second, Nucleic Transformer is tested on longer DNA 36 sequences (300 bp) for classification of viral and non-viral sequences. We show that Nucleic Transformer predicts both 37 viral and non-viral sequences with improved accuracy compared with a previous best computational model [22] . In 38 addition, we demonstrate that a Nucleic Transformer variant can be trained to predict the RNA degradation rates at each 39 position of a given sequence, a task that few computational models have tackled but of great importance especially under 40 current circumstances due to the recent COVID-19 pandemic. All training code for reproducible results is released at 41 https://github.com/Shujun-He/Nucleic-Transformer. 42 2 Methods 43 The Nucleic Transformer architecture combines convolutions and self-attention to capture both local and global 44 dependencies (Figure 1), and can be trained in both supervised and unsupervised fashion to make predictions per 45 sequence and per nucleotide. Its flexible nature also allows injection of information from biophysical models. The 46 self-attention mechanism explicitly models pairwise interactions and provides interpretability that allows more informed 2.1 Benchmark datasets 50 High quality and accessible datasets are imperative in advancing our understanding of DNA and RNA, not only 51 because of the useful information they provide but also because they establish benchmarks where fair comparisons 52 between different models can be made. In this work, we benchmark with 3 high quality and easily accessible datasets, 53 which include a labeled E. coli promoter/non-promoter dataset, a labeled viral/non-viral genome dataset, and an 54 RNA dataset of COVID-19 vaccine candidates with experimentally measured degradation properties. The E. coli 55 promoter/non-promoter dataset and the labeled viral/non-viral genome dataset have been well studied in the literature 56 [19, 20, 23, 24, 22], while the RNA dataset is extremely novel and little existing work has dealt with RNA degradation 57 properties except for very recent works [25].",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 179,
                    "text": "20,",
                    "ref_id": null
                },
                {
                    "start": 465,
                    "end": 469,
                    "text": "[22]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "58 2.1.1 E. coli promoter/non-promoter dataset 59 The E. coli promoter/non-promoter dataset is an experimentally confirmed benchmark dataset widely used in the 60 literature to model and evaluate DNA promoter sequences [26]. All DNA sequences in the dataset were collected 61 from RegulonDB, and sequences were screened by CD-HIT based on redundant sequence identity [27]. This dataset 62 consists of 2,860 promoter sequences and 2,860 non-promoter sequences. All promoter sequences were experimentally 63 confirmed and collected from RegulonDB (version 9.3) [28]. The non-promoters sequences were extracted randomly 64 from the middle regions of long coding sequences and convergent intergenic regions in E. coli K-12 genome [29, 30]. 65 This dataset can be freely downloaded at github. Model performance on this dataset is evaluated using 5-fold cross 66 validation, where the data is split using iterativte stratification [31]. The metrics used for this dataset are accuracy, 67 sensitivity, specificity, and Matthews correlation coefficient (MCC). In addition to cross-validation, we also use an 68 independent test set composed of experimentally verified E. Coli promoters recently added to RegulonDB. 69 3 2.1.2 Viral/non-viral genome dataset 70 The viral/non-viral genome dataset is same as the one used to trained viraminer [22], which consists of 19 different 71 NGS experiments analyzed and labeled by PCJ-BLAST [32] following de novo genome assembly algorithms. This 72 dataset is publicly available at https://github.com/NeuroCSUT/ViraMiner. DNA sequences included in this 73 dataset were cut to 300 bp segments with remaining portions smaller than 300 bp discarded. Further, all sequences that 74 contain \"N\" (unknown with equal probability to any of the four nucleotides) were removed as well. This dataset has 75 approximately 320,000 DNA sequences in total. The main challenge with this dataset is the class imbalance, where 76 only 2% of sequences are of viral origin. The dataset is split into training, validation, and test, where hypertuning was 77 done with the training and validation set, and model performance evaluated on the test set. The metric used for this 78 dataset is AUC (Area Under the Receiver Operating Characteristic Curve). 79 2.1.3 OpenVaccine challenge dataset 80 The COVID-19 pandemic has led to a chaotic 2020, and development of effective vaccines remains a challenge 81 of the highest priority. mRNA is the leading candiate for COVID-19, but they face significant limitations, one of 82 which is the spontaneous degradation of mRNA. The OpenVaccine challenge [33] hosted by the Eterna community 83 sought to rally the data science expertise of Kaggle competitors to develop models that could accurately predict 84 degradation of mRNA. During the 21-day challenge, competitors were provided with 2400 107-bp mRNA sequences 85 with the first 68 base pairs labels with 5 degradation properties at each position. These properties are reactivity, 86 deg_pH10, deg_Mg_pH10 , deg_50C , and deg_Mg_50C. More details on these properties can be found at https: 87 //www.kaggle.com/c/stanford-covid-vaccine/data. 88 Like most Kaggle competitions, the test set was divided into a public test set and a private test set. Results on the public 89 test set was available during the competition, while private test set results were hidden. The final evaluation was done 90 on a portion of the private test set consisting of 3005 130-bp mRNA sequences, whose degradation measurements were 91 conducted during the 21-day challenge and revealed at the end. The test set was subjected to screening based on three 92 criteria: 93 1. Minimum value across all 5 degradation properties must be greater than -0.5 94 2. Mean signal/noise across all 5 degradation properties must be greater than 1.0. [Signal/noise is defined as 95 mean( measurement value over 68 nts )/mean( statistical error in measurement value over 68 nts)]",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "96",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "3. Sequences were clustered into clusters with less than 50% sequence similarity and chosen from clusters with 3 97 or fewer members 98 After screening, only 1172 sequences remained on the test set. Final evaluation was done on 3 of the 5 properties 99 (reactivity, deg_Mg_pH10, and deg_Mg_50C). Unlike the training set, the test set has longer mRNA sequences with 100 more sequence diversity and more measurements (first 91 positions) per sequence; in fact, more predictions had to be 101 made for the test set than there were training samples. The OpenVaccine challenge is thus the most challenging among 102 the tasks tackled in this paper. The metric used for ranking in the competition is MCRMSE (mean columnwise root 103 mean squared error):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "104",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "decision making. In this section, we first describe the benchmark datasets used, then explain our architecture design, 48 and finally detail the training procedures. ",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 121,
                    "text": "48",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "where N t is the number of columns, n the number of positions predicted, y the ground truth, and\u0177 the predicted value. 105 In addition, we also use R2 score (coefficient of determination) during further analysis. In this section, we discuss the theoretical considerations in architecture design. For DNA tasks, the Nucleic Transformer 108 is akin to a language model, where kmers are encoded using 1D convolutions and a transformer encoder stack is used to 109 model pairwise global dependencies. For the RNA task, we modified the Nucleic Transformer by leveraging additional 110 inputs from biophysical models and adding deconvolution layers, which enable predictions per nucleotide. ",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 122,
                    "text": "105",
                    "ref_id": null
                },
                {
                    "start": 457,
                    "end": 460,
                    "text": "109",
                    "ref_id": null
                },
                {
                    "start": 576,
                    "end": 579,
                    "text": "110",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Any DNA seqeuence is a series of nucleotides, each of which can be one of A (adenosine), C (cytidine), G (guanosine), and T (thymine). Therefore, if we consider DNA a langauge that uses only four different characters to encode 114 information, we can model it in similar fashion as a natural langauge. This idea then allows us to agnostically apply 115 Natural Language Processing (NLP) techniques in the domain of deep learning without injection of domain specific 116 knowledge in bioinformatics.",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 230,
                    "text": "114",
                    "ref_id": null
                },
                {
                    "start": 349,
                    "end": 352,
                    "text": "115",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "DNA as a language & RNA as a molecule"
        },
        {
            "text": "Although there are similarities between DNA sequences and natural language, there are also some important differences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "117"
        },
        {
            "text": "Take the English language for example, all words in the vocabulary are combinations of the 26 letters in the alphabet; 119 similarly, a DNA sequence is a sequence of 4 nucleotides. Nevertheless, the English langauge not only contains letters 120 but also spaces that separate words and commas and periods that seperate sentences, whereas comparatively a DNA 121 sequence is simply a sequence of 4 nucleotides. Further, when humans interpret a sentence, words are discretely 122 recognized and each word can be considered a discrete object. As a result, state of art NLP models evaluate languages 123 as collections of words (and punctuations) instead of letters. Since a DNA sequence does not have punctuations, we 124 need to find a way to transform a DNA sequence into \"words\", which we will discuss in detail in the next section.",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 122,
                    "text": "119",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "118"
        },
        {
            "text": "Unlike DNA which is double stranded and relatively stable, RNA is single-stranded and highly promiscuous. While A common method to process DNA sequences in the field of bioinformatics is to transform them into k-mers. For 132 instance, consider a short DNA sequence ATGC. The k-mers in this sequence are subsequences of length k, so ATGC 133 has three 2-mers AT, TG, and GC, two 3-mers ATG and TGC, and one 4-mer ATGC. Converting a DNA sequence into 134 k-mers is analogous to separating a language sentence into words and allows for more efficient extraction of information 135 from DNA. 136 The extraction of k-mers from a DNA sequence can be considered a sliding window of size k taking snapshots of 137 the sequence while moving one position at a time from one end of the sequence to the other, which is conceptually 138 identical to the convolution operation used in deep learning. Consider a simple example of convolution involving a 139 vector S \u2208 R l , where l is the length of the vector, and a convolution kernel K \u2208 R 3 , which convolves over the vector 140 S. If the convolutional kernel strides one position at a time, an output vector of dot products O \u2208 R l\u22122 is computed,",
            "cite_spans": [
                {
                    "start": 575,
                    "end": 578,
                    "text": "135",
                    "ref_id": null
                },
                {
                    "start": 589,
                    "end": 592,
                    "text": "136",
                    "ref_id": null
                },
                {
                    "start": 821,
                    "end": 824,
                    "text": "138",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "125"
        },
        {
            "text": "where p denotes the position in the output vector. In this case, the convolution operation aggregates local information 142 with 3 positions at a time, so if S is a sequence of nucleotides, the convolution operation is essentially extracting 3-mers 143 from the DNA sequence S. Consequently, it is conceptually clear that a convolution kernel of size k can be used to 144 transform a DNA sequence into k-mers. In the next paragraph, we will explain further how this works mathematically.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "125"
        },
        {
            "text": "Since our model takes sequences of DNA/RNA nucleotides as input, we first transform each nucleotide into embeddings 146 of fixed size d model . So now for each sequence we have a tensor I \u2208 R l\u00d7d model , where l is the length of the sequence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "145"
        },
        {
            "text": "Because we are using the transformer encoder architecture, which is permutation invariant, we need to add positional 148 encoding, same as the implementation in [5] :",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 164,
                    "text": "[5]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "147"
        },
        {
            "text": "where pos is the position and i is the channel dimension. Now to create k-mers we perform convolutions on the tensor 150 I without padding and stride = 1. When the convolution operation with kernel size k is performed over I, a new tensor 151 K k \u2208 R (l\u2212k+1)\u00d7d model representing the sequence of k-mers is generated. For the encoder, we implement the vanilla transformer encoder [5] , which uses the multi-head self-attention mechanism. Unlike a single-head attention function, the multi-head self-attention function linearly projects d model -dimensional keys, values and queries into lower dimensional representations. Then the multi-head attention function directly operates on the entire sequence. It has been posited that the multi-head mechanism allows different heads to learn different hidden representations of the input, leading to better performance. The multi-head self-attention mechanism can be summarized in a few equations:",
            "cite_spans": [
                {
                    "start": 379,
                    "end": 382,
                    "text": "[5]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "147"
        },
        {
            "text": "Since we are only using the transformer encoder, Q, K, V come from the same sequence of feature vectors (hence the 164 name self-attention), each of which represents a k-mer with positional encoding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "147"
        },
        {
            "text": "The self-attention mechanism enables each k-mer to attend to all k-mers (including itself), so global dependencies can 166 be drawn between k-mers at any distance. Contrary to recurrence and convolutions, both of which enforce sparse local 167 connectivity, transformers allow for dense or complete global connectivity. The ability to draw global dependencies of 168 transformers is a huge advantage over recurrence and convolutions, both of which struggle with long sequences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "165"
        },
        {
            "text": "The self-attention function is followed by a position-wise feedforward network applied separately and identically to 170 each position:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "169"
        },
        {
            "text": "The position-wise feedforward network is basically two linear transforms with a ReLU activation in between. Con-172 ventionally, the combination of self-attention and position-wise feedforward network is referred to as the transformer 173 encoder layer, and a stack of transformer encoder layers is referred to the transformer encoder. Although we found it sufficient to simply use sequence information for DNA tasks, predicting degradation of RNA requires more than just sequence information. Firstly, we include the predicted structure per position, which describes whether a nucleotide is paired or unpaired with another one via hydrogen bonding. Also, we include the predicted loop type assigned by bpRNA. Additionally, we directly add a modified version of the base-pairing probability matrix into the attention function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "169"
        },
        {
            "text": "where \u03b3 is a learnable parameter and M bpp is the modified base-pairing probability matrix. The original base-pairing 176 probability matrix contains the probabilities for every possible base pair in an RNA sequence and has been used for 177 many RNA informatics tasks. Here in addition to base-pairing probabilities, we also stack inverse, inverse squared, and the base pairing probability matrix cannot be directly added to self-attention matrix. To circumvent this, we also do 2D convolution with the same kernel size as the 1D convolution on the modified base pairing probability matrix without 188 padding, so the dimensionality of the feature map becomes C \u00d7 (L \u2212 k + 1) \u00d7 (L \u2212 k + 1). The attention function 189 now is:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "169"
        },
        {
            "text": "Conceptually, instead of a base pair to base pair interaction mapping, the 2D convolution product of the modified base For DNA classification tasks, we choose Adam [36] , a commonly used optimizer in deep learning with \u03b2 1 = 0.9, 225 \u03b2 2 = 0.99, and = 1e \u2212 8. Weight decay is set to 1e-5. Our learning rate schedule is a stepwise inverse square root 226 decay schedule with warm up. Since we use relatively small batch sizes during training, we adjust the learning rate by a 227 scaling factor C:",
            "cite_spans": [
                {
                    "start": 164,
                    "end": 168,
                    "text": "[36]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "169"
        },
        {
            "text": "In our experiments, C is set 0.1 and warmup_steps is set to 3200. Additionally, we use dropout [37] of probability 0.1 229 in all attention layers, fully connected layers, and positional encoding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "169"
        },
        {
            "text": "For the RNA task, we found Adam to be underfitting. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "230"
        },
        {
            "text": "where \u03b1 and \u03b2 are tunable hyperparameters. If \u03b1 is set to 1 and \u03b2 to infinity, then the loss values stay the same; otherwise 239 gradients from measurements with large errors would be lowered to prevent the neural network from overfitting to 240 experimental errors. For the OpenVaccine dataset, we use \u03b1 = 0.5 and \u03b2 = 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "230"
        },
        {
            "text": "241 Figure 4 : The Nucleic Transformer can be trained in unsupervised, supervised, and semi-supervise fashion.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 4,
                    "end": 12,
                    "text": "Figure 4",
                    "ref_id": null
                }
            ],
            "section": "230"
        },
        {
            "text": "Since in the OpenVaccine challenge, the number of samples in the test set exceeds that in the training set, we use a 243 combination of learning methods ( Figure 4 ). Here we describe those learning methods. Although we also need to make predictions for a degradation property at pH10, none of the biophysical models used learning algorithm is robust to massive label noise, so the non-label-preserving mutations should simply be ignored 279 by the network during training [46] . The number of positions to randomly mutate is a hyperparamter that we denote 280 n mute , with which we experiment to find the best value. ",
            "cite_spans": [
                {
                    "start": 473,
                    "end": 477,
                    "text": "[46]",
                    "ref_id": "BIBREF62"
                }
            ],
            "ref_spans": [
                {
                    "start": 155,
                    "end": 163,
                    "text": "Figure 4",
                    "ref_id": null
                }
            ],
            "section": "Learning"
        },
        {
            "text": "To experiment with k-mers of different lengths, first we set a baseline with no k-mer aggregation, where only single 299 nucleotides with positional encoding are inputted into the transformer. Then we incrementally increase k, and evaluate 300 its effect on cross validation accuracy ( Figure 5) . Additionally, we also run experiments without the transformer encoder 301 to see the improvement the transformer encoder gives, in which case the k-mer aggregation layer is simply followed a 302 global-maxpooling and fully connected layer. Here we used n mute = 15, six transformer encoder layers, d model = 256, 303 n head = 8, and batch size is 24.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 286,
                    "end": 295,
                    "text": "Figure 5)",
                    "ref_id": "FIGREF11"
                }
            ],
            "section": "K-mers of different lengths 298"
        },
        {
            "text": "The model's performance improves initially upon using longer k-mers, and then saturates to a point where using longer 305 k-mers no longer increases performance. We see that aggregating larger kmers (k=8,9) actually led to a decrease in 306 performance, likely due to overfitting resulting from large convolution kernels. Further, we note that the transformer 307 encoder consistently gives a significant boost in accuracy compared to the counterpart without transformer encoder. Additionally, too many mutations lead to underfitting, as there is too much random noise during training leading to poor 316 convergence. Figure 8 : Visualization of an attention weight right before outputting a prediction. A right band can be seen for the 7-mer TTATTAT, which is one mutation away from TATAAT. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 618,
                    "end": 626,
                    "text": "Figure 8",
                    "ref_id": null
                }
            ],
            "section": "304"
        },
        {
            "text": "To further demonstrate the effectiveness of the Nucleic Transformer architecture, we trained Nucleic Transformer 336 models on a viral/non-viral genome dataset previously used to train Viraminer and compare the performance the 2 337 models [22] . ",
            "cite_spans": [
                {
                    "start": 240,
                    "end": 244,
                    "text": "[22]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Viral/non-viral classification 335"
        },
        {
            "text": "Previous works utilizing genomics usually used relatively shallow networks and did not explore the effects of varying 351 the depth or width of neural networks. When hypertuning the Nucleic Transformer, we found that increasing the depth 352 and width of the transformer encoder generally led to better performance on the viral/non-viral dataset (Figure 11, 12) , 353 although we do see that increasing the model width to 1024 actually led to worse results, mostly due to overfitting.",
            "cite_spans": [
                {
                    "start": 364,
                    "end": 367,
                    "text": "353",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 346,
                    "end": 361,
                    "text": "(Figure 11, 12)",
                    "ref_id": "FIGREF18"
                }
            ],
            "section": "Model width and depth 350"
        },
        {
            "text": "Notably, we found that increasing model depth beyond 6 transformer encoder layers actually led to diverged training on can give better results than even the top finish, which used a variety of feature engineering and more than 50 models. 365 We also compare predictions made using secondary structure information from different biophysical models. Finally, 366 we show that pretraining on randomly generated RNA sequences result in very similar performance as pretraining on 367 test set sequences. The biggest challenge in OpenVaccine was that the private test set had sequences that were longer and more diverse. In competition deadline, ensuring a robust testing process.",
            "cite_spans": [
                {
                    "start": 238,
                    "end": 241,
                    "text": "365",
                    "ref_id": null
                },
                {
                    "start": 357,
                    "end": 360,
                    "text": "366",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "354"
        },
        {
            "text": "By the end of the competition, we had trained two sets of models to use in the final submissions, one that was trained 378 directly on short sequences with labels and one that was pretrained with all available sequences before training on on 379 short sequences with labels. Our initial experiments with pretraining did not result in improvements in cross validation 380 or on the public test set, so it was difficult to make a decision purely based on model performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "377"
        },
        {
            "text": "In order to robustly select submissions, we visualized and evaluated the learned attention weights from the transformer 382 encoder ( Figure 13 ). Since we add the BPP matrix and distance matrix as a bias, both learned attention distributions of 383 pretrained and non-pretrained models resembled the BPP and distance matrix, but there were also some key differences.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 134,
                    "end": 143,
                    "text": "Figure 13",
                    "ref_id": null
                }
            ],
            "section": "381"
        },
        {
            "text": "The non-pretrained model paid heavy attention indiscriminately to pairs of positionally close nucleotides, as indicated Results on the private test set validated our selection based on visual inspection of attention weights (Figure 14 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 224,
                    "end": 234,
                    "text": "(Figure 14",
                    "ref_id": null
                }
            ],
            "section": "384"
        },
        {
            "text": "Pretrained models performed significantly better than non-pretrained models on both public test set and private test set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "394"
        },
        {
            "text": "The Figure 15 ). This is somewhat surprising given the that the predictions used as pseudo-labels could only score 411 0.3438 on the private test set. When we compare the R2 scores of supervised only, unsupervised, and semi-supervised 412 learning, we see that unsupervised and semi-supervised resulted in significant improvements over the supervised 413 only ( Figure 16 ). Also, we notice that predictions for deg_Mg_pH10 have significantly larger errors for the other 414 2 properties. This is expected because the biophysical models used are incapable of generating different secondary 415 structure predictions at different pH's.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 4,
                    "end": 13,
                    "text": "Figure 15",
                    "ref_id": "FIGREF11"
                },
                {
                    "start": 362,
                    "end": 371,
                    "text": "Figure 16",
                    "ref_id": null
                }
            ],
            "section": "395"
        },
        {
            "text": "It is important to note that the semi-supervised learning approach requires pseudo-labeling and knowing the test set 417 distribution before hand. Therefore, while it is effective under the competition setting, it is a risky approach and the 418 performance gain may not transfer in real life applications. To explore whether the pretraining performance gain results from knowing the test set distribution, we also generated 431 additional completely random sequences to use for pretraining. The sequences in the test set have more sequence 432 diversity than the training set, and generated sequences are even more diverse (Figure 17) . We repeated the pretraining 433 process with random sequences using the same hyperparamters, our experiments show that pretraining with random 434 sequences instead of test sequences results in almost identical performance and increasing the amount of random 435 sequences for pretraining leads to slightly better performance on both test sets ( Figure 18 ). These results suggest that 436 pretraining improves test set error not because of information leak from knowing the test sequences but rather the model 437 learning the generalized rules of mRNA secondary structure formation. 438 Figure 17 : Tsne plot of RNA sequences in the training set, test set, and randomly generated set Additionally, we also pseudo-labeled random sequences and retrained our models in semi-supervised fashion. In this 439 case, pseudo-labeling with completely random sequences did not lead to significant improvements ( Figure 19) ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 624,
                    "end": 635,
                    "text": "(Figure 17)",
                    "ref_id": null
                },
                {
                    "start": 984,
                    "end": 993,
                    "text": "Figure 18",
                    "ref_id": "FIGREF23"
                },
                {
                    "start": 1227,
                    "end": 1236,
                    "text": "Figure 17",
                    "ref_id": null
                },
                {
                    "start": 1541,
                    "end": 1551,
                    "text": "Figure 19)",
                    "ref_id": "FIGREF16"
                }
            ],
            "section": "416"
        },
        {
            "text": "In this work, we propose the Nucleic Transformer, an effective and yet conceptually simple architecture that is vaccines (Pfizer's vaccine has to be stored as -70 Celsius). One strategy to reduce mRNA hydrolysis is to redesign 461 RNAs to code for the same proteins but form double-stranded regions, which are protected from these degradative 462 processes. Our work can provide guidance and trained models can act as a screening tool in development of more stable 463 mRNA vaccines in the future. It is our hope the Nucleic Transformer can aid design of more stable mRNA vaccines that 464 can withstand harsher conditions than current ones. It is important to note, however, that there is still significant gap 465 between errors on the 107-bp mRNA sequences and the 130-bp mRNA sequences, both due to difference in sequence 466 length and diversity. Actual COVID-19 candidates are even longer and modeling those remains a challenge in the 467 future.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "445"
        },
        {
            "text": "Our results demonstrate that self-attention and convolution are a powerful combination for genomics tasks, enabling 469 learning both global and local dependencies effectively. It has long been posited that the transformer architecture 470 can excel in not only natural language processing but other fields as well, and our work provides evidence for that.",
            "cite_spans": [
                {
                    "start": 236,
                    "end": 239,
                    "text": "470",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "468"
        },
        {
            "text": "One key challenge, however, is the quadratic computational complexity of self-attention, which prohibits training on long sequences (greater than 512 so we plan to apply these linear transformers in the future.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "471"
        }
    ],
    "bib_entries": {
        "BIBREF1": {
            "ref_id": "b1",
            "title": "RNA, and the Flow of Genetic Information",
            "authors": [],
            "year": 2002,
            "venue": "DNA",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Looking beyond the genes: the role of non-coding variants in human 481 disease",
            "authors": [
                {
                    "first": "Malte",
                    "middle": [],
                    "last": "Spielmann",
                    "suffix": ""
                },
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Mundlos",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Human Molecular Genetics",
            "volume": "25",
            "issn": "R2",
            "pages": "157--165",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "The role of non-coding rnas in oncology",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Frank",
                    "suffix": ""
                },
                {
                    "first": "Arul",
                    "middle": [
                        "M"
                    ],
                    "last": "Slack",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Chinnaiyan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Cell",
            "volume": "179",
            "issn": "5",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Xiangyu",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shaoqing",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Attention is all you need. CoRR, abs/1706.03762",
            "authors": [
                {
                    "first": "Illia",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "BERT: pre-training of deep bidirectional 487 transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Deep learning for computational 489 biology",
            "authors": [
                {
                    "first": "Christof",
                    "middle": [],
                    "last": "Angermueller",
                    "suffix": ""
                },
                {
                    "first": "Tanel",
                    "middle": [],
                    "last": "P\u00e4rnamaa",
                    "suffix": ""
                },
                {
                    "first": "Leopold",
                    "middle": [],
                    "last": "Parts",
                    "suffix": ""
                },
                {
                    "first": "Oliver",
                    "middle": [],
                    "last": "Stegle",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Molecular Systems Biology",
            "volume": "12",
            "issn": "7",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Danq: a hybrid convolutional and recurrent deep neural network for quantifying 491 the function of DNA sequences",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Quang",
                    "suffix": ""
                },
                {
                    "first": "Xiaohui",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Nucleic Acids Research",
            "volume": "44",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Deep learning to predict the lab-of-origin of engineered DNA",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "A K"
                    ],
                    "last": "Nielsen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "A"
                    ],
                    "last": "Voigt",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Nat Commun",
            "volume": "9",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Comprehensive evaluation of deep learning architectures 495 for prediction of DNA/RNA sequence binding specificities",
            "authors": [
                {
                    "first": "Ameni",
                    "middle": [],
                    "last": "Trabelsi",
                    "suffix": ""
                },
                {
                    "first": "Mohamed",
                    "middle": [],
                    "last": "Chaabane",
                    "suffix": ""
                },
                {
                    "first": "Asa",
                    "middle": [],
                    "last": "Ben-Hur",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Bioinformatics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A deep learning approach to programmable RNA 497 switches",
            "authors": [
                {
                    "first": "N",
                    "middle": [
                        "M"
                    ],
                    "last": "Angenent-Mari",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Garruss",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "R"
                    ],
                    "last": "Soenksen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nat Commun",
            "volume": "11",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "A deep learning framework to predict binding preference of RNA constituents on 499 protein surface",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lam",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Nat Commun",
            "volume": "10",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Evaluation of deep learning in non-coding RNA classification",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Amin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mcgrath",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "P"
                    ],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Nat Mach",
            "volume": "501",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A survey on deep learning in DNA/RNA 503 motif mining",
            "authors": [
                {
                    "first": "Ying",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Zhen",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Qinhu",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Siguo",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "De-Shuang",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Briefings in Bioinformatics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Deepsite: bidirectional LSTM and CNN models for predicting DNA-protein 505 binding",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Qiao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Int. J. Mach. Learn. & Cyber",
            "volume": "11",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Detection of DNA base modifications by deep recurrent neural network on Oxford 507",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Nanopore sequencing data",
            "authors": [],
            "year": 2019,
            "venue": "Nat Commun",
            "volume": "10",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Identifying centromeric satellites with dna-brnn",
            "authors": [
                {
                    "first": "Heng",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Bioinformatics",
            "volume": "35",
            "issn": "21",
            "pages": "4408--4410",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Deepcpg: accurate prediction of single-cell DNA methylation states 510 using deep learning",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Angermueller",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "J"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Reik",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Genome Biol",
            "volume": "18",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Md Zahid Hossain Khan, and Swakkhar Shatabda. iPromoter-BnCNN: a novel branched 513",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Md Moshiur Rahman",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "CNN-based predictor for identifying and classifying sigma promoters",
            "authors": [],
            "year": null,
            "venue": "Bioinformatics",
            "volume": "07",
            "issn": "2020",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "MULTiPly: a novel multi-layer predictor for discovering general and specific 516 types of promoters",
            "authors": [
                {
                    "first": "Jiangning",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "Cangzhi",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Bioinformatics",
            "volume": "35",
            "issn": "17",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "ipromoter-2l2.0: Identifying promoters and their types by combining smoothing cutting 518 window algorithm and sequence-based features",
            "authors": [
                {
                    "first": "Bin",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Molecular Therapy -Nucleic Acids",
            "volume": "18",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Viraminer: Deep learning on raw dna sequences 520 for identifying viral genomes in human samples",
            "authors": [
                {
                    "first": "Ardi",
                    "middle": [],
                    "last": "Tampuu",
                    "suffix": ""
                },
                {
                    "first": "Zurab",
                    "middle": [],
                    "last": "Bzhalava",
                    "suffix": ""
                },
                {
                    "first": "Joakim",
                    "middle": [],
                    "last": "Dillner",
                    "suffix": ""
                },
                {
                    "first": "Raul",
                    "middle": [],
                    "last": "Vicente",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "PLOS One",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "ipromoter-2l2.0: Identifying promoters and their types by combining smoothing cutting 522 window algorithm and sequence-based features",
            "authors": [
                {
                    "first": "Bin",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Molecular Therapy -Nucleic Acids",
            "volume": "18",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "iPromoter-2L: a two-layer predictor for identifying 524 promoters and their types by multi-window-based PseKNC",
            "authors": [
                {
                    "first": "Bin",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Fan",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "De-Shuang",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Kuo-Chen",
                    "middle": [],
                    "last": "Chou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Bioinformatics",
            "volume": "34",
            "issn": "1",
            "pages": "33--40",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Theoretical basis for stabilizing messenger rna through 527 secondary structure design",
            "authors": [
                {
                    "first": "Po-Ssu",
                    "middle": [],
                    "last": "Dres Parra Sperberg",
                    "suffix": ""
                },
                {
                    "first": "Rhiju",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "iPromoter-2L: a two-layer predictor for identifying 529 promoters and their types by multi-window-based PseKNC",
            "authors": [
                {
                    "first": "Bin",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Fan",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "De-Shuang",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Kuo-Chen",
                    "middle": [],
                    "last": "Chou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Bioinformatics",
            "volume": "34",
            "issn": "1",
            "pages": "33--40",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide 531 sequences",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Godzik",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Bioinformatics",
            "volume": "22",
            "issn": "13",
            "pages": "1658--1659",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "RegulonDB version 9.0: 537 high-level integration of gene regulation, coexpression, motif clustering and beyond",
            "authors": [
                {
                    "first": "V\u00edctor",
                    "middle": [],
                    "last": "Koutoucheva",
                    "suffix": ""
                },
                {
                    "first": "Fabio",
                    "middle": [],
                    "last": "Del Moral-Ch\u00e1vez",
                    "suffix": ""
                },
                {
                    "first": "Julio",
                    "middle": [],
                    "last": "Rinaldi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Collado-Vides",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Nucleic Acids Research",
            "volume": "538",
            "issn": "D1",
            "pages": "133--143",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "iPro54-PseKNC: a sequence-based predictor 540 for identifying sigma-54 promoters in prokaryote with pseudo k-tuple nucleotide composition",
            "authors": [
                {
                    "first": "En-Ze",
                    "middle": [],
                    "last": "Hao Lin",
                    "suffix": ""
                },
                {
                    "first": "Hui",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "Kuo-Chen",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Chou",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Nucleic Acids",
            "volume": "541",
            "issn": "21",
            "pages": "12961--12972",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Identifying sigma70 promoters with novel pseudo nucleotide composition",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "On the stratification of multi-label data",
            "authors": [
                {
                    "first": "Konstantinos",
                    "middle": [],
                    "last": "Sechidis",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Grigorios Tsoumakas, and Ioannis Vlahavas",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Machine Learning and Knowledge Discovery in Databases Lecture Notes in Computer Science",
            "authors": [],
            "year": 2011,
            "venue": "",
            "volume": "546",
            "issn": "",
            "pages": "145--158",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Massively parallel implementation of sequence alignment with 548 basic local alignment search tool using parallel computing in java library",
            "authors": [
                {
                    "first": "Marek",
                    "middle": [],
                    "last": "Nowicki",
                    "suffix": ""
                },
                {
                    "first": "Davit",
                    "middle": [],
                    "last": "Bzhalava",
                    "suffix": ""
                },
                {
                    "first": "Piotr",
                    "middle": [],
                    "last": "Ba\u0142a",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Journal of Computational Biology",
            "volume": "549",
            "issn": "8",
            "pages": "871--881",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Openvaccine: Covid-19 mrna vaccine degradation prediction",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Layer normalization",
            "authors": [
                {
                    "first": "Jimmy",
                    "middle": [
                        "Lei"
                    ],
                    "last": "Ba",
                    "suffix": ""
                },
                {
                    "first": "Jamie",
                    "middle": [
                        "Ryan"
                    ],
                    "last": "Kiros",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Training tips for the transformer model",
            "authors": [
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Popel",
                    "suffix": ""
                },
                {
                    "first": "Ondrej",
                    "middle": [],
                    "last": "Bojar",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Adam: A method for stochastic optimization",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Diederik",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Yoshua Bengio and Yann",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "3rd International Conference on Learning Representations",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Conference Track Proceedings",
            "authors": [],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Dropout: A simple 557 way to prevent neural networks from overfitting",
            "authors": [
                {
                    "first": "Nitish",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Ruslan",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Journal of Machine Learning Research",
            "volume": "15",
            "issn": "56",
            "pages": "1929--1958",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Gradient centralization: A new optimization 560 technique for deep neural networks",
            "authors": [
                {
                    "first": "Hongwei",
                    "middle": [],
                    "last": "Yong",
                    "suffix": ""
                },
                {
                    "first": "Jianqiang",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Xiansheng",
                    "middle": [],
                    "last": "Hua",
                    "suffix": ""
                },
                {
                    "first": "Lei",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Billion-scale semi-supervised 562 learning for image classification",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "Zeki"
                    ],
                    "last": "Yalniz",
                    "suffix": ""
                },
                {
                    "first": "Herv\u00e9",
                    "middle": [],
                    "last": "J\u00e9gou",
                    "suffix": ""
                },
                {
                    "first": "Kan",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Manohar",
                    "middle": [],
                    "last": "Paluri",
                    "suffix": ""
                },
                {
                    "first": "Dhruv",
                    "middle": [],
                    "last": "Mahajan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Rnasoft: a suite of rna secondary structure prediction and design software tools",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Andronescu",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Nucleic Acids",
            "volume": "564",
            "issn": "13",
            "pages": "3416--3422",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Rnastructure: software for rna secondary structure prediction and analysis",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jessica",
                    "suffix": ""
                },
                {
                    "first": "David H",
                    "middle": [],
                    "last": "Reuter",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mathews",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Contrafold: Rna secondary structure prediction without physics-based 568 models",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "B"
                    ],
                    "last": "Do",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "A"
                    ],
                    "last": "Woods",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Batzoglou",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Bioinformatics",
            "volume": "22",
            "issn": "14",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Rna secondary structure packages ranked and 570 improved by high-throughput experiments. bioRxiv",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hannah",
                    "suffix": ""
                },
                {
                    "first": "Wipapat",
                    "middle": [],
                    "last": "Wayment-Steele",
                    "suffix": ""
                },
                {
                    "first": "Rhiju",
                    "middle": [],
                    "last": "Kladwang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "An algorithm for computing nucleic acid base-pairing probabilities including 572 pseudoknots",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Robert",
                    "suffix": ""
                },
                {
                    "first": "Niles",
                    "middle": [
                        "A"
                    ],
                    "last": "Dirks",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pierce",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Journal of Computational Chemistry",
            "volume": "25",
            "issn": "10",
            "pages": "1295--1304",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "Viennarna package 2.0",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Stadler",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hofacker",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Algorithms for Molecular Biology",
            "volume": "6",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Deep learning is robust to massive label noise",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Rolnick",
                    "suffix": ""
                },
                {
                    "first": "Andreas",
                    "middle": [],
                    "last": "Veit",
                    "suffix": ""
                },
                {
                    "first": "Serge",
                    "middle": [
                        "J"
                    ],
                    "last": "Belongie",
                    "suffix": ""
                },
                {
                    "first": "Nir",
                    "middle": [],
                    "last": "Shavit",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "Random sequences rapidly evolve into de novo promoters",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Avihu",
                    "suffix": ""
                },
                {
                    "first": "Eric",
                    "middle": [
                        "J"
                    ],
                    "last": "Yona",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Alm",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gore",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Communications",
            "volume": "578",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "Longformer: The long-document transformer",
            "authors": [
                {
                    "first": "Iz",
                    "middle": [],
                    "last": "Beltagy",
                    "suffix": ""
                },
                {
                    "first": "Matthew",
                    "middle": [
                        "E"
                    ],
                    "last": "Peters",
                    "suffix": ""
                },
                {
                    "first": "Arman",
                    "middle": [],
                    "last": "Cohan",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "Linformer: Self-attention with linear 581 complexity",
            "authors": [
                {
                    "first": "Sinong",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Belinda",
                    "middle": [
                        "Z"
                    ],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Madian",
                    "middle": [],
                    "last": "Khabsa",
                    "suffix": ""
                },
                {
                    "first": "Han",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Hao",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF68": {
            "ref_id": "b68",
            "title": "584 Rethinking attention with performers",
            "authors": [
                {
                    "first": "Jared",
                    "middle": [],
                    "last": "Hawkins",
                    "suffix": ""
                },
                {
                    "first": "Afroz",
                    "middle": [],
                    "last": "Davis",
                    "suffix": ""
                },
                {
                    "first": "Lukasz",
                    "middle": [],
                    "last": "Mohiuddin",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "Lucy",
                    "middle": [],
                    "last": "Belanger",
                    "suffix": ""
                },
                {
                    "first": "Adrian",
                    "middle": [],
                    "last": "Colwell",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Weller",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "Big bird: Transformers for longer sequences",
            "authors": [
                {
                    "first": "Anirudh",
                    "middle": [],
                    "last": "Ravula",
                    "suffix": ""
                },
                {
                    "first": "Qifan",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Li",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Amr",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Overview of the Nucleic Transformer architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "double-stranded DNA forms hydrogen bonds between its complementary bases, single-stranded RNA forms secondary 127 structures by itself. Therefore, it is reasonable to regard DNA as an information storage device, while RNA should 128 be seen as a molecule in the context of mRNA degradation predictions. This simple realization suggests we can use 129 existing biophysical models to inject biophysical knowledge into our deep learning models.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "inverse cubed pairwise distance matrices on top of the original base-pairing probability matrix, where the distance is the 179 the number of covalent bonds between the pair of nucleotides (this can also be considered the path length in an RNA 180 graph where the only edges are the covalent bonds). The inverse distance matrices encode some information about 181 the relative distance between pairs of nucleotides, since pairs of nucleotides with a small number of covelent bonds in 182 between are likely to be closer to each other spatially. Because the distance matrix already encodes information about 183 position, we do not use positional encoding for mRNA. 184 Because 1-D convolution operation used in the Nucleic Transformer does not use padding, the convolution product 185 ends up with reduced dimensionality in the L dimension when the convolution kernel size is bigger than 1. As a result, 186",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "pairing probability matrix can be seen as a kmer to kmer pairwise interaction mapping with matching dimensionality192    to the 1D convolution kmer products. Aside from matching dimensionality, the 2D convolution operation also makes 193 up for some missing information regarding the geometry of mRNA folding. To illustrate this, we visualize an mRNA194 sequence in the OpenVaccine dataset to explain the physical and mathematical reasoning behind the 2D convolution 195 operation (Figure 2). While inspecting the interaction between A-20 (A at position 20), G-21, C-40, and U-41, we 196 can visually see that A-20 and C-40 are quite close to each other and imagine that there is some degree of interaction 197 between them, despite A-20 and C-40 not forming hydrogen bonds. However, looking at the portion of BPP matrix 198 and distance matrix corresponding to the 2x2 connection between A-20 (A at position 20), G-21, C-40, and U-41, we 199 see that neither the BPP matrix nor the distance matrix convey this information, as the component (40,20) has zero or 200 close to zero values on both the BPP matrix and the distance matrix. When a 2x2 convolution kernel operates on the 201 BPP matrix and distance matrix (for illustration purposes here we simply draw a kernel with all values set to unity), it 202 essentially fuses the 4 connections between A-20, G-21, C-40, and U-41, and creates a strong connection between the 2 203 2mers (A-20, G-21 and C-40, U-41). Now it becomes much easier for the network to learn the interaction between 204 A-20 and G-40 (as well as for G-21 and U-41). Physical meaning of 2D convolution on BPP matrix illustrated. Here we visualize the folding of sequence id_0049f53ba. The combination of convolution and self-attention cannot produce nucleotide position wise predictions, since it generates 206 kmer encodings instead single base pair encoding. In order to make predictions per nucleotide position, we introduce 207 additional deconvolution layers to retrieve full dimensional encodings, which allow residual connections of both 1D 208 and 2D encodings before and after the transformer encoder. As a result, both the single nucleotide embeddings and the 209 modified BPP matrix go through deep transforms before outputting predictions. 210Now we can summarize the modified Nucleic Transformer architecture used for the RNA task(Figure 3), which can be 211 seen as a special case of a series of multiple Nucleic Transformers with a single transformer encoder layer followed by 212 a deconvolution layer. Also, because the OpenVaccine challenge requires making predictions at each position of the 213 RNA sequence, it is important for the last transformer encoder layer right before outputting predictions to operate on 214 single nucleotide encodings instead of kmer encodings. With these considerations in mind, we choose a simple strategy 215 to construct the stack of Nucleic Transformers with two main hyperparameters k and n layer set equal to each other 216 (Figure3). The first single layer Nucleic Transformer has k = n layer and we decrease the size of the convolution kernel 217 by 1 for the next single layer Nucleic Transformer. Therefore, when we get to the last Nucleic Transformer in the stack, 218 k becomes 1 and the last Nucleic Transformer is simply a transformer encoder layer with an added bias from the BPP 219 feature map. Nucleic Transformer stack which takes advantage of additional input information from biophysical models.2.3 Training details 221Training of transformers can be tricky and wrong selection of training schedule and hyperparamters can be lead to 222 diverged training [35], so here we detail the training process.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "could generate predictions at different pH's. With 6 packages, we ended with up with 12 secondary structure predictions 265 for each sequence. During training, we randomly select one of the 12 secondary structure predictions for each sample 266 during a forward and backward propagation pass. During validation and testing, we use the averaged predictions made 267 used all 12 secondary structure predictions.268 2.3.5 Random mutations during training 269There is no question that the transformer architecture is extremely powerful. In fact, it has been shown that the270 transformers can learn from the English Wikipedia which contains billions of words in an unsupervised fashion and 271 achieve state-of-the-art results by finetuning on task specific data [6]. It is known that deep learning models can perfectly 272 memorize completely random data and labels, so to combat the memorization effect, we inject noise artificially by 273 randomly mutating positions in the source DNA/RNA sequence before forward and backward propagation during 274 training, similar to bert's pretraining [6]. This injection of random noise is done during DNA supervised learning and 275 RNA unsupervised learning. Note that in all our experiments, we simply randomly mutate nucleotides in randomly 276 selected positions, and because we do not ensure nucleotide at each selected position is changed, the average amount 277 of mutations is 3/4 of n mute . It is true that these random mutations could be non-label-preserving; however, deep 278",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "/non-promoter classification 283 In this section, we first compare the performance of the Nucleic Transformer with the best hyperparameters on E. coli 284 promoter classification with other top results in the literature. Then we explore how a few important hyperparameters 285 affect the accuracy of the results, followed by visualization and analysis of attention weights.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "the Nucleic Transformer to outperform previous results in the literature in E. coli promoter classification 288 (",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Bar chart of accuracy vs the parameter k.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "like most tunable hyperparameters in deep learning, we see from our experiments that there is an optimal value for 310 the number of random mutations during training (Figure 6). Here we used six transformer encoder layers, d model = 256, 311 n head = 8, and batch size is 24. For experiments without the transformer encoder, we simply took out the transformer 312 encoder while keeping everything else the same. Our experiments show that the model tends to overfit to the training 313 data when no random mutations are added during training. Without mutations, the model quickly converges to 1.0 314 accuracy and close to 0 loss on the training set, indicating that the model has simply memorized the training data.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "has been experimentally shown that approximately 10% random sequences can function as promoters[47]. Here we 319 screened one million randomly generated DNA sequences of length 81, and classified them with our trained model 320(Figure 7). We found that despite a balanced distribution of positive and negative examples in the promoter/non-promoter 321 dataset, our model recognized approximately 65% of random DNA sequences as non-promoters. This suggests that our 322 model has some mechanism to guard against random sequences and hence is likely able to generalize well.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "Cross validation accuracy vs number of random mutations during training. Percent of one million randomly generated sequences classified as promoters/non-promoters by the Nucleic Transformer. 3.1.5 Analysis of attention weights and motif extraction 324 By visualizing the attention weight matrix, we see that the Nucleic Transformer often focuses on kmers that resemble 325 consensus promoter motifs (Figure 8). We can also extract motifs the Nucleic Transformer considers the most 326 characteristic of promoters (Figure 9). For each correct prediction of promoters in the validation sets of 5-fold cross 327 validation, we take the column-wise sum of the attention weight matrix and rank the kmers. Then we simply count the 328 amount of times each kmer receives top 3 most attention in all correct predictions. We found the kmers that frequently 329 appear in top 3 resemble the consensus promoter motif TATAAT. In fact, two of the 10 most frequently 7-mers have the 330 exact consensus motif TATAAT, while 5 others contain the motif TAAAAT, TATCAT, TATTAT, TATATT, TATACT, all 331 of which are one mutation away from the consensus motif TATAAT. Deep learning has been criticized as black boxes 332 that cannot be interpreted; however, here we demonstrate that the Nucleic Transformer can be interpreted and useful 333 motifs can even be derived from attention weights directly.334",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Most important 7-mers in promoter classification based on analysis of attention weights.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "end to end, the Nucleic Transformer significantly outperforms the Viraminer counterpart by 3.4% AUC 340 score. When trained with a two-stage training process combining two branches with different hyperparamters and 341 pooling schemes, the Viraminer performed significantly better compared to its end to end counterpart, but the Nucleic 342 Transformer still leads in accuracy even with just one end to end model (k = 13). It is widely known that even just 343 simply averaging results from a few machine learning models trained with slightly different hyperparameters usually 344 leads to better performance, so it is no surprise that Viraminer's strategy to combine two branches with different setups 345 prove successful. For better comparison, we also trained the Nucleic Transformer 2 and 3 times with different ks 346 and averaged the test set predictions. The AUC improved upon by 0.3% upon averaging 2 models (k = 11, 13), but 347 with 3 models averaged (k = 11, 13, 15), the AUC only improved slightly by 0.2%. Here we used n mute = 40, six 348 transformer encoder layers, d model = 512, and n head = 8. Comparison of test set performance between Nucleic Transformer and Viraminer.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "the viral genome dataset, probably because of the huge class imbalance. Although increasing the depth and width both 356 lead to better performance, an argument can be made that it is more efficient to increase the depth of the transformer 357 encoder, because computational complexity and the number of parameters increase quadratically with the width of the 358 transformer encoder. The effect of the number of transformer encoder layers on test AUC performance.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "The effect of the width of transformer encoder layers on test AUC performance.3.3 OpenVaccine challenge 360To further test our ideas, we used an adaption of the Nucleic Transformer in the recent 21-day OpenVaccine challenge,361 hosted by Das Lab at Stanford University [33]. We first show our results using unsupervised learning that led to a 7th 362 finish in 1636 teams of machine learning experts from all over the world and how interpretibility helped our decision 363 making in model selection. Then we demonstrate using a semi-supervised learning approach the Nucleic Transformer364",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "machine learning competitions, competitors would normally have to rely on local cross validation to evaluate model 371 performance. Nevertheless, the training set had noisy training samples and limited sequence diversity, which meant that 372 even local cross validation may not be robust enough. During the course of the competition, each team could make up 373 to 5 submissions per day, and score on the public test set would be displayed for each submission. The final ranking 374 was to be evaluated with a private test set that consisted of more diverse and longer mRNA vaccine sequences and each 375 team could select up to 2 submissions for final evaluation. The score on the private test set would not be shown until 376",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "by the bright stripes parallel and close to the diagonal of the attention matrix. This indicates the non-pretrained model 386 thought that positionally close nucleotides were always important when making predictions on mRNA degradation 387 properties, which seemed highly unlikely. On the other hand, the pretrained model did not show the same bias towards 388 pairs of positionally close nucleotides, and was able to recognize the weak BPP connections which were barely visible 389 on the original BPP matrix. In this case, the model seemed to make more effective use of the BPP matrix generated by 390 biophysical models. Because of these considerations, we favored pretrained models in our final submissions, where 391 one submission was an average of 20 pretrained models, and the other was an average of 20 pretrained models and 20 392 non-pretrained models. Visualization of BPP+distance matrix, attention weights of a non-pretrained Nucleic Transformer, and attention weights of a pretrained Nucleic Transformer",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "Test set performance of OpenVaccine challenge using different pretraining procedures.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "capable of high performance in classifying promoters and viral genome as well as predicting degradation properties 447 of mRNA COVID-19 vaccine candidates per nucleotide. The Nucleic Transformer architecture outperforms other 448 deep learning/non deep learning methods that require hand-crafted features in E. coli promoter classification, while 449 also providing interpretibility and is capable of extracting promoter motifs directly from learned attention. Although 450 always trained end to end, the Nucleic Transformer leads in terms of accuracy in classifying viral genomes compared to 451 previous models such as Viraminer, which requires sophisticated multi-stage training and ensembling. As an additional 452 test, we also participated in the recent OpenVaccine challenge with an adaptation of the Nucleic Transformer and 453 placed 7th out of 1636 teams of top machine learning experts from all over the globe. We also demonstrate with 454 semi-supervised learning, the Nucleic Transformer outperforms even the top solution in the OpenVaccine challenge by 455 a considerable margin. 456 Although classification of promoters and viral genomes have been well studied in the literature, there is no precedence 457 on predictions of mRNA degradation properties per nucleotide. After a chaotic 2020 caused by COVID-19, mRNA 458 vaccines have emerged as a suitable solution to the COVID problem, with companies like Pfizer and Moderna rolling 459 out mRNA vaccines at unprecedented speeds. However, storage and transport remain a challenge with fragile mRNA460",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Now we obtain a collection of k-mers, where each k-mer is represented by a feature vector of size d model . The 1D convolution layers are always followed by a layer Indeed our representation of k-mers deviates from conventional representation of words in deep learning, where each word in the vocabulary directly corresponds to a feature vector in a look up table. The disadvantage of using look up tables for k-mers is that a very small percentage of all possible k-mers are present in any dataset, and there is no way for the network to generalize to unseen k-mers. For instance, if a common promoter motif TATAAT appears in the dataset but a similar motif TATATT does not, there is no way for the network to generalize to the unseen motif TATATT, but by using convolutions, we make it easy for the network to recognize that there is not much difference between TATAAT and TATATT, leading to better generalization. Additionally, embeddings of k-mers of larger sizes require a prohibitively large amount of parameters, since the total possible amount of k-mers for a given k is 4 k .",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Therefore, we switched to a more recent and powerful optimizer, Ranger, which uses gradient centralization from https://github.com/lessw2020/ Ranger-Deep-Learning-Optimizer[38]. As for the training schedule, we used flat and anneal, where training starts with a flat learning rate of 1e-3 and then 75% through all the epochs training proceeds with cosine annealing schedule reducing learning rate down to 0 at the end of training. Weight decay is set to 0.1.Because the OpenVaccine dataset came from experimental measurements that had errors, we adjusted the losses based on the error for each measurement during supervised training:",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Multitasking learning: during pretraining, mutated/masked sequence, structure, and predicted loop type are inputted into the Nucleic Transformer and then the Nucleic Transformer is trained with crossentropy loss to retrieve the correct sequence, structure, and predicted loop type simultaneously at each position of the RNA sequence. During training on ground truth labels and pseudo labels, the Nucleic Transformer is trained to predict 5 different degradation properties simultaneously at each measured position of the RNA sequence.Unsupervised learning: although we did not use pretraining for our DNA tasks, we used all available sequences in the OpenVaccine challenge dataset to pretrain our network on randomly mutated and masked (with NULL token) sequence retrieval loss (basically softmax to retrieve correct nucleotide/structure/loop). During pretraining, the Nucleic Transformer learns the rules of RNA structure, guided by biophysical knowledge provided by biophysical models.Supervised/semi-supervised learning: during supervised learning, the Nucleic Transformer is trained on target values of classification classes or RNA degradation properties. Following RNA supervised learning, the Nucleic Transformer was retrained in semi-supervised fashion on pseudo labels generated by an ensemble of Nucleic Transformers with different depths. Similar to previous work with semi-supervised learning [39], we retrain the models first using pseudo labels at a flat learning rate and then finetune with ground truth labels in the training set with cosine anneal schedule.We used secondary structures predicted by an ensemble of biophysical models including RNAsoft [40], rnastructure [41], CONTRAfold [42], EternaFold [43], NUPACK [44], and Vienna [45]. Arnie https://github.com/DasLab/arnie 261 is used as a wrapper to generate secondary structure predictions. For each sequence, we generated secondary structure 262predictions at 37 and 50 Celsius, since two of the scored degradation properties were measured at different temperatures.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": ". Compared to non-deep learning approaches which use sophisticated hand crafted features, the Nucleic Transformer leads in accuracy by at least 1.6% or more. A more recent model, iPromoter-BnCNN, which also employs structural property properties of DNA such as stability, rigidity, and curvature, is similar in performance to the Nucleic Transformer, although the Nucleic Transformer directly makes predictions from sequence information. The best results Performance of Nucleic Transformer against top results in the literature based on accuracy, MCC, sensitivity, and specificity.On the independent test set(Table 2), which includes recently released experimentally verified promoter samples, the Nucleic Transformer is more accurate than MULTiPly and iPromoter-2L, while only slightly more accurate (by 1 sample) than iPromoter-BnCNN. Performance of Nucleic Transformer on the independent test set against top results in the literature.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Figure 16: R2 score on the OpenVaccine private test set of the Nucleic Transformer. A: supervised only, B: unsupervised, C: semi-supervised3.3.3 Comparison of different biophysical modelsDifferent biophysical models often produce somewhat different secondary structure predictions, so here we compare the MCRMSE of the predictions made using inputs from different biophysical models. Previous study has ranked these biophysical models using high-throughput experiments and found CONTRAfold and RNAsoft to be the most accurate : Performance using inputs generated by different biophysical models to predict mRNA degradation in the OpenVaccine dataset.[43]. Here we find that overall RNAsoft performs the best across different conditions. On the private test set, we see that predictions made using secondary structure information from Vienna and RNAsoft have the lowest MCRMSE without semi-supervised learning. With semi-supervised learning, RNAsoft gives most accurate predictions, while EternaFold and Vienna come close. Additionally, simply the averaging all predictions made using all available packages on the private test set always results in lower MCRMSE than using any single package, indicating that ensembling different biophysical models is a good strategy that would give better predictions than any single model.429 3.3.4 Learning from random sequences",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": ". It is likely that due to the large mutation distance of random sequences compared to train and test sequences, the pseudolabels on the random sequences are not close to ground truth values at all. Therefore, the semi-supervised learning process ended up feeding mostly noise to the model during training, although it did lead to very slight improvement on the private test set over the model without semi-supervised learning.Figure 19: Semi-supervised learning with test sequences compared with random sequences.",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "). Notably, much work has been done in very recent times on reducing the quadratic computational complexity of self-attention to linear to enable training of transformer like self-attention on much longer sequences [48, 49, 50, 51], i.e. linear transformers. Whole genomes and COVID-19 mRNA vaccines both greatly exceed the length limit of full self-attention, and COVID-19 vaccine candidates, in particular, are around 4000 bp long,",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
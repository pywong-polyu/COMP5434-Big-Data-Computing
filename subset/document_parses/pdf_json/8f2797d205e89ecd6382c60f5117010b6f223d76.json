{
    "paper_id": "8f2797d205e89ecd6382c60f5117010b6f223d76",
    "metadata": {
        "title": "Analyzing the Dependency of ConvNets on Spatial Information",
        "authors": [
            {
                "first": "Yue",
                "middle": [],
                "last": "Fan",
                "suffix": "",
                "affiliation": {},
                "email": "yfan@mpi-inf.mpg.de"
            },
            {
                "first": "Yongqin",
                "middle": [],
                "last": "Xian",
                "suffix": "",
                "affiliation": {},
                "email": "yxian@mpi-inf.mpg.de"
            },
            {
                "first": "Max",
                "middle": [
                    "Maria"
                ],
                "last": "Losch",
                "suffix": "",
                "affiliation": {},
                "email": "mlosch@mpi-inf.mpg.de"
            },
            {
                "first": "Bernt",
                "middle": [],
                "last": "Schiele",
                "suffix": "",
                "affiliation": {},
                "email": "schiele@mpi-inf.mpg.de"
            }
        ]
    },
    "abstract": [
        {
            "text": "Intuitively, image classification should profit from using spatial information. Recent work, however, suggests that this might be overrated in standard CNNs. In this paper, we are pushing the envelope and aim to investigate the reliance on spatial information further. We propose to discard spatial information via shuffling locations or average pooling during both training and testing phases to investigate the impact on individual layers. Interestingly, we observe that spatial information can be deleted from later layers with small accuracy drops, which indicates spatial information at later layers is not necessary for good test accuracy. For example, the test accuracy of VGG-16 only drops by 0.03% and 2.66% with spatial information completely removed from the last 30% and 53% layers on CIFAR-100, respectively. Evaluation on several object recognition datasets with a wide range of CNN architectures shows an overall consistent pattern.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The online version of this chapter (https://",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Despite the impressive performances of convolutional neural networks (CNNs) on computer vision tasks [9, 10, 16, 18, 25] , their inner workings remain mostly obfuscated to us, especially how the information is encoded throughout layers. Generally, the majority of modern CNNs for image classification utilize a collection of filters with local receptive fields to capture hierarchical patterns across all the convolutional layers [10, 16, 25] . Such design choices are based on the assumption that spatial information remains important at every convolutional layer, and better representations can be attained by gradually enlarging the receptive field to incorporate more contexts. This further leads to lots of approaches that help capture spatial correlations between features in order to improve model performance [1, 13, 26] . For example, a popular class of those methods is the visual attention mechanism [15, 19] which enables more powerful representations by enhancing the most salient region of the image.",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 104,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 105,
                    "end": 108,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 109,
                    "end": 112,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 113,
                    "end": 116,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 117,
                    "end": 120,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 430,
                    "end": 434,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 435,
                    "end": 438,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 439,
                    "end": 442,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 817,
                    "end": 820,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 821,
                    "end": 824,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 825,
                    "end": 828,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 911,
                    "end": 915,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 916,
                    "end": 919,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "However, recent works on restricting the receptive field of CNN architectures for scrambled inputs [2] or using wavelet feature networks of shallow depth [20] , have all found it to be possible to acquire competitive performances on the respective tasks. This raises doubts on the necessity of spatial information for classification and whether the network can still maintain the performance when the spatial information is completely removed from the training process. Fig. 1 . Shuffling the feature maps from the last 54% layers in VGG-16 randomly and spatially only reduces the final test accuracy by 2.66% (from 74.10% to 71.44%) on CIFAR-100, and the training processes look surprisingly similar, which implies that spatial information may not be necessary for good classification accuracy.",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 102,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 154,
                    "end": 158,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 470,
                    "end": 476,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "In this work, we re-design the structure of the network to separate the spatial information and channel-wise information independently, with the goal of analyzing the dependency of the network on them. Spatial information refers to the spatial ordering on the feature map. To this end, we propose channel-wise shuffle to eliminate channel information, and spatial shuffle, patch-wise spatial shuffle and GAP+FC to eliminate spatial information. Surprisingly, we find that the spatial information is not necessary at later layers, and the modified CNNs, i.e. without accessing any spatial information at later layers, can still achieve competitive results on several object recognition datasets. As an example, Fig. 1 shows the training processes of a standard VGG-16 and a modified VGG-16 with spatial shuffle on CIFAR-100. In the shuffled VGG-16, feature maps must first go through a random spatial shuffle operation before convolved with the filters from the last 54% layers. Interestingly, the test accuracy only drops 2.66%, and the training process is nearly identical to the standard VGG-16. This observation generalizes to various CNN architectures: removing spatial information from the last 30% layers gives a surprisingly little test accuracy decrease within 1% across architectures and datasets, and the accuracy decrease is still within 7% even if the last 50% layers are manipulated. This indicates that spatial information is overrated for standard CNNs and not necessary to reach competitive performances. Finally, our investigation on the detection task shows that although the unavailability of spatial information at later layers does hinder the CNN to localize objects, the impact is not as fatal as expected; at the same time, the classification ability of the model is not affected.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 710,
                    "end": 716,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "The main contributions of our work are as follows: we find that spatial information at later layers is not really necessary for good classification test accuracy and that even though the depth of the network plays an important role, later layers do not require spatial integration. As a side effect, GAP+FC leads to a smaller model with fewer parameters with small test accuracy drops.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Intuitively, object recognition benefits from gradually enlarged receptive field and spatial integration. For that reason extensive efforts have been made to enhance the aggregation of spatial information in the decision-making progress of CNNs. [5, 32] have made attempts to generalize the strict spatial sampling of convolutional kernels to allow for globally spread out sampling, and [31] have spurred a range of follow-up work on embedding global context layers with the help of spatial down-sampling. Another emerging interest of augmenting CNNs with self-attention has also made progress in several vision tasks. [27] presents a non-local operation that computes the response at a position as a weighted sum of the features at all positions to capture long-range dependencies and shows that self-attention is an instantiation of their non-local operations. [3] show improvements on image classification and achieve state-of-the-art results on video action recognition tasks with a variant of non-local operations. Even a fully attentional model is verified to be effective for various visual tasks [21] .",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 249,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 250,
                    "end": 253,
                    "text": "32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 387,
                    "end": 391,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 619,
                    "end": 623,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 863,
                    "end": 866,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1104,
                    "end": 1108,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "While all of these works have improved on a related classification metric in some way, it is not entirely evident whether the architectural changes alone can be credited, as there is an increasing number of work on questioning the importance of the extent of spatial information for common CNNs. One of the most recent observations by [2] indicates that the VGG-16 architecture trained on ImageNet is invariant to scrambled images to a large extent. Furthermore, they construct a modified ResNet architecture with a limited receptive field as small as 33 \u00d7 33, similar to the style of the traditional Bag-of-Visual-Words and reach competitive results on ImageNet. In contrast to their work, we make a clear distinction between first and last layers, and we show empirically spatial information at last layers are not necessary for good test accuracy. [23] assumes that current CNNs do not respect the spatial information due to the pooling operation; CNNs look for features in the image without paying attention to their pose during prediction. This limitation motivates the work of [23] where they make use of dynamic routing among capsules to encode the spatial information. Moreover, the widely used global average pooling in most recently proposed architectures [10, 17] implies that collapsing spatial information at the very end does not affect the test accuracy. On a related note, [8] indicates that models trained solely on ImageNet do not learn shape sensitive representations with constructing object-texture mismatched images, which would be expected to require global spatial information. Instead, the models are mostly sensitive to local texture features.",
            "cite_spans": [
                {
                    "start": 335,
                    "end": 338,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 851,
                    "end": 855,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1266,
                    "end": 1270,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "text": "17]",
                    "ref_id": null
                },
                {
                    "start": 1389,
                    "end": 1392,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Our work aims to push the envelope further to investigate the necessity of spatial information in the processing pipeline of CNNs. While related work has put the attention mainly on altering the input and does not differentiate between last and first layers, we are interested in taking measures that remove the spatial information at different intermediate layers to shed light on how CNNs process spatial information, evaluating its importance and providing insights for architectural design choices.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this section, we design methods to systematically study the phenomenon found in Fig. 1 that spatial information appears to be neglectable to some extent. We test how information is represented throughout the network's layers by discarding spatial or channel information in different ways in intermediate layers and applying them to well-established architectures. Experiments are conducted on object recognition and detection tasks. Section 3.1 elaborates details on our approaches, and the experimental setup is discussed in Sect. 3.2. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 83,
                    "end": 89,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Methods and Experimental Setup"
        },
        {
            "text": "We propose four different methods, namely channel-wise shuffle, spatial shuffle, patch-wise spatial shuffle, and GAP+FC, to remove either spatial or channel information from the training. Spatial information here refers to the awareness of the relative spatial position between activations on the same feature map, and channel information stands for the dependency across feature maps. The left part of Fig. 2 illustrates an example of VGG-16 with its last two layers modified by GAP+FC or any of the three shuffle methods.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 403,
                    "end": 409,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Approaches to Constrain Information"
        },
        {
            "text": "Spatial Shuffle extends the ordinary convolution operation by prepending a random spatial shuffle operation to permute the input to the convolution. As illustrated in Fig. 2 : Given an input tensor of size c \u00d7 h \u00d7 w with c being the number of feature maps for a convolutional layer, we first take one feature map from the input tensor and flatten it into a 1-d vector with h \u00d7 w elements, whose ordering is then permuted randomly. The resulting vector is finally reshaped back into h \u00d7 w and substitute the original feature map. This procedure is independently repeated c times for each feature map so that activations from the same location in the previous layer are misaligned, thereby preventing the information from being encoded by the spatial arrangement of the activations. The shuffled output becomes the input of an ordinary convolutional layer in the end. Even though shuffling itself is not differentiable, gradients can still be propagated through in the same way as pooling operations. Therefore it can be embedded into the model directly for end-to-end training. As the indices are recomputed within each forward pass, the shuffled output is also independent across training and testing steps. Images in the same batch are shuffled in the same way for the sake of simplicity since we find empirically that it does not make a difference whether the images in the same batch are shuffled in different ways.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 167,
                    "end": 173,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Approaches to Constrain Information"
        },
        {
            "text": "Patch-Wise Spatial Shuffle is a variant of spatial shuffle. In contrast, patchwise spatial shuffle does not perform on a global scale but a local scale by dividing the feature map into grids. Each patch in the grid is subsequently shuffled independently. Afterwards, an ordinary convolution is performed as usual. Note that the two operations are equivalent when the patch size is the same as the feature map size. Figure 2 demonstrates an example of patch-wise spatial shuffle with a 2 \u00d7 2 patch size, where the random permutation of pixel locations is restricted within each patch.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 415,
                    "end": 423,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Approaches to Constrain Information"
        },
        {
            "text": "Channel-Wise Shuffle is used to investigate the importance of channel information which is normally deemed as essential [28] [29] [30] . It keeps the spatial ordering of activations and randomly permutes the ordering of feature maps to prevent the model from utilizing channel information. An illustration can be seen in Fig. 2 , channel-wise shuffle is also performed independently across training and testing steps.",
            "cite_spans": [
                {
                    "start": 120,
                    "end": 124,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 125,
                    "end": 129,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 130,
                    "end": 134,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [
                {
                    "start": 321,
                    "end": 327,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Approaches to Constrain Information"
        },
        {
            "text": "Shuffle is an intuitive way of destroying spatial information. However, shuffling introduces undesirable randomness into the model; non-deterministic feature maps from an image lead to fluctuations in the model prediction, so an evaluation needs multiple forward passes to acquire an estimate of the mean of the output. A simple deterministic alternative achieving a similar goal is to deploy Global Average Pooling (GAP) after an intermediate layer, and all the subsequent ones are substituted by fully connected layers. Compared to Spatial Shuffle that introduces an extra computational burden at each forward pass, it is a much more efficient way to avoid learning spatial information at intermediate layers because it shrinks the spatial size of all subsequent feature maps to one; therefore, the number of FLOPs and parameters are also reduced.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "GAP+FC denotes Global Average Pooling and Fully Connected Layers. Spatial"
        },
        {
            "text": "This section details the experimental setup for the classification and object detection tasks. We test different architectures on three datasets: CIFAR-100, Small-ImageNet-32x32 [4] , and Pascal VOC 2007 + 2012. Small-ImageNet-32x32 is a down-sampled version of the original ImageNet (from 256 \u00d7 256 to 32 \u00d7 32). We report top-1 accuracy and mAP [6, 7] in classification and detection experiments respectively. We will take an existing architecture and apply the modification to different layers. The rest of the setup and hyper-parameters for modified architectures remain the same as the original architectures.",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 181,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 346,
                    "end": 349,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 350,
                    "end": 352,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Setup"
        },
        {
            "text": "Classification: For the VGG architecture, the modification is only performed on the convolutional layers, as illustrated in Fig. 2 . For the ResNet architecture, one bottleneck sub-module is considered as a single piece, and the modification is applied onto the 3 \u00d7 3 convolutions within the sub-module since they are the only operations with spatial extent. Features that go through the skip connection branch are also shuffled in the shuffle experiments to prevent the model from learning to ignore the information from the residual branch. The rest of the configuration remains the same (see supplemental material for an example of modified ResNet-50 architecture).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 124,
                    "end": 130,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Experimental Setup"
        },
        {
            "text": "For CIFAR-100 and Small-ImageNet-32x32 experiments, the original ResNet architecture down-samples the input image by a factor of 32 and gives 1 \u00d7 1 feature maps at last layers, therefore shuffling is noneffective. To make shuffling non-trivial, we set the first convolution in ResNet to 3 \u00d7 3 with stride 1 and the first max-pooling layer is removed so that the final feature map size is 4 \u00d7 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup"
        },
        {
            "text": "To alleviate the effect of mismatched training details, we first reproduce the reported results for all experiments and then train our modified architectures under the same training setting. All models in the same set of experiments (e.g. VGG-16 on CIFAR-100) use the same set of hyper-parameters, and they share the same initialization from the same random seed. During testing, we make sure to use a different random seed than during training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup"
        },
        {
            "text": "We use the training set and validation set of VOC 2012+2007 as the training data and report mAP on VOC 2007 test set. We shuffle the last layer in the backbone model to test the robustness of localization against the absence of spatial information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Detection:"
        },
        {
            "text": "We first compare the test accuracy of VGG-16 on CIFAR-100 with spatial or channel information missing from a different number of last layers in Sect. 4.1. An in-depth study of our main observations on CIFAR-100 and Small-ImageNet-32x32 for VGG-16 and ResNet-50 is conducted in Sect. 4.2. In Sect. 4.3, we investigate the model robustness against the loss of spatial information in various degree by controlling the amount of spatial information that passes through the network. Finally, we present the detection results on VOC datasets in Sect. 4.4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "In this section, we first investigate the invariance of pre-trained models to the absence of the spatial or channel information at test time, then we impose this invariance at training time with methods in Sect. 3.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spatial and Channel-Wise Shuffle on VGG-16"
        },
        {
            "text": "Our baseline is a VGG-16 trained on CIFAR-100 that achieves 74.10% test accuracy. We first test its robustness against the absence of the channel information at test time by substituting the last 30% convolutional layers with the channel-wise shuffle convolution. As is expected, the test accuracy drops to 1.04% (Table 1) , which is the same as the random guessing on CIFAR-100. Following the same training scheme of the baseline, we then train another VGG-16 with channel-wise shuffle added to its last 30% convolutional layers. This model can reach around 67% test accuracy no matter whether channel-wise shuffle is applied at test time. However, it still performs significantly worse than the baseline, which indicates that the expressiveness of the model is much limited without utilizing the ordering of feature maps even though the spatial information is preserved. The very slow decrease of the test accuracy of spatial shuffle implies a far less important role of spatial information for classification. The test accuracy is not much affected, given that the spatial shuffle modifies 31% of its layers. Even with 54% later layers shuffled spatially, the test accuracy only decreases by 2.66%, and the same number of the test accuracy decrease in channel-wise shuffle happens when the last layer is modified.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 313,
                    "end": 322,
                    "text": "(Table 1)",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Shuffle the Last 30% Layers Channel-Wise:"
        },
        {
            "text": "To systematically study the impact of spatial and channel information, we gradually increase the number of modified layers from the last in VGG-16 and report the corresponding test accuracy in Fig. 3 . All models are trained with the same setup, and shuffling is performed both at training and test time; the x-axis is the percentage of modified layers counting from the last layer on with 0 referring the baseline. Besides an overall decreasing trend for both shuffling with the increase of the percent of modified layers, the test accuracy of spatial shuffle drops unexpectedly slowly, e.g. merely 2.66% test accuracy drop when up to 54% of layers from the last are shuffled spatially. Likewise, when spatial information is removed from the last 77% layers, it still has a reasonable test accuracy (57.05%), whereas the test accuracy of channel-wise shuffle is only 4.84%.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 193,
                    "end": 199,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Shuffle Other Layers:"
        },
        {
            "text": "Discussion: This indicates that although a standard model makes use of both spatial dimension and channel dimension to encode information, the spatial information plays a surprisingly less pivotal role than the channel information. The model is even able to adapt to the complete absence of spatial information at later layers if spatial information is removed explicitly at training time, which strengthens the claims from [2, 23] that CNNs intrinsically possess invariance to the spatial relationship among features to some extent. Moreover, the unsuccessful adaptation to channel-wise shuffle implies that the large model capacity may mainly come from the channel order and shuffling the channel order causes unrecoverable damage to the model.",
            "cite_spans": [
                {
                    "start": 424,
                    "end": 427,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 428,
                    "end": 431,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Shuffle Other Layers:"
        },
        {
            "text": "In this section, we design more experiments to study the reliance of different layers on spatial information: we modify the last convolutional or bottleneck layers of VGG-16 or ResNet-50 by Spatial Shuffle (both at training and test time) and GAP+FC such that the spatial information is removed in different ways. Our modification on the baseline model always starts from the last layer and is consecutively extended to the first layer. The modified networks are then trained on the training set with the same setup and evaluated on the hold-out validation set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spatial Information at Later Layers is Not Necessary"
        },
        {
            "text": "Small-ImageNet VGG-16 CIFAR100 Small-ResNet-50 VGG-16 CIFAR100 Small- Fig. 4 . Classification results of GAP+FC and spatial shuffle for VGG-16 and ResNet-50 on CIFAR-100 and Small-ImageNet-32x32. The x-axis is the percent of modified layers/sub-modules counting from the last one. Models on the same dataset are trained with the same setup. It can be observed consistently across experiments that the baseline test accuracy is preserved for a long time even though spatial information is eliminated from the last several layers by spatial shuffle or GAP+FC, suggesting that spatial information at later layers is not necessary for good test accuracy. The difference between the baseline models and the models whose latter half of the layers are modified by GAP+FC or spatial shuffle is, however, still in a reasonable range between 2.48% (ResNet-50 with spatial shuffle on CIFAR-100) to 6.92% (ResNet-50 with GAP+FC on Small-ImageNet-32x32).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 70,
                    "end": 76,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "CIFAR100"
        },
        {
            "text": "Results of VGG-16 and ResNet-50 on CIFAR-100 and Small-ImageNet-32x32 are shown in Fig. 4 . The x-axis is the percent of modified later layers, and 0 is the baseline model test accuracy without modifying any layer. As we can see, Spatial Shuffle and GAP+FC have a similar overall behaviour consistently across architectures and datasets: the baseline test accuracy is retained for a long time before it starts to decrease with the increase of the percent of modified layers. When the last 30% layers are modified by GAP+FC or spatial shuffle, there is no or little test accuracy decrease across experiments (0.17% for ResNet-50 on CIFAR-100 and 1.44% for VGG-16 on Small-ImageNet with spatial shuffle). And the test accuracy decrease is still in a reasonable range (2.48% with spatial shuffle on CIFAR-100 and 6.92% for GAP+FC on Small-ImageNet-32x32 for ResNet-50), even with around half of the last layers modified. At 77% to 81% of the modified later layers, the test accuracy just starts to show a significant difference to the baseline in the range of 8.58% (ResNet-50 with spatial shuffle on CIFAR-100) to 20.21% (VGG-16 with GAP+FC on Smalll-ImageNet-32x32).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 83,
                    "end": 89,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Results on CIFAR-100 and Small-ImageNet-32x32:"
        },
        {
            "text": "Our experiments here clearly show that spatial information can be neglected from a significant number of later layers with no or small test accuracy drop if the invariance is imposed at training, which suggests that spatial information at last layers is not necessary for good test accuracy. We should, however, notice that it does not indicate that models whose prediction is based on spatial information can not generalize well. Besides, unlike the common design manner that layers at different depth inside the network are normally treated equally, e.g. the same module is always used throughout the architecture [12, 14, 24] , our observation implies it is beneficial to have different designs for different layers since there is no necessity to encode spatial information in the later layers. As a side effect, GAP+FC can reduce the number of model parameters with little test accuracy drop. For example, GAP+FC achieves nearly identical results (46.05%) to the VGG-16 baseline (46.59%), while reducing the number of parameters from 37.70M to 29.31M on Small-ImageNet-32x32.",
            "cite_spans": [
                {
                    "start": 616,
                    "end": 620,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 621,
                    "end": 624,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 625,
                    "end": 628,
                    "text": "24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Results on CIFAR-100 and Small-ImageNet-32x32:"
        },
        {
            "text": "In this section, we study the relation between the model test accuracy and the amount of spatial information that propagates throughout a network. The latter is controlled by patch-wise spatial shuffle with different patch sizes. The larger the patch size is, the less the preserved spatial information. Patch-wise spatial shuffle reduces to spatial shuffle when the patch size is the same as the feature map size, in which case no spatial information remains. Our experiments are conducted on CIFAR-100 for VGG-16 and ResNet-50, and we only shuffle a single layer at a time since the model is not able to recover the \"damage\" caused by shuffling an early layer (see more in the supplemental material).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Patch-Wise Spatial Shuffle"
        },
        {
            "text": "The result of patch-wise spatial shuffling of different patch sizes is shown in Fig. 5 . We can see that the patch size does not make much difference in terms of the test accuracy at later layers, e.g. results of patch size 2, 4 and 8 for ResNet-50 at 8-14 layers are similar. However, the test accuracy has a rapid decrease with the increase of the patch size at first layers, indicating a relatively important role of spatial information at first layers. Nevertheless, this role might not be as much important as what is commonly believed, as the ResNet-50 still has 40.76% test accuracy when the input image is completely shuffled.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 80,
                    "end": 86,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Patch-Wise Spatial Shuffle"
        },
        {
            "text": "Object detection should intuitively suffer more from spatial shuffling than classification since the spatial information should help to localize objects. In this section, we show some initial results on Pascal VOC [6, 7] .",
            "cite_spans": [
                {
                    "start": 214,
                    "end": 217,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 218,
                    "end": 220,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Detection Results on VOC Datasets"
        },
        {
            "text": "We design an analogue to YOLO [22] as our detection model. The architecture consists of a backbone and a detection head; the backbone is a ResNet-50 without the classifier, and the detection head has three bottlenecks and a 3 \u00d7 3 convolutional layer whose outputs is in the same format as [22] . Different to [22] , we deploy a 3 \u00d7 3 convolution instead of a fully connected layer in the end to output the final detection results. The latter gives the model potential access to the object feature, which may be exploited by the model to predict its location. In order to prevent the undesirable shortcut, we use a 3 \u00d7 3 convolution so that the prediction of a bounding box at a certain location does not depend on all activation on the feature map.",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 34,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 289,
                    "end": 293,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 309,
                    "end": 313,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Detection Results on VOC Datasets"
        },
        {
            "text": "By using a pre-trained ResNet-50 on ImageNet, we can reach 66% mAP on VOC2007 test set after fine-tuning, which is the same as the number in [22] . To avoid pretraining a spatially shuffled model on ImageNet, we compare a spatially shuffled model and a non spatially shuffled model, both trained from Only a single layer is shuffled at a time. Layer index 13 and 16 stand for the last layer of VGG-16 and ResNet-50, respectively. With the increase of the patch size, the test accuracy decreases faster at first layers than that at last layers. It is interesting to see that both models' test accuracy do not fall into the random guess (16.02% for VGG-16 and 40.76% for ResNet-50) at layer index one and patch size 32, where the input image is completely shuffled. scratch on VOC. Our models are trained for 500 epochs with exponentially decaying learning rate starting from 0.001. Our baseline model achieves 50% mAP on VOC2007 test set without using an ImageNet pre-trained backbone. The result of the shuffled model, where we apply random shuffle to the last layer of the backbone, is 34%. While this sounds like a large drop, it turns out that the classification performance is essentially preserved and only the localization performance is suffering. To analyze this effect in detail, we use the method and tools proposed in [11] . The diagnosis tool classifies each prediction from the model as either correct prediction or a type of error based on its class label and IoU with the ground truth. More details can be found in [11] .",
            "cite_spans": [
                {
                    "start": 141,
                    "end": 145,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1329,
                    "end": 1333,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1530,
                    "end": 1534,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Detection Results on VOC Datasets"
        },
        {
            "text": "The results in Fig. 6 right show that the misclassification to the wrong class and background are of similar percents for both models, and the localization error doubles for the shuffled model (an increase from 14.2% to 28.4%). Though random shuffling indeed affects the model's localization ability, it is unexpected that the effect is not fatal. Because random shuffling switches features, it is highly likely the model trained with spatial shuffle has to predict the correct bounding box for one object based on some other features. We should also notice that a prediction is counted as a localization error if it has the correct class label and the IoU to the ground truth is less than 0.5. Therefore, classification-wise speaking, the shuffled model got 73.7% (45.3% + 28.4%) of its predictions correct, which is at the same level as the baseline (73.3% = 59.1% + 14.2%).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 15,
                    "end": 21,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Detection Results on VOC Datasets"
        },
        {
            "text": "Qualitative Results: Figure 6 left shows some qualitative results from both models. Those examples are the first 11 images in the VOC2007 test set. We can see that the localization error actually mainly comes from small objects for which the shuffled model tends to predict several bounding boxes on one object, and the bounding box of the relatively big object is not really off, e.g. the shuffled model managed to localize the dining table in the middle right image and the horse in the middle left image while the baseline can not.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 21,
                    "end": 29,
                    "text": "Figure 6",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Detection Results on VOC Datasets"
        },
        {
            "text": "To conclude, we empirically show that a significant number of later layers of CNNs are robust to the absence of spatial information, which is commonly assumed to be important for object recognition tasks. Modern CNNs can tolerate the loss of spatial information from the last 30% of layers at around 1% accuracy drop; and the test accuracy only decreases by less than 7%, when spatial information is removed from the last half of layers on CIFAR-100 and Small-ImageNet-32x32. Though the depth of the network is essential for good test accuracy, later layers do not require spatial integration.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Inside-outside net: detecting objects in context with skip pooling and recurrent neural networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bell",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lawrence Zitnick",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Bala",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2874--2883",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Approximating CNNs with bag-of-local-features models works surprisingly well on imageNet",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Brendel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bethge",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A\u02c62-nets: double attention networks",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kalantidis",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "352--361",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Chrabaszcz",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Loshchilov",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1707.08819"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Deformable convolutional networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "764--773",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "The PASCAL Visual Object Classes Challenge",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Everingham",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "K I"
                    ],
                    "last": "Williams",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Winn",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "The PASCAL Visual Object Classes Challenge 2012 (VOC 2012) Results",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Everingham",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "K I"
                    ],
                    "last": "Williams",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Winn",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Geirhos",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Rubisch",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Michaelis",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bethge",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "A"
                    ],
                    "last": "Wichmann",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Brendel",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Donahue",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "580--587",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Diagnosing error in object detectors",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hoiem",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chodpathumwan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fitzgibbon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lazebnik",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Perona",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sato",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ECCV 2012",
            "volume": "7574",
            "issn": "",
            "pages": "340--353",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-33712-3_25"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "MobileNets: efficient convolutional neural networks for mobile vision applications",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Howard",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1704.04861"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Squeeze-and-excitation networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "7132--7141",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1mb model size",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "N"
                    ],
                    "last": "Iandola",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Moskewicz",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ashraf",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "J"
                    ],
                    "last": "Dally",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Keutzer",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Advances in Neural Information Processing Systems",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jaderberg",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2017--2025",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Imagenet classification with deep convolutional neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 25th International Conference on Neural Information Processing Systems",
            "volume": "1",
            "issn": "",
            "pages": "1097--1105",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Fully convolutional networks for semantic segmentation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Shelhamer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3431--3440",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Recurrent models of visual attention",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Mnih",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Heess",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Graves",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2204--2212",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Scaling the scattering transform: deep hybrid networks",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Oyallon",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Belilovsky",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zagoruyko",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "5618--5627",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Stand-alone self-attention in vision models",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ramachandran",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Bello",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Levskaya",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shlens",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.05909"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "You only look once: unified, real-time object detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Divvala",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "779--788",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Dynamic routing between capsules",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sabour",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Frosst",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Guyon",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [
                        "V"
                    ],
                    "last": "Luxburg",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wallach",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vishwanathan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "30",
            "issn": "",
            "pages": "3856--3866",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sandler",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zhmoginov",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "C"
                    ],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "4510--4520",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Very deep convolutional networks for large-scale image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1409.1556"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Going deeper with convolutions",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Non-local neural networks",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "7794--7803",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Aggregated residual transformations for deep neural networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1492--1500",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Interleaved group convolutions",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "J"
                    ],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "4373--4382",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "ShuffleNet: an extremely efficient convolutional neural network for mobile devices",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "6848--6856",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Pyramid scene parsing network",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2881--2890",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Deformable convnets v2: more deformable, better results",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "9308--9316",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An example of VGG-16 modified by our methods. The leftmost architecture shows the modification (in red) from GAP+FC, where the last two convolutional layers are replaced by fully-connected layers after a GAP layer. The middle architecture shows the modification (in red) from shuffle conv, where the last two convolutional layers are replaced by one of the shuffling methods and an ordinary convolution. Spatial shuffle randomly and independently permutes pixels on each feature map at a global scale in the sense that a pixel can end up anywhere on the feature map. Patch-wise shuffle first divides the feature map into grids; then it randomly permutes the pixel locations within each grid independently. Channel shuffle randomly permutes the order of feature maps, leaving the spatial ordering unchanged.(Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Classification accuracy of VGG-16 on CIFAR-100 with different shuffle schemes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The result of patch-wise spatial shuffling of VGG-16 and ResNet-50 on CIFAR-100.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Left: Qualitative detection results on the VOC 2007 test set. Examples are the first 11 images in the test set. The left result is from the baseline, and the right result is from the shuffled model. Right: Detection error analysis of our baseline and the shuffled model shows a doubled localization error in the shuffled model and the rest types of error are in the same level as the baseline.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Top-1 accuracy of VGG-16 on CIFAR-100 with spatial/channel-wise shuffle enabled at either training or test time for the last 30% layers. A model from standard training does not possess robustness against spatial shuffle (23.49%) and channel-wise shuffle (1.04%). However, when imposed in training, the model achieves 74.07% test accuracy for spatial shuffle and 67.56% for channel-wise shuffle, showing impressive robustness to the loss of spatial information.Shuffle the Last 30% Layers Spatially: As a comparison to channel shuffle, we repeat the same experiment on spatial shuffle, and the result is presented in the second half ofTable 1. No shuffle \u2192 spatial shuffle of the pre-trained VGG-16 gives 23.49% test accuracy, which is similar to the test accuracy of a one-hidden-",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
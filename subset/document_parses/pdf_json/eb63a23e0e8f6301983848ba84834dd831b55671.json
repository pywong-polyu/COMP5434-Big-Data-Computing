{
    "paper_id": "eb63a23e0e8f6301983848ba84834dd831b55671",
    "metadata": {
        "title": "Neural Text Classification and Stacked Heterogeneous Embeddings for Named Entity Recognition in SMM4H 2021",
        "authors": [
            {
                "first": "Usama",
                "middle": [],
                "last": "Yaseen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Siemens AG Munich",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": "usama.yaseen@siemens.com"
            },
            {
                "first": "Stefan",
                "middle": [],
                "last": "Langer",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Siemens AG Munich",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": "langer.stefan@siemens.com"
            }
        ]
    },
    "abstract": [
        {
            "text": "This paper presents our findings from participating in the SMM4H Shared Task 2021. We addressed Named Entity Recognition (NER) and Text Classification. To address NER we explored BiLSTM-CRF with Stacked Heterogeneous Embeddings and linguistic features. We investigated various machine learning algorithms (logistic regression, Support Vector Machine (SVM) and Neural Networks) to address text classification. Our proposed approaches can be generalized to different languages and we have shown its effectiveness for English and Spanish. Our text classification submissions (team:MIC-NLP) have achieved competitive performance with F1-score of 0.46 and 0.90 on ADE Classification (Task 1a) and Profession Classification (Task 7a) respectively. In the case of NER, our submissions scored F1score of 0.50 and 0.82 on ADE Span Detection (Task 1b) and Profession Span detection (Task 7b) respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The ubiquity of social media has led to massive user-generated content across various platforms. Twitter is a popular micro-blogging platform that allows its users to publish tweets up to 280 characters. The common public uses Twitter to share life-related personal and professional experiences with others. Personal experiences often involve health-related incidents including mentions of adverse drug effect (ADE); this information is crucial to study Pharmacovigilance. In the context of the COVID-19 pandemic, the professional experiences may include information about professions and occupations which are vulnerable due to either direct exposure to the virus or due to the associated mental health issues; detecting vulnerable occupations is critical to adopt necessary preventive measures.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recent research focuses on mining Twitter data for adverse drug effect detection (Jiang and Zheng, 2013; Adrover et al., 2015; Onishi et al., 2018) .",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 104,
                    "text": "(Jiang and Zheng, 2013;",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 105,
                    "end": 126,
                    "text": "Adrover et al., 2015;",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 127,
                    "end": 147,
                    "text": "Onishi et al., 2018)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The distinctive style of communication on Twitter presents unique challenges including informal (brief) text, misspellings, noisy text, abbreviations, data sparsity, colloquial expressions and multilinguality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We participate in the following two tasks organized by SMM4H workshop 2021 (Magge et al., 2021) :",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 95,
                    "text": "(Magge et al., 2021)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Task Description and Contribution"
        },
        {
            "text": "(1) Task 1: Classification, Extraction and Normalization of Adverse Effect mentions in English tweets (2) Task 7: Identification of professions and occupations in Spanish tweets . Task 1 consists of three sub-tasks, (a): ADE tweet classification, (b): ADE span detection, (c): ADE resolution; whereas Task 7 consists of two sub-tasks: (a): Tweet classification (b): Profession/occupation span detection. For both tasks, we participate in sub-tasks (a) and (b). The Task 1a and Task 7a is a text classification problem while Task 1b and Task 7b is a Named Entity Recognition problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task Description and Contribution"
        },
        {
            "text": "Following are our multi-fold contributions: 1. To address NER tasks, we have employed a neural network based sequence classifier, i.e. BiLSTM-CRF and investigated various heterogeneous embeddings. We further investigated the combination of character embeddings, static word embeddings and contextualized embeddings in a stacked format. We also incorporated linguistic features such as part-of-speech tags (POS), orthographic features etc. We apply the proposed modelling approaches to both English and Spanish texts. In Profession span detection (Task 7b) our submission (team:MIC-NLP) achieved the F1-score of 0.824 which is 6 points higher than the arithmetic median of all the submissions; in case of ADE span detection our submission scored F1-score of 0.50, around 8 points higher than the arithmetic median of the participating submissions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task Description and Contribution"
        },
        {
            "text": "2. To address text classification tasks, we investi- gated various machine learning algorithms like logistic regression, SVM and neural network with various word and sentence embeddings. In ADE tweet classification (Task 1a) our submission (team:MIC-NLP) scored F1-score of 0.46, approximately 2 points higher than the arithmetic median of participating submissions; in case of tweet classification (task 7a) our system achieved the F1-score of 0.90 which is 5 points higher than the arithmetic median of all submissions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task Description and Contribution"
        },
        {
            "text": "In the following sections we discuss our proposed model for named entity recognition and text classification. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "We explored traditional machine learning algorithms like logistic regression, SVM and neural network based architecture with various word and sen- tence embeddings for text classification. The SVM was trained with Radial Basis Function (RBF) Kernel with the value of penalty parameter C determined by grid search for each dataset. Our best model was a Neural Network with contextualized embeddings (Devlin et al., 2019; Liu et al., 2019) . Since both datasets (Task 1a and Task 7a) were highly imbalanced, we employed higher class weights for minority classes to train the final models.",
            "cite_spans": [
                {
                    "start": 398,
                    "end": 419,
                    "text": "(Devlin et al., 2019;",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 420,
                    "end": 437,
                    "text": "Liu et al., 2019)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Text Classification"
        },
        {
            "text": "Bagging is a useful technique to reduce the variance of the learning algorithm without impacting bias. We employed a variant of Bagging (Breiman, 1996) such that every data point in the training set is part of the development set at least once and vice versa. We created three data folds and trained the model using optimal configuration on each fold, inference on the test set involves majority voting among the three trained models. For NER, we perform majority voting at the token level for each test data point. In cases when voting results in a tie, we take the prediction of the confident model, we treat the model trained on original data split as the confident model. In the case of an ensemble for text classification, we followed the straight forward approach of majority voting at sentence level for each test data point.",
            "cite_spans": [
                {
                    "start": 136,
                    "end": 151,
                    "text": "(Breiman, 1996)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Ensemble Strategy"
        },
        {
            "text": "Data: We employed bagging (discussed in section 3.3) to split the annotated corpus into 3-folds. For ADE span detection (Task 1b) and Profession span detection (Task 7b) we perform sentence splitting, word tokenization, computing orthographic features and POS tagging. We do not perform any pre-processing for ADE classification (Task 1a) and Tweet classification (Task 7a).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset and Experimental Setup"
        },
        {
            "text": "ADE Classification (Task 1a): The dataset consists of tweets in the English language and the task is to detect tweets containing adverse drug effect. The dataset contains two classes, ADE and NoADE. The dataset is highly imbalanced with only 1235 tweets of type ADE out of total 17385 tweets in the train set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset and Experimental Setup"
        },
        {
            "text": "ADE Span Detection (Task 1b): The dataset consists of only one entity type ADE. The train set contains 1717 entity mentions of ADE (see Table   Features Task 1b ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 136,
                    "end": 152,
                    "text": "Table   Features",
                    "ref_id": null
                }
            ],
            "section": "Dataset and Experimental Setup"
        },
        {
            "text": "Profession Classification (Task 7a): The dataset consists of tweets in the Spanish language and the task is to detect tweets containing mention of profession/occupation. The dataset contains two classes. The dataset is highly imbalanced with only 1393 tweets containing a positive mention out of 6000 tweets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "1)."
        },
        {
            "text": "Profession Span Detection (Task 7b): The dataset consists of four entity types with few mentions of type FIGURATIVA as shown in Table 1 . Entities of type ACTIVIDAD and FIGURATIVA are ignored in the evaluation of this shared task but we still treat them as regular entities.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 128,
                    "end": 135,
                    "text": "Table 1",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "1)."
        },
        {
            "text": "Experimental Setup: We found contextualized embeddings to be very helpful in identifying entities and text classification; all our experiments used pre-trained contextualized embeddings. We employ RoBERTa (Gururangan et al., 2020) for Task 1a and Task 1b; we use multi-lingual BERT (Devlin et al., 2019) for Task 7a and Spanish BERT (Ca\u00f1ete et al., 2020) for Task 7b. We do not finetune embeddings in our experiments. We don't employ any strategy for handling imbalanced classes for NER but have used class weighting by a factor of 10 for all positive classes for text classification. Table 2 lists the best configuration of hyperparameters for all the tasks.",
            "cite_spans": [
                {
                    "start": 282,
                    "end": 303,
                    "text": "(Devlin et al., 2019)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 333,
                    "end": 354,
                    "text": "(Ca\u00f1ete et al., 2020)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [
                {
                    "start": 585,
                    "end": 592,
                    "text": "Table 2",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "1)."
        },
        {
            "text": "We perform various experiments to investigate the impact of features on performance on the development set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results on Development Set"
        },
        {
            "text": "NER: Table 3 shows the score on the development set for Task 1b and Task 7b. Observe that fastText embeddings (row r2) outperform glove embeddings (row r1) for Task 1b. Subsequently, fastText embeddings with BytePair embeddings (row r4) provide an improvement over only fast- Text (row r2) and the combination of fastText with Character embeddings (row r3). The contextualized embeddings (row r5) provide an improvement over the combination of fastText with BytePair embeddings. In row r6, we employ BERT, fastText and BytePair embeddings in a stacked format leading to the best f1-score for both Task 1b and Task 7b.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 5,
                    "end": 12,
                    "text": "Table 3",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Results on Development Set"
        },
        {
            "text": "Text Classification: Table 4 shows the score on the development set for Task 1a and Task 7a.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 21,
                    "end": 28,
                    "text": "Table 4",
                    "ref_id": "TABREF8"
                }
            ],
            "section": "Results on Development Set"
        },
        {
            "text": "Observe that BERTSentEmb provides improvement over fastTextSentEmb for both logistic regression and SVM. Similarly, BERTWordEmb-Sum further improves BERTSentEmb. BERTSen-tEmb uses BERT's CLS representation whereas BERTWordEmbSum is computed by average of the token-wise embeddings of pre-trained BERT as discussed in Rogers et al.. Neural Network with BERT achieves the best result for both datasets. Table 5 shows the comparison of our submissions with the arithmetic median of the participating teams for all the tasks. Our submissions achieve the overall best F1-score than the arithmetic median for all the tasks showing compelling advantage. For Task 1a, the precision of our system is lower than the arithmetic median but this is compensated by the improvement in recall. For all the tasks, the precision is higher than the recall but overall precision and recall are balanced.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 401,
                    "end": 408,
                    "text": "Table 5",
                    "ref_id": "TABREF10"
                }
            ],
            "section": "Results on Development Set"
        },
        {
            "text": "In this paper, we described our system with which we participate in Task 1(Adverse Drug Effect Classification and Extraction) and Task 7 (Identification of professions and occupations in Spanish Tweets) in the SMM4H Shared Task 2021. Our NER system employed stacked heterogeneous em- beddings to extract entities in English and Spanish text. Our NER system demonstrates a competitive performance with F1-score of 0.50 and 0.82 on ADE Span Detection (Task 1b) and Profession/Occupation span detection (Task 7b) respectively. Our text classification system employed contextualized embeddings with Neural Network as a classifier to achieve a competitive performance with F1-score of 0.46 and 0.90 on ADE Classification (Task 1a) and Profession/Occupation classification (Task 7a) respectively. In future, we would like to improve error analysis to further enhance our NER and text classification models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Identifying adverse effects of hiv drug treatment and associated sentiments using twitter",
            "authors": [
                {
                    "first": "Cosme",
                    "middle": [],
                    "last": "Adrover",
                    "suffix": ""
                },
                {
                    "first": "Todd",
                    "middle": [
                        "J"
                    ],
                    "last": "Bodnar",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Telenti",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Salath\u00e9",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "1",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Enriching word vectors with subword information",
            "authors": [
                {
                    "first": "Piotr",
                    "middle": [],
                    "last": "Bojanowski",
                    "suffix": ""
                },
                {
                    "first": "Edouard",
                    "middle": [],
                    "last": "Grave",
                    "suffix": ""
                },
                {
                    "first": "Armand",
                    "middle": [],
                    "last": "Joulin",
                    "suffix": ""
                },
                {
                    "first": "Tom\u00e1s",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "5",
            "issn": "",
            "pages": "135--146",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Bagging predictors",
            "authors": [
                {
                    "first": "Leo",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Machine Learning",
            "volume": "24",
            "issn": "",
            "pages": "123--140",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Spanish pre-trained bert model and evaluation data",
            "authors": [
                {
                    "first": "Jos\u00e9",
                    "middle": [],
                    "last": "Ca\u00f1ete",
                    "suffix": ""
                },
                {
                    "first": "Gabriel",
                    "middle": [],
                    "last": "Chaperon",
                    "suffix": ""
                },
                {
                    "first": "Rodrigo",
                    "middle": [],
                    "last": "Fuentes",
                    "suffix": ""
                },
                {
                    "first": "Jou-Hui",
                    "middle": [],
                    "last": "Ho",
                    "suffix": ""
                },
                {
                    "first": "Hojin",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "Jorge",
                    "middle": [],
                    "last": "P\u00e9rez",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "1",
            "issn": "",
            "pages": "4171--4186",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/N19-1423"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Don't stop pretraining: Adapt language models to domains and tasks",
            "authors": [
                {
                    "first": "Ana",
                    "middle": [],
                    "last": "Suchin Gururangan",
                    "suffix": ""
                },
                {
                    "first": "Swabha",
                    "middle": [],
                    "last": "Marasovic",
                    "suffix": ""
                },
                {
                    "first": "Kyle",
                    "middle": [],
                    "last": "Swayamdipta",
                    "suffix": ""
                },
                {
                    "first": "Iz",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                },
                {
                    "first": "Doug",
                    "middle": [],
                    "last": "Beltagy",
                    "suffix": ""
                },
                {
                    "first": "Noah",
                    "middle": [
                        "A"
                    ],
                    "last": "Downey",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Smith",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "volume": "2020",
            "issn": "",
            "pages": "8342--8360",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages",
            "authors": [
                {
                    "first": "Benjamin",
                    "middle": [],
                    "last": "Heinzerling",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Strube",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Mining twitter data for potential drug effects",
            "authors": [
                {
                    "first": "Keyuan",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Yujing",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "ADMA",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Neural architectures for named entity recognition",
            "authors": [
                {
                    "first": "Guillaume",
                    "middle": [],
                    "last": "Lample",
                    "suffix": ""
                },
                {
                    "first": "Miguel",
                    "middle": [],
                    "last": "Ballesteros",
                    "suffix": ""
                },
                {
                    "first": "Sandeep",
                    "middle": [],
                    "last": "Subramanian",
                    "suffix": ""
                },
                {
                    "first": "Kazuya",
                    "middle": [],
                    "last": "Kawakami",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Dyer",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "",
            "issn": "",
            "pages": "260--270",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "authors": [
                {
                    "first": "Yinhan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Myle",
                    "middle": [],
                    "last": "Ott",
                    "suffix": ""
                },
                {
                    "first": "Naman",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "Jingfei",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "Mandar",
                    "middle": [],
                    "last": "Joshi",
                    "suffix": ""
                },
                {
                    "first": "Danqi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Omer",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "Mike",
                    "middle": [],
                    "last": "Lewis",
                    "suffix": ""
                },
                {
                    "first": "Luke",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                },
                {
                    "first": "Veselin",
                    "middle": [],
                    "last": "Stoyanov",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Overview of the sixth social media mining for health applications (# smm4h) shared tasks at naacl 2021",
            "authors": [
                {
                    "first": "Arjun",
                    "middle": [],
                    "last": "Magge",
                    "suffix": ""
                },
                {
                    "first": "Ari",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "Ivan",
                    "middle": [],
                    "last": "Flores",
                    "suffix": ""
                },
                {
                    "first": "Ilseyar",
                    "middle": [],
                    "last": "Alimova",
                    "suffix": ""
                },
                {
                    "first": "Mohammed",
                    "middle": [
                        "Ali"
                    ],
                    "last": "Al-Garadi",
                    "suffix": ""
                },
                {
                    "first": "Antonio",
                    "middle": [],
                    "last": "Miranda-Escalada",
                    "suffix": ""
                },
                {
                    "first": "Zulfat",
                    "middle": [],
                    "last": "Miftahutdinov",
                    "suffix": ""
                },
                {
                    "first": "Eul\u00e0lia",
                    "middle": [],
                    "last": "Farr\u00e9-Maduell",
                    "suffix": ""
                },
                {
                    "first": "Salvador",
                    "middle": [
                        "Lima"
                    ],
                    "last": "L\u00f3pez",
                    "suffix": ""
                },
                {
                    "first": "Juan",
                    "middle": [
                        "M"
                    ],
                    "last": "Banda",
                    "suffix": ""
                },
                {
                    "first": "Karen",
                    "middle": [
                        "O"
                    ],
                    "last": "Connor",
                    "suffix": ""
                },
                {
                    "first": "Abeed",
                    "middle": [],
                    "last": "Sarker",
                    "suffix": ""
                },
                {
                    "first": "Elena",
                    "middle": [],
                    "last": "Tutubalina",
                    "suffix": ""
                },
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Krallinger",
                    "suffix": ""
                },
                {
                    "first": "Davy",
                    "middle": [],
                    "last": "Weissenbacher",
                    "suffix": ""
                },
                {
                    "first": "Graciela",
                    "middle": [],
                    "last": "Gonzalez-Hernandez",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the Sixth Social Media Mining for Health Applications Workshop & Shared Task",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "The profner shared task on automatic recognition of professions and occupation mentions in social media: systems, evaluation, guidelines, embeddings and corpora",
            "authors": [
                {
                    "first": "Antonio",
                    "middle": [],
                    "last": "Miranda-Escalada",
                    "suffix": ""
                },
                {
                    "first": "Eul\u00e0lia",
                    "middle": [],
                    "last": "Farr\u00e9-Maduell",
                    "suffix": ""
                },
                {
                    "first": "Salvador",
                    "middle": [
                        "Lima"
                    ],
                    "last": "L\u00f3pez",
                    "suffix": ""
                },
                {
                    "first": "Vicent",
                    "middle": [],
                    "last": "Briva-Iglesias",
                    "suffix": ""
                },
                {
                    "first": "Marvin",
                    "middle": [],
                    "last": "Ag\u00fcero-Torales",
                    "suffix": ""
                },
                {
                    "first": "Luis",
                    "middle": [],
                    "last": "Gasc\u00f3-S\u00e1nchez",
                    "suffix": ""
                },
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Krallinger",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the Sixth Social Media Mining for Health Applications Workshop & Shared Task",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Dealing with medication non-adherence expressions in Twitter",
            "authors": [
                {
                    "first": "Takeshi",
                    "middle": [],
                    "last": "Onishi",
                    "suffix": ""
                },
                {
                    "first": "Davy",
                    "middle": [],
                    "last": "Weissenbacher",
                    "suffix": ""
                },
                {
                    "first": "Ari",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "O&apos;",
                    "middle": [],
                    "last": "Karen",
                    "suffix": ""
                },
                {
                    "first": "Graciela",
                    "middle": [],
                    "last": "Connor",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gonzalez-Hernandez",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop & Shared Task",
            "volume": "",
            "issn": "",
            "pages": "32--33",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "A primer in bertology: What we know about how BERT works",
            "authors": [
                {
                    "first": "Anna",
                    "middle": [],
                    "last": "Rogers",
                    "suffix": ""
                },
                {
                    "first": "Olga",
                    "middle": [],
                    "last": "Kovaleva",
                    "suffix": ""
                },
                {
                    "first": "Anna",
                    "middle": [],
                    "last": "Rumshisky",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Trans. Assoc. Comput. Linguistics",
            "volume": "8",
            "issn": "",
            "pages": "842--866",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "describes the architecture of our model, where we design a sequence tagger to extract entities. The architecture of our model is a standard BiLSTM-CRF(Lample et al., 2016) model with stacked heterogeneous embeddings and linguistic features as input. The stacked embeddings consists of Byte-Pair subword embeddings(Heinzerling and Strube, 2018), fastText subword embeddings(Bojanowski et al., 2017) and contextualized word embeddings(Devlin et al., 2019;Liu et al., 2019). The linguistic features include POS, capitalization features and orthographic features.",
            "latex": null,
            "type": "figure"
        },
        "TABREF2": {
            "text": "Dataset statistics for NER.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Hyper parameter settings for NER and Text classification.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "BERT + fastText + BytePair .77/.37/.50 .84/.78/.81",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Scores on dev set using different features for BiLSTM-CRF on Task 1b and Task 7b.",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "Scores on dev set using different features on Task 1a and Task 7a.",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "Tasks Arithmetic Median MIC-NLP P/R/F1 P/R/F1 r1 Task 1a .50/.40/.44 .47/.45/.46 r2 Task 1b .49/.45/.42 .55/.45/.50 r3 Task 7a .91/.85/.85 .94/.85/.90 r4 Task 7b .84/.72/.76 .85/.79/.82",
            "latex": null,
            "type": "table"
        },
        "TABREF10": {
            "text": "Comparison of our system (team:MIC-NLP) with the arithmetic median of the participating teams. Scores on test set for Task 1a, Task 1b, Task 7a and Task 7b.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This research was supported by Bundesministerium f\u00fcr Wirtschaft und Energie (bmwi.de), grant 01MD19003E (PLASS, plass.io) at Technology -Siemens AG, Munich Germany.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgment"
        }
    ]
}
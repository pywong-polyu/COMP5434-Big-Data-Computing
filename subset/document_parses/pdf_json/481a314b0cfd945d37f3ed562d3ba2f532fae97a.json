{
    "paper_id": "481a314b0cfd945d37f3ed562d3ba2f532fae97a",
    "metadata": {
        "title": "TA-BiLSTM: An Interpretable Topic-Aware Model for Misleading Information Detection in Mobile Social Networks",
        "authors": [
            {
                "first": "Shuyu",
                "middle": [],
                "last": "Chang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Rui",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Haiping",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "\u00b7",
                "middle": [
                    "Jian"
                ],
                "last": "Luo",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "As essential information acquisition tools in our lives, mobile social networks have brought us great convenience for communication. However, misleading information such as spam emails, clickbait links, and false health information appears everywhere in mobile social networks. Prior studies have adopted various approaches to detecting this information but ignored global semantic features of the corpus and lacked interpretability. In this paper, we propose a novel end-to-end model called Topic-Aware BiLSTM (TA-BiLSTM) to handle the problems above. We firstly design a neural topic model for mining global semantic patterns, which encodes word relatedness into topic embeddings. Simultaneously, a detection model extracts local hidden states from text content with LSTM layers. Then, the model fuses those global and local representations with the Topic-Aware attention mechanism and performs misleading information detection. Experiments on three real datasets prove that the TA-BiLSTM could generate more coherent topics and improve the detecting performance jointly. Furthermore, case study and visualization demonstrate that the proposed TA-BiLSTM could discover latent topics and help in enhancing interpretability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Mobile social networks have brought us great facilities for acquiring information. Inevitably, a vast amount of useless misleading information, such as spam emails, clickbait links, and false health information, is created. S. Chang This information will deceive us to do things with ill consequences. Table 1 gives two examples of how the meanings of content mislead people and impact categories in the Webis-Clickbait-17 dataset. In general, misleading information is deceptive, which makes it hard to distinguish the difference between two kinds of posts (positive and negative). Thus, how to detect misleading information effectively is challenging. Also, developing an efficient approach with high performance for misleading information detection is particularly essential.",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 232,
                    "text": "Chang",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 302,
                    "end": 309,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "Existing work on misleading information detection could be categorized into two types: machine learningbased approaches and deep learning-based approaches. Approaches based on machine learning often build document representations depending on different feature engineering techniques [10, 26, 35] . Various algorithms such as Labeled-LDA [35] and GBDT [2] also help enhance detection accuracy. Unfortunately, these approaches heavily rely on people to design sophisticated features and will cause lousy performance in a complex context. Deep learning-based approaches extract semantic features from content by multiple non-linear units to solve the above problems. Convolutional neural networks [1, 17] , recurrent neural networks [23] , and a combination of the two [22] are commonly used frameworks. Still, these approaches are limited to local semantic information and severely lack interpretability due to the complex structures. To address the above limitations, we propose a novel model called Topic-Aware BiLSTM (TA-BiLSTM) to add corpus-level topic relatedness and enhance interpretability. Specifically, the TA-BiLSTM is decomposed into two parts: a neural topic model module and a text classification module. Assuming that a multi-layer neural network can approximate the document's topic distribution, we model the topic by Wasserstein autoencoder (WAE) [37] . Neural topic model module constructs the topic distribution on latent space and reconstructs the document representation. The topic distribution could be transformed into the topic embedding provided for the attention mechanism concurrently. Unlike variational autoencoder-based approaches previously [29, 36] , our model minimizes the Maximum Mean Discrepancy regularizer [15] based on Optimal Transport theory [39] to reduce Wasserstein distance between the topic distribution and Dirichlet prior.",
            "cite_spans": [
                {
                    "start": 284,
                    "end": 288,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 289,
                    "end": 292,
                    "text": "26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 293,
                    "end": 296,
                    "text": "35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 338,
                    "end": 342,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 352,
                    "end": 355,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 695,
                    "end": 698,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 699,
                    "end": 702,
                    "text": "17]",
                    "ref_id": null
                },
                {
                    "start": 731,
                    "end": 735,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 767,
                    "end": 771,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 1673,
                    "end": 1677,
                    "text": "[29,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1678,
                    "end": 1681,
                    "text": "36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1745,
                    "end": 1749,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "text": "[39]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Furthermore, the text classification module utilizes a two-layer bidirectional LSTM based on the Topic-Aware attention mechanism to extract semantic features. This attention mechanism incorporates topic relatedness information while calculating the representation. Finally, we input representations to the classifier for misleading information detection. To balance the two task learning, we leverage a dynamic strategy to control the importance of these objectives. We concentrate on the neural topic model preferentially, then simultaneously train the classification objective and topic modeling objective.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The main contributions of our work are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We propose a novel end-to-end framework Topic-Aware BiLSTM for misleading information detection. \u2022 We introduce a new Topic-Aware attention mechanism to encode the document's local semantic and global topical representation. \u2022 Experiments are conducted on three public datasets to verify the effectiveness of our Topic-Aware BiLSTM model in terms of topic coherence measures and classification metrics. \u2022 We select representative cases from different datasets for visualization, demonstrating that the Topic-Aware BiLSTM enhances interpretability than other traditional approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The remainder of the paper is organized as follows: Section 2 reviews the relevant work, and Section 3 introduces preliminary techniques. Section 4 introduces the methodology of Topic-Aware BiLSTM model. Experiments and result analysis are given in Section 5. Lastly, in Section 6, we conclude the paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our work is related to three lines of research which are misleading information detection, topic modeling and attention mechanism.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Misleading information detection models could be categorized as two streams based on implementation techniques: machine learning-based approaches and deep learningbased approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Misleading Information Detection"
        },
        {
            "text": "Generally, machine learning-based approaches need to design the specific representation of texts. For example, Liu et al. [26] employs both the local and the global features via Latent Dirichlet Allocation and utilizes Adaboost to detect spammer. Likewise, Chakraborty et al. [7] uses multinomial Naive Bayes classifiers for pruned features of Clickbait data. Different models of this branch could also result in different detection performance. Song et al. [35] proposes the labeled latent Dirichlet allocation to mine the latent topics from user-generated comments and filter social spam. Biyani et al. [2] uses Gradient Boosted Decision Trees [11] to detect clickbait in news streams. Similarly, Elhadad et al. [10] detects misleading information about COVID-19 through constructing a voting mechanism. However, approaches of this branch often require sophisticated feature engineering and could not capture deep semantic patterns.",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 126,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 276,
                    "end": 279,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 458,
                    "end": 462,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 605,
                    "end": 608,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 646,
                    "end": 650,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 714,
                    "end": 718,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Misleading Information Detection"
        },
        {
            "text": "Thanks to the rapid development of deep representation learning, approaches such as convolutional neural networks, recurrent neural networks have been applied to extract semantic representation from text directly. Agrawal [1] and Hai-Tao et al. [17] utilize a convolutional neural network to detect misleading information from clickbait. Kumar et al. [23] adopts a bidirectional LSTM with an attention mechanism to learn a word contributing to the clickbait score in a different manner. Jain et al. [22] constructs a deep learning architecture based on convolutional layers and long short-term memory layers. Nevertheless, deep learningbased approaches often have complex structures and severely lack interpretability. Thus, we integrate the neural topic model to provide corpus-level semantic information and enhance interpretability.",
            "cite_spans": [
                {
                    "start": 222,
                    "end": 225,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 245,
                    "end": 249,
                    "text": "[17]",
                    "ref_id": null
                },
                {
                    "start": 351,
                    "end": 355,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 499,
                    "end": 503,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Misleading Information Detection"
        },
        {
            "text": "Given a collection of documents, each document will discuss different topics. Topic modeling is an efficient technique which could mine latent semantic patterns from corpus.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Modeling"
        },
        {
            "text": "Latent Dirichlet Allocation (LDA) [3] is the most publicly used traditional probabilistic generative model that can perform topic mining. Unlike traditional graphical topic models, Miao et al. [29] proposes a neural topic model NVDM based on variational autoencoders (VAE). Variational autoencoders use KL divergence to measure the distance between the topic distribution and Gaussian prior. ProdLDA [36] utilizes the approximated Dirichlet prior through Laplace approximation and improves the topic quality. On the other hand, Wang et al. proposes ATM [43] , BAT, and Gaussian-BAT [44] in an adversarial manner. Wang et al. [42] also extends the ATM model for open event extraction. Inspired by ATM model, Hu et al. [20] attempts to improve topic modeling with cycleconsistent adversarial training and names this approach ToMCAT. Zhou et al. [49] extends this line of work by taking into account documents and words as nodes in the graph. Further, autoencoders could be trained stably and reduce the document's representation dimensionally [25] to extract the most effective information [48] . So Nan et al. [31] incorporates adversarial training into Wasserstein autoencoder framework and proposes W-LDA model for unsupervised topic extraction.",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 37,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 193,
                    "end": 197,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 400,
                    "end": 404,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 553,
                    "end": 557,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 582,
                    "end": 586,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 625,
                    "end": 629,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 717,
                    "end": 721,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 843,
                    "end": 847,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1109,
                    "end": 1113,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modeling"
        },
        {
            "text": "The attention mechanism is a brain processing mechanism unique to human vision originally. When we see a picture in life, our brain will prioritize the main content in the image, ignoring the background and other irrelevant information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Mechanism"
        },
        {
            "text": "Inspired by this mechanism of the human brain, various attention mechanisms have achieved success in natural language processing tasks, such as sentiment analysis [45] and machine translation [27] . The typical attention mechanism only pays attention to word-level dependencies and assigns weights so that the model could highlight key elements of sentences [18] . Further, the hierarchical attention mechanism [47] uses twolayer attention, which is successively applied at the word level and sentence level to generate the document representation with rich semantics. Besides, Vaswani et al. [38] proposes a self-attention mechanism to deal with the increasing length of text. Self-attention calculates associations between words in a sentence directly. Previous work [16, 41] has shown that topic information could improve the semantic representation of text with the help of attention mechanisms. Nevertheless, to our best knowledge, no relevant work has been conducted on misleading information detection, so we explore and study in this work.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 167,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 192,
                    "end": 196,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 358,
                    "end": 362,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 411,
                    "end": 415,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 593,
                    "end": 597,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 769,
                    "end": 773,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 774,
                    "end": 777,
                    "text": "41]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [],
            "section": "Attention Mechanism"
        },
        {
            "text": "Latent Dirichlet Allocation (LDA) is the most commonly used generative model for topic extraction. Assuming that a document can be represented by the probability distribution over topics, and each topic can be represented by the probability distribution over words. To learn the topic better, LDA utilizes the Dirichlet distribution as prior over latent space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Latent Dirichlet Allocation"
        },
        {
            "text": "LDA uses \u03b8 d to denote the topic distribution of a document d and z n to represent a topic allocation of the word w n . Thus, the generative process of documents is shown in Algorithm 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Latent Dirichlet Allocation"
        },
        {
            "text": "Here, Dir(\u03b1 ) is the Dirichlet prior distribution, \u03b1 signifies the hyper-parameter of Dirichlet prior, and \u03b8 d is the topic distribution of document d sampled from Dirichlet prior. z n denotes the topic allocation of each position n in the document, and w n is a word randomly generate from multinomial distribution. \u03d5 i is a topic-word distribution of the i-th topic, and \u03d5 z n is one column in the matrix. LDA infers these parameters in an unsupervised manner. After model training, we can obtain representative words with high probabilities in each topic, and these words represent the semantic meaning of each topic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Latent Dirichlet Allocation"
        },
        {
            "text": "As text is sequential data, and small changes of word order will affect the meaning of the entire sentence. However, traditional feedforward neural networks cannot directly extract the word dependency of context. Thus, researchers develop sequential models such as Recurrent Neural Networks (RNN) to extract sequential and contextual features from these data [21] . The RNN comprises an input layer, a hidden layer and an output layer. However, as the length of sentences increases, the training process will appear gradient disappearance and gradient explosion. The Long Short-Term Memory (LSTM) [19] adds a cell state to store long-term memory [13] , which could deal with this problem.",
            "cite_spans": [
                {
                    "start": 359,
                    "end": 363,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 597,
                    "end": 601,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 646,
                    "end": 650,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Long Short-Term Memory"
        },
        {
            "text": "Assuming that x j \u2208 R D w represents a word embedding of the j -th word in the content and D w is the dimension of word embeddings. LSTM feeds in word embeddings as a sequence and calculates the hidden state h j \u2208 R D h for each word, where D h is the dimension of hidden states. The calculation procedure follows below equations:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Long Short-Term Memory"
        },
        {
            "text": "are learnable parameters, and \u03c3 (\u00b7) is sigmoid function. Forget gate f j determines the information that needs to be retained from the cell state C j \u22121 . Input gate i j controls the proportion of new information stored in the new candidate C j . Lastly, LSTM constrains the hidden state of the current node through output gate o j . The elaborated design of its structure enables LSTM could learn longer dependencies and better semantic representation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Long Short-Term Memory"
        },
        {
            "text": "In this section, we first introduce the Topic-Aware BiLSTM (TA-BiLSTM) model. As depicted in Fig text corpus. The text classification module utilizes a twolayer BiLSTM network based on the Topic-Aware attention mechanism to detect misleading information from text.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 93,
                    "end": 96,
                    "text": "Fig",
                    "ref_id": null
                }
            ],
            "section": "Methodology"
        },
        {
            "text": "As shown in the left panel of Fig. 1 , its structure is composed of an encoder and a decoder. (1) Encoder takes the V -dimensional x bow of the document as the input and transforms it into a topic distribution \u03b8 with K dimension through two fully connected layers. (2) Decoder takes the encoded topic distribution \u03b8 as the input, then reconstructs the documentx bow with reconstruction distribution x re . After decoded by the first layer, the topic embedding v t is collected. Besides, to ensure the quality of extracted topics, we use the Wasserstein distance to conduct prior matching in latent topic space.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 30,
                    "end": 36,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Neural Topic Model"
        },
        {
            "text": ".., d n }, the encoder utilizes its bag-ofwords representation x bow as input, where the weights are calculated by TF-IDF formulation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "where c ij indicates the number of the word w i appearing in document d j , and k c kj is the total number of words in document d j . |C d | indicates the total number of documents in the corpus, and j : w i \u2208 d j represents the number of documents containing word w i .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "bow refers to the semantic relevance of the i-th word in the vocabulary in document d j .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "According to Eqs. 7 and 8, each document could be represented as x bow \u2208 R V , where V indicates the vocabulary size.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "The encoder firstly maps x bow into the D s -dimensional semantic space through following transformation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "where W s \u2208 R D s \u00d7V and b s \u2208 R D s are the weight matrix and bias term of the fully connected layer, h s is the hidden state normalized by batch normalization BN(\u00b7), leak denotes the hyper-parameter of LeakyReLU activation, and o s represents the output of the layer. Subsequently, the encoder projects the output vector o s into a K-dimensional document-topic distribution \u03b8 e :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "where W o \u2208 R K\u00d7D s and b o \u2208 R K are the weight matrix and bias term of the fully connected layer, \u03b8 e denotes the topic distribution corresponding to the input x bow and the k-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "e means the proportion of k-th topic in the document.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "We add noise to document-topic distribution to draw more consistent topics. We randomly sample a noise vector \u03b8 n from the Dirichlet prior and merge it with \u03b8 e . The calculation is defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "where \u03b7 \u2208 [0, 1] denotes the mixing proportion of noise. The encoder transforms the bag-of-words representation into topic distribution which perceives the semantic information in latent space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder Network"
        },
        {
            "text": "The decoder takes the topic distribution \u03b8 as input. And then, two fully connected layers reconstruct the document's word representationx bow . After the transformation of first layer, v t serves as the topic embedding of the input document and is provided for the attention mechanism.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decoder Network"
        },
        {
            "text": "The decoder firstly transforms the topic distribution \u03b8 into the D t -dimensional topic embedding space:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decoder Network"
        },
        {
            "text": "where W t \u2208 R D t \u00d7K and b t \u2208 R D t are the weight matrix and bias of the fully connected layer, h t is the hidden vector normalized by batch normalization BN(\u00b7). The v t is activated by the LeakyReLU and then used in Topic-Aware attention mechanism. Subsequently, the decoder transforms the hidden vector h t into V -dimensional reconstruction distribution:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decoder Network"
        },
        {
            "text": "where W r \u2208 R V \u00d7D t and b r \u2208 R V are the weight matrix and bias, and x re is the reconstruction distribution. The decoder is an essential part of the neural topic model. After model training, it could generate the words corresponding to each topic. We input one-hot vectors into the decoder to obtain the word distribution of each topic. Here, we use 10 words with the highest probability of each topic to represent its semantic meaning. Based on the topic distribution and the semantics of topics, interpretable word-level information could be provided for classifying documents in the detection process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decoder Network"
        },
        {
            "text": "Since the Dirichlet distribution is commonly regarded as the prior of multinomial distribution, choosing this prior has substantial advantages [40] . To match the encoded topic distribution to Dirichlet prior, we add a regularizer in TA-BiLSTM. Thus, the training process minimizes the regularization term based on the Maximum Mean Discrepancy (MMD) [15] to reduce the Wasserstein distance, which measures the divergence between the topic distribution \u03b8 and randomly samples \u03b8 from prior.",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 147,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 350,
                    "end": 354,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Prior Distribution Matching"
        },
        {
            "text": "Regarding the kernel function is k : \u00d7 \u2192 R, the MMD based regularizer could be defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prior Distribution Matching"
        },
        {
            "text": "where H is the Reproducing Kernel Hilbert Space (RKHS) of real-valued functions mapping to R. k(\u00b7, \u00b7) implies the kernel function of this space, and k(\u03b8, \u00b7) maps \u03b8 to the features on the high-dimensional space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prior Distribution Matching"
        },
        {
            "text": "As distributions in the latent space are matched with the Dirichlet prior on the simplex, we choose the information diffusion kernel [24] as the kernel function. This function is susceptible to points near the simplex boundary and has better effects on sparse data. The detailed calculation equation is:",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 137,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Prior Distribution Matching"
        },
        {
            "text": "When performing distribution matching, we employ the Dirichlet distribution, \u03b1 means hyper-parameter, then \u03b8 can be sampled by the following equations:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prior Distribution Matching"
        },
        {
            "text": "where \u03b8 (i) denotes the value of the i-th dimension of \u03b8 , \u03b1 (i) means the hyper-parameter of the i-th dimension of the Dirichlet distribution, \u03b8 represents a sample sampled from the Dirichlet prior, and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prior Distribution Matching"
        },
        {
            "text": ". Given M encoded samples and M samples sampled from Dirichlet prior, MMD could be calculated by the following unbiased estimation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prior Distribution Matching"
        },
        {
            "text": "where {\u03b8 1 , \u03b8 2 , ..., \u03b8 M } \u223c Q are the samples collected from the encoder, and Q is the encoded distribution of samples. {\u03b8 1 , \u03b8 2 , ..., \u03b8 M } \u223c P are sampled from the prior distribution P .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prior Distribution Matching"
        },
        {
            "text": "In this subsection, we will introduce the text classification module. As depicted in the right panel of Fig. 1 , we utilize a two-layer BiLSTM based on the Topic-Aware attention mechanism. Because of the complex context of misleading information, we incorporate corpus-level topic features by this mechanism to obtain richer semantic representation. Then, we use a classifier with two fully connected layers to detect misleading information.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 104,
                    "end": 110,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Text Classification Model"
        },
        {
            "text": "Bag-of-words representation is sparse, and the typical solution approach to the sparsity problem is computational intelligence [46] like word embedding technology. Word2vec [30] and GloVe [32] utilize words as the smallest unit for training, while the fastText [4] splits words into n-gram subwords to construct vectors.",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 131,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 173,
                    "end": 177,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 188,
                    "end": 192,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 261,
                    "end": 264,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "BiLSTM"
        },
        {
            "text": "Considering that there are many out-of-vocabulary words in misleading information, we use the embedding layer initialized by the pre-trained fastText. Suppose the word sequence of a document d = {w 1 , w 2 , ..., w m }, w i represents the i-th word in the content. After transforming each word to a one-hot vector, the embedding layer could map words to their corresponding vectors x embed \u2208 R D w , where D w is the dimension of embedding space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BiLSTM"
        },
        {
            "text": "Then, we utilize a two-layer BiLSTM to extract semantic features, and each layer contains bidirectional LSTM units. This bidirectional structure implements the semantic contextual representation of misleading information. The network takes x embed in the order of the content as input and gets each word's hidden state. If the definition of LSTM unit is simplified as LSTM(\u00b7), the hidden state h of each word could be calculated by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BiLSTM"
        },
        {
            "text": "where h f 1 , h f 2 \u2208 R D h are vectors calculated by the forward LSTM, and h b1 , h b2 \u2208 R D h are vectors calculated by the backward LSTM. h \u2208 R 2\u00d7D h +D w is the hidden state that combines the word embedding and the bidirectional LSTM.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BiLSTM"
        },
        {
            "text": "Generally, the attention mechanism is similar to human behavior when reading a sentence, evaluating how important each word is by giving a weight to each part [50] ; the higher value is, the more important the word will be. In the typical attention-based model, the alignment score of each word is calculated as:",
            "cite_spans": [
                {
                    "start": 159,
                    "end": 163,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "Topic-Aware Attention Mechanism"
        },
        {
            "text": "where q \u2208 R D h are learnable parameters. However, typical attention mechanisms could not utilize external information, so we design the Topic-Aware attention mechanism to incorporate topic features while calculating the misleading information representation. In this way, we integrate the neural topic module and the text classification module to train the entire model end-to-end.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic-Aware Attention Mechanism"
        },
        {
            "text": "The attention weights a for each word are calculated based on the similarity between the topic embedding v t and hidden states H = {h 1 , h 2 , ..., h L } in the last layer of BiLSTM, where L represents the max sentence length in batch.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic-Aware Attention Mechanism"
        },
        {
            "text": "Specifically, TA-BiLSTM counts the attention weight a i based on the alignment score between the hidden state h i and the topic embedding v t , where i = {1, 2, ..., L}. We set D t = D h and use the following equation to calculate the alignment score:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic-Aware Attention Mechanism"
        },
        {
            "text": "where W a \u2208 R D h \u00d7D h and b a \u2208 R D h are learnable parameters. The larger the value of f (h , v t ), the greater the probability of misleading information implied by the corresponding word. Then, the document representation could be summarized based on the alignment scores above:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic-Aware Attention Mechanism"
        },
        {
            "text": "where a (i) is the weight of the hidden state h i of the i-th word, and v d \u2208 R D h contains both semantics of hidden states and topic information embedded by the neural topic model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic-Aware Attention Mechanism"
        },
        {
            "text": "In this paper, the text which contains misleading information is taken as a positive example. We apply two fully connected layers and a sigmoid activation function to convert the document representation v d into the probability for classification. Therefore, the higher value of the output, the more possible this document containing misleading information. The prediction process could be defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Classifier"
        },
        {
            "text": "are learnable parameters, and\u0177 is the predicted probability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Classifier"
        },
        {
            "text": "In multi-task learning framework, models are optimized for multiple objectives jointly. Our proposed framework mainly has two training objectives: neural topic modeling objective and misleading information detection objective.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training Objective"
        },
        {
            "text": "For the neural topic modeling, its objective includes the reconstruction term and the MMD based regularization term. It is defined as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training Objective"
        },
        {
            "text": "where c(x bow , x re ) is the reconstruction loss, x (i) bow denotes the weight of the i-th word in the vocabulary, and x (i) re denotes the probability of the i-th word in reconstruction distribution. In our implementation, we follow W-LDA and multiply a scaling factor \u03bc = 1/(l log V ) to balance the two terms, where l indicates the average sentence length in each batch and V indicates the vocabulary size.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training Objective"
        },
        {
            "text": "For classification objective, we measure the binary crossentropy between the target label and the predicted output:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training Objective"
        },
        {
            "text": "where y i is the ground truth, and\u0177 i represents the predicted probability of the i-th document. N means the total number of document in the corpus. To balance the two task specific objectives, we adopt a dynamic strategy to control the weights of objectives above. The neural topic model is mainly concerned in the early stage, and then we pay more attention to train the classification objective. Thus, the total training objective is formed as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training Objective"
        },
        {
            "text": "where \u03bb is a hyper-parameter that dynamically balances the two objectives.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training Objective"
        },
        {
            "text": "We set \u03bb to a slight value in the early stage, allowing the framework to train neural topic model preferentially. Later, we change \u03bb to 1, shifting the focus to multi-task learning, and train the classifier and the neural topic model jointly.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training Objective"
        },
        {
            "text": "We conduct experiments on three public datasets about misleading information to evaluate the effectiveness of the proposed TA-BiLSTM model. [28] is an English public spam dataset compiled in 2006. Ham emails are collected from the mailboxes of six employees in Enron Corporation. Spam messages are obtained from four sources: SpamAssassin corpus, Honeypot project, spam collection of Bruce Guenter, and spam collected by third parties. These emails were sent and received between 2001 and 2005. The dataset consists of six sub-datasets, which are combined into a whole dataset for experiments. [9] . The Text Retrieval Conference (TREC) is a series of seminars, which mainly focuses on the problems and challenges in information retrieval research. The 2007 TREC conference held a spam filtering competition and published this dataset. The dataset includes complete mail information such as sending and receiving addresses, time, HTML code. In the experiments, we retain content in the main body and ignore other information.",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 144,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 594,
                    "end": 597,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Webis-Clickbait-17 [33] contains a total of 19,538 Twitter posts with links from 27 major news publishers in the United States. These posts were published between November 2016 and June 2017. Five annotators from Amazon Mechanical Turk marked whether articles in these links were misleading information. We use the content of articles linked in the post for detection.",
            "cite_spans": [
                {
                    "start": 19,
                    "end": 23,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "TREC Public Spam"
        },
        {
            "text": "Due to noisy data such as blanks and duplicate documents in three datasets, the statistics of preprocessed datasets are listed in Table 2 . We arrange 2/3 of the data as the training set and 1/3 of the data as the test set.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 130,
                    "end": 137,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "TREC Public Spam"
        },
        {
            "text": "In the experiments, all datasets use package enchant to check the spelling of words. Each word is reverted to base form with no inflectional suffixes by the en core web lg model of package spacy. We utilize package gensim to obtain the word embedding matrix and initialize the embedding layer.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Configuration"
        },
        {
            "text": "For the neural topic model, we set the number of topics K to 50 and the dimension D s of the fully connected layer in the encoder to 256. The dimension D t of the topic embedding is equal to the dimension D h of the hidden state h . We make Dirichlet prior as sparse as possible and set the Dirichlet hyper-parameter \u03b1 to 0.001. The proportion of noise \u03b7 that adds to topic distribution is defined as 0.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Configuration"
        },
        {
            "text": "For text classification model, we apply 300-dimensional pre-training fastText word embeddings [14] , that is, D w is set to 300. The dropout of the BiLSTM layer is 0.3, and the dimension D m in the classifier is 64. The weight matrixes in BiLSTM are initialized by orthogonal initialization, and the parameters in the Topic-Aware attention mechanism are initialized by uniform initialization.",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 98,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Model Configuration"
        },
        {
            "text": "During model training, the hyper-parameter \u03bb is set to 1e-8 initially, and when the training reaches the last 20 Epochs, \u03bb is set to 1. Adam optimizer with a learning rate of 1e-4 to train the parameters of the neural topic model and with a learning rate of 5e-5 to train other parameters. The batch size is 16. The computer CPU is Intel Xeon (Skylake) Platinum 8163, and the operating system is Ubuntu 20.04 64-bit. All models are implemented with PyTorch and run on an NVIDIA V100 32G graphic card.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Configuration"
        },
        {
            "text": "We choose Naive Bayes, Support Vector Machine, Decision Tree, Random Forest four machine learning models for comparison.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "Naive Bayes [28] is a probabilistic model. By learning the joint probability distribution of the input and output of the training data, the model computes the label with the largest posterior probability of the predicted data. Positive samples refer to misleading information, while negative ones are opposite SVM [8] is a linear binary classification model defined in the feature space. It uses a kernel function to find a hyperplane to separate the two categories, and maximizes the interval between the data and the plane.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 16,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 314,
                    "end": 317,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "Decision Tree [6] adopts a tree structure and uses layered inferences on the data to achieve the final classification, so it has good interpretability.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 17,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "Random Forest [5] is an ensemble learning method containing multiple decision trees. The model trains each decision tree independently, and the result is determined by the category with the most output of decision trees. Besides, we also compare our model with following deep learning-based baselines.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 17,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "BiLSTM uses a BiLSTM network without attention mechanism. The hidden state of words in the document is averaged as the classifier's input.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "Attention-BiLSTM uses a BiLSTM network based on a traditional attention mechanism and inputs the classifier after the weighted summation of each word's hidden state.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "In the aspect of topic modeling, we compare our model with the following neural topic models. LDA 1 [3] extracts topics based on the co-occurrence information of words in the document. We use package gensim to implement this model. NVDM 2 [29] comprises an encoder network and a decoder network, inspired by the variational autoencoder based on Gaussian prior distribution. 3 [31] is the prototype of our model, which uses Wasserstein autoencoder and Dirichlet prior distribution to mine topic information.",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 103,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 239,
                    "end": 243,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 374,
                    "end": 375,
                    "text": "3",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 376,
                    "end": 380,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "BAT [44] applies bidirectional adversarial training with Dirichlet prior for neural topic modeling.",
            "cite_spans": [
                {
                    "start": 4,
                    "end": 8,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [],
            "section": "W-LDA"
        },
        {
            "text": "The last three neural topic models mentioned above adopt a neural network structure similar to our model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "W-LDA"
        },
        {
            "text": "In the experiments, we mainly evaluate the classification performance of the text classification model and the topic quality of the neural topic model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "For classification, we compare three widely used performance metrics: accuracy, precision, and F1-score. Accuracy refers to the proportion of correctly classified samples to the total number. The calculation is:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "where N is the total number of samples, and I(\u00b7) depicts the indicator function. When \u00b7 is true, the function equals 1; otherwise, it is equal to 0. In binary classification, we generally divide the combination of predicted labels and ground truths into four types, namely True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN). True or False means whether the prediction is correct, Positive or Negative means whether the forecast result is a positive or negative sample. These four categories respectively correspond to the number of samples that meet the condition, so the sum of four values equals N. Based on the above, the definition of precision is:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "Precision is the number of correct labels divided by the number of all predicted positive results, and recall is the fraction of true positive samples predicted to be positive. So the precision and recall are a set of contradictory measures. To comprehensively consider the precision and recall metrics, we also evaluate the effectiveness with the F1-score. The definition is below:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "Under the same experimental conditions, the higher above metrics, the better classification performance. For topic quality, we utilize two standard metrics C V and C A of topic coherence 4 [34] . Here we choose 10 representative words for each topic as word sets and respectively compute C V to measure semantical support for one word in each set. Variously, C A compares pairs of single words in each topic's set to evaluate the coherence between words. To this end, we apply the two metrics to quantify the quality of topic modeling comprehensively.",
            "cite_spans": [
                {
                    "start": 189,
                    "end": 193,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "In this section, we present the experimental results and corresponding analysis of proposed TA-BiLSTM model in terms of classification performance and topic quality. The first four items are machine learning models, and the last two items are deep learning models for ablation study All significant information has been bolded Table 3 lists the results of classification performance on three used public datasets compared with different baselines. We could observe that the TA-BiLSTM model could obtain better results in accuracy, precision and F1-score. Specifically, the bag-of-words representation limits the traditional machine learning approaches. The precision of Random Forest on the Clickbait-17 dataset is higher because the model only selects confirmed positive samples to minimize the number of FP. Therefore, the accuracy of Random Forest is not high, and the F1-score is lower than other approaches.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 327,
                    "end": 334,
                    "text": "Table 3",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "Moreover, we conduct ablation study by comparing BiLSTM and Attention-BiLSTM to verify the outperforming of the Topic-Aware attention mechanism. We could observe that the results are better than those of machine learning-based approaches, indicating that richer semantic feature representation, especially context information, could improve classification performance. Compared with the BiLSTM, the results of Attention-BiLSTM show slight improvements, indicating that the attention mechanism assigns more weights to specific words to provide a more suitable document representation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Classification Performance"
        },
        {
            "text": "Furthermore, in the comparison of Attention-BiLSTM and TA-BiLSTM, we observe that accuracy increases 0.64%, 1.12%, 3.11% and F1-score increases 0.63%, 0.99%, 4.95% for the latter on the three datasets, respectively. The significant improvements show that Topic-Aware attention mechanism could incorporate topic information into classification module. Moreover, the topic information could indeed help TA-BiLSTM to provide more suitable representations for misleading information detection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Classification Performance"
        },
        {
            "text": "The calculation of attention mechanism often incorporates supervision signal from a document, which will be helpful for mining latent semantic patterns in topic modeling procedure. Thus, we also evaluate the quality of topics in this subsection. Table 4 presents the results of different topic coherence metrics C A and C V comparing with other topic modeling baselines on three datasets. The five topics on the Enron Spam dataset are \"college\", \"conference\", \"politics\", \"prize-winning\", and \"loan\", on the 2007 TREC dataset are \"weather\", \"sports\", \"computer\", \"software\" and \"mathematics\", and on the Clickbait-17 dataset are \"politics\", \"sports\", \"medicine\", \"flight\" and \"crime\"",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 246,
                    "end": 253,
                    "text": "Table 4",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Topic Quality Comparison"
        },
        {
            "text": "Compared with the topics extracted by W-LDA on Enron Spam dataset, the C A of TA-BiLSTM has increased by 5.81%, and the C V metric has risen by 11.53%. On the 2007 TREC dataset, C A is almost the same as the W-LDA, but the C V has increased by 13%. We also present the comparison with BAT. It obtains slightly higher than W-LDA and LDA on Clickbait-17, but our model improves C A and C V by 2.31% and 3.06%.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Quality Comparison"
        },
        {
            "text": "Ignoring NVDM with poor performance, Table 5 lists the top-10 representative words with the highest probability for each topic on three datasets. Thus, we could compare the quality of performance intuitively. Generally, compared with other models, we could realize that the topics generated by TA-BiLSTM have fewer irrelevant words and higher semantic coherence.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 37,
                    "end": 44,
                    "text": "Table 5",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "Topic Quality Comparison"
        },
        {
            "text": "The topic words of NVDM are not very consistent because it employs Gaussian prior to mimic Dirichlet in topic distribution space. As the proposed TA-BiLSTM utilizes Dirichlet as prior distribution in topic space, it could obtain coherent topics than NVDM. Meanwhile, the supervision signal also helps the TA-BiLSTM to surpass LDA, W-LDA and BAT in topic modeling evaluation. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Quality Comparison"
        },
        {
            "text": "To further validate the robustness of TA-BiLSTM, we conduct hyper-parameter analysis in this subsection. Concretely, parameter analysis on three parameters (the number of topics K, the dimension of hidden states h and the proportion of noise \u03b7) has been carried out. Firstly, the number of topics K is set to 30, 50, 80 and 100, respectively. The quantitative results on three datasets are reported in Table 6 and visualized in Fig. 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 402,
                    "end": 409,
                    "text": "Table 6",
                    "ref_id": "TABREF7"
                },
                {
                    "start": 428,
                    "end": 434,
                    "text": "Fig. 2",
                    "ref_id": null
                }
            ],
            "section": "Hyper-Parameter Analysis"
        },
        {
            "text": "For Enron Spam and 2007 TREC datasets, we could observe that TA-BiLSTM performs fairly stable on three metrics. For Clickbait-17 dataset, the classification performance is more sensitive to changes of K, which may be caused by the complicity of the dataset. It is worth mentioning that optimal numbers of topics over datasets are different (50 on Enron Spam, 80 on 2007 TREC and 50 on Clickbait-17). If this number is too large, the model is not interpretable, and if the number is too small, the model training will be negatively affected [12] . Thus, we set the number of topics K to 50 in our experiments.",
            "cite_spans": [
                {
                    "start": 540,
                    "end": 544,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Hyper-Parameter Analysis"
        },
        {
            "text": "Similarly, we conduct parameter analysis on the dimension of hidden states h . It has been set to 25, 50, 75, 100 and 150 respectively. And the corresponding statistics are listed in Table 7 . By comparing the results, we could observe that simple models perform better on Enron Spam and 2007 TREC datasets. While dealing with Clickbait-17, classification performance improves with the increasing of model complexity. This may be also caused by the complexity of Clickbait-17 dataset which needs a more complicated model to fit the data.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 183,
                    "end": 190,
                    "text": "Table 7",
                    "ref_id": "TABREF8"
                }
            ],
            "section": "Hyper-Parameter Analysis"
        },
        {
            "text": "We further investigate the impact of different proportions of noise \u03b7 on the performance. In detail, we compute the metrics of classification and topic modeling separately with five proportion settings [0, 0.1, 0.2, 0.3, 0.4]. The detailed comparison is shown in Table 8 . It can be concluded that adding a proper proportion of noise to the topic distribution upgrades the quality of topic modeling on all datasets. However, not the optimal parameter for the topic mining has the same consequence on classification performance. Topic coherence is better when the proportion is set to 0.1 or 0.2, while less noise is helpful for the Topic-Aware attention mechanism to preserve topic features and prediction. Hence we set the proportion of noise to 0.1 for better comprehensive results in the experiments.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 263,
                    "end": 270,
                    "text": "Table 8",
                    "ref_id": "TABREF9"
                }
            ],
            "section": "Hyper-Parameter Analysis"
        },
        {
            "text": "To validate that proposed TA-BiLSTM could indeed improve the model interpretability, we conduct case study and visualization in this subsection. Figure 3a shows an advertising email for an online pharmacy in the Enron Spam dataset. As Topic 8 represents drugs, we could infer that this email may discuss related topics. Also, we could find various drug names appeared in its text content. Likewise, Fig. 3b depicts a web page content from Clickbait-17 which entices people to buy cosmetics. We can also find relevant words from Topic 15 and Topic 45, such as 'carpet', 'fashion', 'beauty', 'makeup'.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 145,
                    "end": 154,
                    "text": "Figure 3a",
                    "ref_id": null
                },
                {
                    "start": 399,
                    "end": 406,
                    "text": "Fig. 3b",
                    "ref_id": null
                }
            ],
            "section": "Case Study and Visualization"
        },
        {
            "text": "Thus, the above two examples show that corpus-level topic relatedness could really improve model interpretability. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case Study and Visualization"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Clickbait Detection Using Deep Learning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Agrawal",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 2nd International Conference on Next Generation Computing Technologies (NGCT). IEEE",
            "volume": "",
            "issn": "",
            "pages": "268--272",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Amazing Secrets for Getting More Clicks: Detecting Clickbaits in News Streams Using Article Informality",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Biyani",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Tsioutsiouliklis",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Blackmer",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Latent dirichlet allocation",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "Y"
                    ],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J Mach Learn Res",
            "volume": "3",
            "issn": "",
            "pages": "993--1022",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Enriching word vectors with subword information",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bojanowski",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Grave",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Joulin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Transa Assoc Comput Linguist",
            "volume": "5",
            "issn": "",
            "pages": "135--146",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Random Forests",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Mach Learn",
            "volume": "45",
            "issn": "",
            "pages": "5--32",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Classification and Regression Trees",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "J"
                    ],
                    "last": "Stone",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Olshen",
                    "suffix": ""
                }
            ],
            "year": 1984,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Stop clickbait: Detecting and preventing clickbaits in online news media",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chakraborty",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Paranjape",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kakarla",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ganguly",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE/ACM International conference on advances in social networks analysis and mining",
            "volume": "",
            "issn": "",
            "pages": "9--16",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "LIBSVM: A library for support vector machines",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "J"
                    ],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "ACM Trans Intell Syst Technol (TIST)",
            "volume": "2",
            "issn": "3",
            "pages": "1--27",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "TREC 2007 Spam Track Overview. In: In The Sixteenth Text RETrieval Conference",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "V"
                    ],
                    "last": "Cormack",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Detecting Misleading Information on COVID-19",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "K"
                    ],
                    "last": "Elhadad",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "F"
                    ],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Gebali",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Access",
            "volume": "8",
            "issn": "",
            "pages": "165201--165215",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Stochastic gradient boosting",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Friedman",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Comput Stat Data Anal",
            "volume": "38",
            "issn": "4",
            "pages": "367--378",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Collaborative Learning-based Industrial IoT API Recommendation for Software-defined Devices. The Implicit Knowledge Discovery Perspective",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "Rjd",
                    "middle": [],
                    "last": "Barroso",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hussain",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "The Cloud-edge-based Dynamic Reconfiguration to Service Workflow for Mobile Ecommerce Environments: A QoS Prediction Perspective",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "ACM Trans Internet Technol (TOIT)",
            "volume": "21",
            "issn": "1",
            "pages": "1--23",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Learning word vectors for 157 languages",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Grave",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bojanowski",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Joulin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the eleventh international conference on language resources and evaluation (LREC",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A kernel Two-Sample test",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gretton",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Borgwardt",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Rasch",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "J Mach Learn Res",
            "volume": "13",
            "issn": "25",
            "pages": "723--773",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Multi-Task Learning with mutual learning for joint sentiment classification and topic detection",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Gui",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Trans Knowl Data Eng",
            "volume": "",
            "issn": "",
            "pages": "1--1",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "An attention-based neural framework for uncertainty identification on social media texts",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Tsinghua Sci Technol",
            "volume": "25",
            "issn": "1",
            "pages": "117--126",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural Comput",
            "volume": "9",
            "issn": "8",
            "pages": "1735--1780",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Neural topic modeling with cycle-consistent adversarial training",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "9018--9030",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Ssur: an approach to optimizing virtual machine allocation strategy based on user requirements for cloud data center",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hussain",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Trans Green Commun Netw",
            "volume": "5",
            "issn": "2",
            "pages": "670--681",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Spam detection in social media using convolutional and long short term memory neural network",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Jain",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Agarwal",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Ann Math Artif Intell",
            "volume": "85",
            "issn": "1",
            "pages": "21--44",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Khattar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gairola",
                    "suffix": ""
                },
                {
                    "first": "Kumar",
                    "middle": [],
                    "last": "Lal",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Varma",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "1225--1228",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Information Diffusion Kernels",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lafferty",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lebanon",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Proceedings of the 15th International Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "391--398",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "An energy-efficient data collection scheme using denoising autoencoder in wireless sensor networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Tsinghua Sci Technol",
            "volume": "24",
            "issn": "1",
            "pages": "86--96",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Detecting \"Smart\" Spammers on Social Network: A Topic Model Approach",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Itti",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the NAACL Student Research Workshop",
            "volume": "",
            "issn": "",
            "pages": "45--50",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Effective Approaches to Attention-based Neural Machine Translation",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "T"
                    ],
                    "last": "Luong",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "1412--1421",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Spam filtering with naive Bayes-Which naive bayes? In: CEAS",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Metsis",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Androutsopoulos",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Paliouras",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "17",
            "issn": "",
            "pages": "28--69",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Neural variational inference for text processing",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Miao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Blunsom",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "International Conference on Machine Learning. PMLR",
            "volume": "",
            "issn": "",
            "pages": "1727--1736",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Distributed Representations of Words and Phrases and their Compositionality",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems",
            "volume": "2",
            "issn": "",
            "pages": "3111--3119",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Topic modeling with wasserstein autoencoders",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Nan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nallapati",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "6345--6381",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "GloVe: Global Vectors for Word Representation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Crowdsourcing a large corpus of clickbait on twitter",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Potthast",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Gollub",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Komlossy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Schuster",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wiegmann",
                    "suffix": ""
                },
                {
                    "first": "Epg",
                    "middle": [],
                    "last": "Fernandez",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hagen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Stein",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "1498--1507",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Exploring the Space of Topic Coherence Measures",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "R\u00f6der",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Both",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hinneburg",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "399--408",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Who are the spoilers in social media marketing? Incremental learning of latent semantics for social spam detection",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "Ryk",
                    "middle": [],
                    "last": "Lau",
                    "suffix": ""
                },
                {
                    "first": "Rcw",
                    "middle": [],
                    "last": "Kwok",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mirkovski",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Dou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Electron Commerce Res",
            "volume": "17",
            "issn": "1",
            "pages": "51--81",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Autoencoding variational inference for topic models",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Sutton",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Wasserstein Auto-Encoders",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Tolstikhin",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Bousquet",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gelly",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schoelkopf",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Attention is All you Need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "Kaiser",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "6000--6010",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Rethinking LDA: Why Priors Matter",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "M"
                    ],
                    "last": "Wallach",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Mimno",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mccallum",
                    "suffix": ""
                }
            ],
            "year": 1973,
            "venue": "Proceedings of the 22nd International Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "An End-to-end Topic-Enhanced Self-Attention Network for Social Emotion Classification",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of The Web Conference",
            "volume": "2020",
            "issn": "",
            "pages": "2210--2219",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Open Event Extraction from Online Text using a Generative Adversarial Network",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Deyu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
            "volume": "",
            "issn": "",
            "pages": "282--291",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "ATM: Adversarial-Neural Topic Model. Inf Process Manag",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "56",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Neural topic modeling with bidirectional adversarial training",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "340--350",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Attention-based LSTM for Aspect-level Sentiment Classification",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "606--615",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "An approach to alleviate the sparsity problem of hybrid collaborative filtering based recommendations: The product-attribute perspective from user reviews",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Mob Netw Appl",
            "volume": "25",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Hierarchical attention networks for document classification",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dyer",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Smola",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Hovy",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "",
            "issn": "",
            "pages": "1480--1489",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Qos Prediction for Service Recommendation With Features Learning in Mobile Edge Computing Environment",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Mai",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Trans Cogn Commun Netw",
            "volume": "6",
            "issn": "4",
            "pages": "1136--1145",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Neural topic modeling by incorporating document relationship graph",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "3790--3796",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "A novel approach to workload prediction using attention-based lstm encoder-decoder network in cloud environment",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "EURASIP J Wirel Commun Netw",
            "volume": "2019",
            "issn": "1",
            "pages": "1--18",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": ". 1, our proposed TA-BiLSTM could be divided into two parts: a neural topic model and a text classification model. The topic module employs a neural topic model to discover latent topics from The overall architecture of TA-BiLSTM: (a)Neural Topic Model on the left; (b)Text Classification Model on the right. MLP and f MLP are multilayer perceptron, v t denotes the topic embedding, and v d means the document's representation, which is computed through attention weights a",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "and R. Wang contributed equally to this work. School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing 210003, Jiangsu, China Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing 210003, Jiangsu, China",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Content from different websites may carry normal or misleading information",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Statistics of three preprocessed datasets",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Misleading information detection performance on the three datasets",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Topic coherence scores of various topic models on the three datasets, a higher value means more coherent topics",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "LDArepublican senate vote bill democrat senator committee sen congress democratic coach football sport yard final lose player championship title loss health care patient medical food doctor disease reduce mental increase flight travel airport airline plane passenger fly ban board return charge arrest border crime car criminal driver prison county prosecutor W-LDA repeal republican care bill health insurance affordable lawmaker legislation coverage baseball league player club pitch major sport fan hit minor",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "DatasetsModels Topics medication prescription med drug generic doctor pharmacy sexual ship medicine flight airline passenger plane airport pilot fly aircraft carrier crew Clickbait-17 suspect arrest officer injure shoot authority truck kill gun wound BAT health care insurance bill republican repeal affordable tax vote law tournament ball coach player shot basketball final league seed guard die doctor condition cancer hospital medical brain tweet surgery staff flight airline passenger plane airport pilot board fly seat air shooting suspect shoot arrest kill gun prison murder charge incident TA-BiLSTM freedom democracy inauguration speech crowd party politician protester supporter protest ball baseball basketball player tournament court supreme shoot hall shooting disease patient cancer medicine medical diagnose drug doctor treatment study flight airline passenger plane airport pilot aircraft seat crew fly prison crime sentence prosecutor drug jail murder inmate convict arrest",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Parameter analysis of the number of topics K on three datasets All significant information has been bolded Mobile Netw Appl Fig. 2 Illustration of different numbers of topics K on three datasets. Each of these subfigures is constituted by four components. The first one depicts how TA-BiLSTM performance varies with different numbers of topics and others depict the comparison with baselines on three classification metrics Accuracy, Precision and F1-score",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "Parameter analysis of the dimension of hidden states h on three datasets All significant information has been bolded",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "Parameter analysis of the proportion of noise \u03b7 on three datasets All significant information has been bolded",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "(b) Clickbait-17",
            "cite_spans": [],
            "ref_spans": [],
            "section": "More from now until the new you'll probably be busy with parties and gatherings. But between deciding on what dress to wear and which dish to figuring out how to do your makeup is likely to fall last on your list. Want a gold smoky eye for dinner with family or glitter lips for new year's? We've scoured the runways and red carpets to help you find the perfect holiday makeup for every affair. Let's keep in follow yahoo beauty on and stunning and simple ways to do winter hair and makeup this is the only blush palette. You'll need this winter beauty gifts so pretty, you don't need to wrap them classic red lipsticks. You probably own and new ones, you should try vamp burgundy lip."
        },
        {
            "text": "In this paper, we proposed the Topic-Aware BiLSTM (TA-BiLSTM) model, an end-to-end framework. TA-BiLSTM contains a neural topic model and a text classification model, which explores corpus-level topic relatedness to enhance misleading information detection. Meanwhile, the supervision signal could be incorporated into topic modeling process to further improve the topic quality. Experiments on three English misleading information datasets demonstrate the superiority of TA-BiLSTM compared with baseline approaches. Additionally, we analyze multiple hyper-parameters in detail and select specific topic examples for visualization. More recently, classification and topic modeling on short texts are still challenging tasks. Our future study would pay more attention to detect misleading information from the short text on social media platforms.Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ]
}
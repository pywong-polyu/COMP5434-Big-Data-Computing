{
    "paper_id": "4c39ffb240da72c23d606d68399395e039596120",
    "metadata": {
        "title": "Motion-compensated online object tracking for activity detection and crowd behavior analysis",
        "authors": [
            {
                "first": "Ashish",
                "middle": [],
                "last": "Singh",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Patel",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Ranjana",
                "middle": [],
                "last": "Vyas",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "\u00b7",
                "middle": [
                    "O P"
                ],
                "last": "Vyas",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "\u00b7",
                "middle": [],
                "last": "Muneendra Ojha",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Vivek",
                "middle": [],
                "last": "Tiwari",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "It is a nontrivial task to manage crowds in public places and recognize unacceptable behavior (such as violating social distancing norms during the COVID-19 pandemic). In such situations, people should avoid loitering (unnecessary moving out in public places without apparent purpose) and maintain a sufficient physical distance. In this study, a multi-object tracking algorithm has been introduced to improve short-term object occlusion, detection errors, and identity switches. The objects are tracked through bounding box detection and with linear velocity estimation of the object using the Kalman filter frame by frame. The predicted tracks are kept alive for some time, handling the missing detections and short-term object occlusion. ID switches (mainly due to crossing trajectories) are managed by explicitly considering the motion direction of the objects in real time. Furthermore, a novel approach to detect unusual behavior of loitering with a severity level is proposed based on the tracking information. An adaptive algorithm is also proposed to detect physical distance violation based on the object dimensions for the entire length of the track. At last, a mathematical approach to calculate actual physical distance is proposed by using the height of a human as a reference object which adheres more specific distancing norms. The proposed approach is evaluated in traffic and pedestrian movement scenarios. The experimental results demonstrate a significant improvement in the results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "intervention to detect and identify potentially dangerous situations, which is very error-prone and costly [39] . Thus, an automatic surveillance system [63] is required, which can process a massive amount of incoming data and generate meaningful information by isolating the suspicious events for ensuring overall safety. Furthermore, detecting certain activities like loitering and physical distancing violation becomes undoubtedly crucial in the time of natural pandemic like COVID-19. 1 Thus, an intelligent system must identify such behavior in public places to ensure safety and security with law enforcement.",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 111,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 153,
                    "end": 157,
                    "text": "[63]",
                    "ref_id": "BIBREF63"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "However, detecting such activities requires spatial and temporal information, which consists of the interaction of numerous objects in the same frame and the subsequent temporal frames requiring accurate tracking of the multiple objects.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Multi-object tracking becomes complex in a dense environment where many objects with similar appearances are present, leading to object occlusion (due to self or other multiple nearby objects) and frequent crossing trajectories. In such an environment, most of the tracking algorithm fails to perform accurately in achieving robust multi-object tracking [37] . Moreover, due to the emergence of Deep Neural Network (DNN) models, various automatic object detection approaches like Yolov3 [52] , single-shot object detector [36] , etc., showed their applicability with high accuracy in real time [64, 67] . However, occasionally missed detection, false detection, and multiple detections still occur due to shortterm object occlusion, which causes tracks misleading and hampering the activity detection. Most of the work in literature is for pedestrian detectors, using frame by frame high-quality object detectors [13, 18] , and then associating with online and offline trackers [5, 67, 69] . The majority of the approaches are used either appearance-based methods [1, 7, 9, 11, 66] or Kalman filter [30] , and particle filter [29] . These methods fail where objects targets are too close, or object movement is nonlinear. Few approaches use tracking by comparing the trajectories (dynamics similarity) but are too complex.",
            "cite_spans": [
                {
                    "start": 354,
                    "end": 358,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 487,
                    "end": 491,
                    "text": "[52]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 522,
                    "end": 526,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 594,
                    "end": 598,
                    "text": "[64,",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 599,
                    "end": 602,
                    "text": "67]",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 913,
                    "end": 917,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 918,
                    "end": 921,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 978,
                    "end": 981,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 982,
                    "end": 985,
                    "text": "67,",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 986,
                    "end": 989,
                    "text": "69]",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 1064,
                    "end": 1067,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1068,
                    "end": 1070,
                    "text": "7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1071,
                    "end": 1073,
                    "text": "9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1074,
                    "end": 1077,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "text": "66]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1126,
                    "end": 1130,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The presented study is focused on improving the object detection method and wrong association of the track, which normally occurs due to crossing trajectories of multiple objects. Furthermore, we proposed a method for loitering detection and physical distancing detection. The proposed method for finding physical distancing is often sufficient to comply with norms of the COVID-19 pandemic, where the norms are relative to the dimensions of the objects. However, certain scenarios/use-cases may require more specific distancing norms requiring identification of actual physical distance. Thus, we proposed an algorithm to estimate the actual physical distance between adults by using the height of the human as a reference. The major contributions are summarized as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "-An algorithm for better localization of an object in the video through a data association approach to estimate object tracks that alleviate ID Sw. Furthermore, the proposed approach handles short-term object occlusion by keeping the estimated track alive for a certain number of frames or until the object is detected again. -The tracking algorithm to identify suspicious activity (\"loitering\") in a video and assigned a severity level (high/medium/low) using an online approach. -A method to identify the physical distancing in the crowd by using reference thresholding. -A method to estimate actual physical distance using reference objects.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The proposed approach performs well to improve detection errors. It is validated by recognizing the activity of loitering and achieved state-of-the-art results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The remaining of the paper is organized as follows. The existing literature work is presented in Sect. 2, the multiobject tracking methodology, along with a novel algorithm for identifying loitering and violation of physical distancing in Sect. 3 . Experimental results on various datasets and their discussion are presented in Sect. 4 . Section 5 concludes the paper by highlighting the possible future directions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The major focus of the study is online object tracking for crowd behavior analysis. In this regard, literature discussion has been categorized into two segments, first with the tracking algorithms including object detection and, later, crowd behavior analysis (loitering and physical distancing).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "Plenty of Deep Neural Network (DNN) [12, 25, 28] architectures were proposed to extract the bounding box and further classification. The object detection methods can be viewed in two ways. The first set of methods requires the generation of region proposals where the probability of locating an object is high. These region proposals of the objects are passed to a neural network that further classifies the object in a predefined class. The R-CNN [25] is the first and state-of-the-art method. Furthermore, many other methods are proposed, which are incremental development over the R-CNN which includes spatial pyramid pooling (SPPnet) [28] , region-based fully convolutional network (R-FCN) [12] , feature pyramid networks (FPN) [34] , Fast R-CNN [24] , Faster R-CNN [54] , and Mask R-CNN [27] . The second type of detection method extracts the bounding box location of the object and class of the object. The major approached are Multi-Box [17] , Deconvolutional Single Shot Detector (DSSD) [23] , Attention Net [68] , G-CNN [43] , Single Shot Multi-Box Detector (SSD) [36] , Deeply Supervised Object Detectors (DSOD) [59] , Yolo [50] , and yolov2 [51] . The YOLOv3 [52] has been used in the proposed work as it is a robust and accurate method for object detection with bounding box extraction.",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 41,
                    "end": 44,
                    "text": "25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 45,
                    "end": 48,
                    "text": "28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 448,
                    "end": 452,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 638,
                    "end": 642,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 694,
                    "end": 698,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 732,
                    "end": 736,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 750,
                    "end": 754,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 770,
                    "end": 774,
                    "text": "[54]",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 792,
                    "end": 796,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 944,
                    "end": 948,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 995,
                    "end": 999,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "text": "[68]",
                    "ref_id": "BIBREF68"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1122,
                    "end": 1126,
                    "text": "[59]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 1134,
                    "end": 1138,
                    "text": "[50]",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 1152,
                    "end": 1156,
                    "text": "[51]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "text": "[52]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [],
            "section": "Brief survey on object detection approaches"
        },
        {
            "text": "The track of the object and the temporal relations between the objects spanning over multiple frames is widely used for behavior analysis of the crowd at public places ( [2, 49] ). Tu et al. [62] highlighted the inability of CNNs for modeling temporal information, and they proposed an action stage that emphasized spatiotemporal vector created by aggregating local feature descriptors. Basly et al. [4] proposed a Residual Convolutional Network-based deep temporal residual system for recognition day to day activities. They have used LSTM for extracting temporal features from the video. However, detecting unusual and abnormal behavior is still challenging as it requires tremendous training data. Dawn et al. [14] surveyed the various spatiotemporal interest points (STIP)based techniques for human action recognition which uses spatiotemporal information. Mabrouk et al. [38] proposed a method for violence detection by utilizing global and local features (optical flow and STIP). Ramfrez et al. [20] proposed recognition of group social behavior based on Individual Profiles (IP) to classify as Equally Interested (EI), Balanced Interest (BI), Imbalance Interests (UI), and Chatting (CH). The IP is categorized as Exploring/Interested/Distracted or Disoriented by ranking the features (trajectory, distance, speed, and gaze) using centered kernel alignment.",
            "cite_spans": [
                {
                    "start": 170,
                    "end": 173,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 174,
                    "end": 177,
                    "text": "49]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 191,
                    "end": 195,
                    "text": "[62]",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 400,
                    "end": 403,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 713,
                    "end": 717,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 876,
                    "end": 880,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 1001,
                    "end": 1005,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Recent works on object tracking"
        },
        {
            "text": "Feng et al. [19] proposed a unified MOT algorithm by utilizing long-term and short-term cues. It classifies potential user and causer using a Switch Aware Classifier (employ features of the target and switcher), sub-net-based single object tracking by capturing short-term cues, and a re-identification based subnet for extracting long-term cues. Furthermore, many approaches for multi-object tracking are proposed by generating detections and associating them to establish the track hypothesis. However, two approaches are followed for associating detections with the track. First, associate detections frame by frame locally, while others follow a global approach. Wu et al. [65] followed the approach of associating detections with object hypotheses locally by defining an affinity measure based on position, color, and size. Jeany et al. [60] proposed a tracking algorithm by associating detections; using Quad-CNN, which performs the association of objects across subsequent frames by learning quadruplet losses and the appearances of the target object (including their temporal adjacencies). The quadruplet loss adds an extra constraint that forces proximate detections to locate near compared to the target, which has a significantly sizable temporal gap. Bewley et al. [8] proposed a real-time MOT approach by using a robust detector for object detection and then assigning the track based on the Hungarian method and Kalman filter by considering the velocity of the objects. Wojke et al. [64] improved their previous approach [8] by integrating appearance information to handle the object occlusion and reduce the number of identity switches.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 16,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 677,
                    "end": 681,
                    "text": "[65]",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 842,
                    "end": 846,
                    "text": "[60]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 1277,
                    "end": 1280,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1497,
                    "end": 1501,
                    "text": "[64]",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 1535,
                    "end": 1538,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Recent works on object tracking"
        },
        {
            "text": "Dicle et al. [16] proposed an algorithm that employs the motion dynamics to distinguish targets having a similar appearance, minimize target identification error, use Generalized Linear Assignment (GLA) for identifying dynamics. The approach is independent of track length and powerful to capture the motion dynamics of the target. However, it is computationally costly. Bae et al. [3] discussed an online algorithm for multi-target tracking using discriminative appearance learning. Firstly, the confidence of the track is calculated using continuity and detectability. The track grows incrementally by associating with detections, and linear discriminant analysis selects the target where appearance is similar. It shows track confidence estimation is computationally costly and also highly dependent on the quality of detectors, fails to handle object occlusion and crossing trajectory. The frame-by-frame association suffers from the drift when multiple objects are too close. This problem is addressed by optimizing multiple trajectories in [53] , and joint probabilistic data association filters [22] by generating a single state hypothesis for association likelihood. It is further improved in [31, 55] tracking and demonstrated substantial results. Leibe et al. [32] used quadratic Boolean programming to perform association globally (processed the entire video as a batch). Zhang et al. [69] introduced network flow formulations to perform multi-target data association. However, these algorithms are not applicable in online real-time scenarios.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 382,
                    "end": 385,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "text": "[53]",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 1102,
                    "end": 1106,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "text": "[31,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1206,
                    "end": 1209,
                    "text": "55]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 1270,
                    "end": 1274,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1396,
                    "end": 1400,
                    "text": "[69]",
                    "ref_id": "BIBREF69"
                }
            ],
            "ref_spans": [],
            "section": "Recent works on object tracking"
        },
        {
            "text": "To the best of our knowledge, no work has been sighted that uses the direction of the objects to handle crossing trajectories. It is also worth exploring frame interleaving to improve the detections errors. The major focus of the work is the estimation of accurate tracking information for activity recognition to analyze crowd behavior.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Recent works on object tracking"
        },
        {
            "text": "Anomalous crowd behavior analysis is challenging in intelligent surveillance systems due to the lack of a standard definition of anomaly and limited generalized examples. Chang et al. [10] proposed a technique employing deep autoencoder and k-means clustering to detect anomalies in video. The spatiotemporal information is divided into two sub-processes; one autoencoder for spatial information and the other for temporal information. The consecutive frames are processed on temporal auto-encoder while spatial autoencoder processes the last individual frame. The optical flow is constructed by utilizing the RGB difference by taking the last frame while the temporal part process the consecutive frames. As a result, abnormal events tend to have distinctive spatial and temporal characteristics that produces significant reconstruction error. Furthermore, deep k-means clusters are designed to extract the common factors of variation on the dataset trained on general events. The anomaly is detected by computing the deviation of the representation with the cluster along with reconstruction error.",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 188,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Crowd behavior analysis survey"
        },
        {
            "text": "Loitering is one of the most common unusual behaviors from the perspective of surveillance systems [39] . Thus, intime identification of such loitering (anomalous behavior) can mitigate many potential dangerous scenarios. Nam [44] proposed an algorithm for loitering detection by associating tracks in the crowded scene using a histogram of oriented gradients. Consequently, versatile loitering [57] is proposed by Arivazhagan et al. by using wavelet transformation for blob detection and further classifying them as pedestrian tracks using an SVM classifier. However, such methods suffer in the presence of multiple pedestrians in the vicinity. Loitering detection through frame-by-frame tracking is efficient, but it predominantly depends on the tracking accuracy [42] . The misses can occur due to object occlusions and detection errors, especially in crowded scenes. The proposed tracking algorithm can recover the missed tracks and handle object occlusion. Recent works on loitering detection rely on the trajectory and duration of stay of the person, but it is insufficient, as the stay duration depends on various parameters like the environment of the place, number of people in the place, or a visual salience place. In such cases, a person can spend significantly more time than just crossing the place. Thus, the proposed work has given the major focus on loitering detection and further assigned a severity level as low, medium, or high based on the time spent. The initial threshold is assigned and updated online after the end of each track.",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 103,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 226,
                    "end": 230,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 395,
                    "end": 399,
                    "text": "[57]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 766,
                    "end": 770,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Crowd behavior analysis survey"
        },
        {
            "text": "It is worth to note physical distancing in public places is a must to protect against the various infectious diseases that spread through the air. However, physical distancing violation is common in public places like malls, railway stations, airports, etc. Mercaldo et al. [40] proposed a simple method for physical distancing estimation using object detection with YOLO and Euclidean distance between the centroid of the detected object. However, a predefined threshold is not effective as the dimensions of the object and threshold depend on the position of the camera. Sugianto et al. [61] proposed to identify the physical distancing by performing a homographic transformation. However, the homographic transformation has certain limitations and may not work accurately in crowd surveillance scenarios. Saponara et al. [58] introduced a system for real-time crowd behavior analysis for tackling from COVID-19 pandemic. The method estimates physical distancing using the Euclidean distance between the centroid of the detected bounding box of the objects. However, Euclidean is only effective in very limited planar scenarios. Zuo et al. [70] proposed a method to calculate the real-time distance by finding a \"box\" with four hyper-planes; two pedestrians belong to two of the hyperplanes, and the other two hyperplanes are perpendicular and intersect with the pedestrians. The distance is estimated by calculating the rate at which the height of the object in pixel changes. However, the different locations usually have different views of perspective and maybe re-positioned at different places. So, it is not always possible to find a box with such conditions. Furthermore, the method is computationally costly to perform in real time. Gupta et al. [26] discussed a novel method to calculate the actual physical distance of each object to the camera by considering the focal length and distance with the bounding box of the objects. Furthermore, the distance between the objects is estimated by the absolute horizontal pixel distance. However, such a method is only effective in planar scenarios where the object size remains the same at all positions, often infeasible in a real-time environment. Thus, we propose an algorithm for analyzing crowd behavior by identifying anomalous behavior of loitering and detecting violation of physical distancing by object detection and utilizing the tracking information. The threshold for physical distancing is set adaptively, independent of the viewpoint, and proportionate to the size of the object.",
            "cite_spans": [
                {
                    "start": 274,
                    "end": 278,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 589,
                    "end": 593,
                    "text": "[61]",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 824,
                    "end": 828,
                    "text": "[58]",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 1142,
                    "end": 1146,
                    "text": "[70]",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 1756,
                    "end": 1760,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Brief survey on physical distance estimation approaches"
        },
        {
            "text": "The major contribution and methodology have been categorized into four problems: object detection/tracking, loitering classification, adaptive threshold-based, and actual physical distance estimation. Moreover, an adaptive threshold-based method is well suitable where norms are relative to the dimensions of the objects. However, certain scenarios/use-cases may require more specific distancing norms (actual physical distance). Thus, an algorithm to measure the actual physical distance is also proposed. The framework of the proposed tracking-based activity detection and behavior analysis is subdivided into three parts: object detection, object tracking, and crowd behavior analysis as depicted in Fig. 1 . In the first part of the figure, the object detection algorithm (YOLOv3 [52] ) details are presented by including the layered model. Further, the information on the tracking algorithm is presented, which involves the prediction step followed by data association. The tracking information is additionally utilized for crowd behavior analysis to identify the activity of loitering and physical distancing with actual physical distance estimation presented in the third part. The key features and parameters employed in crowd behavior analysis are also given under each step. The key feature contains the adaptive threshold techniques for assigning a severity level and adaptive threshold for physical distancing based on the size of the bounding box with the actual physical distance by taking the height of the human as a reference in the case of adults. Figure 2 demonstrates a better understanding of the proposed object tracking approach. Firstly, the objects in each frame of the videos are identified using a detection algorithm. The tracking information of the objects is generated by combining the predictions (motion compensation) and a data association algorithm. The proposed motion estimation approach can handle short-term object occlusion, including detection error (missing, double, and false detections). The identity switches (due to crossing trajectories) are handled by including the direction of the motion. The proposed tracking approach consists mainly of object detection, track prediction, and association of detections with tracks. However, the physical distance estimation approach has been discussed separately at last.",
            "cite_spans": [
                {
                    "start": 784,
                    "end": 788,
                    "text": "[52]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [
                {
                    "start": 703,
                    "end": 709,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 1566,
                    "end": 1574,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Proposed work"
        },
        {
            "text": "An object is first identified and then extracted along with its bounding box using the DNN-based object detection method. YOLOv3 [52] was employed for object detection, which is a Fast RCNN [24] based framework, pre-trained to detect 80 classes of Microsoft Coco dataset [35] . It is worth mentioning that various similar methods were tested in this scenario and found YOLOv3 is outperformed. However, there may have a chance to see performance variation with respect to other datasets and scenarios. The model worked well to detect the car, person, and bus but was found less effective for the truck. In this regard, a separate training has been carried out explicitly with 5000 images of the truck (learning rate was set to 0.02). The batch size was set to 100 with ten epochs and 50 iterations per epoch (500 iterations in total). Furthermore, few MOT benchmark datasets have public detections, consisting of pre-existing detections for benchmarking the results. However, object detection methods have some limitations and may not perform accurately due to detection errors, which are summarized as below:",
            "cite_spans": [
                {
                    "start": 129,
                    "end": 133,
                    "text": "[52]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 190,
                    "end": 194,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 271,
                    "end": 275,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Object detection"
        },
        {
            "text": "-Missed Detection-When an object in an image goes undetected. It usually happens due to lack of training or object occlusion. -Small Detection-A small portion of the object (small bounding box) is detected along with the full part of the object due to object occlusion, lack of clear visibility, or poor image resolution. -Large Detection-A bigger bounding box is also generated when two objects are close by along with individual detections.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Object detection"
        },
        {
            "text": "-Double Detection-Two bounding box of the object is generated; one of each type may occur due to misclassification.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fig. 2 Object tracking approach workflow"
        },
        {
            "text": "The proposed tracking algorithm handles such errors by employing frame interleaving.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fig. 2 Object tracking approach workflow"
        },
        {
            "text": "The tracking algorithm is based on the Kalman filter [30] which can handle the continuous variable for estimating state variables from noisy observations over time. For example, looking for a bird movement so that we do not lose its sight. Kalman filter uses linear Gaussian distributions, such that the subsequence state is a linear function of the present state with some Gaussian noise. The algorithms predict the state variables comprising the width, height, and location of the object of the next state based on the previous states. The new state of the object is used in the data association algorithm. It consists of three steps: predict, data association, and update.",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 57,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Object tracking"
        },
        {
            "text": "The new trajectory is initialized in a case where detection of the next frame does not match with existing trajectories. The old trajectories are terminated only when associations do not occur with new objects for a certain number of frames (frame interleaving). The pixel coordinate of the center of the object in the current frame is calculated using an extracted bounding box, denoted by (x t and y t ). The predicted and actual coordinate of the object in the subsequent frame is denoted byx,\u0177\" x t+1 , and y t+1 . The track of the object is assigned with an identifier denoted by track_id (referred to as tracks). Prediction of the object in the next frame is performed using Kalman filter-based estimator. It has a horizontal and a vertical component denoted by The initial estimate of the model state X , matrix of error, and covariances C between the estimated state variables are measured by multiple iterations of the algorithm. It will have blended in enough observations to make it reasonably insensitive to have precise initialization details.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Object tracking"
        },
        {
            "text": "The state variables of the object in the subsequent frame are predicted by the estimator function as shown in Eq. (2) and (3). X p n and C p n denote the estimated state and covariance matrix at the nth frame, whileX n\u22121 and\u0108 n\u22121 represent the state variable and covariance matrix of the previous frame. F n\u22121 matrix is of dimension mxm, calculated as Jacobian matrix of the non-linear function f by linearizing the model which is used to predict the state n th using the previously estimated state (n \u2212 1) state. Thus, the bounding box of the object is estimated using the predicted state variables.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prediction"
        },
        {
            "text": "The existing tracks are associated with the detections in the next frame by calculating intersection over union (IoU) with each object and matching the direction of the track. Each track is assigned with a direction as shown in Table 1 . D x and D y represent the difference between horizontal pixel and vertical pixel of the object in the current frame and previous frame. The direction is calculated by obtaining the difference (D x and D y ) followed by the assignment of direction based on Table 1 . The direction of the track is only updated when there exists a unique match based on the IoU matrix.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 228,
                    "end": 235,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 494,
                    "end": 501,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Direction and IoU based data association"
        },
        {
            "text": "In the case of multiple matches, the direction is calculated by associating the track with both the detections. Finally, a track where direction remains the same is selected for the object. Consequently, a new track id is assigned to the detected object If an association is not successful with any detections. The threshold value for IoU is denoted by I oU th . The associations are successful if Eq. (4) is satisfied. The association algorithm is demonstrated in Fig. 3 which works based on the following cases:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 465,
                    "end": 471,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Direction and IoU based data association"
        },
        {
            "text": "1. Unique match -In case of only one match on IoU greater than I oU th , then the object is included in the track, and the direction of the track is updated as in Table 1 exists more than one match on I oU greater than I oU th , then the direction is calculated by associating all the matched tracks, and the direction of the track is updated as in Table 1 . However, for all associations, the direction is updated, which leads to a change in the direction of the track. In such cases, the track which results in maximum I oU is selected. 4. No Match -A new track is created with a track_id and direction id is set to 0.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 163,
                    "end": 170,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 349,
                    "end": 356,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Direction and IoU based data association"
        },
        {
            "text": "I oU n < I oU th (4) I OU th and threshold for substantial distance is set to 0.6 and 5 (obtained experimentally for all scenarios).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Direction and IoU based data association"
        },
        {
            "text": "The inclusion of direction while assigning tracks to detections reduces the number of ID Sw due to more than one object being too close, dominant in case of crossing trajectories, shown in Fig. 4 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 189,
                    "end": 195,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Direction and IoU based data association"
        },
        {
            "text": "The direction of each track is calculated based on the rules below also listed in Table 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 82,
                    "end": 89,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Direction and IoU based data association"
        },
        {
            "text": "If D x and D y are positive and magnitude is substantially high then this direction is assigned with id 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "North East:"
        },
        {
            "text": "If D x is positive and only D x have substantial magnitude, then this direction is assigned with id 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "East:"
        },
        {
            "text": "If D x is positive while D y is negative, and both have substantially high magnitude then this direction is assigned with id 3. 4. North: If D y is positive and only D y have substantial magnitude, then this direction is assigned with id 4. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "South East:"
        },
        {
            "text": "It has been observed that the majority of the detection algorithms miss the object in a few frames due to changes in intensity and luminosity. Thus, frame interleaving is proposed to keep the tracks active even direct association is not possible. This is predominant and useful in case of missing detection for a certain number of frames. Furthermore, the missing detections may also occur due to short-term object occlusion. In this regard, the existing track will be kept active till those many frames where the detection error probability is high. The optimal value of the hyper-parameter (\u03b3 ) is obtained experimentally by repeating the experiment for a certain number of trials. The value of hyperparameter depends on the environment, speed of the vehicle, etc. The proposed optimization of keeping tracks active (frame interleaving) handles short-term object occlusion and missing detections, giving the object more precise tracking.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Frame interleaving for handling detection errors and object occlusion"
        },
        {
            "text": "The velocity estimators are updated in case of successful association with the pre-existing tracks by adjusting the variation between actual and predicted detection using Eqs. (5), ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimator update"
        },
        {
            "text": "An ideal tracking algorithm should correctly detect all the objects and estimate their position precisely with their tracking information over time by assigning a unique identifier for each tracked object sequence. Furthermore, it should generate only one trajectory per object. Thus, the following parameters are considered to evaluate its effectiveness.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation metric"
        },
        {
            "text": "-False Positive -The number of tracks which do not associate with the real object. is the measure combining false-positives, missed targets, and the number of identity switches given by (8) , m t , f p t , g t , and mme t are the number of misses, of false positives, ground truth, and mismatches, respectively, with time t.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation metric"
        },
        {
            "text": "-MT -Mostly tracked targets. It is the ratio of ground truth trajectories covering more than 80%. -MOTP [6] -Multiple Object Tracking Precision (MOTP) specified in (9), ct is the number of matches over the time period t, and d i t represents the distance between the object and its corresponding hypothesis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation metric"
        },
        {
            "text": "A time-stamp-based approach has been introduced to identify loitering. The time spent by a person is calculated using the function of the length of track and frames per second of the captured frame, given by Eq. (10) . T o refers to the time spent, and the number of frames per second is denoted by f ps. The threshold parameters are calculated for initial n sample tracks only (training phase of threshold parameters). The value of n controls the parameter for detecting the activity of loitering.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Enhanced loitering classification through dynamic thresholding"
        },
        {
            "text": "The initial value of the threshold is set to 10, 20, and 30 seconds for low, medium, and high severity levels, respectively. The threshold for severity is updated after each iteration by calculating the deviation from the time of stay of a current person with the average time of stay. Mean is given by \u03bc and calculated using (11) and updated after every iteration.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Enhanced loitering classification through dynamic thresholding"
        },
        {
            "text": "Thus, threshold values are denoted by T h L , T h M , and T h H , given by (12), (13) , and (14) for low, medium, and high level of severity, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Enhanced loitering classification through dynamic thresholding"
        },
        {
            "text": "T h L = max (2\u03bc n+1 , 10)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Enhanced loitering classification through dynamic thresholding"
        },
        {
            "text": "T h M = max (3\u03bc n+1 , 20)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Enhanced loitering classification through dynamic thresholding"
        },
        {
            "text": "The thresholds are measured experimentally and very reasonable for a given environment as it depends on the basic principle of unusual behavior.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Enhanced loitering classification through dynamic thresholding"
        },
        {
            "text": "Monitoring public places to ensure physical distancing in a real-time environment is considered one of the most challenging tasks. It comes with an enormous challenge as estimating physical distance is almost infeasible without a reference object. Furthermore, the size of the object varies with pixel location as the viewpoint of the camera changes. However, the dimension of the object is correlated in close proximity (a small region of interest) having similar dimensions. The region of interest is defined as a small area where the dimension of a type of object is relatively similar. Thus, our proposed algorithm exploits this property and utilizes the dimensions of the target object to identify if physical distancing. We propose an algorithm to detect the physical distancing violation both spatially and temporally. The algorithm identifies physical distance by calculating the physical threshold as a function of the object's size, as demonstrated in Fig. 5 . The visualization and idea of the algorithm are shown in Fig. 6 . Two possible scenarios are demonstrated with different viewpoints. It is evident that the number of pixels required by a reference object varies according to the angle, which is captured by the variation in objects' size. The description of the algorithm 1 is as follows:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 962,
                    "end": 968,
                    "text": "Fig. 5",
                    "ref_id": null
                },
                {
                    "start": 1028,
                    "end": 1034,
                    "text": "Fig. 6",
                    "ref_id": null
                }
            ],
            "section": "Adaptive threshold based physical distancing detection"
        },
        {
            "text": "Input: Bounding box of detected person along with the frame id and track id Output: Set the Physical Distancing parameter of objects and tracks ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1: Physical Distancing Detection"
        },
        {
            "text": "The proposed method for finding physical distancing can identify the activity to safeguard from COVID-19 pandemic where the norms are relative to the dimensions of the objects. However, certain scenarios/use-cases may require more specific distancing norms requiring identification of actual physical distance. Thus, we propose to calculate the actual physical distance between person i and j is calculated using Eq. (15) denoted by P D i j is a function of the average height of the human being, the height of the person in the frame, and the Euclidean distance between them, calculated in meters. At each pixel, the number of pixels contained by the reference object varies depending on the viewpoint angle. As the object goes farther from the viewpoint, the viewpoint angle and number of pixels contained by the object also decreases; thus, the size of the reference object is directly proportional to the viewing angle. This is evident in Fig. 6 , in scenario B, the viewing angle is less than the viewpoint angle in scenario A. Thus, the proposed algorithm is adaptive and calculates the actual physical distance at each pixel change based on Eq. (15) . The average height of the person (adults) acting as a reference from the real world is a constant and kept as 1.65 m [45] for this experiment which is consistent with our dataset as well. However, the bounding box height is a little larger than the actual object. Thus we have kept the height of the bounding box to 1.8 m as a reference for our calculation.",
            "cite_spans": [
                {
                    "start": 1276,
                    "end": 1280,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [
                {
                    "start": 943,
                    "end": 949,
                    "text": "Fig. 6",
                    "ref_id": null
                }
            ],
            "section": "Actual physical distance estimation"
        },
        {
            "text": "In the case of another type of object, such as a car that has similar dimensions, the actual physical distance can be estimated using its dimension as a reference. Furthermore, in the case of the objects whose real-world dimensions are not known, then a normalized physical distance can be calculated by using the height of the object in one location of the image to the other. This distance can be converted to real-world metrics like (meters, inches) when height in the real world is established. One such scenario is a school environment where the height of the children is less and has variation but has a common height in a group (a particular class/age group). In such cases, the normalized physical distance is estimated as the children belonging to the same group will possess similar height.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Actual physical distance estimation"
        },
        {
            "text": "The proposed algorithm is adaptive and calculates the actual physical distance between two persons, given by Eq. (15) . The Euclidean distance between the center of the objects is calculated at each pixel value, by varying the value of y by 1 after each instant keeping \u0394y to 1 and calculating the corresponding value of x at that position using Eq. (16), where \u0394x refers to a change in the values of x after changing the y by 1. This is repeated from vertical coordinate of object i to vertical coordinate of object j (o iy to o j y ) to get the distance from object i to object j. The distance after each iteration is normalized with the ratio of half of the human height and the number of pixels denoting half of the height at that instant as the upper half of the objects is only considered from the center.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Actual physical distance estimation"
        },
        {
            "text": "Equation (16) is derived from the equation of the line between two points, joining the center of the i th and j th object, m is the slope between two points o i x , o iy , and o j x , o j y given by Eq. (17) , and c is a constant. The constant c in Eq. (16) is calculated by assigning x with o i x , y with o iy and m from Eq. (17) in Eq. (16) . The vertical point of the bounding box of the person after each update is denoted by y t , which is calculated by putting the value of x from Eq. (16) in line joining the coordinated of top of the i th object and j th object. The algorithm is demonstrated in Fig. 7 . The ratio of human height and number of pixels spanned by the height at that instant normalizes the distance after each iteration, multiplied by two as only one-half of the object is considered.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 605,
                    "end": 611,
                    "text": "Fig. 7",
                    "ref_id": null
                }
            ],
            "section": "Actual physical distance estimation"
        },
        {
            "text": "Thus, our approach finds the actual physical distance between the people using the height of the person as a reference.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Actual physical distance estimation"
        },
        {
            "text": "The proposed approach is evaluated on different datasets in various scenarios of the public domain along with the specifically tailored PETS2006 and PETS2016 datasets for performance evaluation for tracking and surveillance. The effectiveness of the object tracking approach is demonstrated by detecting the activity of loitering, which uses the tracking information as a baseline. Thus, the proposed approach is evaluated in three basic steps from the perspective of activity detection; object detection, object tracking, and loitering detection. A comparison with current methods is performed on the benchmark dataset, demonstrating the effectiveness of the proposed approach. Furthermore, the performance of the physical distancing algorithm is demonstrated on various benchmark datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and discussion"
        },
        {
            "text": "In this work, various datasets are used as listed in Table 2 consisting of traffic environment, warehouse environment, crowd scene, and parking environment for demonstrating the applicability of the proposed approach in broad scenarios. The PETS dataset is also used, a benchmark dataset for performance evaluation of object tracking, and consists of various activities such as loitering. Furthermore, the proposed tracking and physical distancing approach including the estimation of actual physical distance is validated on MOT benchmark datasets [41] (PETS09-S2L2, ETH Crossing, AVG-TownCentre, TUD-Stadmitte, and Venice-2) [21] (D5) (sequence S1-T1-C(S1), S2-T3-C (S2) and S3-T7-A (S3))",
            "cite_spans": [
                {
                    "start": 549,
                    "end": 553,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 627,
                    "end": 631,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 53,
                    "end": 60,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "The dataset consists of a left luggage scenario with varied complexity consisting of the multiple moving person in public places. Each sequence contains a calibrated scenario of person(s) with abandoned luggage and loitering.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "A multi-camera dataset contains various scenarios around a parked vehicle. In total 22 different scenarios of various abnormal activities were recorded to identify various potential threats. However, in this work we have used only a part of dataset with sequences containing the activity of loitering. Fig. 7 Demonstration of estimation of actual physical distance by using reference object (person) between two persons demonstrating the effectiveness of the approach. The MOT benchmark datasets already have object detections, thus isolating the bias on object detection method for measuring tracking performance.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 302,
                    "end": 308,
                    "text": "Fig. 7",
                    "ref_id": null
                }
            ],
            "section": "PETS-2016 [48] (D6) (sequence 03_06 and 14_05)"
        },
        {
            "text": "The objects are detected on each image of the video frame using Yolov3 [52] , which detects the objects along with its bounding box. The proposed model can detect and extract bounding box of vehicles (car, bus, truck including rear, front, and side) and person. The effectiveness of the detection algorithm is demonstrated by calculating precision and recall as shown in Table 3 . ",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[52]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [
                {
                    "start": 371,
                    "end": 378,
                    "text": "Table 3",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "Object detection"
        },
        {
            "text": "The detections of each video sequence frame are passed as the input to the object tracking framework. The detections are associated with a track; subsequently, the algorithm adds the tracking id. The benchmark parameter is calculated using motmetrics 2 which is a python library.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tracking"
        },
        {
            "text": "The proposed object tracking approach achieves high accuracy on benchmark parameters on different datasets. The tracking results on sequences of PETS2006 and PETS2016 dataset is entirely accurate, laying the strong foundation for activity detection. The results of the proposed tracking approach are demonstrated in Table 4 . A comparison is also performed with the recent object tracking approaches on the dataset D3 (PNNL2) in Table 6 . Our proposed method gives better accuracy and also reduces the total number of ID Sw significantly. The number of ID Sw is significantly reduced as the proposed approach employs direction and IoU based data association along with the frame interleaving, which is very useful in case of crossing trajectories and detection errors. The hyper-parameter (\u03b3 ) further enhances the tracking by configuring the frame interleaving. (\u03b3 ) is set experimentally, keeping the tracks active even in case of no association is possible, primarily due to short-term occlusion and detection errors. The effect of maintaining the tracks active is shown in Table 5 on dataset D4. The D4 dataset has 15 frames per second. The duration is kept in terms of a number of frames with a variation of 5, ranging from 0 to 25. It is evident that the best results are obtained at \u03b3 = 20, found after experimenting through multiple possible values and iterations. Furthermore, the hyper-parameter value depends on the target environment, datasets, and detection method, so it needs to be set experimentally.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 316,
                    "end": 323,
                    "text": "Table 4",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 429,
                    "end": 436,
                    "text": "Table 6",
                    "ref_id": null
                },
                {
                    "start": 1077,
                    "end": 1084,
                    "text": "Table 5",
                    "ref_id": "TABREF7"
                }
            ],
            "section": "Tracking"
        },
        {
            "text": "Moreover, to demonstrate the effectiveness of the proposed approach on benchmark datasets that have public detections is shown in Table 7 . The proposed approach outperforms method [8] producing less number of ID Sw and tracking the objects more accurately. Both the methods are evaluated on the dataset having public detections (detections already provided with the dataset). This way, the dependency of the detections is eliminated as certain detection algorithms may be better than the other, thus avoiding ambiguity in tracking accuracy making difficult to analyze the performance of the tracking algorithm.",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 184,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 130,
                    "end": 137,
                    "text": "Table 7",
                    "ref_id": null
                }
            ],
            "section": "Tracking"
        },
        {
            "text": "We have demonstrated the loitering detection with various sequences of PETS 2006 and PETS 2016 dataset as shown in Table 8 . The value Yes corresponds to Loiter while No is marked when it not classified as Loiter. For each sequence, first of all, objects are detected using YOLOv3 [52] . Subsequently, the track is assigned to detections using our proposed tracking algorithm. The time spent by the person is calculated using Eq. (10), which is further classified as a loiter based on the threshold values, calculated adaptively.",
            "cite_spans": [
                {
                    "start": 281,
                    "end": 285,
                    "text": "[52]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [
                {
                    "start": 115,
                    "end": 122,
                    "text": "Table 8",
                    "ref_id": "TABREF8"
                }
            ],
            "section": "Loitering detection"
        },
        {
            "text": "The existing approach is better than the versatile loitering because it worked on GAIT feature. A person can still be a loiter if he walks fast and moves around the same place. In this case, it would not be classified as loiter because detection is based on GAIT feature corresponding to slow walk. Furthermore, the area under trajectory can be less when he still moves to and from along a straight line. Thus, the proposed approach of loitering detection based on the time of stay is superior to other methods of classifying by calculating the shape of the trajectory and GAIT features. Figures 8 and 9 demonstrate the loitering behavior in the sequence 03-06 and 12-05 with high severity as the time Fig. 11 Demonstration of physical distancing and actual distance on PETS09 Dataset spent by the person is more than the threshold which was set adaptive. In the first part of the figures, the key-frames of the sequence are shown at the interval of two seconds involving loitering. In the second part, the track of the person is plotted. The approximate center position of the person in half a second is shown with a dot.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 588,
                    "end": 603,
                    "text": "Figures 8 and 9",
                    "ref_id": null
                },
                {
                    "start": 702,
                    "end": 709,
                    "text": "Fig. 11",
                    "ref_id": null
                }
            ],
            "section": "Loitering detection"
        },
        {
            "text": "The proposed algorithm for violation of physical distancing detection is evaluated on four benchmark datasets as shown in Table 9 . The table presents precision and recall in the spatial direction, i.e., in a single video frame. However, the results significantly depend on the detection accuracy. The percentage of track that has violated physical distancing presents the overall idea about following the norms in a temporal direc-tion. The average count of the person present in a video frame is also listed, giving information about the occupancy of the crowd in a public place. The dataset D5 only has one sequence with multiple persons, while dataset D3 (PNNL2) has multiple persons.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 122,
                    "end": 129,
                    "text": "Table 9",
                    "ref_id": "TABREF9"
                }
            ],
            "section": "Physical distancing"
        },
        {
            "text": "The physical distancing and actual distance of the various MOT 16 benchmark dataset also listed in Table 9 are shown in Figs. 10, 11, 12, and 13. The first image in the figure shows the status of the person. If the distance is maintained larger than the threshold, the bounding box is labeled green, otherwise red. Furthermore, a physical distancing violation is shown with a red line drawn between the objects.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 99,
                    "end": 106,
                    "text": "Table 9",
                    "ref_id": "TABREF9"
                }
            ],
            "section": "Physical distancing"
        },
        {
            "text": "The second set of the image shows the actual physical distance in meters from one specific object with others. It is assumed that the height of the objects (person in this case) has similar dimensions. Furthermore, while estimating the Fig. 12 Demonstration of physical distancing and actual distance on TUD-Stadmitte Dataset actual physical distance between the objects, the real-world size of the object needs to be known.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 236,
                    "end": 243,
                    "text": "Fig. 12",
                    "ref_id": null
                }
            ],
            "section": "Physical distancing"
        },
        {
            "text": "In some cases, for identifying the physical distancing when both the target objects and the camera lie on the same line of sight, the distance between the objects will not give accurate results. This type of scenario cannot be solved using a single camera. Thus, multiple cameras need to be installed to get more precise results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Physical distancing"
        },
        {
            "text": "The proposed framework uses detected objects along with a bounding box for generating tracking information and crowd behavior analysis. The objects are detected on an input video frame which is passed to the online tracking algorithm in the pipeline, which assigns a track-id in real time. The objects with the bounding box and track-id are further passed to the crowd behavior analysis. The object tracking algorithm works in three parts where prediction thorough Kalman filter have linear bound, data association have polynomial time complexity on O(t 2 ) and update step requires linear time proportional to the number of active tracks. The loitering detection involves linear computational complexity proportional to the number of active tracks (t) followed by updating threshold for severity level. The physical distancing detection has time complexity with O(n 2 ) as for each bounding box, it calculates distance with all other existing bounding boxes in a frame. The actual physical distance estimation also has the time complexity of O(n 2 ) is the distance between two objects are estimated where n is the number of objects.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Runtime"
        },
        {
            "text": "The whole framework is tested on systems with varied configurations and performs in real time on an HD stream (1280 \u00d7 720 resolution) with 30 fps on a system with 3rd generation Intel Xeon processor and NVIDIA Titan 1080 Ti GPU (11GB). Table 10 demonstrates the computation time taken for tracking and crowd behavior analysis along with the object detection. It is to be noted the time taken by object detection significantly depends on the GPU and outperforms with NVIDIA Titan 1080 Ti GPU achieving the performance required for real-time scenarios for HD video. The runtime for tracking and crowd behavior analysis is independent of GPU. It only depends on the CPU and performs reasonably well with 8GB of RAM and 10th generation Intel i5 and i7 processors.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 236,
                    "end": 244,
                    "text": "Table 10",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Runtime"
        },
        {
            "text": "In this paper, we have explored the approach for multi-object tracking to handle short-term object occlusion, detection errors, and ID Sw to generate accurate tracks of the objects for analyzing crowd behavior in public places. Firstly, the object bounding box is extracted using a detection algorithm. The bounding box of the objects in the subsequent frame is predicted using a linear velocity estimate of the object based on Kalman filter, which is later on compared and replaced with actual detections in case an association is found. Whenever data association is not feasible, the existing predicted tracks are kept alive for some time, handling the missing detections and short-term object occlusion. ID Sw are handled by explicitly considering the motion direction of the objects at the time of association with the detections, which mainly occurs due to crossing trajectories. The proposed approach is evaluated in traffic environment and pedestrian movement scenarios and achieves high accuracy, which can be explicitly used for event detection. A trajectory-based method to detect the activity of loitering is also proposed capable of classifying the loitering at three levels of suspicion (low, medium, and high). Furthermore, the accuracy of the proposed loitering detection approach demonstrates the fruitfulness of the proposed work. Moreover, an adaptive algorithm for identifying physical distancing is proposed, which utilizes the object detection and tracking information to calculate the actual distance between the persons and the statics about the place if it is too much crowded by counting the average number of persons present at the moment. The proposed work opens up a plethora of use-cases for detecting usual and unusual activities/events by using the tracking information of the objects in a surveillance environment. In future work, the work can be extended to track multiple types of objects simultaneously and handle long-term object occlusion. The use of context and tracking information can be vital in detecting activities in real-time and complex scenarios. Furthermore, due to the lack of a standard benchmarking dataset with the labeled actual distance between the objects, complete validation will be taken up as future work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and future work"
        },
        {
            "text": "Funding This study does not receive any funding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and future work"
        },
        {
            "text": "Conflict of interest All authors have participated in the design and approval of the final manuscript version. This manuscript has not been submitted to, nor is under review at, another journal or other publishing venue. The contents of this manuscript are not copyrighted, submitted, or published elsewhere, while acceptance by the Journal is under consideration. There are no directly related manuscripts or abstracts, published or unpublished, by any authors of this paper. All authors declares that he/she has no conflict of interest.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Declarations"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Discrete-continuous optimization for multi-target tracking",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Andriyenko",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Schindler",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1926--1933",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Expert video-surveillance system for real-time detection of suspicious behaviors in shopping malls",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Arroyo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "J"
                    ],
                    "last": "Yebes",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "M"
                    ],
                    "last": "Bergasa",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "G"
                    ],
                    "last": "Daza",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Almaz\u00e1n",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Expert Syst. Appl",
            "volume": "42",
            "issn": "21",
            "pages": "7991--8005",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Bae",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Yoon",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "1218--1225",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Dtr-har: deep temporal residual representation for human activity recognition",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Basly",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ouarda",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "E"
                    ],
                    "last": "Sayadi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ouni",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Alimi",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Vis. Comput",
            "volume": "",
            "issn": "",
            "pages": "1--21",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Stable multi-target tracking in real-time surveillance video",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Benfold",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Reid",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "CVPR 2011",
            "volume": "",
            "issn": "",
            "pages": "3457--3464",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Evaluating multiple object tracking performance: the clear mot metrics. EURASIP J. Image Video Process",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Bernardin",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Stiefelhagen",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Tracking large variable numbers of objects in clutter",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Betke",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "E"
                    ],
                    "last": "Hirsh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bagchi",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "I"
                    ],
                    "last": "Hristov",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "C"
                    ],
                    "last": "Makris",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "H"
                    ],
                    "last": "Kunz",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Simple online and realtime tracking",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bewley",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ge",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ott",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ramos",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Upcroft",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE international conference on image processing (ICIP)",
            "volume": "",
            "issn": "",
            "pages": "3464--3468",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A generative statistical model for tracking multiple smooth trajectories",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Brau",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dunatunga",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Barnard",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Tsukamoto",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Palanivelu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "CVPR 2011",
            "volume": "",
            "issn": "",
            "pages": "1137--1144",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Clustering driven deep autoencoder for video anomaly detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "329--345",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Multitarget data association with higher-order motion models",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "T"
                    ],
                    "last": "Collins",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "2012 IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "1744--1751",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "R-fcn: Object detection via region-based fully convolutional networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1605.06409"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Histograms of oriented gradients for human detection",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Dalal",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Triggs",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)",
            "volume": "1",
            "issn": "",
            "pages": "886--893",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "A comprehensive survey of human action recognition with spatio-temporal interest point (stip) detector",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "D"
                    ],
                    "last": "Dawn",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Shaikh",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Vis. Comput",
            "volume": "32",
            "issn": "3",
            "pages": "289--306",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Gmmcp tracker: Globally optimal generalized maximum multi clique problem for multiple object tracking",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dehghan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "M"
                    ],
                    "last": "Assari",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "4091--4099",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2015.7299036"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "The way they move: Tracking multiple targets with similar appearance",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dicle",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [
                        "I"
                    ],
                    "last": "Camps",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sznaier",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the IEEE international conference on computer vision",
            "volume": "",
            "issn": "",
            "pages": "2304--2311",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Scalable object detection using deep neural networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Erhan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Toshev",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2155--2162",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2014.276"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Object detection with discriminatively trained part-based models",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "F"
                    ],
                    "last": "Felzenszwalb",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "B"
                    ],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Mcallester",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "32",
            "issn": "9",
            "pages": "1627--1645",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Multi-object tracking with multiple cues and switcher-aware classification",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ouyang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1901.06129"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Video-based social behavior recognition based on kernel relevance analysis",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fern\u00e1ndez-Ram\u00edrez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "\u00c1lvarez-Meza",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pereira",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Orozco-Guti\u00e9rrez",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Castellanos-Dominguez",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Vis. Comput",
            "volume": "36",
            "issn": "8",
            "pages": "1535--1547",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Pets 2006 benchmark data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ferryman",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Sonar tracking of multiple targets using joint probabilistic data association",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Fortmann",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bar-Shalom",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Scheffe",
                    "suffix": ""
                }
            ],
            "year": 1983,
            "venue": "IEEE J. Ocean. Eng",
            "volume": "8",
            "issn": "3",
            "pages": "173--184",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Dssd : Deconvolutional single shot detector",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "Y"
                    ],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ranga",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Tyagi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Berg",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Fast r-cnn",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "2015 IEEE International Conference on Computer Vision (ICCV)",
            "volume": "",
            "issn": "",
            "pages": "1440--1448",
            "other_ids": {
                "DOI": [
                    "10.1109/ICCV.2015.169"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Donahue",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "580--587",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Sdmeasure: A social distancing detector",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kapil",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Kanahasabai",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Joshi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Joshi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "2020 12th International Conference on Computational Intelligence and Communication Networks (CICN)",
            "volume": "",
            "issn": "",
            "pages": "306--311",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Mask r-cnn",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Gkioxari",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "42",
            "issn": "2",
            "pages": "386--397",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2018.2844175"
                ]
            }
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "37",
            "issn": "9",
            "pages": "1904--1916",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Condensation-conditional density propagation for visual tracking",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Isard",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Blake",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Int. J. Comput. Vis",
            "volume": "29",
            "issn": "1",
            "pages": "5--28",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "New results in linear filtering and prediction theory",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "E"
                    ],
                    "last": "Kalman",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "S"
                    ],
                    "last": "Bucy",
                    "suffix": ""
                }
            ],
            "year": 1961,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Multiple hypothesis tracking revisited",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ciptadi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Rehg",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE international conference on computer vision",
            "volume": "",
            "issn": "",
            "pages": "4696--4704",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Coupled detection and trajectory estimation for multi-object tracking",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Leibe",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Schindler",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "2007 IEEE 11th International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Learning to associate: Hybridboosted multi-target tracker for crowded scene",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nevatia",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2953--2960",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2009.5206735"
                ]
            }
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Feature pyramid networks for object detection",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Hariharan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Belongie",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2117--2125",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Microsoft coco: common objects in context",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Maire",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Belongie",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hays",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Perona",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Zitnick",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "740--755",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Ssd: Single shot multibox detector",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Erhan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Reed",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "Y"
                    ],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Berg",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "21--37",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Multiple object tracking: a literature review",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xing",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Milan",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "K"
                    ],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Spatio-temporal feature using optical flow based distribution for violence detection",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Mabrouk",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Zagrouba",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Pattern Recognit. Lett",
            "volume": "92",
            "issn": "",
            "pages": "62--67",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Abnormal behavior recognition for intelligent video surveillance systems: a review",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Mabrouk",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Zagrouba",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Expert Syst. Appl",
            "volume": "91",
            "issn": "",
            "pages": "480--491",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "A proposal to ensure social distancing with deep learning-based object detection",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Mercaldo",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Martinelli",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Santone",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "2021 International Joint Conference on Neural Networks (IJCNN)",
            "volume": "",
            "issn": "",
            "pages": "1--5",
            "other_ids": {
                "DOI": [
                    "10.1109/IJCNN52387.2021.9534231"
                ]
            }
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Mot16: A benchmark for multi-object tracking",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Milan",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Leal-Taixe",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Reid",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Schindler",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "A survey of vision-based trajectory learning and analysis for surveillance",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "T"
                    ],
                    "last": "Morris",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Trivedi",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "IEEE Trans. Circuits Syst. Video Technol",
            "volume": "18",
            "issn": "8",
            "pages": "1114--1127",
            "other_ids": {
                "DOI": [
                    "10.1109/TCSVT.2008.927109"
                ]
            }
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "G-cnn: An iterative grid based object detector",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Najibi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rastegari",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "S"
                    ],
                    "last": "Davis",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "2369--2377",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2016.260"
                ]
            }
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Loitering detection using an associating pedestrian tracker in crowded scenes",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Nam",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Multimed. Tools Appl",
            "volume": "74",
            "issn": "9",
            "pages": "2939--2961",
            "other_ids": {
                "DOI": [
                    "10.1007/s11042-013-1763-7"
                ]
            }
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "A century of trends in adult human height",
            "authors": [
                {
                    "first": ")",
                    "middle": [],
                    "last": "Ncd-Risc",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "R F C"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "5",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.7554/eLife.13410"
                ]
            }
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Video representation and suspicious event detection using semantic technologies",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Merlino",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Bruneo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Puliafito",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Vyas",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ojha",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Semantic Web",
            "volume": "12",
            "issn": "3",
            "pages": "467--491",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Vehicle tracking and monitoring in surveillance video",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [
                        "P"
                    ],
                    "last": "Vyas",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ojha",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 IEEE Conference on Information and Communication Technology",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {
                "DOI": [
                    "10.1109/CICT48419.2019.9066256"
                ]
            }
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Pets 2016: Dataset and challenge",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Patino",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Cane",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vallee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ferryman",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "volume": "",
            "issn": "",
            "pages": "1240--1247",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPRW.2016.157"
                ]
            }
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Trajectory based unusual human movement identification for video surveillance system",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Rai",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Kolekar",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Keshav",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Mukherjee",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Progress in Systems Engineering",
            "volume": "",
            "issn": "",
            "pages": "789--794",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "You only look once: Unified, real-time object detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Divvala",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "779--788",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2016.91"
                ]
            }
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Yolo9000: Better, faster, stronger",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "6517--6525",
            "other_ids": {
                "DOI": [
                    "10.1109/CVPR.2017.690"
                ]
            }
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Yolov3: An incremental improvement",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.02767"
                ]
            }
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "An algorithm for tracking multiple targets",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Reid",
                    "suffix": ""
                }
            ],
            "year": 1979,
            "venue": "IEEE Trans. Autom. Control",
            "volume": "24",
            "issn": "6",
            "pages": "843--854",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Faster r-cnn: towards realtime object detection with region proposal networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "39",
            "issn": "6",
            "pages": "1137--1149",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2016.2577031"
                ]
            }
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Joint probabilistic data association revisited",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Rezatofighi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Milan",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dick",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Reid",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "3047--3055",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Performance measures and a data set for multi-target, multi-camera tracking",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ristani",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Solera",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zou",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Cucchiara",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Tomasi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision-ECCV 2016 Workshops",
            "volume": "",
            "issn": "",
            "pages": "17--35",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Versatile loitering detection based on non-verbal cues using dense trajectory descriptors",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "S"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Multimedia Tools and Applications",
            "volume": "78",
            "issn": "8",
            "pages": "10933--10963",
            "other_ids": {
                "DOI": [
                    "10.1007/s11042-018-6618-9"
                ]
            }
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Implementing a realtime, ai-based, people detection and social distancing measuring system for Covid-19",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Saponara",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Elhanashi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gagliardi",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "J. Real-Time Image Process",
            "volume": "",
            "issn": "",
            "pages": "1--11",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Dsod: Learning deeply supervised object detectors from scratch",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xue",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE International Conference on Computer Vision (ICCV)",
            "volume": "",
            "issn": "",
            "pages": "1937--1945",
            "other_ids": {
                "DOI": [
                    "10.1109/ICCV.2017.212"
                ]
            }
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Multi-object tracking with quadruplet convolutional neural networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Son",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Baek",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "5620--5629",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "Privacy-preserving ai-enabled video surveillance for social distancing: responsible design and deployment for public spaces",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sugianto",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Tjondronegoro",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Stockdale",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "I"
                    ],
                    "last": "Yuwono",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Information Technology & People",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Action-stage emphasized spatiotemporal vlad for video action recognition",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dauwels",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Image Process",
            "volume": "28",
            "issn": "6",
            "pages": "2799--2812",
            "other_ids": {
                "DOI": [
                    "10.1109/TIP.2018.2890749"
                ]
            }
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "A survey on activity recognition and behavior understanding in video surveillance",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vishwakarma",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Agrawal",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Vis. Comput",
            "volume": "29",
            "issn": "10",
            "pages": "983--1009",
            "other_ids": {}
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "Simple online and realtime tracking with a deep association metric",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Wojke",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bewley",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Paulus",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE international conference on image processing (ICIP)",
            "volume": "",
            "issn": "",
            "pages": "3645--3649",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nevatia",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Int. J. Comput. Vision",
            "volume": "75",
            "issn": "2",
            "pages": "247--266",
            "other_ids": {}
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "Efficient track linking methods for track graphs using network-flow and set-cover techniques",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "H"
                    ],
                    "last": "Kunz",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Betke",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "CVPR 2011",
            "volume": "",
            "issn": "",
            "pages": "1185--1192",
            "other_ids": {}
        },
        "BIBREF67": {
            "ref_id": "b67",
            "title": "Multi-object tracking through occlusions by local tracklets filtering and global tracklets association with detection responses",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xing",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ai",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lao",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1200--1207",
            "other_ids": {}
        },
        "BIBREF68": {
            "ref_id": "b68",
            "title": "Attentionnet: Aggregating weak directions for accurate object detection",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yoo",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Paek",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "S"
                    ],
                    "last": "Kweon",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "2015 IEEE International Conference on Computer Vision (ICCV)",
            "volume": "",
            "issn": "",
            "pages": "2659--2667",
            "other_ids": {
                "DOI": [
                    "10.1109/ICCV.2015.305"
                ]
            }
        },
        "BIBREF69": {
            "ref_id": "b69",
            "title": "Global data association for multiobject tracking using network flows",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nevatia",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "Reference-free video-to-real distance approximation-based urban social distancing analytics amid covid-19 pandemic",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zuo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kurkcu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ozbay",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "J. Transp. Health",
            "volume": "21",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Fig. 1 A proposed framework for activity detection and crowd behavior analysis",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "x \u03b4 and y \u03b4 . The bounding box of the object is represented with left, top, width and height of the object (denoted by o l , o t , o w and o h ). The horizontal and vertical coordinates of the center of the object are given by o x and o y . The state equation of Kalman filter is given by Eq. (1) of m components. Each state has four components, width, height, horizontal, and vertical pixel positions. After each iteration, we get a new set of q number of observations denoted by y containing quantities denoted by h(x). The h(x) depend non-linearly on the state, with observation errors \u00fd having a square covariance matrix (C o ) of size qxq.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Association of objects based on IoU threshold scenario corresponds to the possibility of crossing trajectories. However, in the case of multiple tracks with no change in direction, it may occur when tracks are moving very close with almost in a similar direction. 3. Multiple matches with direction update -When there",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Handling",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "False Negative -The number of target detections which are not associated with any tracks, but in actual have a valid association. It also referred as missed targets. -ID Sw [33] -The number of identity switches for all the objects by counting the total number of association of the same object with different tracks. -IDF1 [56] -The ratio of correctly identified detections with the average of ground truth and computed detections. -MOTA [44] -Multiple Object Tracking Accuracy (MOTA)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "j \u2190 1 to n do 9 if (i! = j)&&(o i pd == 0) then 10 o j x = o jle f t + o jright \u2212o jle f t 2 ; 11 o j y = o jtop + o jbottom \u2212o jtop 2 ; 12E D i j = (o j x \u2212 o i x ) 2 + (o j y \u2212 o iy ) 2 ; 13 if E D i j <= T h pd then 14 o i pd = 1;The set of frames of a video scene with tracked objects along with its bounding box are given as input.-The flag is set for the objects which do not follow the social distancing as per the algorithm 1. -The threshold for physical distancing is adaptive based on the width and height of the object. The objects which are near from camera have a larger size as compared",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Demonstration Demonstration of adaptive physical distancing algorithm in two possible scenario to farther objects due to changes in camera viewpoint. Thus, a common threshold cannot fit all the objects in a frame. However, the size of the person is proportional to the width and height of the object, irrespective of the location in the frame. -The violation in physical distancing is calculated by finding the Euclidean distance with all other objects denoted by E D i j and comparing it with the adaptive threshold based on the size of the object. -The threshold is kept as the sum of height and width of the person as the height of the person is around 5-6 feet, and width is 3 feet, sufficient and adaptive based on the location of the objects in the image. -Violation of physical distancing at temporal level is identified by identifying the physical distancing flag of all the objects included in the track. If even a single object in a track has a flag as set, the track is classified as violated physical distancing. -Total number of objects present in the frame is also calculated to identify the overall level of crowding of the place.-The objects that violate the physical distancing have the value of flag as one, while for others that follow physical distancing, the flag value is zero.The tracking id of an object is represented by t and goes up to T . The total number of frames is F, and a frame is represented using f . The number of objects in a frame is n and given byO. The pixel value of the center of the object is given by o x and o y . The width and height of the object are given by o width and o height . The bounding box of the object is represented by o le f t , o top , o right , and o bottom . The flag representing the physical distancing of an object and track is denoted by o pd and track pd .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Demonstration Demonstration of loitering detection based on the length of the track of the person on PETS 2016 1405 TRK2 dataset",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Fig. 13 Demonstration of physical distancing and actual distance on Venice-2 dataset",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": ". 2. Multiple matches with no direction update -In the case when there exists more than one match on I oU greater than I oU th , then the direction is calculated by associating the object with each track in from the list of tracks in which I oU is greater than I oU th . The direction of the track is updated based onTable 1. In this case, one or more tracks exist when the updated directions remain the same after associating the object with the tracks. If there is only one track where the direction is not changed, that track is selected; if multiple such tracks are found, then the track that resulted in maximum I oU is selected. This",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "North West:If D x is negative while D y is positive, and their magnitude is substantially high then this direction is assigned with id 6. 7. West:If D x is negative and only D x have substantial magnitude, then this direction is assigned with id 7. 8. South West: If D x and D y are negative and their magnitude is substantially high, then this direction is assigned with id 8. 9. No Change: If both D x and D y have substantially low magnitude, no change in direction is performed.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Detail description of various datasets used in this paper",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Trained model performs well over different dataset covering multiple scenarios and domain. The trained model is capable of detecting objects to generate tracking information.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Precision and Recall of detections on various datasets",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Evaluation of the approach on benchmark parameters",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Effect of \u03b3 on MOT benchmark parameter over Dataset D4",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "Analysis of loitering detection with versatile loitering[57]",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "Fig. 10 Demonstration of physical distancing and actual distance on AVG-TownCenter Dataset",
            "latex": null,
            "type": "table"
        },
        "TABREF10": {
            "text": "Average runtime of the framework in different configurations for one input frame",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}
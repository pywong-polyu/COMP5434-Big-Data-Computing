{
    "paper_id": "010a2ead4aed8fe864a1bfcabfc051acb6b6af22",
    "metadata": {
        "title": "UWB-gestures, a public dataset of dynamic hand gestures acquired using impulse radar sensors",
        "authors": [
            {
                "first": "Shahzad",
                "middle": [],
                "last": "Ahmed",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Dingyang",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Junyoung",
                "middle": [],
                "last": "Park",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Sung",
                "middle": [
                    "Ho"
                ],
                "last": "Cho",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "In the past few decades, deep learning algorithms have become more prevalent for signal detection and classification. To design machine learning algorithms, however, an adequate dataset is required. Motivated by the existence of several open-source camera-based hand gesture datasets, this descriptor presents UWB-Gestures, the first public dataset of twelve dynamic hand gestures acquired with ultrawideband (UWB) impulse radars. The dataset contains a total of 9,600 samples gathered from eight different human volunteers. UWB-Gestures eliminates the need to employ UWB radar hardware to train and test the algorithm. Additionally, the dataset can provide a competitive environment for the research community to compare the accuracy of different hand gesture recognition (HGR) algorithms, enabling the provision of reproducible research results in the field of HGR through UWB radars. Three radars were placed at three different locations to acquire the data, and the respective data were saved independently for flexibility.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Hand gesture recognition (HGR) provides a convenient and natural method of human-computer interaction. User-friendly interfaces for human-machine interactions can be built using hand gestures. In HGR, gesture data are first acquired using a suitable sensor, and then patterns within the acquired sensory signals are recognized to identify different hand movements. Several sensors exist for data acquisition, including wearable devices 1 , cameras 2 , and radar sensors. Recently, radar has been considered a key enabling technology for HGR due to its many benefits over other sensors; for example, radar is less prone to lightning conditions than camera-based sensors. In addition, radar-based HGR systems do not require wearable devices. Currently, several commercial devices (such as Google Pixel 4 smartphones) are equipped with built-in radar for HGR.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Deep learning algorithms have shown great potential for the recognition and classification of hand gestures. Recent studies demonstrated a high classification accuracy for ultra-wideband (UWB) radar-based hand gesture classification 3,4 , where several sliding and circular hand motions were classified using deep learning approaches. Similarly, researchers from Google designed and manufactured a miniature radar named Soli solely for hand gesture recognition and sensing using a random forest classifier driven by low-dimensional features 5 . Recently, Wang et al. 6 used the same Soli radar sensor 5 to classify eleven different gestures using a convolutional neural network (CNN)-based classifier. Another study 7 recognized six different gestures with radar sensors intended for vehicular and infotainment interfaces, and the classification output (class) was fed to an Android system to perform the desired operation based on gestures. Furthermore, a system called Radar Categorization for Input & Interaction (RadarCat) 8 was established to provide a set of applications, including gesture recognition-based random forest classifiers. Recently, Park et al. 9 focused on providing both radar hardware and a recognition algorithm for hand gestures; six different gestures were classified using long short-term memory (LSTM). The aforementioned studies suggest that machine learning-based solutions will enable radar sensors to have considerable positive impacts on HGR. However, machine learning algorithms are based on learning paradigms and therefore require some overhead, such as sufficient computing power and the need for a sufficiently large dataset to train the algorithm. Without an adequate hardware assembly and acquisition environment, it is often not possible to develop and test deep learning frameworks. Thus, to build HGR algorithms without purchasing hardware, a public dataset of hand gestures acquired through radar sensors is needed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Several (signal and image) datasets for training deep learning algorithms are available to the public for download. For instance, ImageNet 10 and LabelMe 11 provide large collections of images intended for use in visual object",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "recognition. These datasets eliminate the need to acquire images to test different machine learning algorithms and simultaneously provide a competitive platform for comparing the performance of different algorithms in similar environments. Recently, a small collection of governmental response events for COVID-19 12 was released, and over 13,000 policy announcements were made by the governments of 195 countries; this public dataset can be used to train a CNN for the detection of COVID-19. Similarly, PhysioBank 13 presents a collection of over 75 datasets containing samples of different biomedical signals, such as cardiopulmonary and neural signals, from both patients and healthy individuals. However, few vision-based hand-gesture datasets exist; among them are the Cambridge Hand Gesture Database (released in 2009) containing nine hundred sequences of images for nine different hand gesture classes 14 , MSRGesture3D (released in 2012) 15 and EgoGesture 16 (released in 2018). Furthermore, the RGBD-HuDaAct 17 dataset provides a human activity recognition dataset acquired with a video camera and a depth sensor. Pisharady and Serbeck 18 reported a comprehensive review of all available vision-based hand gesture datasets, and recently, a dataset of continuous-wave radar datasets for vital signs and heartbeats with six different human subjects recorded over 223 minutes was released 19 . However, no such public radar dataset exists for hand gestures. For all the studies regarding HGR with radar and other radio sensors, researchers first collect training data before testing their algorithms.",
            "cite_spans": [
                {
                    "start": 314,
                    "end": 316,
                    "text": "12",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 909,
                    "end": 911,
                    "text": "14",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 946,
                    "end": 948,
                    "text": "15",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 964,
                    "end": 966,
                    "text": "16",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1145,
                    "end": 1147,
                    "text": "18",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1395,
                    "end": 1397,
                    "text": "19",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "In this paper, we present the first-ever dataset of hand gestures collected using ultra-wideband (UWB) impulse radar. We expect that this dataset may eliminate the need to acquire data for algorithm testing and will provide a competitive environment for the research community to compare the accuracy of existing and newly proposed algorithms. The overall summary and the scope of this study are presented in Fig. 1 . Three different radar sensors operating independently in a monostatic configuration are deployed, and the data from each radar sensor are saved separately in the repository. Consequently, the evaluation of HGR algorithms can be performed either by using a single radar sensor or by exploiting signals from multiple radar sensors simultaneously. An application example of a CNN-based classifier, as explained in Fig. 1 , is also demonstrated in a subsequent section.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 409,
                    "end": 415,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 829,
                    "end": 835,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": ""
        },
        {
            "text": "Literature survey-based selection of hand gestures. First, we performed a brief literature review to select hand gestures since there is no existing standard for selecting radar sensor-collected hand gestures to test HGR algorithms. Researchers always select a gesture set randomly to evaluate their algorithms. However, studies suggest that the nature of gestures is usually the same, i.e., swiping, sliding, pushing and cyclic rotation, among other movements. For example, Kim and Tomajian 20 used 10 gestures of a similar nature to perform HGR using Doppler radar. Khan et al. 21 used UWB radar to classify five gestures comprising hand sliding and pointing gestures acquired from three different human volunteers. Similarly, Ryu et al. 22 constructed a feature-based gesture recognition algorithm and tested it on 7 hand gestures, including moving the hand left, right, up, down, clockwise, and counterclockwise and pushing the hand. Recent studies on radar sensor-based HGR 20-26 used gestures of a similar nature to test their algorithms. Nevertheless, although the gestures were similar in nature, the data acquisition environment and type of radar differed among each study. As a result, the performance evaluation of each new algorithm varies as a function of the recorded dataset. Moreover, in all of the aforementioned studies, the datasets used for training and evaluating the algorithms consisted of a small number of samples. For example, the dataset used in Fhager et al. 26 comprises only 180 samples per gesture. Ryu et al. 22 performed each of their 7 gestures only 15 times for training purposes. A small number of training samples and human participants may cause a machine learning algorithm to be biased and lead to overfitting given only the known data samples; as a result, the algorithm may not be robust enough against unknown test data samples. To cope with the aforementioned challenges, the presented dataset has the following features:",
            "cite_spans": [
                {
                    "start": 580,
                    "end": 582,
                    "text": "21",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 740,
                    "end": 742,
                    "text": "22",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1487,
                    "end": 1489,
                    "text": "26",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1541,
                    "end": 1543,
                    "text": "22",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "\u2022 UWB-Gestures contains most (if not all) of the previously used gestures in radar-based HGR studies, as there is no procedural standard for acquiring hand gestures. \u2022 Multiple volunteers were used to acquire the data to provide larger intragesture variations. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Hardware Software Fig. 1 Summary of the overall workflow: collection of the UWB-Gestures dataset using three UWB radars and an application example of classification with a DCNN.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 18,
                    "end": 24,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Radar Radar"
        },
        {
            "text": "\u2022 Multiple (three) radar sensors were used for data acquisition, and the data of each radar sensor are separately accessible to provide flexibility in terms of preferences for hardware placement and number of radar sensors. Multiple radar sensors also permit hand tracking and trajectory prediction.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Radar Radar"
        },
        {
            "text": "In this paper, we present the first-ever public dataset (called UWB-Gestures) of twelve different dynamic hand gestures, as presented in Fig. 2 (a-l), where the gestures were acquired with UWB impulse radar. The modality of the video of each of the performed gestures is available at (http://casp.hanyang.ac.kr/uwbgestures). We selected eight swiping (sliding) gestures, as shown in Fig. 1 Fig. 2 (i-k) present clockwise rotation (CW), counterclockwise (CCW) rotation, and an inward push gesture. One empty gesture was added for each user to permit gesture and nongesture classification. The hands shown in Fig. 2 represent the starting point of each gesture. As stated earlier, these gestures were carefully selected based on the preferences of other researchers.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 137,
                    "end": 143,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 383,
                    "end": 389,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 390,
                    "end": 396,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 607,
                    "end": 613,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Radar Radar"
        },
        {
            "text": "participants. The data were gathered from eight different participants to introduce more intragesture variations. Table 1 presents the details of each individual participant that can be used for analyzing intragesture and intergesture variations with respect to different hand sizes. The average age of the participants was 25.75 years old, and the average body mass index (BMI) was 22.19 \u00b1 5 kg/m 2 . Although most of the human participants involved in the data acquisition process were from research occupations, they were provided with basic training prior to data acquisition. Data acquisition hardware. For data acquisition, we selected the XeThru X4 UWB impulse radar sensor from Novelda (Norway) due to its small size and extensive usage in different radar sensor-based applications, such as gesture recognition 3,21,25 , vital sign monitoring 27, 28 and automobiles 29 . The detailed technical specifications of the radar sensor are listed in Table 2 . As shown in Table 2 , the Novelda radar sensor is a UWB impulse radar sensor with a bandwidth of 2 GHz centered at a frequency of 8.745 GHz. The connectivity diagram is shown in",
            "cite_spans": [
                {
                    "start": 851,
                    "end": 854,
                    "text": "27,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 855,
                    "end": 857,
                    "text": "28",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 874,
                    "end": 876,
                    "text": "29",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [
                {
                    "start": 114,
                    "end": 121,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 951,
                    "end": 958,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 973,
                    "end": 980,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Radar Radar"
        },
        {
            "text": "Empty gesture",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Radar Radar"
        },
        {
            "text": "Inward push www.nature.com/scientificdata www.nature.com/scientificdata/ Fig. 3(a) , which demonstrates that the Novelda radar sensor consists of a pair of transmitter (Tx) and receiver (Rx) antennas and a DSP microcontroller that is further connected to a host computer, where MATLAB is used to collect and save the data. The front and back of the complete radar chip are shown in Fig. 3 (b). The dataset was collected at Hanyang University, Seoul, South Korea.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 73,
                    "end": 82,
                    "text": "Fig. 3(a)",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 382,
                    "end": 388,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Radar Radar"
        },
        {
            "text": "Unlike traditional narrowband radar sensors, UWB radar transmits a signal with a wide frequency spectrum for a very short duration. For every sequence of transmitted impulse-like signals, the corresponding received signal x[n] consists of the reflected signal from N different paths and an additive noise term 30 . As a result, the received UWB signal is a linear combination of these N delayed and distorted signals and can be represented by:",
            "cite_spans": [
                {
                    "start": 310,
                    "end": 312,
                    "text": "30",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Radar Radar"
        },
        {
            "text": "represents the estimate of the transmitted pulse shape received at the receiver that is usually distorted due to several different factors, such as the reflection, refraction and scattering coefficients of objects, and N represents additive noise. In addition, a ni and T i represent the scaling factor and duration, respectively, of the signal.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Radar Radar"
        },
        {
            "text": "The terms N and K in Eq. (1) represent the rows and columns of the 2D received radar signal matrix, known as the fast time and slow time, respectively 30 . Here, the fast time (rows) of the radar signal matrix expresses the distance of the hand from the radar, while the slow time (columns) represents the frames transmitted by the radar (the duration of the hand gesture). The signal represented in Eq. (1) contains both the reflections from the target (hand) and the unwanted reflections from static objects within the operational area of the radar sensor, such as the human body. These unwanted reflections are usually termed clutter. The final 2D matrix containing the received hand reflections for one single gesture movement against an individual radar signal can be expressed as: www.nature.com/scientificdata www.nature.com/scientificdata/ Data acquisition setup. The conceptual acquisition setup is presented in Fig. 4(a) , which comprises 3 radars named R L , R T and R R (placed at the left, top and right sides, respectively, of the experimental setup). All three radars operate independently in a monostatic configuration, and signal transmission and reception are performed independently by each radar. The gestures were performed in the middle of all three radars. The distance between the left and right radars was 1.1 meters, and the distance between the midpoint of the horizontal radars and the top radar was 0.55 meters. Figure 4(b) illustrates the actual experimental setup and the data matrices against all three radars for gesture 1 (LR swipe). As shown in Fig. 4(b) , as the hand moves from left to right, the target signal of the left radar (R L ) can be seen moving away from it. In contrast, the target signal can be seen moving towards R R . For the case of the radar used, a distance of 1 meter contains 156 fast times. Every sample consists of 90 slow time frames, which is equivalent to a duration of 4.5 seconds.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 921,
                    "end": 930,
                    "text": "Fig. 4(a)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1441,
                    "end": 1452,
                    "text": "Figure 4(b)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1580,
                    "end": 1589,
                    "text": "Fig. 4(b)",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Radar Radar"
        },
        {
            "text": "The UWB-Gestures dataset is available for download at Figshare 31 (https://doi.org/10.6084/ m9.figshare.12652592). The data are placed in two separate folders to comply with the file size limit. Since the data were gathered using MATLAB, the stored files are MAT files. Additionally, to ensure license-free distribution of the dataset, we converted the dataset to a comma-separated values (CSV) file. For clarity, the modality video of each gesture is available on our homepage. The structure of the data descriptor is shown in Fig. 5 . The dataset contains eight directories corresponding to each of the individual participants listed in Table 1 . Each folder also contains two directories containing the raw data and clutter-removed data. The raw data comprise the recorded gestures in raw form, whereas the clutter-removed data consist of a preprocessed version of the raw data. During preprocessing, the clutter is estimated using the loopback filter 29 based on the following principle:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 528,
                    "end": 534,
                    "text": "Fig. 5",
                    "ref_id": null
                },
                {
                    "start": 639,
                    "end": 646,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Data Records"
        },
        {
            "text": "n n n where c represents the clutter term, which is extracted using the previously estimated clutter and the current received radar signal x[n], and the alpha (\u03b1) term represents the weighting factor that controls the learning rate of the filter. Particularly, for our dataset, alpha was chosen as 0.9. The estimated clutter c is then subtracted from the received radar signal x to obtain the clutter-free output y. www.nature.com/scientificdata www.nature.com/scientificdata/ Note that all the variables representing each radar value are saved as separate CSV files, resulting in three times more CSV files than MAT files. All the samples of each gesture are placed in a 2D file with the fast time on the horizontal axis and the slow time on the vertical axis. As stated above, every group of 90 slow-time values constitutes 1 gesture sample. A MATLAB script to access and view the hand gesture samples is also included and discussed in a later section in detail.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Records"
        },
        {
            "text": "technical Validation signal pattern analysis. Figure 6 presents the signal patterns for all the remaining gestures. As seen in Fig. 6 , each hand gesture movement corresponds to a distinctive pattern. As a practical example, for the LR swipe and RL swipe cases, the right and left radar sensors show opposite patterns, whereas radar 3 shows a straight vertical line. On the other hand, for the UD swipe and DU swipe cases, the left and right radars show a straight vertical line pattern, while radar 3 showed a varying pattern. Similar variations can be observed for each gesture. Similarly, Fig. 6 (i) and (j) present the radar images corresponding to clockwise and counterclockwise rotational gestures, respectively. www.nature.com/scientificdata www.nature.com/scientificdata/ example with a cNN-based classifier. To provide an example of applying the proposed dataset, we implement a novel DCNN-based classifier with four hidden layers, as shown in Fig. 7 . The radar data matrix is converted into images, and these images are fed as input to the DCNN architecture. We perform classification using only a single radar sensor (left radar). Consequently, the size of the input layer to the DCNN is 90 \u00d7 189 (fast time \u00d7 slow time of the single radar data matrix). We employ a 3 \u00d7 3 convolutional filter for each of the four hidden layers. The learning rate is set to 0.01, and 30 epochs are used for training purposes. Table 3 presents the classification accuracy of the 4-layer DCNN algorithm shown in Fig. 7 with input from only the left radar sensor (R L ). The first column of each row represents the original class, whereas the first row represents the predicted class of gestures. The diagonal values represent the overall success rate, and the values found elsewhere are the classification errors. In Table 3 , the diagonal terms representing the success rate are marked in bold for clarity. The classifier based on the 4-layer DCNN architecture yielded an accuracy of 94% for the single radar sensor.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 46,
                    "end": 54,
                    "text": "Figure 6",
                    "ref_id": null
                },
                {
                    "start": 127,
                    "end": 133,
                    "text": "Fig. 6",
                    "ref_id": null
                },
                {
                    "start": 592,
                    "end": 598,
                    "text": "Fig. 6",
                    "ref_id": null
                },
                {
                    "start": 953,
                    "end": 959,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1421,
                    "end": 1428,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 1505,
                    "end": 1511,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1810,
                    "end": 1817,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Data Records"
        },
        {
            "text": "The files uploaded to Figshare 31 also contain two independent programs for data visualization and for the example demonstration of the DCNN network presented in the above section. The first program can be used to generate the distance vs. time graph for user 1, as shown in Fig. 6 . The dataset, its subfolders and the code (having the 'm' file extension) are extracted to the same directory where the code can be executed. After running the code, the userinterface instructions and comments in the code can be used to plot the distance-time (fast time vs. slow time) samples of the hand gestures. The same program can be used to plot the graphs for the other human volunteers as well. The second program is uploaded to a separate directory called \"Exemplary CNN demonstration\", which can generate results similar to those shown in Table 3 . Note that the exact accuracy may vary across trials. Overall classification accuracy for a single radar 94% Table 3 . Classification accuracy of the CNN-based classifier.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 275,
                    "end": 281,
                    "text": "Fig. 6",
                    "ref_id": null
                },
                {
                    "start": 833,
                    "end": 840,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 951,
                    "end": 958,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Code availability"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Wearable sensor-based hand gesture and daily activity recognition for robot-assisted living",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Sheng",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans",
            "volume": "41",
            "issn": "",
            "pages": "569--573",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Depth camera-based 3D hand gesture controls with immersive tactile feedback for natural mid-air gesture interactions",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Sensors",
            "volume": "15",
            "issn": "",
            "pages": "1022--1046",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Finger-counting-based gesture recognition within cars using impulse radar with convolutional neural network",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ghaffar",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hussain",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Sensors",
            "volume": "19",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Robust gesture recognition using millimetric-wave radar system",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hazra",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Santra",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE sensors letters",
            "volume": "2",
            "issn": "",
            "pages": "1--4",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Soli: ubiquitous gesture sensing with millimeter wave radar",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lien",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ACM Transactions on Graphics (TOG)",
            "volume": "35",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lien",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Poupyrev",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Hilliges",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 29th Annual Symposium on User Interface Software and Technology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Gesture recognition using mm-wave sensor for human-car interface",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "A"
                    ],
                    "last": "Smith",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Csech",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Murdoch",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Shaker",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Sensors Letters",
            "volume": "2",
            "issn": "",
            "pages": "1--4",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Radar categorization for input & interaction",
            "authors": [
                {
                    "first": "H.-S",
                    "middle": [],
                    "last": "Yeo",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Flamich",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Schrempf",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Harris-Birtill",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Quigley",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Radarcat",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 29th Annual Symposium on User Interface Software and Technology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A Time Domain Artificial Intelligence Radar System Using 33-GHz Direct Sampling for Hand Gesture Recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Journal of Solid-State Circuits",
            "volume": "55",
            "issn": "",
            "pages": "879--888",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Imagenet: A large-scale hierarchical image database",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "248--255",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "LabelMe: a database and web-based tool for image annotation",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "C"
                    ],
                    "last": "Russell",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Torralba",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "P"
                    ],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "T"
                    ],
                    "last": "Freeman",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "International journal of computer vision",
            "volume": "77",
            "issn": "",
            "pages": "157--173",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Covid-19 government response event dataset (coronanet v. 1.0)",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Barcel\u00f3",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Hartnett",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kubinec",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Messerschmidt",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature human behaviour",
            "volume": "4",
            "issn": "",
            "pages": "756--768",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. circulation",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Goldberger",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "",
            "volume": "101",
            "issn": "",
            "pages": "215--220",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Canonical correlation analysis of video volume tensors for action categorization and detection",
            "authors": [
                {
                    "first": "T.-K",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Cipolla",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "31",
            "issn": "",
            "pages": "1415--1428",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Learning hierarchical 3D kernel descriptors for RGB-D action recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Satarboroujeni",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision and Image Understanding",
            "volume": "144",
            "issn": "",
            "pages": "14--23",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Egogesture: a new dataset and benchmark for egocentric hand gesture recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Transactions on Multimedia",
            "volume": "20",
            "issn": "",
            "pages": "1038--1050",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A unified framework for multi-modal isolated gesture recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "Z"
                    ],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "ACM Transactions on Multimedia Computing",
            "volume": "14",
            "issn": "",
            "pages": "1--16",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Recent methods and databases in vision-based hand gesture recognition: A review",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "K"
                    ],
                    "last": "Pisharady",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Saerbeck",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Computer Vision and Image Understanding",
            "volume": "141",
            "issn": "",
            "pages": "152--165",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "A dataset of radar-recorded heart sounds and vital signs including synchronised reference sensor signals",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Scientific data",
            "volume": "7",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Hand gesture recognition using micro-Doppler signatures with convolutional neural network",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Toomajian",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Access",
            "volume": "4",
            "issn": "",
            "pages": "7125--7130",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Hand-based gesture recognition for vehicular applications using IR-UWB radar",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "K"
                    ],
                    "last": "Leem",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Sensors",
            "volume": "17",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Feature-based hand gesture recognition using an FMCW radar and its temporal feature analysis",
            "authors": [
                {
                    "first": "S.-J",
                    "middle": [],
                    "last": "Ryu",
                    "suffix": ""
                },
                {
                    "first": "J.-S",
                    "middle": [],
                    "last": "Suh",
                    "suffix": ""
                },
                {
                    "first": "S.-H",
                    "middle": [],
                    "last": "Baek",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                },
                {
                    "first": "J.-H",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE sensors Journal",
            "volume": "18",
            "issn": "",
            "pages": "7593--7602",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Dynamic continuous hand gesture recognition using FMCW radar sensor",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Latern",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Sensors Journal",
            "volume": "18",
            "issn": "",
            "pages": "3278--3289",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Hand-gesture recognition using two-antenna doppler radar with deep convolutional neural networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Skaria",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Al-Hourani",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lech",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Evans",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Sensors Journal",
            "volume": "19",
            "issn": "",
            "pages": "3041--3048",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Hand Gesture Recognition Using an IR-UWB Radar with an Inception Module-Based Classifier",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Sensors",
            "volume": "20",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Pulsed Millimeter Wave Radar for Hand Gesture Sensing and Classification",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "O"
                    ],
                    "last": "Fhager",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Heunisch",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Dahlberg",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Evertsson",
                    "suffix": ""
                },
                {
                    "first": "L.-E",
                    "middle": [],
                    "last": "Wernersson",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Sensors Letters",
            "volume": "3",
            "issn": "",
            "pages": "1--4",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Preclinical evaluation of a noncontact simultaneous monitoring method for respiration and carotid pulsation using impulse-radio ultra-wideband radar",
            "authors": [
                {
                    "first": "J.-Y",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Scientific reports",
            "volume": "9",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "A pilot study of impulse radio ultra wideband radar technology as a new tool for sleep assessment",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pallesen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Journal of Clinical Sleep Medicine",
            "volume": "14",
            "issn": "",
            "pages": "1249--1254",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Lane detection method with impulse radio ultra-wideband radar and metal lane reflectors",
            "authors": [
                {
                    "first": "D.-H",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Sensors",
            "volume": "20",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Bi-directional passing people counting system based on IR-UWB radar sensors",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "W"
                    ],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Quan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Internet of Things Journal",
            "volume": "5",
            "issn": "",
            "pages": "512--522",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "UWBgesture, a public Dataset of Dynamic Hand-gestures Acquired using Impulse-radar sensors",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Dingyang",
                    "suffix": ""
                },
                {
                    "first": "J.-Y",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.6084/m9.figshare.12652592"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "This research was supported by the Bio & Medical Technology Development Program of the National Research Foundation (NRF) funded by the Korean government (MSIT",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Gesture vocabulary: (a-l) show the twelve selected hand gestures for the UWB-Gestures dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "(a) Connectivity diagram: (a) UWB radar sensor and (b) front and back of the UWB radar sensor. (2021) 8:102 | https://doi.org/10.1038/s41597-021-00876-0",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "As an example demonstrating how to access the subfiles in the folders, the link to access the files containing all the clutter-removed samples of gesture 4 (DU swipe) performed by human volunteer 2 is shown below:\\UWB-Gestures\\HV_02\\ClutterRemovedData_HV02\\HV_02_G4_ClutterRemoved.matHere, HV_02 refers to human volunteer 2, and G4 refers to gesture 4. The final MAT files containing separate variables corresponding to the three different radar systems are expressed as follows:\u2022 Left Radar: HV2_G4_RadarLeft_ClutterRemoved_100samples. \u2022 Top Radar: HV2_G4_RadarTop_ClutterRemoved_100samples. \u2022 Right Radar: HV2_G4_RadarRight_ClutterRemoved_100samples.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "(a) Conceptual diagram of the hardware environment consisting of three radar sensors operating independently in a monostatic configuration; (b) demonstration of gesture 1 (LR swipe) in an actual experimental environment along with the signal patterns of all three radars.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The structure of the UWB-Gestures dataset. Data matrices for each radar sensor generated for all the performed gesture samples.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Example of the DCNN-based classifier for the proposed data descriptor.",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "Table 1. Details of the included human volunteers.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "We adjusted the range of radar to 1.2 meters, yielding 189 fast-time samples.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Technical specifications of the used radar.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Received: 24 July 2020; Accepted: 19 February 2021; Published: xx xx xxxx",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors declare no competing interests.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Competing interests"
        },
        {
            "text": "Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.The Creative Commons Public Domain Dedication waiver http://creativecommons.org/publicdomain/zero/1.0/ applies to the metadata files associated with this article.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "additional information"
        }
    ]
}
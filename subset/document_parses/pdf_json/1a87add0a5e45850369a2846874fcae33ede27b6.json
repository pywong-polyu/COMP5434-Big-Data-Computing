{
    "paper_id": "1a87add0a5e45850369a2846874fcae33ede27b6",
    "metadata": {
        "title": "Construction material classification on imbalanced datasets for construction monitoring automation using Vision Transformer (ViT) architecture",
        "authors": [
            {
                "first": "Maryam",
                "middle": [],
                "last": "Soleymani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Art University of Tehran",
                    "location": {
                        "settlement": "Tehran",
                        "country": "Iran"
                    }
                },
                "email": "m.soleymani.pm@gmail.com"
            },
            {
                "first": "Mahdi",
                "middle": [],
                "last": "Bonyani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Tabriz",
                    "location": {
                        "settlement": "Tabriz",
                        "country": "Iran"
                    }
                },
                "email": "m_bonyani96@ms.tabrizu.ac.ir"
            },
            {
                "first": "Hadi",
                "middle": [],
                "last": "Mahami",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Art University of Tehran",
                    "location": {
                        "settlement": "Tehran",
                        "country": "Iran"
                    }
                },
                "email": "hadi.mahamipm@gmail.com"
            },
            {
                "first": "Farnad",
                "middle": [],
                "last": "Nasirzadeh",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Deakin University",
                    "location": {
                        "settlement": "Geelong",
                        "country": "Australia"
                    }
                },
                "email": "farnad.nasirzadeh@deakin.edu.au"
            }
        ]
    },
    "abstract": [
        {
            "text": "Nowadays, automation is a critical topic due to its significant impacts on the productivity of construction projects. Utilizing automation in this industry brings about great results, such as remarkable improvements in the efficiency, quality, and safety of construction activities. The scope of automation in construction includes a wide range of stages, and monitoring construction projects is no exception. Additionally, it is of great importance in project management since an accurate and timely assessment of project progress enables managers to quickly identify deviations from the schedule and take the required actions at the right time. In this stage, one of the most important tasks is to daily keep track of the project progress, which is very time-consuming and labor-intensive, but automation has facilitated and accelerated this task. It also eliminated or at least decreased the risk of many dangerous tasks. In this way, the first step of construction automation is to detect used materials in a project site automatically. In this paper, a novel deep learning architecture is utilized, called Vision Transformer (ViT), for detecting and classifying construction materials. To evaluate the applicability and performance of the proposed method, it is trained and tested on three large imbalanced datasets, namely Construction Material Library (CML) and Building Material Dataset (BMD), used in the previous papers, as well as a new dataset created by combining them. The achieved results revealed an accuracy of 100 percent in all parameters and also in each material category. It is believed that the proposed method provides a novel and robust tool for detecting and classifying different material types.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Efficient and effective progress monitoring practices provide information regarding performance deviations to the execution plan , which help the project management office (PMO) towards timely implementation of control actions to minimize the negative impacts [1, 2] . Therefore, project progress monitoring and controlling is one of the most important tasks of construction project management [3, 4] . Construction progress monitoring is currently performed mostly manually, in a labor-intensive and error-prone non-automated process [5] . Accurate and efficient tracking, analysis, and visualization of the as-built (actual) status of buildings under construction are critical components of successful project monitoring [6] . The majority of construction management and monitoring processes are still conducted traditionally with the use of 2D drawings, reports, schedules, and photo logs, making the process complicated and inefficient [7] . Automation of construction progress monitoring aids to improve the accuracy of progress monitoring and controlling. For this purpose, different approaches and systems have been used on construction sites including web-based technologies, cloud computing, Building Information Modelling (BIM), photogrammetry, videogrammetry, laser scanner, and material recognition [4, [7] [8] [9] [10] [11] [12] [13] .",
            "cite_spans": [
                {
                    "start": 260,
                    "end": 263,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 264,
                    "end": 266,
                    "text": "2]",
                    "ref_id": null
                },
                {
                    "start": 394,
                    "end": 397,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 398,
                    "end": 400,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 535,
                    "end": 538,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 723,
                    "end": 726,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 940,
                    "end": 943,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1311,
                    "end": 1314,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1315,
                    "end": 1318,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1319,
                    "end": 1322,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1323,
                    "end": 1326,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1332,
                    "end": 1336,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1337,
                    "end": 1341,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1342,
                    "end": 1346,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the last two decades, advancements in computer processes and digital camera technologies have allowed the construction sector to effectively process and retrieve valuable information from video clips and digital images [1, 14] . In the contemporary era, applications of image processing and computer vision techniques like machine learning and deep learning are considered as an intensive area of research with steady growth in the Architecture, Engineering, Construction, and Facilities Management (AEC/FM) industry [14, 15] . The core technique of image-based methods focuses on using the visual features of building materials such as color, texture, roughness, and projection [3, [6] [7] [8] for automatic classification [15] [16] [17] [18] [19] . In recent years, advancement in deep learning has enabled researchers to develop robust tools for analyzing images at remarkable accuracies [20] . But, Image classification is still a wellknown machine vision problem [11, [21] [22] [23] [24] .",
            "cite_spans": [
                {
                    "start": 222,
                    "end": 225,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 226,
                    "end": 229,
                    "text": "14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 520,
                    "end": 524,
                    "text": "[14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 525,
                    "end": 528,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 682,
                    "end": 685,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 686,
                    "end": 689,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 690,
                    "end": 693,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 694,
                    "end": 697,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 727,
                    "end": 731,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 732,
                    "end": 736,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 737,
                    "end": 741,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 742,
                    "end": 746,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 747,
                    "end": 751,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 894,
                    "end": 898,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 971,
                    "end": 975,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 976,
                    "end": 980,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 981,
                    "end": 985,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 986,
                    "end": 990,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 991,
                    "end": 995,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the current study, we have utilized Vision Transformer (ViT) architecture as a reliable classifier to maximize the accuracy of construction material detection. It must be noted that the variety, quality, and quantity of the employed data for training models have always been the key factors to evaluate the robustness of the implemented architecture, which were fulfilled effectively in this research since all of them were considered though using different imbalanced datasets. In the end, the best result, a 100% accuracy rate, was achieved, which has been unprecedented to get such a result for all of the parameters and also all material classes. As such, Vision Transformer classifier provides a new and promising perspective for automatic building material classification, in support of automated progress monitoring.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the following, prior studies on material recognition and classification are briefly reviewed. The datasets are introduced in Section 3. Then, the methodology and implementation details are described in Section 4. Section 5 is allocated to the results and discussion representing validation experiments on three building material datasets to validate the accuracy and robustness of the introduced method. Lastly, the conclusion and suggestions for future work are presented (Section 6, 7 respectively).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Currently, there are growing interests and trends in modeling the digital 3D world [25] . In addition, in recent decades, building information modeling (BIM) has become a very popular technology in the civil engineering and asset management industries [26, 27] . In the context of project management automation, there have been several attempts to improve the process of scan-to-BIM. Computer vision techniques have been widely applied in construction, from fundamental tasks to high-level applications, promoting automation in construction and yield continuous improvements in safety, quality, and productivity [28] . In terms of an applicationoriented framework for scan-to-BIM, which is proposed by Wang et al., identification of information requirements is the first major step of a scan-to-BIM process since the elements need their respective material information to be recognized [29] . Therefore, automatically identifying construction materials with images from construction sites has attracted a lot of interest in the last two decades. This rising interest is expected from the importance and utility of automation for a wide range of construction applications, from the generation of a 3D as-built model, such as scan-to-BIM process, to automated project monitor and control [17] . In this regard, the related research is reviewed and the results are briefly mentioned. Furthermore, the summary is presented in Table 1 .",
            "cite_spans": [
                {
                    "start": 83,
                    "end": 87,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 252,
                    "end": 256,
                    "text": "[26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 257,
                    "end": 260,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 612,
                    "end": 616,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 886,
                    "end": 890,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1286,
                    "end": 1290,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 1422,
                    "end": 1429,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Background and related work"
        },
        {
            "text": "Building material detection was first proposed by Brilakis et al. (2005) to recover images and identify building elements from two-dimensional images [30, 31] . Then, Zhu and Brilakis presented a method of identifying concrete material regions using machine learning techniques [32] . Ara\u00fajo et al. described a methodology that uses functional machine learning techniques to automatically classify granite at various processing stages based on spectral information captured by a spectrophotometer [33] . Kim et al. proposed an automated color model-based concrete detection method via analyzing the performance of three machine learning algorithms (Gaussian Mixture Modelling (GMM), Artificial Neural Network (ANN) model, and Support Vector Machine (SVM)) [34] . Son et al. introduced the use of a heterogeneous ensemble classifier to improve the detection accuracy of major construction materials such as concrete, steel, and wood on construction sites [17] . Yazdi and Sarafrazi proposed an automated system for the segmentation of concrete images into microstructures using texture analysis via an ANN classifier [35] .",
            "cite_spans": [
                {
                    "start": 50,
                    "end": 72,
                    "text": "Brilakis et al. (2005)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 150,
                    "end": 154,
                    "text": "[30,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 155,
                    "end": 158,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 278,
                    "end": 282,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 497,
                    "end": 501,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 504,
                    "end": 514,
                    "text": "Kim et al.",
                    "ref_id": null
                },
                {
                    "start": 756,
                    "end": 760,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 954,
                    "end": 958,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Background and related work"
        },
        {
            "text": "Han and Golparvar-Fard presented two methods for sampling and recognizing construction material from image-based point cloud data and inferring progress using a statistical representation from the material classification [30] . Dimitrov and Golparvar-Fard presented a robust material classification method (a multiple one-vs.-all x 2 kernel SVM classifier) for semantically-rich as-built 3D modeling and construction monitoring purposes [18] . Han and Golparvar-Fard presented a new appearance-based material classification method for operation-level monitoring of construction progress using BIM and daily construction photologs [16] . Han et al. introduced an improved appearance-based progress monitoring method by removing occlusion; then, they have used the SVM algorithm to classify materials [36] . Yang et al. proposed an image-based 3D modeling and recognizing its surface materials from images taken from various points of view; in this research, SVM algorithm has been used [37] . Rashidi et al. conducted a comparison study to evaluate the performance of different machine learning techniques (Multilayer Perceptron (MLP), Radial Basis Function (RBF), and SVM) for detection of three common categorists of building materials: Concrete, red brick, and OSB boards [14] . Degol et al. employed 3D geometry features with state-of-the-art material classification 2D features and find that both jointly and independently modeling 3D geometry improve mean classification accuracy [38] . Jiang et al. explored the application of a convolutional neural network (CNN) in classifying and identifying asphalt mixtures using the sectional images obtained from the X-ray computed tomography (CT) method [39] . Lee and Park designed and implemented a system that exhibited improved performance on the detection of reinforcing bars [40] . Bunrit et al. proposed an automatic feature extraction method by a novel CNN in transfer learning technique for construction material image classification task [41] . In another study, Bunrit et al. investigated the transfer learning of GoogleNet and ResNet101 that pre-trained on the ImageNet dataset (source task) with a task specific on construction material images classification [42] . Yuan et al. introduced a terrestrial laser scanner (TLS) data-based method for classifying common building materials in which material reflectance, HSV colors(Hue, Saturation, Value), and surface roughness are used as classification features [15] . Mahami et al. employed a new deep learning technique (VGG16) to detect the type of different construction materials accurately [11] .",
            "cite_spans": [
                {
                    "start": 221,
                    "end": 225,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 437,
                    "end": 441,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 630,
                    "end": 634,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 799,
                    "end": 803,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 806,
                    "end": 817,
                    "text": "Yang et al.",
                    "ref_id": null
                },
                {
                    "start": 985,
                    "end": 989,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 1274,
                    "end": 1278,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1485,
                    "end": 1489,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 1701,
                    "end": 1705,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 1835,
                    "end": 1848,
                    "text": "Bunrit et al.",
                    "ref_id": null
                },
                {
                    "start": 1995,
                    "end": 1999,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 2219,
                    "end": 2223,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 2468,
                    "end": 2472,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 2602,
                    "end": 2606,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Background and related work"
        },
        {
            "text": "Utilizing only proprioceptive force data acquired from an autonomous digging system and three machine learning algorithms (KNN(K Nearest Neighbor), ANN, k-means), Fernando and Marshal presented a classification methodology for excavation material identification [43] . Davis et al. designed and described a deep CNN to identify seven typical Construction and Demolition Waste (C&DW) classifications (both single and mixed disposal) using digital images of waste deposited in a construction site bin [44] . Alaloul and Qureshi used an ANN model to classify some construction materials [1] . The literature review indicated that material recognition had been regarded in many studies, but some shortcomings still need to be addressed. Some of the previous studies were restricted to just one specific material [32-35, 39, 40, 43] or at least a limited number of material categories [1, 14, 17, 37, 41, 42] . Furthermore, some studies employed tiny datasets, that is to say, a few images and sometimes containing just one material category [35] . It must be noted that tiny datasets could lead to biased training since the performance of the trained model can vary for new material from the same class but with a different color or shape [44] . And, results may be affected due to the lack of a sufficient variety of light conditions in the images of the dataset too [14] .",
            "cite_spans": [
                {
                    "start": 262,
                    "end": 266,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 499,
                    "end": 503,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 584,
                    "end": 587,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 808,
                    "end": 827,
                    "text": "[32-35, 39, 40, 43]",
                    "ref_id": null
                },
                {
                    "start": 880,
                    "end": 883,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 884,
                    "end": 887,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 888,
                    "end": 891,
                    "text": "17,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 892,
                    "end": 895,
                    "text": "37,",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 896,
                    "end": 899,
                    "text": "41,",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 900,
                    "end": 903,
                    "text": "42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 1037,
                    "end": 1041,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 1235,
                    "end": 1239,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Background and related work"
        },
        {
            "text": "Additionally, the accuracy rates resulted from other studies whose datasets were more acceptable, in terms of the number of images and classes of materials, were 90.8% to 97.35% [11, 15, 16, 18, 30, 36, 38] . The biggest challenge of these studies is related to some materials with similar shape and texture. For instance, in an experiment, grass, form work, and brick have been detected with the most accuracy rate due to their unique pattern or texture; nevertheless, soil, cement, and concrete are recognized with much error owing to their similar shapes and textures [18] . In another study, gravel, sand, cement-granular, and asphalt have been problematic in achieving a 100% accuracy rate [11] . Consequently in this research, some of the acceptable datasets of prior studies are utilized to train, test, and validate ViT model as a reliable classifier to maximize the accuracy rate of material recognition; and finally, the comparison among results is presented.",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 182,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 183,
                    "end": 186,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 187,
                    "end": 190,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 191,
                    "end": 194,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 195,
                    "end": 198,
                    "text": "30,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 199,
                    "end": 202,
                    "text": "36,",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 203,
                    "end": 206,
                    "text": "38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 571,
                    "end": 575,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 695,
                    "end": 699,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Background and related work"
        },
        {
            "text": "In this study, first, two specific datasets, used be previous studies, were utilized for training and evaluating Vision transformer (ViT) model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "The first is the dataset of [11] created by Mahami et al. (henceforth referred to as Building Material Dataset abbreviated to BMD) consisting of 1231 images of various resolutions, captured from different distances, viewpoints, and light conditions. The dataset deals with 11 classes, applied to identify different materials such as asphalt, brick, wood, etc. that can be seen in Table 2 . Furthermore, the distribution of data in each class was imbalanced, shown better in Figure 1 . The second dataset is the Construction Materials Library (CML) including a wide range of materials and intra-class variability, which was created by Dimitrov and Golparvar-Fard (2014) and improved by Han and Golparvar-Fard (2015), used in [16, 18, 30, 36] for material recognition purposes. The total number of images in the CML is 3266 which have a fixed resolution of 200\uf0cd200 pixels, consisting of 20 classes presented in Table 3 . Also, the class distribution is depicted in Figure 2 . Moreover, [41, 42] employed some parts of the CML too. In addition, from the combination of these datasets, a new composite dataset is created, including 4497 images and 24 classes of materials, as it is shown in Table 4 and Figure 3 . The important characteristic of this dataset is the variety in size and quality of images and also the various class distribution which is interpreted as imbalanced data and leads to a more challenging project as learning from such data is a common problem in classification. In fact, the classification of data with imbalanced class distribution has encountered a significant drawback of the attainable performance by most standard classifier learning algorithms [45] . Moreover, highly imbalanced data poses added difficulty, as most learners will exhibit bias towards the majority class, and in extreme cases, may ignore the minority class altogether [46] .",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 32,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 724,
                    "end": 728,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 729,
                    "end": 732,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 733,
                    "end": 736,
                    "text": "30,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 737,
                    "end": 740,
                    "text": "36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 984,
                    "end": 988,
                    "text": "[41,",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 989,
                    "end": 992,
                    "text": "42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 1674,
                    "end": 1678,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 1864,
                    "end": 1868,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                }
            ],
            "ref_spans": [
                {
                    "start": 380,
                    "end": 387,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 474,
                    "end": 482,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 909,
                    "end": 916,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 963,
                    "end": 971,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1187,
                    "end": 1194,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1199,
                    "end": 1207,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "There are samples of images in material classes of BMD and CML datasets represented in Figure 4 and Figure 5 , respectively. As can be seen, some materials have a unique pattern, such as brick and form work that makes classification less challenging. While others are of similar appearance properties, like sandstorm, gravel, asphalt, cement-granular, and stone-granular, which leads to a more problematic classification. However, we overcame this challenge. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 87,
                    "end": 95,
                    "text": "Figure 4",
                    "ref_id": null
                },
                {
                    "start": 100,
                    "end": 108,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "Many studies in recent years have used famous network architectures like VGG-16 [47] , DenseNet [48] , GoogleNet [49, 50] , and EfficientNet [51] . Typically, these networks are composed of convolutional layers and pooling layers, followed by a few fully connected layers. Several studies have suggested that more accuracy can be achieved by using a deeper network. The spatial stream ConvNet was designed taking into account how to balance performance and computation efficiency [52] , and was based on the network configuration of the VGG-16 [47] , which worked for the classification of the images and was proven to extract the features of images layer by layer. Although the configuration of the VGG-16 is simple and effective, the large number of parameters (140 million) on fully connected layers is the main problem [47] . However, CNNs come with shortcomings. This is because convolution operates on a fixed-size window. Therefore the model cannot recognize long-range spatial relationships between different parts of the images or between individual images. In addition to using CNNs as a training tool, there are other challenges such as layer pooling which leads to information loss, as well as translation invariance which prevents models from dynamically adapting to changes in the input.",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 84,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 96,
                    "end": 100,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 113,
                    "end": 117,
                    "text": "[49,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 118,
                    "end": 121,
                    "text": "50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 141,
                    "end": 145,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 480,
                    "end": 484,
                    "text": "[52]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 544,
                    "end": 548,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 823,
                    "end": 827,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "An important part of deep learning is the use of convolutional neural networks (CNNs)."
        },
        {
            "text": "Attention-based architectures like Transformers [53] have been developed into the Natural Language Processing (NLP) domain to optimize various language-related tasks more effectively, such as translations and text classification. Transformers have resulted in significant performance gains, which have caused a great deal of interest in the computer vision community to apply similar self-attentional models to vision tasks. Several computer vision studies have been inspired by the success of self-attention in NLP that incorporate self-attention in CNN-like architectures [54, 55] . A few works has used the self-attention mechanism [56, 57] as an alternative to the convolution mechanism. Despite their efficiency, these models haven't been able to scale on modern hardware accelerators because they require specialized attention patterns.",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 52,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 574,
                    "end": 578,
                    "text": "[54,",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 579,
                    "end": 582,
                    "text": "55]",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 635,
                    "end": 639,
                    "text": "[56,",
                    "ref_id": null
                },
                {
                    "start": 640,
                    "end": 643,
                    "text": "57]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "An important part of deep learning is the use of convolutional neural networks (CNNs)."
        },
        {
            "text": "The Vision Transformer (ViT) architecture was introduced by Dosovitskiy et al. (2020) [58] for the first time. An attempt is made to design a deep neural network algorithm that does not use convolution operations over large datasets. For this purpose, ViT employs the original transformer which was developed in [53] for NLP tasks with a few changes. The general concept of Vit is to divide the image into patches, then flatten and project them into a Ddimensional embedding space obtaining the so-called patch embedding. In the viewpoint of NLP, a new network will be trained in a supervised learning manner by first treating image patches as tokens (words) and then using them as inputs. Next, it adds positional embedding (a set of learnable vectors allowing the model to retain positional information) and concatenate a (learnable) class token, then let the Transformer encoder do its magic. As opposed to CNNs which function on spatial domain-specific information, transformers work on vectors and require much more and larger datasets to discover knowledge. Finally, a classification head is applied to the class token to obtain the model's logits. The model's performance was acceptable when trained on ImageNet (1M images), great when pre-trained on ImageNet-21k (14M images), and state-of-the-art when pre-trained on Google's internal JFT-300M dataset (300M images) [58] . [53] The main architecture of the transformer encoder comes from [53] . A multiheaded selfattention mechanism is layered on top of different encoder blocks for each block. Prior to being fed into the multi-head self-attention, and also prior to the MLP blocks, layer normalization is applied to the embedded patches. An encoder's general mechanism is illustrated in Figure 6, and Figure 7 provides a general explanation of how encoders, multi-head attention, and scaleddot product attention work.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 90,
                    "text": "[58]",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 312,
                    "end": 316,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 1375,
                    "end": 1379,
                    "text": "[58]",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 1382,
                    "end": 1386,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 1447,
                    "end": 1451,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [
                {
                    "start": 1748,
                    "end": 1757,
                    "text": "Figure 6,",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1762,
                    "end": 1770,
                    "text": "Figure 7",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "An important part of deep learning is the use of convolutional neural networks (CNNs)."
        },
        {
            "text": "An important part of deep learning is the use of convolutional neural networks (CNNs). Many studies in recent years have used famous network architectures like VGG-16 [47] , DenseNet [48] , GoogleNet [49, 50] , and EfficientNet [51] . Typically, these networks are composed of convolutional layers and pooling layers, followed by a few fully connected layers. Several studies have suggested that more accuracy can be achieved by using a deeper network. The spatial stream ConvNet was designed taking into account how to balance performance and computation efficiency [52] , and was based on the network configuration of the VGG-16 [47] , which worked for the classification of the images and was proven to extract the features of images layer by layer. Although the configuration of the VGG-16 is simple and effective, the large number of parameters (140 million) on fully connected layers is the main problem [47] . However, CNNs come with shortcomings. This is because convolution operates on a fixed-size window. Therefore the model cannot recognize long-range spatial relationships between different parts of the images or between individual images. In addition to using CNNs as a training tool, there are other challenges such as layer pooling which leads to information loss, as well as translation invariance which prevents models from dynamically adapting to changes in the input.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 171,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 183,
                    "end": 187,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 200,
                    "end": 204,
                    "text": "[49,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 205,
                    "end": 208,
                    "text": "50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 228,
                    "end": 232,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 567,
                    "end": 571,
                    "text": "[52]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 631,
                    "end": 635,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 910,
                    "end": 914,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "An important part of deep learning is the use of convolutional neural networks (CNNs)."
        },
        {
            "text": "Having pre-trained on large amounts of data and transferred to numerous mid-sized or small image recognition benchmarks, ViT achieves great results in comparison with state-of-the-art CNNs while it needs considerably fewer computational resources to train [58] .",
            "cite_spans": [
                {
                    "start": 256,
                    "end": 260,
                    "text": "[58]",
                    "ref_id": "BIBREF56"
                }
            ],
            "ref_spans": [],
            "section": "An important part of deep learning is the use of convolutional neural networks (CNNs)."
        },
        {
            "text": "All in all, how CNNs have been applied to computer vision tasks has been largely driven by advances in GPU technology and modern computational power. In a wide range of applications, consisting of challenging image detection tasks on ImageNet, several architectures have achieved state-of-the-art performance. Although CNNs have many advantages, they have some limitations, namely locality, translation invariance, and max pooling. Vision transformers are able to resolve CNN challenges by addressing an image first as a sequence of patches, and then decoding them as a standard Transformer encoding, in contrast to many earlier tools inspired by NLP based on image analysis. This strategy is confirmed that can perform as well as CNN on natural image classification tasks when it is trained on large datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "An important part of deep learning is the use of convolutional neural networks (CNNs)."
        },
        {
            "text": "Due to the different resolutions of the images, the only pre-processing done is to resize the images to 224\uf0cd224 pixels. However, the data augmentation utilized includes Flip-Left-to-Right (FlipLR), Flip-UP-to-DOWN (FlipUD), translate, random crop, and Randaug [59] , which are done during training, then the images are normalized. ",
            "cite_spans": [
                {
                    "start": 260,
                    "end": 264,
                    "text": "[59]",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [],
            "section": "Pre-processing and data augmentation"
        },
        {
            "text": "We used the basic Vision Transformer model with a size of 16\uf0cd16 patches to extract the features and the weights of the pre-trained model used on the original images. The model is 25 epochs with a learning rate of 0.0003, the Adam [60] optimization function, batch size 8, and the cross-entropy loss function, which have been employed for training. We also took advantage of Google Colab Graphics processing Tesla K8 Unit for training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training"
        },
        {
            "text": "We used four standard performance metrics, described in equation (1) (1)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Network evaluation"
        },
        {
            "text": "In this section, we first evaluate the model separately on the datasets used by the various partitions that are intended for the data, and finally we examine the reliability of these partitions by the 5-fold cross-validation method. In the following, we create a new and more challenging situation for the model by combining two datasets, and in this case, the model is evaluated by different data divisions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Discussion"
        },
        {
            "text": "In order to evaluate the model more accurately, we evaluated the datasets in the most different divisions of the validation test as well as the 5-fold cross-validation mode, which is shown in Table 5 and Figure 9 . Fusion matrix on the BMD dataset for: (a) data division (b) 5-fold cross-validation. Table 6 . In all these cases, except for one case on two datasets, we obtained an accuracy of 100%. The imbalance in the number of data of datasets, in test phase, can be seen in fusion matrixes in Figure 9 and Figure 10 . Finally, the performance of the proposed method has been compared with the existing state-of-the-art methods listed in Figure 10 . Fusion matrix on the CML dataset for: (a) data division (b) 5-fold cross-validation. Table 7 . Due to the fact that both datasets used do not have a default partition to evaluate the model used, in order to make a fairer comparison with related work, in Table 5 and Figure 9 . Fusion matrix on the BMD dataset for: (a) data division (b) 5-fold cross-validation. Table 6 , we evaluated both datasets in different modes of data segmentation. According to the results obtained, our model was able to achieve 100% accuracy in all but one case. Also, for better evaluation, we used the 5-fold cross-validation method in Table 8 to be able to have a more accurate assessment of the model's behavior relative to different parts of the data. By doing this, we can evaluate the generalizability of the model for different partitions of data and in different conditions. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 192,
                    "end": 199,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 204,
                    "end": 212,
                    "text": "Figure 9",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 300,
                    "end": 307,
                    "text": "Table 6",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 498,
                    "end": 506,
                    "text": "Figure 9",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 511,
                    "end": 520,
                    "text": "Figure 10",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 642,
                    "end": 651,
                    "text": "Figure 10",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 739,
                    "end": 746,
                    "text": "Table 7",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 908,
                    "end": 915,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 920,
                    "end": 928,
                    "text": "Figure 9",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 1016,
                    "end": 1023,
                    "text": "Table 6",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 1269,
                    "end": 1276,
                    "text": "Table 8",
                    "ref_id": "TABREF7"
                }
            ],
            "section": "Evaluating the BMD and the CML datasets"
        },
        {
            "text": "In this section, in order to evaluate the model in a new way, the two datasets are combined to test the model's performance in a more complex way than a separate evaluation for both data. In this case, the number of classes for material detection increased to 24, and also due to the difference in resolution of the two datasets, the way of photography, and the type of texture of different materials, the model has more challenges to achieve 100% accuracy, which are overcome efficiently. In this case, similarly, the model is evaluated in different modes of dataset segmentation, and also to better evaluate and measure the generalizability of the model performance on different parts of the dataset, the 5-fold cross-validation method is used.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Combining Two Datasets"
        },
        {
            "text": "According to the results shown in Table 9 , it can be seen that the model has no over-fit on the test dataset, and the model can generalize to the test and validation dataset for different modes. In addition, the model has reached 100% accuracy on the validation and testing data in the case where 80% of the data is for training, 10% for validation, and 10% for testing, which is the best possible result.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 34,
                    "end": 41,
                    "text": "Table 9",
                    "ref_id": "TABREF8"
                }
            ],
            "section": "Combining Two Datasets"
        },
        {
            "text": "However, in other cases where the model could not reach 100% accuracy on the test dataset, we can point to the imbalance in the number of data in each class of two datasets. Therefore, this imbalance can also be seen in the combined dataset too. As such, data division will be a more complex situation. It creates a combination for the distribution of each material and the texture difference in the dataset, making the model unable to achieve 100% accuracy. As a result, we evaluated the model with a Test Time Augment (TTA) method. In this algorithm, the original image and images augmented by training augment are separately given to the model and the max vote is selected as the final result. Consequently, the accuracy of the model increased in all used divisions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Combining Two Datasets"
        },
        {
            "text": "The 5-fold cross-validation method has also been used to evaluate the model in the combination of two datasets. The model achieved 100% accuracy on folds one and two and 97.32%, 99.10%, and 99.10% on folds 3 to 5. However, according to the results of the 5-fold cross-validation method, it can be concluded that the obtained result represented in Table 9 is reliable and is not affected by the random division of data. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 347,
                    "end": 354,
                    "text": "Table 9",
                    "ref_id": "TABREF8"
                }
            ],
            "section": "Combining Two Datasets"
        },
        {
            "text": "Monitoring construction projects according to the as-planned model is one of the most important activities for project managers. IN fact, accurate and timely assessment of project progress enables managers to make the right and immediate decision, when they encounter deviations. In this way, automated progress monitoring helps them a lot to improve the efficiency and accuracy of this process. One of the controversial and essential issues in automated construction management is material recognition and classification. Advancements in artificial intelligence have revolutionized many fields especially construction management through providing state-of-the-art methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Regarding the limitations of prior studies, this paper aims to introduce a method for maximizing the accuracy of material detection. In recent years, deep learning has aided material recognition a lot. The recent state-of-the-art Deep Learning Vision Transformer model is employed in this paper as a solution to achieve the highest accuracy rate in material recognition. It is applied on three imbalanced datasets of various material classes with different numbers, sizes, and qualities of images in each class, in order to validate the proposed model. Indeed, we utilized CML and BMD datasets as well as a compound dataset of them, in different data segmentation modes, to evaluate the model. Moreover, various data augmentation including Flip-Left-to-Right (FlipLR), Flip-UP-to-DOWN (FlipUD), translate, random crop, and Randaug were employed in order to solve the problems of data shortage, overfitting, and imbalanced classes so that we could keep the model's generalizability high, in the section of testing data. Taking advantage of evaluation metrics such as accuracy, precision, recall, and f1score revealed that ViT model outperforms the other presented state-of-the-art works with different data segmentation modes. Not only did the ViT model achieve a rate of 100% in all four parameters but it accomplished this accuracy rate in all material categories in three different datasets, which is really unprecedented, since identifying materials with similar shape and texture (such as soil, cement, gravel, sand, concrete, and asphalt) in prior studies has been problematic and challenging.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The effectiveness of the proposed model should be tested on other material classes which are more challenging, such as glass whose appearance features like reflection and transparency are very different from other construction materials. Future work may also seek to make up a method to be utilized on a broader scope of material detection. For instance, datasets made up of images that represent more than a single material could be considered to introduce a combined material recognition model. As the next step or to do more complementary work, it could also yield the amount of each material in the image. That would greatly help the automation of construction progress monitoring through presenting more detailed functional information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future works"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Material Classification via Machine Learning Techniques: Construction Projects Progress Monitoring",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Alaloul",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Qureshi",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Artificial Neural Networks and Deep Learning -Applications and Perspective. 2021",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "A Real-Time 4D Augmented Reality System for Modular Construction Progress Monitoring",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Petzold",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "An Automatic Project Progress Monitoring Model by Integrating Auto CAD and Digital Photos",
            "authors": [
                {
                    "first": "Z",
                    "middle": [
                        "A"
                    ],
                    "last": "Memon",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "Z A"
                    ],
                    "last": "Majid",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mustaffar",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Computing in Civil Engineering",
            "volume": "",
            "issn": "",
            "pages": "1--13",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Automated progress controlling and monitoring using daily site images and building information modelling",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Mahami",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Buildings",
            "volume": "9",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Improving progress monitoring by fusing point clouds, semantic data and computer vision. Automation in Construction",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Braun",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2020--116",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Automated progress monitoring using unordered daily construction photographs and IFC-based building information models",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Golparvar-Fard",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pe\u00f1a-Mora",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Savarese",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Journal of Computing in Civil Engineering",
            "volume": "29",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "On-demand monitoring of construction projects through a gamelike hybrid application of BIM and machine learning",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pour Rahimian",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Survey of digital technologies in procurement of construction projects. Automation in Construction",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "O"
                    ],
                    "last": "Ibem",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Laryea",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "46",
            "issn": "",
            "pages": "11--21",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A Review of ICT Technology In Construction",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Adwan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Alsoufi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "International Journal of Managing Information Technology",
            "volume": "8",
            "issn": "",
            "pages": "1--21",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Evaluation of image-based modeling and laser scanning accuracy for emerging automated performance monitoring techniques. Automation in Construction",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Golparvar-Fard",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "20",
            "issn": "",
            "pages": "1143--1155",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Material Recognition for Automated Progress Monitoring using Deep Learning Methods",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Mahami",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.16344"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Progressive 3D reconstruction of infrastructure with videogrammetry. Automation in Construction",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Brilakis",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Fathi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rashidi",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "20",
            "issn": "",
            "pages": "884--895",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Imaging network design to improve the automated construction progress monitoring process. Construction Innovation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Mahami",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "19",
            "issn": "",
            "pages": "386--404",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "An analogy between various machine-learning techniques for detecting construction materials in digital images",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rashidi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "KSCE Journal of Civil Engineering",
            "volume": "20",
            "issn": "4",
            "pages": "1178--1188",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Automatic classification of common building materials from 3D terrestrial laser scan data",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Appearance-based material classification for monitoring of operation-level construction progress using 4D BIM and site photologs",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "K"
                    ],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Golparvar-Fard",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "53",
            "issn": "",
            "pages": "44--57",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Classification of major construction materials in construction environments using ensemble classifiers. Advanced Engineering Informatics",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Son",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "28",
            "issn": "",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Vision-based material recognition for automated monitoring of construction progress and generating building information modeling from unordered site image collections",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dimitrov",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Golparvar-Fard",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Advanced Engineering Informatics",
            "volume": "28",
            "issn": "1",
            "pages": "37--49",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Image-driven fuzzy-based system to construct as-is IFC BIM objects. Automation in Construction",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "92",
            "issn": "",
            "pages": "68--87",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Deep machine learning approach to develop a new asphalt pavement condition index",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Majidifard",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Adu-Gyamfi",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "G"
                    ],
                    "last": "Buttlar",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Construction and Building Materials, 2020",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Deep Learning",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Nature",
            "volume": "521",
            "issn": "",
            "pages": "436--480",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Deep Learning for Neuroimaging-based Diagnosis and Rehabilitation of Autism Spectrum Disorder: A Review",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Khodatars",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Deep learning. Genetic Programming and Evolvable Machines",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Heaton",
                    "suffix": ""
                },
                {
                    "first": "Ian",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Courville",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "19",
            "issn": "",
            "pages": "305--307",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Automated Detection and Forecasting of COVID-19 using Deep Learning Techniques: A Review",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shoeibi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Semantic decomposition and recognition of indoor spaces with structural constraints for 3D indoor modelling. Automation in Construction",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "SCAN-TO-BIM METHODOLOGY ADAPTED FOR DIFFERENT APPLICATION. ISPRS -International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Badenko",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1--7",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Building Information Modelling (BIM) uptake: Clear benefits, understanding its implementation, risks and challenges. Renewable and Sustainable Energy Reviews",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ghaffarianhoseini",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "75",
            "issn": "",
            "pages": "1046--1053",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Virtual prototyping-and transfer learning-enabled module detection for modular integrated construction",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "An Application Oriented Scan-to-BIM Framework. Remote Sensing",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "M.-K",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "11",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Automated monitoring of operation-level construction progress using 4D bim and daily site photologs",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "K"
                    ],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Golparvar-Fard",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Material-Based Construction Site Image Retrieval",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Brilakis",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Soibelman",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shinagawa",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Journal of Computing in Civil Engineering",
            "volume": "19",
            "issn": "4",
            "pages": "341--355",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Parameter optimization for automated concrete detection in image data. Automation in Construction",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Brilakis",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "19",
            "issn": "",
            "pages": "944--953",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Identification of granite varieties from colour spectrum data",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ara\u00fajo",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Sensors",
            "volume": "10",
            "issn": "9",
            "pages": "8572--8584",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Automated color model-based concrete detection in constructionsite images by using machine learning algorithms",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Son",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Journal of Computing in Civil Engineering",
            "volume": "26",
            "issn": "3",
            "pages": "421--433",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Automated segmentation of concrete images into microstructures: A comparative study. Computers and Concrete",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yazdi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sarafrazi",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "14",
            "issn": "",
            "pages": "315--325",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Enhanced Appearance-Based Material Classification for the Monitoring of Operation-Level Construction Progress through the Removal of Occlusions",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "K"
                    ],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Muthukumar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Golparvar-Fard",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Construction Research Congress 2016: Old and New Construction Technologies Converge in Historic",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Towards automatic generation of as-built BIM: 3D building facade modeling and material recognition from images",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "K"
                    ],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "Y"
                    ],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "International Journal of Automation and Computing",
            "volume": "13",
            "issn": "4",
            "pages": "338--349",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Geometry-informed material recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Degol",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Golparvar-Fard",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hoiem",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "29th IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Characterization and identification of asphalt mixtures based on Convolutional Neural Network methods using X-ray scanning images",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Construction and Building Materials",
            "volume": "174",
            "issn": "",
            "pages": "72--80",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Machine learning-based automatic reinforcing bar image analysis system in the internet of things. Multimedia Tools and Applications",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "O"
                    ],
                    "last": "Park",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "78",
            "issn": "",
            "pages": "3171--3180",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Evaluating on the transfer learning of CNN architectures to a construction material image classification tasks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bunrit",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kerdprasop",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kerdprasop",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Journal of Machine Learning and Computing",
            "volume": "9",
            "issn": "2",
            "pages": "201--207",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Improving the representation of cnn based features by autoencoder for a task of construction material image classification",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bunrit",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kerdprasop",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kerdprasop",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Advances in Information Technology",
            "volume": "11",
            "issn": "4",
            "pages": "192--199",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "What lies beneath: Material classification for autonomous excavators using proprioceptive force sensing and machine learning",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Fernando",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Marshall",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "The classification of construction waste material using a deep convolutional neural network",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Davis",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "CLASSIFICATION OF IMBALANCED DATA: A REVIEW",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Yanmin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "K C W A M S K"
                    ],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "International Journal of Pattern Recognition and Artificial Intelligence",
            "volume": "23",
            "issn": "04",
            "pages": "687--719",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Survey on deep learning with class imbalance",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "M"
                    ],
                    "last": "Khoshgoftaar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of Big Data",
            "volume": "6",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Zisserman Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1409.1556"
                ]
            }
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Densely Connected Convolutional Networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Going Deeper with Convolutions",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1409.4842"
                ]
            }
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1602.07261"
                ]
            }
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Rethinking Model Scaling for Convolutional Neural Networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [
                        "V"
                    ],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "Efficientnet",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 36th International Conference on Machine Learning",
            "volume": "97",
            "issn": "",
            "pages": "6105--6114",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Detecting distraction of drivers using Convolutional Neural Network",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Masood",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Pattern Recognition Letters",
            "volume": "139",
            "issn": "",
            "pages": "79--85",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Attention Is All You Need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1706.03762"
                ]
            }
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Non-local Neural Networks",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1711.07971"
                ]
            }
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "End-to-End Object Detection with Transformers",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Carion",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.12872.56"
                ]
            }
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.07853"
                ]
            }
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dosovitskiy",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2010.11929"
                ]
            }
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Practical automated data augmentation with a reduced search space",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "D"
                    ],
                    "last": "Cubuk",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Adam: A Method for Stochastic Optimization",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR). 2014. p",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.6980"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The distribution of images of the BMD dataset among classes",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The distribution of images of the CML dataset among classes",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The distribution of images of the combined dataset among classes The categories of materials in the BMD dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The categories of materials in the CML dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Schematic illustration of ViT model[58]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Multi-head attention mechanism",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Some samples of applied augmentations (randaug, random-crop, translate). Each row illustrates a sample image and the applied augmentation is shown on top of each column.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "to (4): accuracy, precision, recall, F1-score as well as confusion matrix. The elements to calculate the mentioned metrics are True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) that are defined as follows to evaluate the performance of the model: \u2022 True Positive (TP) = correct material prediction \u2022 False Positive (FP) = incorrect material prediction \u2022 True Negative (TN) = correctly predicts the negative material class \u2022 False Negative (FN) = incorrectly predicts the negative material class = ( + ) \u2044",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Fusion matrix on the BMD dataset for: (a) data division (b) 5-fold cross-validation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Fusion matrix on the CML dataset for: (a) data division (b) 5-fold cross-validation.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The summary of related studies on building materials classification",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The number of images in each class of the BMD dataset",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "The number of images in each class of the CML dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "The number of images in each class of the combined dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Results obtained in different modes of data division for training and testing the BMD dataset (in terms of percentage)",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Results obtained in different modes of data division for training and testing the CML dataset (in terms of percentage)",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Comparison of the results of utilizing ViT on the BMD and the CML datasets with those of related work",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "The percentage of correctly recognized materials using 5-fold cross-validation on accuracy.",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "Results obtained in different modes of data division for training and testing the combined dataset (in terms of percentage)",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors would like to thank Dr. Kevin K. Han for providing access to the CML dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgement"
        }
    ]
}
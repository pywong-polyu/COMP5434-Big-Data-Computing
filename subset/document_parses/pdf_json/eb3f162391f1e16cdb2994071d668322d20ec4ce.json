{
    "paper_id": "eb3f162391f1e16cdb2994071d668322d20ec4ce",
    "metadata": {
        "title": "Group selection and shrinkage with application to sparse semiparametric modeling",
        "authors": [
            {
                "first": "Ryan",
                "middle": [],
                "last": "Thompson",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Farshid",
                "middle": [],
                "last": "Vahid",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Sparse regression and classification estimators capable of group selection have application to an assortment of statistical problems, from multitask learning to sparse additive modeling to hierarchical selection. This work introduces a class of group-sparse estimators that combine group subset selection with group lasso or ridge shrinkage. We develop an optimization framework for fitting the nonconvex regularization surface and present finite-sample error bounds for estimation of the regression function. Our methods and analyses accommodate the general setting where groups overlap. As an application of group selection, we study sparse semiparametric modeling, a procedure that allows the effect of each predictor to be zero, linear, or nonlinear. For this task, the new estimators improve across several metrics on synthetic data compared to alternatives. Finally, we demonstrate their efficacy in modeling supermarket foot traffic and economic recessions using many predictors. All of our proposals are made available in the scalable implementation grpsel.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Group selection (structured selection) arises in connection with a myriad of statistical problems, e.g., multitask learning (Obozinski et al. 2006) , sparse additive modeling (Ravikumar et al. 2009 ), and hierarchical selection (Lim and Hastie 2015) . Even sparse linear modeling can involve structured selection, such as when a categorical predictor is represented as a sequence of dummy variables. In certain domains, groups may emerge naturally, e.g., disaggregates of the same macroeconomic series or genes of the same biological path. The prevalence of such problems motivates principled estimation procedures capable of encoding structure into the fitted models they produce.",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 147,
                    "text": "(Obozinski et al. 2006)",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 175,
                    "end": 197,
                    "text": "(Ravikumar et al. 2009",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 228,
                    "end": 249,
                    "text": "(Lim and Hastie 2015)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Given response y = (y 1 , . . . , y n ) \u2208 R n , predictors X = (x 1 , . . . , x n ) \u2208 R n\u00d7p , and nonoverlapping groups G 1 , . . . , G g \u2286 {1, . . . , p}, group lasso (Yuan and Lin 2006; Meier et al. 2008 ) solves:",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 187,
                    "text": "(Yuan and Lin 2006;",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 188,
                    "end": 205,
                    "text": "Meier et al. 2008",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(1.1)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "where \u03b2 k \u2208 R p k are the coefficients indexed by G k , \u03bb 1 , . . . , \u03bb g are nonnegative tuning parameters, and : R 2 \u2192 R + is a loss function, e.g., square loss for regression or logistic loss for classification. 1 Group lasso couples coefficients via their l 2 -norm so that all predictors in a group are selected together. Just as lasso (Tibshirani 1996) is the continuous relaxation of the combinatorially-hard problem of best subset selection (\"best subset\"), so is group lasso the relaxation of the combinatorial problem of group subset selection (\"group subset\"):",
            "cite_spans": [
                {
                    "start": 341,
                    "end": 358,
                    "text": "(Tibshirani 1996)",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "x i \u03b2, y i + g k=1 \u03bb k 1( \u03b2 k 2 = 0).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(1.2) Unlike group lasso, which promotes group sparsity implicitly by nondifferentiability of the l 2 -norm at the null vector, group subset explicitly penalizes the number of nonzero groups. Consequently, one might interpret group lasso as a compromise made in the interest of computation. However, group lasso has a trick up its sleeve that group subset does not: shrinkage. Shrinkage estimators such as lasso and ridge (Hoerl and Kennard 1970) are more robust than best subset to noise (Breiman 1996; Hastie et al. 2020 ). This consideration motivates us to shrink the group subset estimator:",
            "cite_spans": [
                {
                    "start": 422,
                    "end": 446,
                    "text": "(Hoerl and Kennard 1970)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 489,
                    "end": 503,
                    "text": "(Breiman 1996;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 504,
                    "end": 522,
                    "text": "Hastie et al. 2020",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "x i \u03b2, y i + g k=1 \u03bb 0k 1( \u03b2 k 2 = 0) + g k=1 \u03bb qk \u03b2 k q 2 , q \u2208 {1, 2}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(1.3)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In contrast to group lasso and group subset, (1.3) directly controls both group sparsity and shrinkage via separate penalties. When q = 1, the shrinkage penalty is group lasso, and when q = 2, the penalty is ridge. The combination of best subset and lasso or ridge in the unstructured setting results in good predictive models with low false positive selection rates . Unfortunately, the estimators (1.3), including group lasso and group subset as special cases, do not accommodate overlap among groups. Specifically, if two groups overlap, one cannot be selected independently of the other. To encode sophisticated structures, such as hierarchies or graphs, groups must often overlap. To address this issue, one can introduce group-specific vectors \u03bd (k) \u2208 R p (k = 1, . . . , g) that are zero everywhere except at the positions indexed by G k . Letting V be the set of all tuples\u03bd := (\u03bd (1) , . . . , \u03bd (g) ) satisfying this property, group subset with shrinkage becomes:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "x i \u03b2, y i + g k=1 \u03bb 0k 1( \u03bd (k) 2 = 0) + g k=1 \u03bb qk \u03bd (k) q 2 , q \u2208 {1, 2}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(1.4)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The vectors \u03bd (1) , . . . , \u03bd (g) are a decomposition of \u03b2 into a sum of latent coefficients that facilitate selection of overlapping groups. For instance, if three predictors, x 1 , x 2 , and x 3 , are spread across two groups, G 1 = {1, 2} and G 2 = {2, 3}, then \u03b2 1 = \u03bd",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(1) 1 , \u03b2 2 = \u03bd (1) 2 + \u03bd",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(2) 2 , and \u03b2 3 = \u03bd (2) 3 . Since \u03b2 2 has a separate latent coefficient for each group, G 1 or G 2 can be selected independently of the other. This latent coefficient approach originated for group lasso with Jacob et al. (2009) and Obozinski et al. (2011) . When all groups are disjoint, (1.4) reduces exactly to (1.3).",
            "cite_spans": [
                {
                    "start": 208,
                    "end": 227,
                    "text": "Jacob et al. (2009)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 232,
                    "end": 255,
                    "text": "Obozinski et al. (2011)",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This paper develops computational methods and statistical theory for group subset with shrinkage. Via the formulation (1.4), our work accommodates the general overlapping groups setting. On the computational side, we develop algorithms that scale to compute quality (approximate) solutions of the combinatorial optimization problem. Our framework comprises coordinate descent and local search and applies to general smooth convex loss functions (i.e., regression and classification), building on recent advances for best subset Hazimeh and Mazumder 2020) . In contrast to existing computational methods for group subset (Guo et al. 2014; Bertsimas and King 2016) , which rely on branch-and-bound or commercial mixed-integer optimizers, our methods scale to instances with millions of predictors or groups. We implement our framework in the publicly available R package grpsel. On the statistical side, we establish new error bounds for group subset with and without shrinkage. The bounds apply in the overlapping setting and allow for model misspecification. The analysis sheds light on the advantages of group selection and the benefits of shrinkage.",
            "cite_spans": [
                {
                    "start": 528,
                    "end": 554,
                    "text": "Hazimeh and Mazumder 2020)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 620,
                    "end": 637,
                    "text": "(Guo et al. 2014;",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 638,
                    "end": 662,
                    "text": "Bertsimas and King 2016)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The new estimators have application to a broad range of statistical problems. We focus on sparse semiparametric modeling, a procedure wherein y is modeled via a sum of functions j f j (x j ), and f j can be zero, linear, or nonlinear. Chouldechova and Hastie (2015) and Lou et al. (2016) estimate these flexible models using group lasso with overlapping groups and regression splines. After conducting synthetic experiments on the efficacy of our estimators in fitting these models, we carry out two empirical studies. The first study involves modeling supermarket foot traffic using sales volumes on different products. Only a fraction of supermarket products are traded in volume, necessitating sparsity. The second study involves modeling recessionary periods in the economy using macroeconomic series. The macroeconomic literature contains many examples of sparse linear modeling (De Mol et al. 2008; Li and Chen 2014 ), yet theory does not dictate linearity. Together these studies suggest semiparametric models are an excellent compromise between fully linear and fully nonparametric alternatives.",
            "cite_spans": [
                {
                    "start": 235,
                    "end": 265,
                    "text": "Chouldechova and Hastie (2015)",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 270,
                    "end": 287,
                    "text": "Lou et al. (2016)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 884,
                    "end": 904,
                    "text": "(De Mol et al. 2008;",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 905,
                    "end": 921,
                    "text": "Li and Chen 2014",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Independently and concurrently to this work, Hazimeh et al. (2021) study computation and theory for group subset with nonoverlapping groups. Their algorithms likewise build on Hazimeh and Mazumder (2020) but apply only to square loss regression. In another recent preprint, Zhang et al. (2021) propose a computational \"splicing\" technique for group subset that appears promising, though they do not consider overlapping groups or shrinkage.",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 66,
                    "text": "Hazimeh et al. (2021)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 176,
                    "end": 203,
                    "text": "Hazimeh and Mazumder (2020)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 274,
                    "end": 293,
                    "text": "Zhang et al. (2021)",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The paper is structured as follows. Section 2 presents computational methods, Section 3 provides statistical theory, Section 4 describes simulation experiments, Section 5 reports data analyses, and Section 6 closes the paper. All proofs are relegated to the appendices.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This section introduces our optimization framework and its key components: coordinate descent and local search. The framework applies to any smooth convex loss function (z, y).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Computation"
        },
        {
            "text": "The discussion below addresses the specific cases of square loss (z, y) = (y \u2212 z) 2 /2, which is suitable for regression, and logistic loss (z, y) = \u2212y log(z) \u2212 (1 \u2212 y) log(1 \u2212 z), which is suitable for classification. Both loss functions are implemented in grpsel.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Computation"
        },
        {
            "text": "The algorithms are presented for nonoverlapping groups; however, this does not limit their applicability since the overlapping groups problem can be transformed into one involving disjoint groups. LetX := (X 1 , . . . , X g ) be the predictor matrix formed by concatenating the submatrices of X corresponding to groups G 1 , . . . , G g . Since the predictors are replicated as many times as they appear across groups, a new set of groups that do not overlap can be constructed. The algorithms are then run onX with the new groups. Finally, the coefficients on X are recovered by summing the coefficients on each replicated column inX. Jacob et al. (2009) suggest this approach for group lasso. The matrixX, however, will not be full rank, nor will a submatrix corresponding to any subset of overlapping groups. Our algorithmic analysis takes this fact into account.",
            "cite_spans": [
                {
                    "start": 636,
                    "end": 655,
                    "text": "Jacob et al. (2009)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Computation"
        },
        {
            "text": "Throughout this section, X is assumed to have columns with mean zero and unit l 2 -norm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Computation"
        },
        {
            "text": "Coordinate descent algorithms are optimization routines that minimize along successive coordinate hyperplanes. The coordinate descent scheme developed here iteratively fixes all but one group of coordinates (a coordinate group) and minimizes in the directions of these coordinates. The group subset problem (1.4) can be expressed as the unconstrained minimizer of the objective function F (\u03b2) := L(\u03b2) + \u2126(\u03b2), where the loss function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "x i \u03b2, y i , and the regularization function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "The regularization function can be written directly in terms of \u03b2 k since the groups are transformed such that they do not overlap. The function F (\u03b2) is a sum of smooth convex and discontinuous nonconvex functions and is hence discontinuous nonconvex. The minimization problem with respect to group k is min \u03be\u2208R p k F (\u03b2 1 , . . . , \u03b2 k\u22121 , \u03be, \u03b2 k+1 , . . . , \u03b2 g ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "(2.1)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "The complexity of this coordinate-wise minimization depends on and X. In the case of square loss, the minimization involves a least-squares fit in p k coordinates, taking O(p 2 k n) operations. To bypass these involved computations, a partial minimization scheme is adopted whereby each coordinate group is updated using a single gradient descent step taken with respect to that group. This scheme results from a standard technique of replacing the objective function with a surrogate function that is a quadratic upper bound. To this end, we require Lemma 2.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "Lemma 2.1. Let L : R p \u2192 R be a continuously differentiable function. Suppose there exists a c k > 0 such that the gradient of L with respect to group k satisfies the Lipschitz property",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "for all \u03b2 \u2208 R p and\u03b2 \u2208 R p that differ only in group k. Then it holds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "for anyc k \u2265 c k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "Lemma 2.1 is the block descent lemma of Beck and Tetruashvili (2013) , which holds under a Lipschitz condition on the group-wise gradients of L(\u03b2). This condition is satisfied for square loss with c k = \u03b3 2 k and for logistic loss with c k = \u03b3 2 k /4, where \u03b3 k is the maximal eigenvalue of X k X k . Using the result of Lemma 2.1, a quadratic upper bound of F (\u03b2), treated as a function in group k, is given b\u0233 Fc k (\u03b2;\u03b2) :=Lc k (\u03b2;\u03b2) + \u2126(\u03b2).",
            "cite_spans": [
                {
                    "start": 40,
                    "end": 68,
                    "text": "Beck and Tetruashvili (2013)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "(2.3)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "Thus, in place of the minimization (2.1), we use the minimization",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "This new problem admits a simple analytical solution, given by Proposition 2.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "Proposition 2.1. Define the thresholding function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "and (x) + is shorthand for max(x, 0). Then the coordinate-wise minimization problem (2.4) is solved by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "Proposition 2.1 states that a minimizer is given by appropriately thresholding a gradient descent update to coordinate group k. For both square and logistic loss, the gradient \u2207 k L(\u03b2) can be expressed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "where r = y \u2212 X\u03b2 for square loss and r = y \u2212 (1 + exp(\u2212X\u03b2)) \u22121 for logistic loss. Hence, a solution to (2.4) can be computed in as few as O(p k n) operations. Algorithm 1 now presents the coordinate descent scheme. Several algorithmic optimizations and heuristics can improve performance; these are discussed in Section 2.3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "end if converged then break end return \u03b2 (m)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coordinate descent"
        },
        {
            "text": "While Algorithm 1 may appear as an otherwise standard coordinate descent algorithm, the presence of the group subset penalty complicates the analysis of its convergence properties. No standard convergence results directly apply; e.g., Tseng (2001) that applies to group lasso cannot be invoked immediately here. Hence, we work towards establishing some properties tailored to Algorithm 1.",
            "cite_spans": [
                {
                    "start": 235,
                    "end": 247,
                    "text": "Tseng (2001)",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "Two results are presented. Lemma 2.2 establishes convergence of the sequence of objective values. Theorem 2.1 establishes convergence of the sequence of iterates to a stationary point of F (\u03b2) that satisfies a certain coordinate-wise property. A point \u03b2 with nonzero coordinate groups A is said to be a stationary point of F (\u03b2) if \u2207 A L(\u03b2 ) = 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "Lemma 2.2. Letc k \u2265 c k for all k = 1, . . . , g. Then the sequence of objective values {F (\u03b2 (m) )} m\u2208N produced by Algorithm 1 is decreasing, convergent, and satisfies the inequality",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "Lemma 2.2 is derived from the result of Lemma 2.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "Theorem 2.1. Letc k > c k for all k = 1, . . . , g. Then the sequence of iterates {\u03b2 (m) } m\u2208N produced by Algorithm 1 converge to a solution \u03b2 that is a stationary point of F (\u03b2) satisfying the fixed point equations",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "To prove Theorem 2.1, Lemma 2.2 is used to show that the active set stabilizes during coordinate descent. This property allows the group subset penalty to be treated as a fixed quantity after some finite number of iterations, and in turn, opens up the results of Tseng (2001) .",
            "cite_spans": [
                {
                    "start": 263,
                    "end": 275,
                    "text": "Tseng (2001)",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "The fixed point equations in Theorem 2.1 have the interpretation that the limit point of the iterates of Algorithm 1 cannot be improved by partially minimizing in the directions of any coordinate group. This notion is stronger than stationarity alone because all points satisfying (2.6) are stationary, but not all stationary points satisfy (2.6).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "In the case of overlapping groups, the matrixX := (X 1 , . . . , X g ) is rank deficient, as are any of its submatrices corresponding to groups that overlap. This matter raises a question as to whether the convergence results established above still hold. As summarized in the following remark, the answer is in the affirmative.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "Remark 2.1. Let G 1 , . . . , G g be a set of overlapping groups. Then Theorem 2.1 remains valid when Algorithm 1 is run with the predictor matrixX := (X 1 , . . . , X g ), where X k contains the columns of X indexed by G k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "This claim follows from an inspection of the relevant proofs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convergence analysis"
        },
        {
            "text": "Local search methods have a long history in combinatorial optimization. Here we present a local search method tailored specifically to the group subset problem. The proposed method generalizes an algorithm that first appeared in Beck and Eldar (2013) for solving instances of unstructured sparse optimization. Hazimeh and Mazumder (2020) and Dedieu et al. (2020) adapt it to best subset with promising results. The core idea is simple: given an incumbent solution, search a neighborhood local to that solution for a minimizer with lower objective value by discretely optimizing over a small set of coordinate groups. This scheme turns out to be useful when the predictors are strongly correlated, a situation in which coordinate descent alone may produce a poor solution.",
            "cite_spans": [
                {
                    "start": 229,
                    "end": 250,
                    "text": "Beck and Eldar (2013)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 310,
                    "end": 337,
                    "text": "Hazimeh and Mazumder (2020)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 342,
                    "end": 362,
                    "text": "Dedieu et al. (2020)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Local search"
        },
        {
            "text": "Define the group sparsity pattern of a vector \u03b2 to be the set of nonzero group indices: 2 gs(\u03b2) := {k \u2208 {1, . . . , g} : \u03b2 k 2 = 0}, and define the constraints sets",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Local search"
        },
        {
            "text": "Given a fixed vector \u03b2, a solution to (2.7) is a minimizer among all ways of replacing a subset of active coordinate groups in \u03b2 with a new subset. The complexity of the problem is dictated by s, which controls the size of these subsets. When s = g, the full combinatorial problem whose solution is a global minimizer of the group subset problem is recovered. For s g, a reduced combinatorial problem is obtained whose solution space is usually orders of magnitude smaller than that of the full problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, consider the optimization problem"
        },
        {
            "text": "The limiting case s = 1 admits an efficient computational scheme, given in Algorithm 2, referred to hereafter as local search. Algorithm 2 comprises two low-complexity loops: an outer loop over the active set and an inner loop over the inactive set. Within the inner loop, an active coordinate group is removed, and the objective is minimized in the directions of an inactive coordinate group. This minimization problem can be solved by iterating the thresholding operator (2.5).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, consider the optimization problem"
        },
        {
            "text": "Since Algorithm 2 involves optimizing in one coordinate group only, it need not produce a stationary point even if initialized at one. Algorithm 3 thus combines local search with coordinate descent. The combined algorithm first produces a candidate solution using",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, consider the optimization problem"
        },
        {
            "text": "coordinate descent and then follows up with local search. This scheme is iterated until the solution cannot be improved. Compared with coordinate descent alone, coordinate descent with local search can yield significantly lower objective values in high-correlation scenarios. Empirical comparisons of the algorithms are provided in Section 4. Algorithm 3 is guaranteed to converge because Algorithm 2 never increases the objective value (by construction), Algorithm 1 is convergent, and the objective function is bounded below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now, consider the optimization problem"
        },
        {
            "text": "This section is concluded with details of additional components of the framework. Some components are described in Appendix A.5 in the interest of space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Other components"
        },
        {
            "text": "Suppose the group matrix X k is orthogonal, i.e., X k X k = I. Then, for square loss, the surrogate minimization problem (2.4) exactly minimizes F (\u03b2) with respect to group k when c k = c k . If the groups are not orthogonal, they can be made so using singular value decomposition. The coefficients can then be transformed back to their original basis after the algorithms have run. Experimental evidence suggests orthogonal groups yield higher quality solutions in fewer iterations. In addition, using orthogonal groups is equivalent to using the regularizer \u2126(X\u03b2). That is, orthogonalizing the groups has the effect of penalizing the fits X k \u03b2 k rather than the coefficients \u03b2 k . For many problems, this type of penalization is intuitive; e.g., when X k represents a basis for a function f k , penalizing X k \u03b2 k is equivalent to penalizing the complexity of f k . Simon and Tibshirani (2012) point out this property for group lasso. Our implementation supports automatic orthogonalization.",
            "cite_spans": [
                {
                    "start": 869,
                    "end": 896,
                    "text": "Simon and Tibshirani (2012)",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Orthogonal groups"
        },
        {
            "text": "To ensure equitable penalization among possibly unequally sized groups, the parameters \u03bb 0k and \u03bb qk are configured to reflect the group size p k . Suitable default choices are \u03bb 0k = p k \u03bb 0 , \u03bb 1k = \u221a p k \u03bb 1 , and \u03bb 2k = \u03bb 2 , where \u03bb 0 , \u03bb 1 , and \u03bb 2 are nonnegative. The ridge parameter \u03bb 2k is typically fixed across k since the ridge penalty decouples the groupings. 3 For fixed \u03bb q , we take a sequence {\u03bb",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularization parameters"
        },
        {
            "text": "0 yields\u03b2 = 0, and sequentially warm start the algorithms. That is, the solution for \u03bb (t+1) 0 is obtained by using the solution from \u03bb is always different to that corresponding to \u03bb (t) 0 . Proposition 2.2 presents the details of this method, extending an idea of Hazimeh and Mazumder (2020) for best subset.",
            "cite_spans": [
                {
                    "start": 265,
                    "end": 292,
                    "text": "Hazimeh and Mazumder (2020)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Regularization parameters"
        },
        {
            "text": "Let A (t) be the active set of groups. Then running Algorithm 1 initialized to\u03b2 (t) and using",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularization parameters"
        },
        {
            "text": "produces a solution\u03b2 (t+1) such that\u03b2 (t+1) =\u03b2 (t) for any \u03b1 \u2208 [0, 1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularization parameters"
        },
        {
            "text": "3 The decoupling occurs because g k=1 \u03b2 k 2 2 = \u03b2 2 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regularization parameters"
        },
        {
            "text": "This section presents a finite-sample analysis of the proposed estimators. In particular, we state probabilistic upper bounds for the error of estimating the underlying regression function. These bounds accommodate overlapping groups and model misspecification. The role of structure and shrinkage is discussed, and comparisons are made with known bounds for other estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error bounds"
        },
        {
            "text": "The data is assumed to be generated according to the regression model",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "is iid stochastic noise. This flexible specification encompasses the semiparametric model f 0 (x) = j f j (x j ) (with f j zero, linear, or nonlinear) that is the focus of our empirical studies, and the linear model",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": ". . , f 0 (x n )) be the vector of function evaluations at the sample points. The goal of this section is to place probabilistic upper bounds on f 0 \u2212f 2 2 /n, the estimation error off := X\u03b2. The objects of our analysis are the group subset estimators (1.4). We allow the predictor groups G 1 , . . . , G g to overlap, and do not require the group matrices X 1 , . . . , X g to be orthogonal. To simplify the results and their derivations, we constrain the number of nonzero groups rather than penalize them. To this end, let V(s) be the set of all\u03bd such that at most s groups are nonzero: 4",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "We consider the regular group subset estimator:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "and the shrinkage estimator:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "The analysis focuses on the case q = 1 of group lasso shrinkage. The results derived below apply to global minimizers of these nonconvex problems. The algorithms of the preceding section cannot guarantee such minimizers in general, but experiments in the subsequent section demonstrate they frequently attain them. If global optimality is of foremost concern, the output of the algorithms can be used to initialize a mixed-integer optimizer (see Appendix C.1) which can guarantee a global solution at additional computational expense. In recent work, Fan et al. (2020) show that statistical properties of best subset remain valid when the attained minimum is within a neighborhood of the global minimum. We expect their analysis extends to structured settings.",
            "cite_spans": [
                {
                    "start": 551,
                    "end": 568,
                    "text": "Fan et al. (2020)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "We begin with Theorem 3.1, which characterizes an upper bound for group subset with no shrinkage. The notation p max := max k p k represents the maximal group size. As is customary, we absorb numerical constants into the term C > 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bound for group subset"
        },
        {
            "text": "Theorem 3.1. Let \u03b4 \u2208 (0, 1] and \u03b1 \u2208 (0, 1). Then, for some numerical constant C > 0, the group subset estimator (3.1) satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bound for group subset"
        },
        {
            "text": "The first term on the right-hand side of (3.3) is the error incurred by the oracle in approximating f 0 as X\u03b2. In general, this error is unavoidable in finite-dimensional settings. The three terms inside the brackets have the following interpretations. The first term is the cost of estimating \u03b2; with s active groups, there are at most s \u00d7 p max parameters to estimate. The second term is the price of selection; it follows from an upper bound on the total number of group subsets. The third term controls the trade-off between the tightness of the bound and the probability it is satisfied. Finally, the scalar \u03b1 appears in the bound due to the proof technique (as in, e.g., Rigollet 2015) . When f 0 = X\u03b2 0 , \u03b1 need not appear. Hazimeh et al. (2021) independently state an analogous bound for f 0 = X\u03b2 0 in the case of equisized nonoverlapping groups. In the special case that all groups singletons, (3.3) matches the well-known bound for best subset (Raskutti et al. 2011) .",
            "cite_spans": [
                {
                    "start": 677,
                    "end": 691,
                    "text": "Rigollet 2015)",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 731,
                    "end": 752,
                    "text": "Hazimeh et al. (2021)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 954,
                    "end": 976,
                    "text": "(Raskutti et al. 2011)",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Bound for group subset"
        },
        {
            "text": "Theorem 3.1 confirms group subset is preferable to best subset in structured settings. Consider the following example. Suppose we have g groups each of size p 0 so that the total number of predictors is p = g \u00d7 p 0 . It follows for group sparsity level s that the ungrouped selection problem involves choosing s \u00d7 p 0 predictors. Accordingly, the ungrouped bound scales as sp 0 + sp 0 log(p/(sp 0 )) = sp 0 + sp 0 log(g/s). On the other hand, the grouped bound scales as sp 0 + s log(g/s), i.e., it improves by a factor p 0 of the logarithm term.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bound for group subset"
        },
        {
            "text": "We now establish bounds for group subset with shrinkage. The results are analogous to those established in Mazumder et al. (2020) for best subset with shrinkage. Two results are given, a bound where the error decays as 1/ \u221a n, and another where the error decays as 1/n. Adopting standard terminology (e.g., Hastie et al. 2015) , the former bound is referred to as a \"slow rate\" and the latter bound as a \"fast rate.\"",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 129,
                    "text": "Mazumder et al. (2020)",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 307,
                    "end": 326,
                    "text": "Hastie et al. 2015)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "The slow rate is presented in Theorem 3.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "Theorem 3.2. Let \u03b4 \u2208 (0, 1]. Let \u03b3 k be the maximal eigenvalue of the matrix X k X k /n and \u03bb k \u2265 \u221a \u03b3 k \u03c3 \u221a n p k + 2 p k log(g) + p k log(\u03b4 \u22121 ) + 2 log(g) + 2 log(\u03b4 \u22121 ), k = 1, . . . , g.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "Then the group subset estimator (3.2) with q = 1 satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "with probability at least 1 \u2212 \u03b4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "In the case of no overlap, Theorem 3.2 demonstrates that the shrinkage estimator satisfies the same slow rate as group lasso (Lounici et al. 2011, Theorem 3.1 ). An identical expression to Lounici et al. (2011) for \u03bb k can be stated here using a more intricate chi-squared tail bound in the proof. In the case of overlap, the same slow rate can be obtained for group lasso from Percival (2012, Lemma 4) .",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 158,
                    "text": "(Lounici et al. 2011, Theorem 3.1",
                    "ref_id": null
                },
                {
                    "start": 189,
                    "end": 210,
                    "text": "Lounici et al. (2011)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 378,
                    "end": 402,
                    "text": "Percival (2012, Lemma 4)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "The following assumption is required to establish the fast rate.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "Assumption 3.1. Let s < min(n/p max , g)/2. Then there exists a \u03c6(2s) > 0 such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "Assumption 3.1 is satisfied provided no collection of 2s groups have linearly dependent columns in X. This condition is a weaker version of the restricted eigenvalue condition used in Lounici et al. (2011) and Percival (2012) for group lasso, which (loosely speaking) places additional restrictions on the correlations of the columns in X.",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 209,
                    "text": "Lounici et al. (2011) and",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 210,
                    "end": 225,
                    "text": "Percival (2012)",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "The fast rate is presented in Theorem 3.3. The notation \u03bb max := max k \u03bb k represents the maximal shrinkage parameter.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "Theorem 3.3. Let Assumption 3.1 hold. Let \u03b4 \u2208 (0, 1] and \u03b1 \u2208 (0, 1). Let \u03bb 1 , . . . , \u03bb g \u2265 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "Then, for some numerical constant C > 0, the group subset estimator (3.2) with q = 1 satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "with probability at least 1 \u2212 \u03b4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "Theorem 3.3 establishes that the shrinkage estimator achieves the bound of the regular estimator up to an additional term that depends on \u03bb max and \u03c6(2s). By setting the shrinkage parameters to zero, the dependence on these terms vanishes, and the bounds are identical.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "Theorems 3.2 and 3.3 together show that group subset with shrinkage does no worse than group lasso or group subset. This property is helpful because group lasso tends to outperform when the noise is high or the sample size is small, while group subset tends to outperform in the opposite situation. This empirical observation is consistent with the above bounds since the slow rate (3.4) depends on \u03c3/ \u221a n while the fast rate (3.5) depends on \u03c3 2 /n. Hence, (3.4) is typically the tighter of the two bounds for large \u03c3 or small n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bounds for group subset with shrinkage"
        },
        {
            "text": "This section investigates the computational and statistical performance of the proposed estimators on synthetic data. They are compared against group lasso and group versions of SCAD (Fan and Li 2001) and MCP (Zhang 2010) . The range of tuning parameters for each estimator is as follows.",
            "cite_spans": [
                {
                    "start": 183,
                    "end": 200,
                    "text": "(Fan and Li 2001)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 209,
                    "end": 221,
                    "text": "(Zhang 2010)",
                    "ref_id": "BIBREF42"
                }
            ],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "\u2022 Group subset: a grid of \u03bb 0 chosen adaptively using the method of Proposition 2.2, where the first \u03bb 0 sets all coefficients to zero.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "\u2022 Group subset+lasso (q = 1): a grid of \u03bb 1 containing logarithmically spaced points between \u03bb max 1 and \u03bb min 1 = 10 \u22124 \u03bb max 1 , where \u03bb max 1 is the smallest value that sets all coefficients to zero; and for each value of \u03bb 1 , a grid of \u03bb 0 chosen as above.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "\u2022 Group subset+ridge (q = 2): a grid of \u03bb 2 containing logarithmically spaced points between \u03bb max 2 = 100 and \u03bb min 2 = 10 \u22124 ; and for each value of \u03bb 2 , a grid of \u03bb 0 chosen as above.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "\u2022 Group lasso: a grid of \u03bb containing logarithmically spaced points between \u03bb max and \u03bb min = 10 \u22124 \u03bb max , where \u03bb max is the smallest value that sets all coefficients to zero.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "\u2022 Group SCAD: the same grid of \u03bb as above; and for each value of \u03bb, a grid of the nonconvexity parameter \u03b3 containing logarithmically spaced points between \u03b3 max = 100 and \u03b3 min = 2 + 10 \u22124 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "\u2022 Group MCP: the same grid of \u03bb as above; and for each value of \u03bb, a grid of the nonconvexity parameter \u03b3 containing logarithmically spaced points between \u03b3 max = 100 and \u03b3 min = 1 + 10 \u22124 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "Grids of 100 points are used for the primary tuning parameters (\u03bb 0 , \u03bb) and grids of 10 points for the secondary tuning parameters (\u03bb 1 , \u03bb 2 , \u03b3).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "The algorithms of Section 2 are implemented in grpsel. Its efficacy is demonstrated here.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Computational performance"
        },
        {
            "text": "We begin by evaluating the computational performance of grpsel for group subset relative to Gurobi, a global optimizer. Gurobi is applied to a mixed-integer program representation of the group subset problem (see Appendix C.1). The aim here is to understand any optimality gaps from grpsel. Synthetic datasets are generated according to the regression model",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparisons against a global optimizer"
        },
        {
            "text": "where \u03b5 i \u223c N (0, \u03c3 2 ). The predictor matrix X \u2208 R n\u00d7p is generated by taking n iid draws from N (0, \u03a3) and standardizing the columns to have mean zero and unit l 2 -norm. The correlation matrix \u03a3 is constructed element-wise as \u03a3 i,j = \u03c1 1(i =j) , where \u03c1 \u2208 {0, 0.5, 0.9}. This constant correlation structure is chosen to demonstrate algorithmic performance in a worst-case scenario. The noise parameter \u03c3 is set to achieve a signal-to-noise ratio (SNR) of 10, where SNR := Var(f 0 )/\u03c3 2 and f 0 = X\u03b2 0 here. The number of observations n = 500 and number of predictors p = 1, 000. The predictors are divided into 200 evenly-sized groups and five groups have nonzero coefficients: \u03b2 0 1 = \u00b7 \u00b7 \u00b7 = \u03b2 0 5 = 1. We record the optimality gap (square loss relative to that of Gurobi as evaluated at the true sparsity level) and the run time. Gurobi is set with a time limit of 1200 seconds on 32 cores of an AMD Ryzen Threadripper 3970X.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparisons against a global optimizer"
        },
        {
            "text": "The metrics are aggregated over 10 independent simulations and reported in Table 4 .1: Comparisons of algorithms for the group subset estimator. Metrics are aggregated over 10 synthetic datasets generated with SNR = 10, n = 500, p = 1, 000, g = 200, and p 1 = \u00b7 \u00b7 \u00b7 = p g = 5. Averages are reported next to (one) standard errors in parentheses. Abbreviation \"cd\" is coordinate descent and \"cd+ls\" is coordinate descent with local search.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 75,
                    "end": 82,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Comparisons against a global optimizer"
        },
        {
            "text": "(\u03c1 = 0), all three algorithms are equally effective at optimization. In the moderate-(\u03c1 = 0.5) and high-correlation (\u03c1 = 0.9) scenarios, local search proves its worth, nearly eliminating any gap between grpsel and Gurobi. Gurobi remains the gold standard in the higher correlation scenarios, though it does not terminate in the allocated time. Nevertheless, grpsel can deliver solutions nearly as good as those from Gurobi in a fraction of the time. Moreover, the times quoted for grpsel correspond to an entire regularization path of 100 solutions versus a single point on the path for Gurobi. When the additional burden of cross-validating the tuning parameters is factored in, the run times from Gurobi can become prohibitive.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparisons against a global optimizer"
        },
        {
            "text": "We now compare the computational performance of grpsel against grpreg (Breheny and Huang 2015), a popular implementation for group lasso, group SCAD, and group MCP. These estimators each solve different optimization problems, so it does not make sense to ask whether one implementation is faster than another for the same problem. Rather, the purpose of these comparisons is to provide indications of run time and computational complexity for alternative approaches to group selection. Both grpsel and grpreg are set with a convergence tolerance of 10 \u22124 . All run times and iteration counts are measured with reference to the coordinate descent algorithms of each package over a grid of the primary tuning parameter. Where there is a secondary tuning parameter, the figures reported are averaged over the secondary parameter, e.g., the total time taken to evaluate a 100 \u00d7 10 grid of parameters divided by 10. The results as aggregated over 10 synthetic datasets are reported in Metrics are aggregated over 10 synthetic datasets generated with SNR = 10, \u03c1 = 0, n = 500, g = p/5, and p 1 = \u00b7 \u00b7 \u00b7 = p g = 5. Vertical bars represent averages and error bars denote (one) standard errors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparisons against an existing implementation"
        },
        {
            "text": "p = 10, 000, grpsel can compute an entire regularization path in less than a second. Group subset+lasso and group subset+ridge yield the lowest times overall, requiring fewer iterations to converge thanks to shrinkage. For p = 100, 000, grpsel still takes less than 10 seconds to fit a path. grpreg is also impressive in this scenario, though relative to grpsel it is slower. Group lasso converges in the fewest iterations, closely followed by group subset+ridge and group subset+lasso. Group MCP and group SCAD always take more than a thousand iterations to converge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparisons against an existing implementation"
        },
        {
            "text": "We now study the comparative finite-sample properties of the group estimators in fitting sparse semiparametric models. Recall E[y] = j f j (x j ), and f j can be zero, linear, or nonlinear. We follow the approach of Chouldechova and Hastie (2015) in using overlapping groups and regression splines. Briefly, for every x j , an orthogonal spline is computed and two groups are formed: a linear group containing the first term of the spline (assumed equal to x j ) and a nonlinear group containing all terms of the spline. Due to the linear and nonlinear groups overlapping, the fitf j is nonlinear whenever the nonlinear group is selected regardless of whether the linear group is also selected. The group penalty parameters are scaled to control the trade-off between fitting f j as linear or nonlinear. For group subset or group lasso penalty parameter \u03bb, we set \u03bb k = \u03b1\u03bb for k a linear group and \u03bb k = (1 \u2212 \u03b1)\u03bb for k a nonlinear group. The scalar \u03b1 is cross-validated over the grid {0.25, 0.30, . . . , 0.50}. Taking group subset as an example, when \u03b1 = 0.25, a nonlinear group incurs three times the penalization of a linear group so that it is strongly preferable for x j to enter the model linearly. When \u03b1 = 0.50, the penalization is equal, and x j always enters the model nonlinearly.",
            "cite_spans": [
                {
                    "start": 216,
                    "end": 246,
                    "text": "Chouldechova and Hastie (2015)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Statistical performance"
        },
        {
            "text": "Synthetic datasets are generated according to the regression model",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "where f 0 (x i ) = j f 0 j (x ij ) and \u03b5 i \u223c N (0, \u03c3 2 ). The matrix X = (x 1 , . . . , x n ) \u2208 R n\u00d71000 is treated as fixed and constructed as follows. First, n iid observations are drawn from N (0, \u03a3), where \u03a3 is Toeplitz with elements \u03a3 i,j = \u03c1 |i\u2212j| and \u03c1 \u2208 {0.5, 0.9}. The standard normal distribution function is applied to each column of this sample to produce uniformly distributed variables that conform to \u03a3 (see Falk 1999) . Finally, each column is min-max scaled to the interval [\u22121, 1]. Ten columns are selected at random to construct the response: six using the linear function f (x) = x, two using f (x) = cos(\u03c0x), and two using f (x) = sin(\u03c0x). The remaining columns are unassociated with the response, i.e., f (x) = 0 for these. All functions evaluations are scaled to mean zero and variance one. The noise parameter \u03c3 is chosen to attain SNR = 1. We take n between 100 and 1000 over a grid of seven values equispaced on the log scale.",
            "cite_spans": [
                {
                    "start": 423,
                    "end": 433,
                    "text": "Falk 1999)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "The matrix X is formed by expanding each column of X using a cubic thin-plate spline containing three knots at equispaced quantiles. 5 Four terms are in each spline so that p = 4, 000. The number of groups g = 2, 000.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "As a measure of loss, we report estimation error relative to the null model:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "The splines are computed using the basis_tps function of the R package npreg.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "The best possible relative estimation error is zero and the null value is one. In addition, we report the number of fitted functions that are nonzero. As a measure of support recovery, we report the micro F1-score for the classification of linear and nonlinear functions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "F1-score := 2 \u00b7 True positives 2 \u00b7 True positives + False positives + False negatives .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "The best possible F1-score is zero and the null value is one. These metrics are all evaluated with respect to tuning parameters that minimize five-fold cross-validation error. Coordinate descent with local search is used to compute the group subset estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Setup"
        },
        {
            "text": "The metrics under consideration are aggregated over 10 simulations and reported in Figure 4 .2.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 83,
                    "end": 91,
                    "text": "Figure 4",
                    "ref_id": null
                }
            ],
            "section": "Results"
        },
        {
            "text": "The first column of plots pertains to \u03c1 = 0.5, the moderate configuration of the correlation parameter-the more challenging configuration where \u03c1 = 0.9 relates to the second column. The solid lines are averages and the error bars are standard errors. Consider first the three group subset estimators. Group subset exhibits excellent performance for large n but fares relatively poorly in smaller sample sizes, roughly when n < 400 in the moderate-correlation scenario and later when n < 500 in the high-correlation scenario. Thanks to their shrinkage effect, group lasso and ridge penalization improve the performance of group subset considerably in smaller samples. As n increases, group subset+lasso and group subset+ridge lower the degree of shrinkage, eventually behaving like group subset. In contrast to group subset, the shrinkage estimators start with fairly dense models for small n and move towards sparser models as n increases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Group lasso behaves contrarily to group subset, performing relatively capably for small n but poorly for large n. In addition, it has the unappealing property of failing to converge to the correct sparsity level, leading to mediocre F1-scores. Group SCAD and group MCP try to correct this behavior via their nonconvexity parameters by tapering off the degree of shrinkage. Group MCP is more successful in its attempts than group SCAD. Even so, there remains a gap between group MCP and the group subset estimators. The latter converge earlier to the right sparsity level and the correct model. The gap is most stark in the high-correlation scenario.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Appendix C.2 presents classification results. The findings are broadly consistent with those above.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "This section studies two contemporary problems: modeling foot traffic in major supermarkets and modeling recessions in the business cycle. Both problems are characterized by the availability of many candidate predictors and the possibility for misspecification of linear models. These characteristics motivate consideration of sparse semiparametric models. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data analyses"
        },
        {
            "text": "The first dataset contains anonymized data on foot traffic and sales volumes for a major Chinese supermarket (see Wang 2009). 6 The task is to model foot traffic using the sales volumes of different products. To facilitate managerial decision-making, the fitted model should identify a subset of products that well-predict foot traffic (i.e., it should be sparse). The sample contains n = 464 days. We randomly hold out 10% of the data as a testing set and use the remaining data as a training set. Sales volumes are available for 6,398 products. A four-term thin-plate spline is used for each product, resulting in p = 25, 592 predictors and g = 12, 796 groups. As a measure of predictive accuracy, we report the out-of-sample mean square error:",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 127,
                    "text": "Wang 2009). 6",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Supermarket foot traffic"
        },
        {
            "text": "where T indexes observations in the testing set and\u0177 i is a prediction of y i . We also report the number of fitted functions that are nonzero. As benchmarks, we include random forest and lasso, which respectively produce dense nonparametric models and sparse linear models. In addition, the (unconditional) mean is evaluated as a predictive method to assess the value added by the predictors. Five-fold cross-validation is used to choose tuning parameters. The group subset estimators are computed using coordinate descent (without local search).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supermarket foot traffic"
        },
        {
            "text": "The metrics under consideration are aggregated over 10 training-testing set splits and reported in Table 5 estimators used to fit sparse semiparametric models all lead to lower mean square error than other methods. Group subset+ridge yields the best predictions, followed by group subset+lasso. The latter estimator has the edge of selecting the fewest products among the sparse methods on average. Lasso yields models that are nearly as sparse but at the same time are also less predictive, signifying linearity might be too restrictive. Random forest is markedly worse than any sparse method. It appears only a small fraction of products explain foot traffic, around 2-3%.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 99,
                    "end": 106,
                    "text": "Table 5",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Supermarket foot traffic"
        },
        {
            "text": "The second dataset contains monthly data on macroeconomic series for the United States (see McCracken and Ng 2016) . 7 The dataset is augmented with the National Bureau of Economic Research recession indicator, a dummy variable that indicates whether a given month is a period of recession or expansion. 8 The task is to model the recession indicator using the macroeconomic series. Such models are useful for scenario analysis and for assessing economic conditions in the absence of low-frequency variables such as quarterly GDP growth. The sample contains n = 739 months, ending in September 2020. It includes the COVID-19 recession. We again randomly hold out 10% of the data as a testing set and use the remaining data as a training set. Because there are relatively few recessionary periods, a stratified split is applied so that the proportion of recessions in the testing and training sets are equal. The dataset has 128 macroeconomic series. Applying a four-term thin-plate spline to each series yields p = 512 predictors and g = 256 groups. To evaluate predictive accuracy, we report the out-of-sample mean logistic loss:",
            "cite_spans": [
                {
                    "start": 92,
                    "end": 114,
                    "text": "McCracken and Ng 2016)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 304,
                    "end": 305,
                    "text": "8",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Economic recessions"
        },
        {
            "text": "The remaining metrics and methods are as before. The group subset estimators are computed using coordinate descent with local search.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Economic recessions"
        },
        {
            "text": "The results as aggregated over 10 training-testing set splits are reported in Table 5 .2. Sparse semiparametric models predict recessionary periods well. Group subset+ridge is again best, while lasso is worst, highlighting the value in allowing for nonlinearity. Compared with the other estimators, group subset+ridge delivers almost entirely nonlinear models. Its models are also denser on average. On the other hand, group subset+lasso, group lasso, and group MCP use an even split between linear and nonlinear functions. Group subset+lasso yields the lowest loss among these three, and it may be preferable to group subset+ridge if interpretability is sought. All methods improve on the mean, illustrating the predictive content of monthly series. Unlike the products in the supermarket dataset, the identifiers of the macroeconomic series are available, allowing us to inspect the selected series. In the interest of space, we focus on a subset of six series used in McCracken et al. (2021) as determinants of recessions. For this subset, the type of function fit by each estimator as applied to all n = 739 observations is reported in Table 5 minus federal funds rate) and FEDFUNDS (federal funds rate) are treated nonlinearly by every estimator. PAYEMS (nonfarm employment) is fit linear by some and nonlinear by others. The group subset estimators are the only ones to select W875RX1 (real personal income excluding transfers), both of which use a nonlinear fit. Group subset+ridge is the only estimator to select all six series, including CMRMTSPLx (manufacturing and trade sales) and INDPRO (industrial production). Lasso selects only a single series, suggesting it might be unable to approximate the nonlinear effects adequately.",
            "cite_spans": [
                {
                    "start": 971,
                    "end": 994,
                    "text": "McCracken et al. (2021)",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [
                {
                    "start": 78,
                    "end": 85,
                    "text": "Table 5",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 1140,
                    "end": 1147,
                    "text": "Table 5",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Economic recessions"
        },
        {
            "text": "Despite a broad array of applications, subset selection for grouped predictors is not wellstudied, especially in high dimensions where it has remained computationally illusive. This paper represents an effort to close the gap in the general setting of overlapping groups. Our optimization framework consists of low complexity algorithms that come with convergence results. A theoretical analysis of the proposed estimators illuminates some of their finitesample properties. They estimators behave favorably in simulation, exhibiting excellent support recovery when fitting sparse semiparametric models. In real-world modeling tasks, they improve on popular benchmarks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concluding remarks"
        },
        {
            "text": "Our implementation grpsel is available on the R repository CRAN.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concluding remarks"
        },
        {
            "text": "Proof. Denote by \u03b2 the result of applying the thresholding function (2.5) to\u03b2. Starting from the inequality (2.2) with \u03b2 = \u03b2 , we add \u2126(\u03b2 ) to both sides to obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2 Proof of Lemma 2.2"
        },
        {
            "text": "where the left-hand side follows from definition (2.3). Addingc k /2 \u03b2 k \u2212\u03b2 k 2 2 to both sides and rearranging terms leads to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2 Proof of Lemma 2.2"
        },
        {
            "text": "UsingFc k (\u03b2 ;\u03b2) \u2264Fc k (\u03b2;\u03b2) = F (\u03b2), we reorganize terms to get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2 Proof of Lemma 2.2"
        },
        {
            "text": "and sum both sides of the inequality (A.3) over 1 \u2264 k \u2264 g to get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2 Proof of Lemma 2.2"
        },
        {
            "text": "By definition \u03b7 (m) 0 = \u03b2 (m) and \u03b7 (m) g = \u03b2 (m+1) , establishing {F (\u03b2 (m) )} m\u2208N is decreasing. Since F (\u03b2) is bounded below, {F (\u03b2 (m) )} m\u2208N must converge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2 Proof of Lemma 2.2"
        },
        {
            "text": "The proof relies on the following lemma that the active set must stabilize in finitely many iterations. The lemma is established by contradiction along the lines of Dedieu et al. (2020, Theorem 1) .",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 196,
                    "text": "Dedieu et al. (2020, Theorem 1)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A.3 Proof of Theorem 2.1"
        },
        {
            "text": "Lemma A.1. Letc k > c k for all k = 1, . . . , g. Then the sequence of iterates {\u03b2 (m) } m\u2208N stabilizes to a fixed support within a finite number of iterations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3 Proof of Theorem 2.1"
        },
        {
            "text": "Proof. Suppose the support does not stabilize in finitely many iterations. Choose an m such that gs(\u03b2 (m+1) ) = gs(\u03b2 (m) ). Then at least one group was added or removed from the support, i.e., there is a k such that either (1) \u03b2 ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3 Proof of Theorem 2.1"
        },
        {
            "text": "and, because \u03b2 (m+1) k is the output of the thresholding function (2.5), it holds \u03b2 (m+1) k \u2265 2\u03bb 0k /(c k + 2\u03bb 2k ). These inequalities together imply",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3 Proof of Theorem 2.1"
        },
        {
            "text": "Similar working yields the same inequality for case (2). Forc k > c k , the quantity on the right-hand side is strictly positive. Hence, a change to the support yields a strict decrease in the objective value. However, if the support changes infinitely many times, this contradicts that F (\u03b2) is bounded below. Thus, the support must stabilize in finitely many iterations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3 Proof of Theorem 2.1"
        },
        {
            "text": "We are now ready to prove Theorem 2.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3 Proof of Theorem 2.1"
        },
        {
            "text": "Proof. From Lemma A.1, there exists a finite m such that the subsequence of iterates {\u03b2 (m) } m\u2265m share the same active set, say A. Hence, for all m \u2265 m and k \u2208 A we hav\u0113",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3 Proof of Theorem 2.1"
        },
        {
            "text": "i.e., the group subset penalty can be treated as fixed. Denote by \u2207 2 vFck (\u03b2; \u03b2 (m) ) the second directional derivative ofFc k (\u03b2; \u03b2 (m) ) along the vector v \u2208 R p . The infimum of the minimal eigenvalue of \u2207 2 vFck (\u03b2; \u03b2 (m) ) over all \u03b2 k and v isc k + 2\u03bb 2k . Sincec k > 0 and \u03bb 2k \u2265 0,Fc k (\u03b2; \u03b2 (m) ) is strictly convex. Furthermore, Lemma 2.2 implies the level set {\u03b2 \u2208 R p : F (\u03b2) \u2264 F (\u03b2 (0) )} is bounded when the initialization \u03b2 (0) \u2208 R p , and hence {\u03b2 (m) } m\u2265m is bounded. These conditions are sufficient to invoke Tseng (2001, Theorem 5.1) and establish {\u03b2 (m) } m\u2265m converges to a stationary point \u03b2 ofFc k (\u03b2; \u03b2 ). We conclude by the equalityFc k (\u03b2 ; \u03b2 ) = F (\u03b2 ) that \u03b2 is also a stationary point of F (\u03b2).",
            "cite_spans": [
                {
                    "start": 529,
                    "end": 554,
                    "text": "Tseng (2001, Theorem 5.1)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A.3 Proof of Theorem 2.1"
        },
        {
            "text": "Proof. Under the conditions of Theorem 2.1, Algorithm 1 is guaranteed to converge to a stationary point\u03b2 (t) such that for all k \u2208 A (t) it holds",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 108,
                    "text": "(t)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A.4 Proof of Proposition 2.2"
        },
        {
            "text": "Then initializing Algorithm 1 with\u03b2 (t) and using \u03bb 0 = \u03bb",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.4 Proof of Proposition 2.2"
        },
        {
            "text": "We briefly outline other algorithmic components of grpsel. These components are similar to those used in the coordinate descent literature (Friedman et al. 2007; Breheny and Huang 2011; Hazimeh and Mazumder 2020) .",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 161,
                    "text": "(Friedman et al. 2007;",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 162,
                    "end": 185,
                    "text": "Breheny and Huang 2011;",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 186,
                    "end": 212,
                    "text": "Hazimeh and Mazumder 2020)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "A.5 Other components"
        },
        {
            "text": "While Algorithm 1 is running, the residuals r (m) = y \u2212 X\u03b2 (m) are required in the gradient calculations. Instead of recomputing r (m) during each update, the vector is kept in memory and updated only when \u03b2 (m) changes. When required, the update to r (m) can be performed",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.5.1 Sparse residual updates"
        },
        {
            "text": "Rather than cycling through all g groups in each coordinate descent round, it is convenient to restrict the updates to a smaller set of screened groups. The initialization \u03b2 (0) can be used to compute the group-wise gradients { \u2207 k L(\u03b2 (0) ) 2 / \u221a p k } k \u2208A (0) which are already available as a consequence of selecting \u03bb 0 dynamically (see Proposition 2.2). The inactive groups whose gradients are among the top 500 largest are classed as \"strong,\" in addition to the active set of groups. The remaining groups are classed as \"weak.\" The coordinate descent updates are restricted to the strong groups until convergence is achieved, at which time a further round over the weak groups is performed. If the solution does not change after this further round, convergence is declared. Otherwise, any weak groups that have become active are shifted to the strong set, and the process is repeated. Gradient screening is also used in local search. Rather than searching through all inactive groups in the inner loop of Algorithm 2, only the inactive groups whose gradients are among the largest 5% are enumerated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.5.2 Gradient screening"
        },
        {
            "text": "The solutions produced by coordinate descent often benefit from greedily ordering the groups. At the beginning of the algorithm, the groups are sorted according to their gradients. Any groups in A (0) are placed first. The coordinate descent updates then proceed using this new ordering.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.5.3 Gradient ordering"
        },
        {
            "text": "The set of active groups typically stabilizes after several rounds of coordinate descent updates. At this time, several additional rounds are required for the nonzero coefficients to converge. Rather than cycling through the full set (or screened set) of groups, the updates are restricted to the active groups only, usually a small subset. Once convergence is achieved on the active set, a further round is performed over the inactive set to confirm overall convergence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.5.4 Active set updates"
        },
        {
            "text": "The proof requires the following lemma.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "Lemma B.1. Let \u03b4 \u2208 (0, 1]. Let X \u2208 R n\u00d7p be a fixed matrix and \u03b5 \u2208 R n be a N (0, \u03c3 2 I) random vector. Define \u03b8 := g k=1 \u03bd (k) and the random event A\u03bd := |\u03b5 X\u03b8| \u2265 X\u03b8 2 C 1 \u03c3 sp max + s log g s + log(\u03b4 \u22121 ) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "Then, for some numerical constant C 1 > 0, the probability of the union \u222a\u03bd \u2208V(2s) A\u03bd is at most \u03b4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "Proof. For A \u2286 {1, . . . , g}, a set of active groups, denote by S A := \u222a k\u2208A G k the set of active predictors. Denote the singular value decomposition of X A by U A D A V A , where X A are the columns of X indexed by S A . Define the set of unit vectors B r 2 := {u \u2208 R r : u 2 \u2264 1}, and the set of s group-sparse subsets P(s) := {A \u2286 {1, . . . , g} : |A| = s}. For all\u03bd \u2208 V(2s) such that \u03b8 = 0, it holds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "For any t \u2208 R, this inequality implies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "Applying Boole's inequality to the right-hand side yields with respect to l 2 -norm that satisfies |E |S A | | \u2264 (3/ ) |S A | . Such an E |S A | is guaranteed to exist for \u2208 (0, 1) (Rigollet 2015, Lemma 1.18). Setting = 1/2, it holds for any A \u2208 P(2s) and any z \u2208 E sup u\u2208B |S A | 2 |\u03b5 U A u| \u2264 2 sup",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "Applying Boole's inequality to this bound yields A\u2208P(2s) P 2 sup",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "The cardinality of P(2s) satisfies |P(2s)| = g 2s \u2264 log(eg/(2s)) 2s . For any A \u2208 P(2s), the cardinality of E |S A | satisfies |E |S A | | \u2264 6 |S A | \u2264 6 2spmax . Since U A is orthonormal and z has unit length, the random variable \u03b5 U A z \u223c N (0, \u03c3 2 ). Using a standard Gaussian tail bound (Rigollet 2015, Lemma 1.4), we have P 2|\u03b5 U A z| \u2265 t \u2264 2 exp \u2212 t 2 8\u03c3 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "It follows from the chain of inequalities above P \u222a\u03bd \u2208V(2s) |\u03b5 X\u03b8| \u2265 X\u03b8 2 t \u2264 2 exp \u2212 t 2 8\u03c3 2 + 2sp max log(6) + 2s log eg 2s .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "Setting t \u2265 8\u03c3 2 [log(2) + 2sp max log(6) + 2s log(eg/(2s)) + log(\u03b4 \u22121 )] concludes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "We are now ready to prove Theorem 3.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 Proof of Theorem 3.1"
        },
        {
            "text": "The proof requires the following lemma.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "Lemma B.2. Let \u03b4 \u2208 (0, 1]. Let X \u2208 R n\u00d7p be a fixed matrix and \u03b5 \u2208 R n be a N (0, \u03c3 2 I) random vector. Let \u03b3 k be the maximal eigenvalue of X k X k /n, where X k is the submatrix of X corresponding to group k. Define the random event A k = X k \u03b5 2 \u2265 \u221a n\u03b3 k \u03c3 p k + 2 p k log(g) + p k log(\u03b4 \u22121 ) + 2 log(g) + 2 log(\u03b4 \u22121 ) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "Then the probability of the union \u222a g k=1 A k is at most \u03b4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "Proof. Denote the singular value decomposition of X k by U k D k V k . Using the properties of the operator norm, it holds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "For any t \u2208 R, this inequality implies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "Applying Boole's inequality to the right-hand side yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "Since U k is orthonormal, the random variable U k \u03b5 2 2 /\u03c3 2 \u223c \u03c7 2 (p k ). Using a standard chisquared tail bound (Laurent and Massart 2000, Lemma 1), we have for t = p k + \u221a 2p k x + 2x and x > 0 P U k \u03b5 2 \u2265 \u03c3 \u221a t \u2264 exp(\u2212x).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "It follows from the chain of inequalities above P \u222a g k=1 X k \u03b5 2 \u2265 \u221a n\u03b3 k \u03c3 p k + 2p k x + 2x \u2264 exp(\u2212x + log(g)).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "Setting x \u2265 log(g) + log(\u03b4 \u22121 ) concludes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "We are now ready to prove Theorem 3.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "Proof. For any\u03bd \u2208 V(s) and any \u03b2 \u2208 R p such that \u03b2 = g k=1 \u03bd (k) , we have Finally, putting the bounds together and simplifying the resulting expression, we have 1 n f 0 \u2212 X\u03b2 2 2 \u2264 1 + \u03b1 (1 \u2212 \u03b1)n f 0 \u2212 X\u03b2 2 2 + 4C 2 1 \u03c3 2 \u03b1(1 \u2212 \u03b1)n sp max + s log g s + log(\u03b4 \u22121 )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "holding with probability at least 1 \u2212 \u03b4 for \u03b1 \u2208 (0, 1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Proof of Theorem 3.2"
        },
        {
            "text": "C.1 Mixed-integer program where (\u03b2 k , 1 \u2212 z k ) : SOS-1 is a special ordered set (SOS) constraint of type one and M is an optional upper bound on \u03b2 \u221e . These SOS constraints have the property",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix C Simulations"
        },
        {
            "text": "thereby enforcing group sparsity on \u03b2. The bounding constant M assists the solver in terminating quicker. In the experiments, M = 1.25 \u03b2 \u221e where\u03b2 is the least-squares fit on the true set of groups. When groups overlap, the approach of expanding the predictor matrix as described in Section 2 is applicable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix C Simulations"
        },
        {
            "text": "The response is generated as a Bernoulli variable: P(y i = 1) = 1 1 + exp(\u2212f 0 (x i ))",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Classification results"
        },
        {
            "text": ", i = 1, . . . , n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Classification results"
        },
        {
            "text": "The remainder of the simulation design and evaluation metrics are the same as for regression. Figure C .1 reports the results. Compared with regression, group SCAD now performs better in support recovery, though not as well as the group subset estimators. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 94,
                    "end": 102,
                    "text": "Figure C",
                    "ref_id": null
                }
            ],
            "section": "C.2 Classification results"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "First-order methods in optimization",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Beck",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "MOS-SIAM Series on Optimization",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Sparsity constrained nonlinear optimization: Optimality conditions and algorithms",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Beck",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "C"
                    ],
                    "last": "Eldar",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "SIAM Journal on Optimization",
            "volume": "23",
            "issn": "",
            "pages": "1480--1509",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "On the convergence of block coordinate descent type methods",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Beck",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Tetruashvili",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "SIAM Journal on Optimization",
            "volume": "23",
            "issn": "",
            "pages": "2037--2060",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "OR Forum-An algorithmic approach to linear regression",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Bertsimas",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "King",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Operations Research",
            "volume": "64",
            "issn": "1",
            "pages": "2--16",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Breheny",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Annals of Applied Statistics",
            "volume": "5",
            "issn": "",
            "pages": "232--253",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors",
            "authors": [],
            "year": 2015,
            "venue": "Statistics and Computing",
            "volume": "25",
            "issn": "",
            "pages": "173--187",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Heuristics of instability and stabilization in model selection",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Annals of Statistics",
            "volume": "24",
            "issn": "",
            "pages": "2350--2383",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Testing for structural breaks in dynamic factor models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Breitung",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Eickmeier",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Journal of Econometrics",
            "volume": "163",
            "issn": "",
            "pages": "71--84",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Generalized additive model selection",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chouldechova",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1506.03850"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Forecasting using a large number of predictors: Is Bayesian shrinkage a valid alternative to principal components?",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "De Mol",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Giannone",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Reichlin",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Journal of Econometrics",
            "volume": "146",
            "issn": "",
            "pages": "318--328",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Learning sparse classifiers: Continuous and mixed integer optimization perspectives",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dedieu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hazimeh",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mazumder",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2001.06471"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "A simple approach to the generation of uniformly distributed random variables with prescribed correlations",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Falk",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Communications in Statistics -Simulation and Computation",
            "volume": "28",
            "issn": "",
            "pages": "785--791",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "When is best subset selection the \"best",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2007.01478"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Variable selection via nonconcave penalized likelihood and its oracle properties",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Journal of the American Statistical Association",
            "volume": "456",
            "issn": "",
            "pages": "1348--1360",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Pathwise coordinate optimization",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "H\u00f6fling",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Annals of Applied Statistics",
            "volume": "1",
            "issn": "2",
            "pages": "302--332",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Group subset selection for linear regression",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Berman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Computational Statistics and Data Analysis",
            "volume": "75",
            "issn": "",
            "pages": "39--52",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Best subset, forward stepwise or lasso? Analysis and recommendations based on extensive comparisons",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Statistical Science",
            "volume": "35",
            "issn": "",
            "pages": "579--592",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Statistical learning with sparsity. Chapman & Hall/CRC Monographs on Statistics and Applied Probability",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                },
                {
                    "first": "Wainwright",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Fast best subset selection: Coordinate descent and local combinatorial optimization algorithms",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hazimeh",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mazumder",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Operations Research",
            "volume": "68",
            "issn": "5",
            "pages": "1517--1537",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Grouped variable selection with discrete optimization: Computational and statistical perspectives",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hazimeh",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mazumder",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Radchenko",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2104.07084"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Ridge regression: Biased estimation for nonorthogonal problems",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Hoerl",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "W"
                    ],
                    "last": "Kennard",
                    "suffix": ""
                }
            ],
            "year": 1970,
            "venue": "Technometrics",
            "volume": "12",
            "issn": "1",
            "pages": "55--67",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Group lasso with overlap and graph lasso",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jacob",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Obozinski",
                    "suffix": ""
                },
                {
                    "first": "J.-P",
                    "middle": [],
                    "last": "Vert",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the 26th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "433--440",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Adaptive estimation of a quadratic functional by model selection",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Laurent",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Massart",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Annals of Statistics",
            "volume": "28",
            "issn": "",
            "pages": "1302--1338",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Forecasting macroeconomic time series: Lasso-based approaches and their forecast combinations with dynamic factor models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International Journal of Forecasting",
            "volume": "30",
            "issn": "",
            "pages": "996--1015",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Learning interactions via hierarchical group-lasso regularization",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lim",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Journal of Computational and Graphical Statistics",
            "volume": "24",
            "issn": "",
            "pages": "627--654",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Sparse partially linear additive models",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bien",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Caruana",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gehrke",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Journal of Computational and Graphical Statistics",
            "volume": "25",
            "issn": "",
            "pages": "1026--1040",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Oracle inequalities and optimal inference under group sparsity",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lounici",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pontil",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Van De Geer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Tsybakov",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Annals of Statistics",
            "volume": "39",
            "issn": "",
            "pages": "2164--2204",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Subset selection with shrinkage: Sparse linear modeling when the SNR is low",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mazumder",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Radchenko",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dedieu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1708.03288"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Binary conditional forecasts",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Mccracken",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Mcgillicuddy",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "T"
                    ],
                    "last": "Owyang",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Journal of Business and Economic Statistics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "FRED-MD: A monthly database for macroeconomic research",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Mccracken",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Journal of Business and Economic Statistics",
            "volume": "34",
            "issn": "",
            "pages": "574--589",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "The group lasso for logistic regression",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Meier",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Van De Geer",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "B\u00fchlmann",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
            "volume": "70",
            "issn": "",
            "pages": "53--71",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Group lasso with overlaps: The latent group lasso approach",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Obozinski",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jacob",
                    "suffix": ""
                },
                {
                    "first": "J.-P",
                    "middle": [],
                    "last": "Vert",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1110.0413"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Multi-task feature selection",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Obozinski",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Taskar",
                    "suffix": ""
                },
                {
                    "first": "Jordan",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Theoretical properties of the overlapping groups lasso",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Percival",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Electronic Journal of Statistics",
            "volume": "6",
            "issn": "",
            "pages": "269--288",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Minimax rates of estimation for highdimensional linear regression over q -balls",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Raskutti",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Wainwright",
                    "suffix": ""
                },
                {
                    "first": "Yu",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "57",
            "issn": "",
            "pages": "6976--6994",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Sparse additive models",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ravikumar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lafferty",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wasserman",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
            "volume": "71",
            "issn": "",
            "pages": "1009--1030",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "18.S997: High dimensional statistics. Lecture notes",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Rigollet",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Standardization and the group lasso penalty",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Simon",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Statistica Sinica",
            "volume": "22",
            "issn": "",
            "pages": "983--1001",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Regression shrinkage and selection via the lasso",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
            "volume": "58",
            "issn": "1",
            "pages": "267--288",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Convergence of a block coordinate descent method for nondifferentiable minimization",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Tseng",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Journal of Optimization Theory and Applications",
            "volume": "109",
            "issn": "",
            "pages": "475--494",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Forward regression for ultra-high dimensional variable screening",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Journal of the American Statistical Association",
            "volume": "104",
            "issn": "",
            "pages": "1512--1524",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Model selection and estimation in regression with grouped variables",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
            "volume": "68",
            "issn": "",
            "pages": "49--67",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Nearly unbiased variable selection under minimax concave penalty",
            "authors": [
                {
                    "first": "C.-H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Annals of Statistics",
            "volume": "38",
            "issn": "",
            "pages": "894--942",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Certifiably polynomial algorithm for best group subset selection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Wang",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2104.12576"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "is computed in such a way that the active set of groups corresponding to \u03bb (t+1) 0",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The vertical bars are averages and the error bars are standard errors. For p = 1",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Comparisons of packages and estimators for group selection.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Comparisons of estimators for sparse semiparametric regression. Metrics are aggregated over 10 synthetic datasets generated with SNR = 1, p = 4, 000, and g = 2, 000. Solid lines represent averages and error bars denote (one) standard errors. Dashed lines indicate the true number of nonzero functions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Averages are reported with standard errors in parentheses.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Consider case (1). It follows from Lemma 2.2",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "|\u03b5 X(\u03b2 \u2212 \u03b2)| + 2 g k=1 \u03bb k \u03bd (k) 2 \u2212 \u03bd (k) 2 . (B.1)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Comparisons of estimators for sparse semiparametric classification. Metrics are aggregated over 10 synthetic datasets generated with p = 4, 000 and g = 2, 000. Solid lines represent averages and error bars denote (one) standard errors. Dashed lines indicate the true number of nonzero functions.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Table 4.1. Averages are reported with standard errors in parentheses. In the uncorrelated scenario Optimality gap Run time (secs.)",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": ".3. A checkmark indicates the series entered the model linearly, while a checked box indicates it entered nonlinearly. Except for lasso, T10YFFM (10-year treasury rate",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Estimated function type for select macroeconomic series. Checkmarks indicate a linear fit and checked boxes indicate a nonlinear fit.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "|\u03b5 U A u| \u2265 t |\u03b5 U A u| \u2265 tWe bound the supremum over B |S A | 2 using an -net argument. Let E |S A | be an -net of B",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Ryan Thompson's research was supported by an Australian Government Research Training Program (RTP) Scholarship.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        },
        {
            "text": "A.1 Proof of Proposition 2.1Proof. The subscript k is dropped from c k , \u03bb 0k , \u03bb 1k , and \u03bb 2k to simplify the notation. Since the objective is treated as a function in the kth group of coordinates \u03b2 k only, we hav\u0113where\u03b2 k =\u03b2 k \u2212 1/c\u2207 k L(\u03b2). When \u03bb 1 = 0, it is not hard to see a minimizer ofF c (\u03b2;\u03b2) isWhen \u03bb 0 = 0 and \u03bb 1 > 0 (and hence \u03bb 2 = 0), the minimizer isThis expression follows from the proximal operator for the l 2 -norm (Beck 2017) . Combining (A.1) with (A.2) leads to the result of the proposition.Proof. Take any\u03bd \u2208 V(s) and any \u03b2 \u2208 R p such that \u03b2 = g k=1 \u03bd (k) . Optimality of\u03bd andwhich, after some algebra, leads to) not equal to 0. An application of Lemma B.1 thus yields the high-probability upper boundwhere the last line follows from Minkowski's inequality for l p -norms. Using Young's inequality (2ab \u2264 \u03b1a 2 + \u03b1 \u22121 b 2 for \u03b1 > 0), the first term on the right-hand side is bounded asA bound for the second term on the right-hand side follows similarly. Putting the results together and rearranging terms, we arrive atholding with probability at least 1 \u2212 \u03b4 for \u03b1 \u2208 (0, 1). Taking C \u2265 2C 2 1 completes the proof.The Cauchy-Schwarz inequality and Minkowski's inequality are applied in turn to getApplying Lemma B.2, and using the assumed lower bound on \u03bb k , yieldswith high-probability. Plugging this bound into (B.1), we arrive atholding with probability at least 1 \u2212 \u03b4.",
            "cite_spans": [
                {
                    "start": 438,
                    "end": 449,
                    "text": "(Beck 2017)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Appendix A Computation"
        },
        {
            "text": "Proof. Begin with inequality (B.1). First, we bound the term 2/n|\u03b5 X(\u03b2 \u2212 \u03b2)|. Lemma B.1 gives the high-probability upper boundUsing Young's inequality (2ab \u2264 \u03b1/2a 2 + 2/\u03b1b 2 ), the first term on the right-hand side is bounded asThe second term on the right-hand side is bounded similarly. Now, we bound the remaining term 2 g k=1 \u03bb k ( \u03bd (k) 2 \u2212 \u03bd (k) 2 ) in (B.1). Minkowski's inequality and Assumption 3.1 giveUsing Minkowski's inequality again, we haveTwo applications of Young's inequality yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.3 Proof of Theorem 3.3"
        },
        {
            "text": "All series are made stationary using standard transformations given in McCracken and Ng (2016) . Some series contain missing observations and outliers, which are also treated as missing. These missing values are imputed using the na_kalman function of the R package imputeTS. Following Breitung and Eickmeier (2011) , an observation x i is treated as an outlier if |x i \u2212 Q 2 |/(Q 3 \u2212 Q 1 ) > 6 where Q 1 , Q 2 , and Q 3 are the respective quartiles of the data.",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 94,
                    "text": "McCracken and Ng (2016)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 286,
                    "end": 315,
                    "text": "Breitung and Eickmeier (2011)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "D.1 Macroeconomic data preprocessing"
        }
    ]
}
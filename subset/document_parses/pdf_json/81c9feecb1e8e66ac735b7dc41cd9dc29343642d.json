{
    "paper_id": "81c9feecb1e8e66ac735b7dc41cd9dc29343642d",
    "metadata": {
        "title": "Balancing Efficiency and Comfort in Robot-Assisted Bite Transfer",
        "authors": [
            {
                "first": "Suneel",
                "middle": [],
                "last": "Belkhale",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Cornell University",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Ethan",
                "middle": [
                    "K"
                ],
                "last": "Gordon",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Yuxiao",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Cornell University",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Siddhartha",
                "middle": [],
                "last": "Srinivasa",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Tapomayukh",
                "middle": [],
                "last": "Bhattacharjee",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Dorsa",
                "middle": [],
                "last": "Sadigh",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Cornell University",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": ": Our method finds feasible bite transfer trajectories in simulation. Given the food geometry and pose on the fork, we sample at least N goal food poses that are checked for collisions with the mouth geometry using a learned constraint model. Next, we cluster the goal poses and use heuristic-guided BiRRT to reach cluster centroids with comfort (blue) and bite volume efficiency (orange) heuristics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Abstract-Robot-assisted feeding in household environments is challenging because it requires robots to generate trajectories that effectively bring food items of varying shapes and sizes into the mouth while making sure the user is comfortable. Our key insight is that in order to solve this challenge, robots must balance the efficiency of feeding a food item with the comfort of each individual bite. We formalize comfort and efficiency as heuristics to incorporate in motion planning. We present an approach based on heuristics-guided bi-directional Rapidlyexploring Random Trees (h-BiRRT) that selects bite transfer trajectories of arbitrary food item geometries and shapes using our developed bite efficiency and comfort heuristics and a learned constraint model. Real-robot evaluations show that optimizing both comfort and efficiency significantly outperforms a fixed-pose based method, and users preferred our method significantly more than that of a method that maximizes only user comfort. Videos and Appendices are found on our website.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Imagine a setting where you want to pick up a piece of food, e.g., a baby carrot, from a salad bowl to eat. Nondisabled people might overlook the complexity of this daily task -they might use a fork to pick up the carrot, while carrying a conversation and not paying as much attention on how the carrot is placed on the fork. Regardless of this placement, they move the fork in a manner that is not only efficient in how much food can be eaten but is also comfortable for the duration of the motion. This task presents numerous challenges for more than 12 million people with mobility-related disabilities [1] . Assistive robot arms have the potential to bridge this gap, and therefore provide care for those with disabilities. However, operating these arms can be challenging [2] , [3] . In our initial surveys, people with mobility impairment mentioned the need for intelligent autonomy that optimizes comfort and adapts to the food item being fed. We envision intelligent algorithms that are aware of user comfort without the need for explicit user input. Achieving this level of autonomy presents a number of challenges which carry over to other robotics applications, including: 1) perceiving and choosing the next bite of food on a plate, 2) acquiring the food item with an appropriate tool, 3) transferring these items into the mouth in an efficient and comfortable manner. In recent years, there has been significant advances in food perception and acquisition [4] , [5] . It turns out that the food acquisition strategy (e.g. fork skewering angle) heavily affects a user's comfort during bite transfer [5] ; however, prior bite transfer methods rely on predetermined transfer trajectories for a discrete set of acquisition strategies and food geometries [6] .",
            "cite_spans": [
                {
                    "start": 606,
                    "end": 609,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 777,
                    "end": 780,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 783,
                    "end": 786,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1469,
                    "end": 1472,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1475,
                    "end": 1478,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1611,
                    "end": 1614,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1763,
                    "end": 1766,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "To handle a wide variety of food items and acquisition methods, a bite transfer strategy must optimize its trajectories on the fly by bringing food into a mouth without sacrificing user comfort. However, this is challenging with real world sources of variation (e.g. food geometries, sizes, acquisition poses on the fork, and mouth shapes). Even with one food geometry and acquisition pose, there are often many different \"collision-free\" paths into the mouth, so the feeding agent should filter this solution space intelligently. For instance, consider a vertically aligned baby carrot oriented perpendicular to the fork, as shown in Fig. 1 . There are a wide range of possible feeding paths; some may come too close to a person's face, affecting comfort, while others may only bring the tip of the carrot into the mouth, limiting bite volume.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 635,
                    "end": 641,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Regardless of the orientation or type of food on our fork, caregivers will intuitively balance the bite volume efficiency for a single bite with the comfort of that bite. Motivated by this behavior, we present a bite transfer algorithm for selecting trajectories in a continuous space of mouth sizes, food geometries, and poses. Our approach (Section III) takes as input a food mesh and an acquisition pose on the fork from the real world, and generates an analogous simulation environment. We learn a constraint model to sample goal food poses near the mouth, and perform motion planning based on a novel set of heuristics (Section IV) to shape the perceived comfort and bite volume efficiency of each transfer. To our knowledge, our approach is the first to formulate comfort and efficiency for bite transfer, to consider non-bite sized food items, and to work for a continuum of acquisition poses and food geometries. We demonstrate our algorithm in practice through a limited user study (Section V). Our results show that while comfort alone and efficiency alone are able to outperform fixed trajectories on average, our approach of blending comfort and efficiency is the only method to outperform a fixed pose baseline with statistical significance. We run our method on various food items of differing geometries and scales in simulation (Appendix IV).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "II. RELATED WORK Our work draws inspiration not only from the state-of-theart in the robot-assisted feeding literature but also from the shared autonomy and general robot-human handovers. Robot-assisted Feeding: Bite acquisition and transfer. Several specialized feeding devices for people with disabilities have come to market in the past decade. Although several automated feeding systems exist [7], [8], [9], [10], they lack widespread acceptance as they use minimal autonomy, demanding a time-consuming food preparation process [11] , or pre-cut packaged food and cannot adapt the bite transfer strategies to large variations due to pre-programmed movements. Existing autonomous robot-assisted feeding systems such as [4] , [5] , [12] , and [13] can acquire and feed a fixed set of food items, but it is not clear whether these systems can adapt to different food items that are either not bite-sized and require multiple bites or require other bite transfer strategies. Feng et al. [4] and Gordon et al. [14] developed an online learning framework using the SPANet network and showed acquisition generalization to previously-unseen food items, but did not address the bite transfer problem. Gallenberger et al. [5] showed a relationship exists between bite acquisition and transfer, but did not propose how to transfer bites for non bite-sized items in such a setting. Our paper aims to close this gap in bite transfer by developing a context-aware framework for robot-assistive feeding which generalizes to food items that are not bite-sized. Shared Autonomy for Robotic Assistance. Adding autonomy to provide robotic assistance to tasks by inferring human intent is a well-studied field [15] , [2] , [16] , [17] , [18] , [19] , [20] , [3] . This is especially relevant for precise manipulation tasks such as bite acquisition or bite transfer during robotassisted feeding, while drawing parallels to other tasks such as peg-in-hole insertion [21] , [22] . For example, there has been work on using the concept of shared autonomy for bite acquisition tasks such as stabbing a bite, scooping in icing, or dipping in rice [23] , where the researchers combined embeddings from a learned latent action space with robotic teleoperation to provide assistance. Unlike this body of work, this paper focuses on completely autonomous bite transfer of food items by keeping in mind our end user population, which may have severe mobility limitations. Robot-human Handovers. There are many works analyzing robot-human handovers, but most of the studies focus on objects that are handed over without using an intermediate tool [24] , [25] , [26] in a single attempt. In this paper, we focus on tool-mediated handover of food-items that may not be bite-sized, and thus may require multiple handover attempts. The feeding handover situation poses an additional challenge of transferring to a constrained mouth, instead of a hand [5] . Gallenberger et al. [5] explore the problem of bitetransfer by providing the insight that bite transfer depends on bite acquisition and thus the transfer trajectories are not only food-item dependent but are also based on how a food was acquired. Cakmak et al. [27] study the handover problem in an application-agnostic way, where they identify human preferences for object orientations and grasp types. Similarly, Aleotti et al. [28] confirmed that orienting items in specific ways can make handover easier. Canal et al. [29] take a step further and explore how bite transfer can change with personal preferences. In our paper, we focus on tool-mediated bite-transfer of food items that may not be bite-sized and hence may require multiple transfer attempts.",
            "cite_spans": [
                {
                    "start": 532,
                    "end": 536,
                    "text": "[11]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 722,
                    "end": 725,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 728,
                    "end": 731,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 734,
                    "end": 738,
                    "text": "[12]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 745,
                    "end": 749,
                    "text": "[13]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 987,
                    "end": 990,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "text": "[14]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1216,
                    "end": 1219,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "text": "[15]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1701,
                    "end": 1704,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1707,
                    "end": 1711,
                    "text": "[16]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1714,
                    "end": 1718,
                    "text": "[17]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1721,
                    "end": 1725,
                    "text": "[18]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1728,
                    "end": 1732,
                    "text": "[19]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1735,
                    "end": 1739,
                    "text": "[20]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1742,
                    "end": 1745,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1948,
                    "end": 1952,
                    "text": "[21]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1955,
                    "end": 1959,
                    "text": "[22]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 2125,
                    "end": 2129,
                    "text": "[23]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 2619,
                    "end": 2623,
                    "text": "[24]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 2626,
                    "end": 2630,
                    "text": "[25]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 2633,
                    "end": 2637,
                    "text": "[26]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 2919,
                    "end": 2922,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 2945,
                    "end": 2948,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 3186,
                    "end": 3190,
                    "text": "[27]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 3355,
                    "end": 3359,
                    "text": "[28]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 3447,
                    "end": 3451,
                    "text": "[29]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "A caregiver can guide a food item into their patient's mouth agnostic to the orientation of the food on the fork -they do not spend minutes optimizing how the food should be placed on the fork for the most optimal transfer. In this section, we first formalize the goal of food acquisitionagnostic bite transfer, and then discuss our approach.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. CONTEXT-AWARE MULTI-BITE TRANSFER"
        },
        {
            "text": "To begin bite transfer iteration, we are given a 3D mesh M food of the food item, the constant pose p f \u2208 R 6 of the food item on the fork, a kinematics model for the robot and fork system with corresponding mesh M R , and the pose estimate of the mouth p m \u2208 R 6 . To capture the motion of the food item into the mouth, we want to find waypoints of the food item over time, represented by poses p \u2208 R 6 . Additionally, we assume the mouth can be represented by a simple elliptical tube M mouth , where the ellipse axes are in the face plane, and open mouth dimensions d m \u2208 R 2 are specified per end user. These inputs are visualized in our PyBullet-based simulation environment in Fig. 2 . We outline our method for acquiring these inputs in Appendix III. Given these inputs, we formulate the goal of bite transfer as finding a sequence of food poses T = {p 0 , . . . , p L\u22121 } of varying length L respecting a set of physical constraints C and cost function J (T ), shown in Eq. (1) . For the task of bite transfer, C consists of physical constraints. C 0 (Eq. (2)) and C 1 (Eq. (3)) ensure no collisions between the mouth mesh M mouth of dimensions d m with pose p m and the food mesh M food for each pose p i \u2208 T as well as the robot-fork mesh M R respectively. C 2 (Eq. (4)) constrains the final food pose to be near the mouth opening, i.e., p G . = p L\u22121 is in the support of goal pose distribution D g .",
            "cite_spans": [
                {
                    "start": 982,
                    "end": 985,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 683,
                    "end": 689,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "A. Bite Transfer Problem Formulation"
        },
        {
            "text": "When taking a bite, a person intuitively simulates the physics of their mouth's interaction with a carrot on our fork, regardless of the carrot's orientation, or where their arm starts. They might initially visualize where the carrot should be in the mouth and work backwards to find the most comfortable and efficient path. Our approach captures this intuition. Our simulation environment (see Fig. 2 ) reflects the real world setup, where the mouth is replaced with a static elliptical mouth model, allowing us to simulate the interactions between the human mouth and the food item.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 395,
                    "end": 401,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "A. Bite Transfer Problem Formulation"
        },
        {
            "text": "Our approach in Fig. 1 consists of three phases: sampling, clustering, and planning. Since the space of feasible goal food poses in the mouth is continuous, we outline two efficient goal sampling methods (Projection & Learned Constraints), which leverage simulation to batch sample from a set of \"feasible\" orientations and offsets from the mouth, defined by the distribution D g , and then check these samples against the constraints C to generate a varied set of feasible goal food poses p G near the mouth. Next, we cluster the constraint satisfying poses into a set of K goals with broad coverage over D g . We use heuristic guided bidirectional rapidly-exploring random trees to search for paths to goal food poses within the mouth that respect the physical constraints C. We guide the addition of new nodes to the h-BiRRT with a cost-to-come function h and a cost-so-far function g, where the sum of h and g defines the overall predicted cost J of a node in the h-BiRRT produced graph:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 16,
                    "end": 22,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "A. Bite Transfer Problem Formulation"
        },
        {
            "text": "Section IV discusses how we incorporate comfort and efficiency into h and g. Here, we first outline a method for generating goal poses p G \u223c D g to satisfy the constraints C. Sampling Food Objects with Projection. When sampling goal food poses, there are certain fork orientations that are impossible or unsafe for the arm to reach (e.g. the fork pointing backwards relative to the face). We restrict the orientations of the robot end effector to be within a spherical cut centered on the into-mouth axis, and position offsets from the mouth center are bounded. These bounds form the uniform goal distribution D g . The full sampling algorithm is outlined in the appendix in Algorithm 1. We first generate batches of food goal poses from D g and check for collision, repeating this process until reaching N collision-free samples or timing out. Since a person's true mouth cavity fits within the tube-like elliptical mouth in simulation, which has a constant cross section in the mouth plane (see Fig. 2 ), we accelerate the 3D collision check by slicing the food by the mouth plane and then projecting the inner food mesh vertices for each goal pose onto the mouth plane (Projection). The second image from the left in Fig. 7 (Appendix I) shows the slicing plane, with a sample carrot geometry. Finally we can check if the vertices are within the 2D mouth cross section to detect if the goal pose is collision-free. Improved Sampling via Learned Constraints. While Projection checks samples for collision faster than a na\u00efve 3D collision check, it still has significant and high variance lag (Fig. 2) . We thus propose a sampling method, Learned Constraints, that learns to predict constraint values (e.g., collision prediction) from 1M sim samples, with model inputs (d m , M food , p f , p G ). In Fig. 2 , the right plot shows that a learned collision predictor significantly reduces sampling time. In Appendix I, we provide further details and show that Learned Constraints maintain sample quality (e.g., predictive accuracy) and end-to-end trajectory performance (e.g., comfort & efficiency costs). Clustering Goal Food Poses. Once we have timed out or reached N collision free samples, we consolidate these goal poses into a representative set over D g for the planning step. We use a standard implementation of k-mediods, although any mediod clustering algorithm can be substituted. Motion Planning with Heuristic-Guided BiRRT. Once collision-free goal food poses have been generated and clustered, we must find trajectories to reach these goals. We adapt Rapidly-exploring Random Trees (RRT) for our motion planning. Inspired by Lavalle et al. [30] , who used bi-directional search ideas to grow two RRTs, we used one tree from the start state p 0 and the other from the goal state p k G . To bias the two search trees towards each other, we take a heuristics-based approach [31] . See Appendix II for details on our heuristic-guided implementation. Designing the heuristic cost functions will be discussed next.",
            "cite_spans": [
                {
                    "start": 2653,
                    "end": 2657,
                    "text": "[30]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 2884,
                    "end": 2888,
                    "text": "[31]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [
                {
                    "start": 997,
                    "end": 1003,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1220,
                    "end": 1226,
                    "text": "Fig. 7",
                    "ref_id": null
                },
                {
                    "start": 1593,
                    "end": 1601,
                    "text": "(Fig. 2)",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1801,
                    "end": 1807,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "A. Bite Transfer Problem Formulation"
        },
        {
            "text": "The solution space of feasible transfer trajectories is often large: a small strawberry can be eaten in a wide variety of fork orientations due to its size and inherent symmetry.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. COMFORT & EFFICIENCY IN MOTION PLANNING"
        },
        {
            "text": "Our approach narrows this solution space with comfort and efficiency heuristics during motion planning. One intuitive formulation is the path cost g being the distance between food poses (Eq. (6)), with the heuristic h being the distance to the goal pose (Eq. (7)).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. COMFORT & EFFICIENCY IN MOTION PLANNING"
        },
        {
            "text": "While this cost function guides hRRT to the goal pose, finding the shortest distance path in food pose space ignores both comfort and bite volume efficiency. Consider the vertically oriented carrot in the first row of Fig. 3 . If we sample a goal pose with the carrot oriented into the mouth and just past the teeth, a straight path to this goal is optimal in distance cost, but the person can barely take a bite; to add, the end effector would be close to the user's face, which could be considered uncomfortable. From our initial surveys with users with mobility limitations, we indeed conclude that comfort and efficiency are essential during bite transfer. One participant comments, \"The orientation should be comfortable for the utensil and the food ... the arm should maintain a low profile to not obstruct sight ... it should be fast...\". To this end, we develop two competing cost functions to shape the trajectories produced by hRRT: (1) bite efficiency, capturing the percentage of the food inside the mouth at the end of a trajectory, and (2) trajectory comfort, capturing the perceived user comfort along a given trajectory.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 218,
                    "end": 224,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "IV. COMFORT & EFFICIENCY IN MOTION PLANNING"
        },
        {
            "text": "The most efficient goal pose brings the most food into a person's mouth, which can be measured in the real world by comparing the food mesh before and after each bite. In simulation, we approximate the new food mesh without knowing the biting physics for a user. Instead, we assume a bite slices the food mesh in the face plane (see Figure 7 ). Let V i be the volume of the food geometry, and V f be the remaining volume after the bite. We estimate the efficiency cost of goal poses with Eq. (8). The n-root (n = 3 in practice) of the volume ratio amplifies the cost difference between goal poses of lower final volumes (high efficiency) to more noticeably bias RRT growth towards the most efficient goal poses. The resulting costs are in Eq. (9) & (10).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 333,
                    "end": 341,
                    "text": "Figure 7",
                    "ref_id": null
                }
            ],
            "section": "A. Modeling Efficiency"
        },
        {
            "text": ") Note that this cost only considers the goal pose, rather than the entire trajectory. We empirically found that other notions of efficiency applied to paths, like trajectory execution time, or the distance of the path traveled, do not vary as much between outputs of h-BiRRT, and so yield less impact on the quality of trajectories produced.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Modeling Efficiency"
        },
        {
            "text": "A trajectory that brings the arm too close within a user's personal space could influence the user's perceived safety, even if the transfer efficiency is high. We develop a notion of comfort that draws from proxemics literature in human-robot interaction, a well-studied field [32] for tasks such as in social robot navigation [33] . Building off the notion of \"personal space\", we hypothesize that trajectories should stay within a conic region stemming from the mouth, with a wide cross-sectional area further from the face that narrows towards the mouth. Prior work in human factors for social navigation has shown that a person's comfortable personal space can be different for each cardinal direction, usually being larger within a person's visual field than outside [33] . Building on this intuition, we skew the cone down, away from the visual field. We define a spatial cost function resembling an elliptical Gaussian at each cross section centered along the mouth axis ( Fig. 4) . We posit that the upward direction relative to a person's face, which is closer to the visual field, should penalize deviation from the mouth axis more than in the downward direction. For a distance z \u2208 R + along the mouth axis and offset from the mouth axis x \u2208 R 2 in the cross section plane, we define the spatial comfort cost in Eq. (11) .",
            "cite_spans": [
                {
                    "start": 277,
                    "end": 281,
                    "text": "[32]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 327,
                    "end": 331,
                    "text": "[33]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 772,
                    "end": 776,
                    "text": "[33]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "text": "(11)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 980,
                    "end": 987,
                    "text": "Fig. 4)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "B. Modeling Comfort and Personal Space"
        },
        {
            "text": "Here, \u03a3(x) is a piece-wise covariance matrix in the face plane. In our experiments, we used a diagonal covariance matrix, with smaller variances above the mouth horizontal plane than below, and equal variances left and right. This cost and the mouth axis can be visualized in Fig. 4 . Our comfort cost is applied on both the food item mesh and the entire simulated robot mesh and fork. In essence, we create a low resolution depth image from the perspective of the mouth and apply our cost function on the 3D location of each pixel. For a given food pose p and the corresponding simulated robot mesh, we cast rays in simulation along the mouth axis, starting at an N \u00d7 M grid of points relative to the mouth center and on the face plane (e.g. z = 0), ending at a fixed maximum distance along the mouth axis z max . The set of hit points from this ray cast, denoted H = {h j \u2208 R 3 }, are passed into the cost function in Eq. (11) and normalized by the total number of points (Eq. (12) ). This comfort cost is incorporated as a distance-weighted edge cost in hRRT with weight \u03b3 C , and can be included in the heuristic as an additional goal cost using weighting \u03b2 C (Eq. (13) & (14)).",
            "cite_spans": [
                {
                    "start": 924,
                    "end": 928,
                    "text": "(11)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 979,
                    "end": 983,
                    "text": "(12)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 276,
                    "end": 282,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "B. Modeling Comfort and Personal Space"
        },
        {
            "text": "Here, J C (p j , p j+1 ) is shorthand for the comfort cost at The fundamental trade off between average comfort and efficiency costs for a grid of chosen relative weightings of comfort and efficiency, with costs from our h-BiRRT method averaged over 500+ initial food geometries and poses in simulation. Teal represents high ratios of comfort to efficiency, and orange the opposite. Our weights are the green dot at the elbow of this trade-off, achieving low efficiency cost and low comfort cost. Right: Sample trajectories for Fixed Pose (top) and Comfort+Efficiency (bottom) for the Vertical food geometry (see Section V). While Fixed pose (baseline) incurs high comfort cost (close to user's face), our method finds a trajectory that is both comfortable and efficient for the user. the midpoint of these two food poses. We denote this formulation as \"comfort only,\" since there is no consideration of efficiency here. Incorporating comfort alone can yield trajectories that keep the robot within the cone comfort region, but often this generates final goal poses that would not be easy to bite. Next, we will discuss incorporating both efficiency and comfort as costs for h-BiRRT.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Modeling Comfort and Personal Space"
        },
        {
            "text": "Ideally, an assistive robot would be able to feed bites of food with both comfort and efficiency in mind. In order to maximize both comfort and efficiency, we can put together the comfort costs (Eq. (13) & (14)) and efficiency costs (Eq. (9) & (10)), yielding the cost functions for h-BiRRT in Eq. (15) & (16), where the weightings \u03b2 E , \u03b2 C , and \u03b3 C emphasize the efficiency at the goal, comfort at the goal, and comfort along the trajectory, respectively. Fig. 3 , we plot the average comfort and efficiency scores for our h-BiRRT pipeline over a grid of weight values for \u03b2 E , \u03b2 C , and \u03b3 C and over a large number of initial food poses and geometries (e.g., carrots, strawberries, celery, cantaloupes) in simulation. Refer to Appendix IV for quantitative results and example trajectories for each food type. Optimizing for efficiency only finds trajectories with the highest comfort costs but lowest efficiency costs, and vice versa for comfort only. This demonstrates that there is in fact a trade-off in comfort and efficiency costs when running h-BiRRT with our heuristic functions. Our approach will choose the \"elbow\" of this trade-off, balancing both efficiency and comfort.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 459,
                    "end": 465,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "C. Trading off Comfort and Efficiency"
        },
        {
            "text": "We conducted a user study with six non-disabled participants to evaluate the perceived comfort and efficiency with our real world setup 1 In Appendix III, we discuss our real world system design, and how we ensure user safety during our user studies. Key parameter choices are shown in Table  II in the Appendix. We consider carrots of varying sizes and 1 We decided to recruit non-disabled participants due to Covid-19 & safety concerns. Please see Appendix V for further discussion. fixed acquisition poses, visualized in the first row of Fig. 5 : Vertical, Horizontal, Roll & Pitch, and Yaw. Users were instructed to sit still facing the robot, and to take a bite of each food item after each trajectory if they felt comfortable to do so. In addition, an emergency stop button was placed next to them for added assurance. See Appendix V for more user study details. We evaluated the following methods: 1) FixedPose (F): We fix the final orientation of the food item independent of the pose of the food item on the fork and the food size. This final orientation is hard-coded for a specific type of food and is inspired by the taxonomy of food manipulation strategies developed in [6] . 2) EfficiencyOnly (E): Our approach with h-BiRRT and only efficiency costs, Eq. (9) and (10).",
            "cite_spans": [
                {
                    "start": 136,
                    "end": 137,
                    "text": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 354,
                    "end": 355,
                    "text": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1183,
                    "end": 1186,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 286,
                    "end": 295,
                    "text": "Table  II",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 541,
                    "end": 547,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "A. Experimental Setup"
        },
        {
            "text": "comfort costs, Eq. (13) and (14). 4) Comfort+Efficiency (CE): Our approach with both efficiency and comfort in mind: the h-BiRRT cost functions use both efficiency and reward, Eq. (15) and (16). For each food pose and method, we evaluate two trajectories end-to-end with each user. After two trajectories for a given method, we ask a series of questions to gauge the user's perceived comfort of each trajectory, and the ease with which they were able to take a bite. We compare responses to these questions, in terms of Comfort (the average user rating of comfort for each evaluated trajectory, from 1 to 5, with 5 being the most comfortable), Ease (average rating of their ease of taking a bite for each evaluated trajectory, normalized from 1 to 5, with 5 being the best), Rank (Average relative rank of each method, from 1 to 4), and Safety (Average user rating of safety, from 1 to 5).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3) ComfortOnly (C): Our approach with h-BiRRT and only"
        },
        {
            "text": "The ease rating, comfort rating, and approach rank are summarized in Fig. 5 . Importantly, our real world evaluation pipeline (Appendix III) was perceived as safe to the user regardless of food geometry or method, achieving an average Safety rating of 4/5. Despite the limited sample size, our method (CE) significantly outperforms the fixed baseline (F) for all three metrics (example in Fig. 3) , and consistently outperforms comfort-only (C) and efficiency-only (E), significantly so in Rank ratings. Additionally, efficiency-only (E) did not perform as well as comfort-only in Comfort ratings. This supports our hypothesized connection between user comfort perception and our comfort model (Eq. (11)). The data is consistent with our hypothesis that, while optimizing over individual metrics (C and E) provides some improvement over the baseline, joint optimization performs even better in creating trajectories robust to real-world variation. We suspect that this is due to the large space of possible trajectories, where maximizing for only comfort puts no guarantee on efficiency, and vice versa.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 69,
                    "end": 75,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 389,
                    "end": 396,
                    "text": "Fig. 3)",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "B. Results"
        },
        {
            "text": "Qualitatively, our comfort model's sensitivity to objects above mouth level fits with user expectations. When asked about low-ranked trajectories, users stated that they believed \"the robot should have approached from underneath,\" or that they \"didn't like when [the robot] came up close to [their] face.\" Users were more likely to instinctively move backwards when approached from above, near the face, and lean in when approached from below. In Fig. 6 , we visualize a sample real world trajectory produced by each method for a Vertical pose. FixedPose is neither maximally efficient (carrot only partially fits in mouth) nor comfortable (robot is too close to face). Common quantitative metrics like time and path length are not as informative in gauging comfort, so we limit our evaluation to these qualitative metrics. Appendix V elaborates on quantitative and qualitative metrics and outlines how our approach naturally extends to the multi-bite setting with sample real world evaluations.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 447,
                    "end": 453,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "B. Results"
        },
        {
            "text": "VI. DISCUSSION Summary. In this work, we presented an approach based on motion planning for transferring food items to a person's mouth with a continuous space of possible acquisition angles. During planning, we narrow down the solution space of possible trajectories into the mouth with an awareness of both bite efficiency and user comfort. Our user study demonstrates that our method that considers comfort and efficiency jointly provides significantly more preferable trajectories compared to a fixed pose baseline. Furthermore, our method with comfort and efficiency consistently outperforms when considering only comfort or only efficiency. Limitations. One limitation of our method is the assumption that the mouth can be represented by a rigid elliptical tube, and that the food item is also rigid. In reality, the human mouth and the food item can both be deformable, which expands the set of \"collision-free\" paths into the mouth.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Results"
        },
        {
            "text": "Furthermore, our user study only involved six non-disabled users due to Covid-19 related policies. In future work we plan to evaluate with more users, including users with mobility-impairment disabilities. However, we are excited that even with the given sample size, our method improves on the state-of-the-art with statistical significance. Additional details for our approach are described in the following sections. Appendix I covers the Projection and Learned Constraints sampling approaches in greater detail, including quantitative comparisons. Appendix II provides details for the h-BiRRT algorithm used for motion planning, for example how the cost heuristics in Section IV are incorporated into RRT growth with key parameter choices. Appendix III explains our real world robotic pipeline for autonomous feeding, involving gravity compensation, food geometry perception, trajectory following, force-reactive control, and facial keypoint tracking. Appendix IV outlines simulation experiments, with quantitative and qualitative information to supplement our real world analysis. Finally, user study details including demographics and survey questions are covered in Appendix V along with a discussion of how our algorithm naturally extends to the multi-bite setting using examples from our user study.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Results"
        },
        {
            "text": "Our projection sampling algorithm (Algorithm 1) begins by generating batches of food goal pose samples from D g (Line 4), and performing collision checks on each food pose with the mouth (Lines 6, 7, 8), and repeating this process until reaching N valid samples or timing out. We observed that performing 3D collision checks was the time bottleneck for each sampling iteration. After reaching N samples, finding the representative goal set can be accomplished by any clustering algorithm that returns cluster centers from within the original set of food poses, since it may be the case that cluster centers outside of this set do not satisfy the same collision checks. The clustering method is referenced in the final line of Algorithm 1. D g is defined by fixed positional offsets from the mouth center and a spherical cut of feasible orientations for the fork. As an aside, while fixed positional offsets were sufficient for most food geometries, we noticed that using these fixed 3D offsets with very large geometries or small mouths occasionally leads to a sparse set collisionfree poses, since these geometries may need more \"wiggle\" room near the mouth to generate even a few collision free samples. One could compensate for this by adjusting this 3D offset as a function of the food geometry bounding box.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX I SAMPLING ALGORITHMS"
        },
        {
            "text": "We further improve upon the efficiency of sampling with a learned constraint model, replacing lines 6 & 7 with a forward pass of the constraint model and filtering by the outputs. As outlined in Fig. 2 in the text, sampling with a learned model greatly improves the speed of the full algorithm. We use a 3-layer NN to predict collisions from food pose on fork p f , food mesh M food , mouth dimensions d m , and a desired goal food pose p G . We trained on 1 million samples of just the final pose of the robot and the corresponding simulated constraint label. Our data collection in simulation involved variations in not just food type and initial pose on the fork, but also in food scaling in all 3 axes and in mouth elliptical cross section dimensions. We chose the bounds for each of these factors to be a super set of those likely to be encountered in the real world. Note that each of these variables is passed in as input to the constraint model and are accessible in the real world. Thus, this model can be readily used in real world settings that allow access to these state variables. The main sim-to-real gap here involves the elliptical mouth assumption, which we hope to tackle in future work. During training, we accounted for label shift by weighting constraint satisfying and non-constraint satisfying samples proportionally to their occurrence in the simulation dataset. Figure 8 shows the holdout prediction accuracies and cross entropy loss during the training process. Table I shows that the constraint predictor is accurate at a slightly lower rate than the projection sampling method (i.e. high sample quality); however, the faster sampling time for the constraint model allows for more iterations of sampling in order to offset the lower prediction accuracy (although this fact is not leveraged for Table I ). Due to the imperfect predictions, the Learned Constraints model on its own may result in collision generating goal poses after clustering. To prevent these poses from being evaluated, we run an additional constraint check on the cluster centers to filter out any prediction errors with no loss in final goal pose quality.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 195,
                    "end": 201,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1388,
                    "end": 1396,
                    "text": "Figure 8",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1489,
                    "end": 1496,
                    "text": "Table I",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 1822,
                    "end": 1829,
                    "text": "Table I",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "APPENDIX I SAMPLING ALGORITHMS"
        },
        {
            "text": "Additionally, we show that the final h-BiRRT generated trajectories achieve similar comfort and efficiency costs in simulation compared to the projection-based sampling method (i.e. high trajectory quality). In summary, the learned For our h-BiRRT implementation, outlined in Algorithm 2, we sample goal poses as described previously, and produce a goal tree associated with each of the sampled goal poses (Line 5). To bias the start to each goal tree, we leverage a heuristics-based approach [31] with a parameter m q that is used to weight the Voronoi region associated with each node, such that",
            "cite_spans": [
                {
                    "start": 493,
                    "end": 497,
                    "text": "[31]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "APPENDIX I SAMPLING ALGORITHMS"
        },
        {
            "text": "where c i is the total cost associated with node i, c * is the total cost of the optimal path from start to goal, and c max is the highest total cost of any node. We calculate total cost c as the sum of cost-to-come g and the heuristic cost-to-go h. Specifically for the goal tree, we use a simple distance heuristic from goal to start, and for the start tree, we use the novel comfort and efficiency heuristics. For each tree, we use k-Nearest Neighbors to select nearest nodes in the tree to expand. At every iteration, we run the bi-directional RRT planner for the same start tree, but different sampled goal trees, where the probability of sampling a goal tree at any iteration is proportional to its goal cost (Lines 5, 7). This ensures that we eventually reach every goal tree from the start while prioritizing reaching the least costly goal pose first. Picking a trajectory. Once hRRT has completed, the resulting tree has a smoothed and low cost path to each of the K goal poses that were generated from sampling. Each of these trajectories has an associated cost defined by evaluating g on the entire trajectory, and adding any costs associated with that trajectory's goal pose. The final trajectory that will be evaluated on the real robot arm is the trajectory with the lowest overall cost. Key Parameter Choices. Our key cost parameters are shown below. N is the number of collision free samples to generate before clustering; K is the number of clusters generated after sampling; n is power to which the bite efficiency ratio (final over initial) is raised in the efficiency cost; \u03b1, r up , r down , r side parameterize the comfort cost; \u03b2 E is the goal efficiency contribution; \u03b2 C is the goal comfort contribution; and \u03b3 C is the edge comfort contribution. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX I SAMPLING ALGORITHMS"
        },
        {
            "text": "In order for an assistive feeding platform to be adopted, users need to feel safe while our algorithm is operating in the real world. In the context of assistive feeding, the autonomous robot must execute a complex end-effector trajectory produced by our method and simultaneously react to the highly sensitive human mouth without causing discomfort. We designed a robotic system to evaluate expressive trajectories with an awareness of users and the forces they exert on the utensil.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "The trajectory execution operates as follows. At initialization, the force/torque readings from the end-effector are decoupled from gravity forces/torques of the unknown food object by estimating the food mass from a sequence of end effector poses. Using an RGB-D camera, we estimate the 3D mesh of the food item on the fork. Once our algorithm produces a sequence of waypoints for the food item, we smooth the trajectory and interpolate through the waypoints using bounded end effector velocities. Along the trajectory, force and torque readings along with robot state are utilized by a force-reactive admittance controller to safely navigate the forces applied by users' mouths in end-effector space. We delve into each of these components in the following Fig. 9 : Our real world evaluation setup consists of a Franka Emika Panda robotic arm, a fork attached to an ATI Mini45 6-axis F/T Sensor via a custom 3D printed mount, and an external Intel Realsense RGB-D camera. The F/T sensor enables the robot arm to interact with the user's mouth through a force-reactive controller. The external camera enables food geometry perception at the start of each bite transfer and visual servoing based on facial keypoints during trajectory execution.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 759,
                    "end": 765,
                    "text": "Fig. 9",
                    "ref_id": null
                }
            ],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "sections, and our real world setup is shown in Fig. 9 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 47,
                    "end": 53,
                    "text": "Fig. 9",
                    "ref_id": null
                }
            ],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "Food Mass Gravity Compensation. We use an ATI F/T 6-axis Mini45 sensor (see Fig. 9 ) to receive force and torque readings from a custom fork mounted on the robot's end-effector. Before compensation, these readings have an unknown bias and do not take gravity into account. At the beginning of every episode, we cycle through end-effector poses to estimate the inherent bias of the force torque sensor readings along with the food mass currently on the fork. Note that this procedure is not specific to feeding and can be extended to robotic systems with unknown sensor biases and initial gravity forces.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 76,
                    "end": 82,
                    "text": "Fig. 9",
                    "ref_id": null
                }
            ],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "Upon acquiring a new bite of food, the end-effector cycles through several pose quaternions q i \u2208 R 4 for i \u2208 1 . . . N and records the raw force and torque readings f i \u2208 R 3 and t i \u2208 R 3 at each pose. Let T ee\u2192s \u2208 R 3x3 be the fixed known rotational transform from the robot end-effector coordinate frame to the sensor coordinate frame, and let T w\u2192ee i \u2208 R 3x3 be the known rotational transform from the world frame to the robot's current end-effector orientation q i . Additionally, let the force sensor bias be f b and the torque bias be t b .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "Assuming an unknown mass m and a known torque radius r (the distance to the tip of the fork) in the end-effector frame +y direction, and a fixed & known gravity in the world frame -z direction g, we know that,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "Here, m, f b , and t b yield 7 unknown scalars. Note that Eq. (18) and (19) are linear in these unknowns, and can thus be rearranged into the form Ax = b with x \u2208 R 7 . We then set up a system of linear equations for each robot pose q i , and the corresponding force measurements. With multiple poses, we can solve the least squares approximation to estimate the unknowns in the presence of sensor noise. With the F/T readings properly adjusted, we can proceed to estimating the 3D geometry of the food item. Food Perception. Food shape can vary greatly between different food items, even of the same food type. In the multiple bite formulation, this food geometry can change over time, and so 3D geometry must be re-evaluated after each bite in order to obtain the most accurate simulation environment for searching for trajectories into the mouth. Our food perception pipeline is as follows: we use a fixed externally mounted RGB-D camera to take several images of the food item for multiple fork orientations. We assume we can extract the pixel region of the food item. For the purposes of our study, where food items have a single color, we extract the food region using HSV and depth filtering, but any segmentation model can be used to extract the food item from each image. Then we use the RGB-D image to reconstruct a point cloud for each image, and we stitch these pointclouds together with knowledge of the camera frame pose relative to the robot frame of reference (see Fig. 10 ).",
            "cite_spans": [
                {
                    "start": 62,
                    "end": 66,
                    "text": "(18)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 71,
                    "end": 75,
                    "text": "(19)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 1481,
                    "end": 1488,
                    "text": "Fig. 10",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "For the purposes of demonstrating our approach, our food geometry estimation procedure assumes that the food item can be sufficiently represented by a bounding box or cylinder, seen in Fig. 10 , although in the future we hope to make this pipeline more robust as well as capable of representing more complex 3D mesh geometries. Importantly, overestimating the bounding region of a food item, e.g. with a convex hull, does not hinder our ability to produce valid collision free trajectories. The expressiveness of trajectories produced by our method scales with the accuracy of the mesh geometries we get from the real world. Trajectory Following. Once we have recovered the food geometry and pose on the fork, our algorithm samples a set of valid goal poses of the food item, and then finds trajectories to each goal pose as discussed in Sections III and IV, in the form of discrete waypoints to the goal pose. In order to ensure user comfort and consistency, we fix the velocity norm in end effector space along the trajectory. We smooth the output of the h-BiRRT, and generate an interpolated waypoint for each time step of trajectory evaluation bounded by the max velocity norm. We proceed to the next waypoint when the end effector comes within a \"follow\" radius. We found that using a proportional (P) controller to generate velocities along the interpolated trajectory was sufficient to stay within the follow radius. Final performance (lower is better) over 500+ evaluated sim trajectories for three cost regimes over cost weights \u03b2 C , \u03b2 E , and \u03b3 C , relative to our chosen values. (\u00b5 \u00b1 \u03c3) for h-BiRRT produced comfort cost (normalized) and efficiency cost (normalized). From left to right: our algorithm, at the elbow of the trade-off of comfort and efficiency; high comfort weighting regime, with low comfort cost but high efficiency cost; high efficiency weighting regime, with low efficiency cost but high comfort cost; distance only regime, with no comfort or efficiency heuristics.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 185,
                    "end": 192,
                    "text": "Fig. 10",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "Despite moving at relatively slow speeds, the human mouth is very sensitive to any forces being applied on it, especially once the fork has entered the mouth. We implemented a simple force-reactive controller that filters velocities from the trajectory following controller, discussed in the next section. Force-Reactive Controller. The biased and gravity compensated F/T readings are brought into the robot's frame of reference. We use a force admittance controller to convert forces and torques experienced by the fork to linear and angular velocity commands for the robot. In practice, we used only linear velocity commands from the force readings, yielding the following control equations for the end effector velocities:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "This PID controller aims to keep the forces applied by the fork within a band {\u2212F th , F th } to prevent causing discomfort to the user. This threshold is determined by the sensitivity of the human mouth. In practice we set this to be around 0.25 N. If the forces are outside of this band, the algorithm ignores the velocity command from the trajectory following algorithm, and instead returns the force-reactive controller command. Facial Keypoint Visual Servoing. Another important aspect of ensuring user comfort is knowing the position of human mouth in 3D space relative to the robot arm. One common approach is to mount a camera on the end effector of the robot, and track keypoints on a person's face using facial detection algorithms. However, our method leverages a high degree of mobility at the end effector, meaning the human mouth or face may not always be fully in view or at an upright orientation. To ensure consistent facial tracking, we mount an RGB-D camera at an external position, which measures the 3D position of the person's head and mouth. Our trajectory waypoints are adjusted relative to this known 3D mouth position, but velocities are computed in the global frame to ensure the motion is smooth and of bounded velocity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX III ROBOTIC SYSTEM FOR AUTONOMOUS FEEDING"
        },
        {
            "text": "Different Food Geometries. To illustrate our flexibility in regards to food geometries, we model four geometry types of random size in simulation that each have a unique 3D shape: a carrot (cylinder); a piece of cantaloupe (trapezoidal prism); a stalk of celery (half of a cylindrical tube); and a strawberry (\"teardrop\" shape). These geometries are visualized in Fig. 11 . The initial and final poses for a sample trajectory under our method for each food type are also shown in this figure. Notably, our algorithm is able to bring each food item at least partially into the mouth, picking a final goal pose and planning a trajectory that is both comfortable and efficient. The trade-off plots from Fig. 3 in the text were generated using these food geometries of various scales, and thus represent a super-set of the generated trajectories in the user study experiments in Section V, which considers carrots alone.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 364,
                    "end": 371,
                    "text": "Fig. 11",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 700,
                    "end": 706,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "APPENDIX IV SIMULATION EXPERIMENTS"
        },
        {
            "text": "Comfort and Efficiency. For a wide range of values of \u03b2 E , \u03b2 C , and \u03b3 C , we ran 500+ trajectories in simulation for a range of initial food poses, types (see above), and geometry sizes, recording the average comfort cost, efficiency cost, and distance costs. Fig. 3 , discussed previously, illustrates the results of this parameter sweep, and Table III provides the cost values for this sweep under our weights (elbow of trade-off), high relative comfort weighting, and high relative efficiency weighting. As our intuition suggests, we find that there exists a trade-off between comfort and efficiency under our formulation. Since respecting a user's personal space can yield end effector orientations that bring the food into the mouth with low efficiency, we note that trajectories that are more comfortable on average have lower efficiency, and vice versa. Additionally, distance-only heuristics produce notably higher comfort and efficiency costs, suggesting that only using the distance heuristic produces trajectories far from the Pareto front of comfort and efficiency.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 262,
                    "end": 268,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 346,
                    "end": 355,
                    "text": "Table III",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "APPENDIX IV SIMULATION EXPERIMENTS"
        },
        {
            "text": "Design. Our user study involved six non-disabled participants, (ages 19, 22, 23, 23, 53, & 55, three women and three men). Three had experience interacting with robots, and three did not. All had experience feeding other people. Due to COVID-19 & safety concerns, we were unable to recruit people with motor impairment into our user study. We have been limited even in the number and population of non-disabled users over the past year. Additionally, we believe the problem of assistive feeding is important beyond its impact on users with disabilities. Specifically, real-world evaluation with non-disabled users still captures the complex interactions between the mouth and the robot, and provides insights about formalizing comfort for the general public. Similar comfort formalisms might even extend to other real-world human-robot applications, as well as to assistive feeding beyond the disabled population.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX V USER STUDY"
        },
        {
            "text": "Per user, we ran two iterations for each method (FixedPose, ComfortOnly, EfficiencyOnly, Comfort+Efficiency), all for each of the carrot food poses: The size of each carrot we used varied between each experiment, and we biased the Yaw category towards bigger carrots for added diversity. For sanitation, the forks were cleaned after every two iterations, and food items were replaced if not eaten. The following scale questions were used to evaluate the user's perception of comfort after every two iterations (i.e. for one method and food geometry combination): 1) Ease: I was able to easily take a bite (1 to 5). 2) Comfort: I was able to comfortably take a bite (1 to 5). 3) Safety: I felt safe while being fed (1 to 5).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX V USER STUDY"
        },
        {
            "text": "After each method for a given food geometry (eight total iterations), the user was asked to rank the methods in order of preference. Additionally, they were asked to comment on how they would change the robot trajectories if they were feeding someone who cannot feed themselves. Finally, they were given the opportunity to provide additional comments about the methods & robot system. Objective Metrics. We considered several objective metrics before choosing to use subjective metrics for our evaluation. We will now discuss the shortcomings of various objective metric options: elapsed time, path length, trajectory jerk, applied force, & vital data. We find that elapsed time depends more on the distance of travel for the robot than the difficulty of taking a bite. The path length, in turn, depends only on the change in joint state from the initial to goal robot configurations, and therefore both elapsed time and path length vary in predictable and uninformative ways. Specifically, path traversal time varied from 9 seconds to 17 seconds for each transfer, while bite time remained roughly constant at 1-1.5 seconds. For trajectory jerk, our planned trajectories are all quite smooth by design, so this metric is equally uninformative. Applied force is also not informative since each bite taken induces a significant impulse on the fork, often noticeably larger than the minor interaction forces that might make transfer uncomfortable. Vital data would be quite interesting to analyze, but we believe that subjective questionnaires are the most informative avenue for gauging perceived comfort, as prior work has also noted. Further complicating many of these metrics, users might decide to not take a bite of the food during the study, for example if the presented orientation is too difficult. Thus, these objective metrics would fail to capture the user's real opinions about these trajectories, whereas subjective metrics can be informative in all cases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX V USER STUDY"
        },
        {
            "text": "Safety Considerations. In order to ensure the user's safety, we took several precautions during the experiment, in addition to those described in Appendix III. The user sits facing the robot. The proctor sits on the left side of the table from the user's perspective. The emergency-stop button is placed either in the hands of the user, or next to them on a side table, depending on the user's preference. A fork (a cut-out of a standard metal table fork) is mounted via a custom 3D mount on the force/torque sensor, which is in turn attached to a shaft that the Franka Panda gripper can hold. Importantly, the fork mechanism is not rigidly attached to the Panda arm, and thus allows the fork to interact with the mouth much more forgivingly. In addition to the force reactive control which runs throughout the experiment, this mechanism is designed to break from the gripper when too much force is applied (although this scenario was never encountered). This experiment was conducted under IRB-52441.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "APPENDIX V USER STUDY"
        },
        {
            "text": "Results. Due to the measurement of 3 different variables (Rank, Ease, and Comfort), we apply a Bonferroni correction to our significance threshold for all hypotheses (P < 0.01). First, given two independent variables (Food Fig. 12 : An example of a food geometry that is too big to comfortably fit in the mouth in a single iteration. Our method (CE) was evaluated on this larger carrot over multiple iterations. Our food perception re-evaluates the 3D geometry at each feeding iteration (the first and third images from the left), allowing our algorithm to find new trajectories at each feeding iteration that balance making progress at each bite (efficiency) and respecting user personal space (comfort). Two iterations were required using our method, and the final pose of each trajectory is shown in the second and fourth images from the left.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 223,
                    "end": 230,
                    "text": "Fig. 12",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "APPENDIX V USER STUDY"
        },
        {
            "text": "Pose and Method), we analyzed our data with a Two-Way ANOVA with repeated measures. This determined that Method alone had a significant effect on Rank, Comfort, and Ease (P < 0.001). We therefore averaged out the Food Pose and analyzed Method vs (Rank, Comfort, Ease) with a One-Way ANOVA with correlated samples. Significant results (as determined by a Tukey HSD Test, P < 0.01) are marked in Figure 5 with an asterisk. For the reader's convenience, the significant pairings are listed in Table IV We did not treat multiple user ratings as independent data. We note that, while these results are significant despite our limited sample size (N = 6), this does not necessarily mean that they generalize to the target population of individuals with limited upper-body mobility.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 394,
                    "end": 402,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 490,
                    "end": 498,
                    "text": "Table IV",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "APPENDIX V USER STUDY"
        },
        {
            "text": "Our results support the hypothesis that combining comfort and efficiency heuristics facilitated more comfortable bite transfer for non-disabled participants than either heuristic on their own. One interesting finding is that the Comfortonly model does not perform significantly better than the Efficiency-only model in Comfort ratings. We note that a user's perceived comfort is multi-faceted, and includes factors beyond just the trajectory taken for bite transfer. For example, forces applied on the mouth due to inefficient food orientations, jitter of the robot arm due to the reactive controller, and minor positioning errors in the real world can often play a role in user comfort. In this work, we attempt to formalize the trajectory-based component of comfort; we demonstrate that when combined with Efficiency heuristics, these Comfort heuristics perform better than either heuristic does on its own, often with statistical significance Multi-Bite. Often, a single bite of food might be too large or not oriented on the fork in the most optimal way. In these cases, it is necessary for intelligent bite transfer algorithms to recognize that the food item cannot be consumed in a single bite and then plan in a closed loop over each bite transfer iteration, continually re-planning based on how the food geometry changed after the last bite. Since our algorithm takes as input the full 3D mesh, our method is capable of dealing with the large changes in food geometry that can occur after each bite. A sample multi-bite transfer in the real world that uses our algorithm with both comfort and efficiency is shown in Fig. 12 . When feeding themselves a carrot of this size, a person might take a greedy approach to taking multiple bites, making as much progress as one can at every iteration without ever sacrificing one's comfort until the food item has been fully consumed -while it might be possible to bring this entire carrot into the mouth in one bite (i.e. high efficiency), one can imagine this might not be comfortable for the user. Our algorithm plans over multiple bites in a similarly greedy fashion: we consider only the current food geometry, and then make a decision over what goal and subsequent trajectory to take to minimize our overall cost for the bite iteration. After the user takes a bite, we repeat this process. By incorporating both comfort and efficiency into our cost function, our approach naturally extends to this multi-bite planning process as shown with the sample trajectory in Fig. 12 . As a result, our method required only two bites to feed the entire carrot on the fork and was able to keep the user comfortable throughout.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1624,
                    "end": 1631,
                    "text": "Fig. 12",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 2519,
                    "end": 2526,
                    "text": "Fig. 12",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "APPENDIX V USER STUDY"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Americans with disabilities: 2010",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Brault",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Current population reports",
            "volume": "7",
            "issn": "",
            "pages": "70--131",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Controlling assistive robots with learned latent actions",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Losey",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Srinivasan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mandlekar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Garg",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sadigh",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA)",
            "volume": "",
            "issn": "",
            "pages": "378--384",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Learning user-preferred mappings for intuitive robot control",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Losey",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bohg",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sadigh",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Robot-assisted feeding: Generalizing skewering strategies across food items on a realistic plate",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "K"
                    ],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schmittle",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Bhattacharjee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Srinivasa",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Transfer depends on acquisition: Analyzing manipulation strategies for robotic feeding",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gallenberger",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Bhattacharjee",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Srinivasa",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACM/IEEE International Conference on Human-Robot Interaction",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Towards robotic feeding: Role of haptics in fork-based food manipulation",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Bhattacharjee",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Srinivasa",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Robotics and Automation Letters",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Learning haptic representation for manipulating deformable food objects",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "C"
                    ],
                    "last": "Gemici",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Saxena",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "volume": "",
            "issn": "",
            "pages": "638--645",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Towards assistive feeding with a General-Purpose mobile manipulator",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "K"
                    ],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "M"
                    ],
                    "last": "Erickson",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Kemp",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Algorithms, Implementation, and Studies on Eating with a Shared Control Robot Arm",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "V"
                    ],
                    "last": "Herlant",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Adaptive robot-assisted feeding: An online learning framework for acquiring previously unseen food items",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "K"
                    ],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Bhattacharjee",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Barnes",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Srinivasa",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "A policy-blending formalism for shared control",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "D"
                    ],
                    "last": "Dragan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Srinivasa",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "The International Journal of Robotics Research",
            "volume": "32",
            "issn": "7",
            "pages": "790--805",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Eye-hand behavior in human-robot shared manipulation",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "Aronson",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Santini",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "C"
                    ],
                    "last": "K\u00fcbler",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Kasneci",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Srinivasa",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Admoni",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction",
            "volume": "",
            "issn": "",
            "pages": "4--13",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Human-in-the-loop optimization of shared autonomy in assistive robotics",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gopinath",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jain",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "D"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Robotics and Automation Letters",
            "volume": "2",
            "issn": "1",
            "pages": "247--254",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Anticipatory robot control for efficient human-robot collaboration",
            "authors": [
                {
                    "first": "C.-M",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Mutlu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 11th ACM/IEEE international conference on human-robot interaction (HRI)",
            "volume": "",
            "issn": "",
            "pages": "83--90",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Shared autonomy via hindsight optimization for teleoperation and teaming",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Javdani",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Admoni",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pellegrinelli",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Srinivasa",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Bagnell",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "The International Journal of Robotics Research",
            "volume": "37",
            "issn": "7",
            "pages": "717--742",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Autonomy infused teleoperation with application to brain computer interface controlled manipulation",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Muelling",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Venkatraman",
                    "suffix": ""
                },
                {
                    "first": "J.-S",
                    "middle": [],
                    "last": "Valois",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Downey",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weiss",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Javdani",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hebert",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Schwartz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "L"
                    ],
                    "last": "Collinger",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Bagnell",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Autonomous Robots",
            "volume": "41",
            "issn": "6",
            "pages": "1401--1422",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Making sense of vision and touch: Selfsupervised learning of multimodal representations for contact-rich tasks",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Srinivasan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Savarese",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Fei-Fei",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Garg",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bohg",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 International Conference on Robotics and Automation (ICRA)",
            "volume": "",
            "issn": "",
            "pages": "8943--8950",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Enabling robot teammates to learn latent states of human collaborators",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Unhelkar",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Guan",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Shared autonomy with learned latent actions",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "J"
                    ],
                    "last": "Jeon",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Losey",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sadigh",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.03210"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Human interaction with a service robot: mobile-manipulator handing over an object to a human",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Agah",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Tanie",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Proceedings of International Conference on Robotics and Automation",
            "volume": "1",
            "issn": "",
            "pages": "575--580",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Human-robot interaction for cooperative manipulation: Handing objects to one another",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Edsinger",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Kemp",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "RO-MAN 2007 -The 16th IEEE International Symposium on Robot and Human Interactive Communication",
            "volume": "",
            "issn": "",
            "pages": "1167--1172",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Humanrobot interaction in handing-over tasks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Huber",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rickert",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Knoll",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brandt",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Glasauer",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "RO-MAN 2008 -The 17th IEEE International Symposium on Robot and Human Interactive Communication",
            "volume": "",
            "issn": "",
            "pages": "107--112",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Human preferences for robot-human hand-over configurations",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cakmak",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Srinivasa",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "K"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Forlizzi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kiesler",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on",
            "volume": "",
            "issn": "",
            "pages": "1986--1993",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Comfortable robot to human object hand-over",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Aleotti",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Micelli",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Caselli",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication",
            "volume": "",
            "issn": "",
            "pages": "771--776",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Personalization framework for adaptive robotic feeding assistance",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Canal",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Aleny\u00e0",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Torras",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "22--31",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Rrt-connect: An efficient approach to single-query path planning",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "J"
                    ],
                    "last": "Kuffner",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "M"
                    ],
                    "last": "Lavalle",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065)",
            "volume": "2",
            "issn": "",
            "pages": "995--1001",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Approaches for heuristically biasing rrt growth",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Urmson",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Simmons",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No. 03CH37453)",
            "volume": "2",
            "issn": "",
            "pages": "1178--1183",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Influences on proxemic behaviors in human-robot interaction",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Takayama",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Pantofaru",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "5495--5502",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Social robot navigation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kirby",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Simmons",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Forlizzi",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Left: PyBullet sim with robot mesh (Franka Emika Panda) M R , food object mesh M food (e.g. carrot) at pose p, and mouth mesh M mouth (cylindrical tube, radii from dm) at pose pm. Right: End-to-end algorithm timing for learned constraint model compared to projection-based sampling (100 trajectories each).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Spatial comfort cost (red higher, green lower). The steeper cost gradient in the upward direction than downward ensures trajectories near the face (e.g.,Fig. 3) have high \"comfort\" cost.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Left:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "User study quantitative results. Each non-highlighted plot shows the average comfort rating, ease rating, and rank between trajectory types across 4 different food poses, with the range across all 6 users plotted as error bars. The highlighted plots on the left show the average across all food poses, with error bars representing 95% confidence. Significant results, as determined by two-way ANOVA with repeated measures, Tukey HSD test, and Bonferroni correction (P < 0.01), are marked with an asterisk. We do not treat multiple ratings as independent. Despite the limited sample size (N = 6), trajectories from the combined comfort and efficiency method perform significantly better than the baseline fixed pose approach across all three metrics. Notably, the efficiency-only method often performs worse than comfort-only in comfort ratings. See Appendix V for significance testing details and more analysis.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Example trajectories optimized for each metric. The efficiency metric (b) rotates the carrot sideways so as much can be consumed in one bite as possible. The comfort metric (c) penalizes more complicated trajectories where the robot body is likely to encroach on the face. The combined metric (d) results in a fairly straight trajectory that still ends with a sideways carrot.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "mesh Mfood, Constraints C, Goal sampling distribution Dg, Mouth dm and pm, Clustering method k-mediods 2: P = {} 3: while P has less than N poses do return cluster centers \u2190 k-mediods(P )Fig. 7: An example of what \"slicing\" a food geometry, in this case a carrot, with the mouth plane, the gray cross sectional plane, looks like in simulation when calculating the efficiency score with simplified mouth and carrot geometries. The efficiency cost J E (p G ), defined in Eq. (8), uses the ratio of final volume V f and initial volume V i , shown here on the right and left sides, respectively.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Holdout training plots for the learned collision prediction model. From left to right: Holdout collision prediction accuracy during training for colliding samples (0 to 1); Holdout collision prediction accuracy during training for non-colliding samples (0 to 1); BCE holdout loss. constraint sampling achieves similar quality trajectories while greatly reducing the algorithm execution time. APPENDIX II MOTION PLANNING ALGORITHMS Algorithm 2 Heuristic-Guided Bi-directional RRT (h-BiRRT) 1: Given K goal poses {p k G }, cost-to-come h, cost-so-far g, initial food pose p0, sampling distribution DT , hRRT connect 2: Start tree: S = Tree(p0) 3: Goal trees: G = {G k = Tree(p k G )} 4: while S not connected to each G k do 5: Sample unconnected goal tree Gi \u223c G by goal cost 6: TA, TB \u2190 sortByLength(S, Gi) 7: connect(TA, TB, g, h) 8: return Smoothed paths from p0 to each goal p k G",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Left: HSV + Depth filtering example for a carrot. Right: Point cloud reconstruction example, with estimated oriented bounding box in green.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Starting and ending poses for our method (CE) evaluated in simulation with models for many different basic geometries: a carrot (top left), slice of cantaloupe (top right), celery (bottom left), and strawberry (bottom right). Given the specific food item's mesh M food , our method is able to find paths that bring a large portion of each food item into the mouth while simultaneously respecting the user's personal space. 1) Vertical: the carrot begins along the up-down axis, perpendicular to the fork. 2) Horizontal: the carrot begins along the left-right axis, perpendicular to the fork. 3) Roll & Pitch: and the orientation includes a roll angle left and pitch angle up. 4) Yaw: the orientation is rotated about the vertical axis with respect to Horizontal.",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "Learned Constraints vs. Projection sampling over 100 evaluated trajectories. (\u00b5 \u00b1 \u03c3) for sampling time, constraint prediction accuracy, RRT comfort cost (normalized), RRT efficiency cost (normalized).",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Key parameters for our method.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Rank CE vs C CE vs E CE vs F C vs F E vs F",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Pairwise significance test results for each method, under each user rating metric. Methods are Comfort+Efficiency (CE), EfficiencyOnly (E), ComfortOnly (C), & FixedPose (F). A vs B indicates method A > method B.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
{
    "paper_id": "3766ca59ae0cff7fc939497d638a2f1b8c5befdd",
    "metadata": {
        "title": "Learning via variably scaled kernels",
        "authors": [
            {
                "first": "C",
                "middle": [],
                "last": "Campi",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universit\u00e0 di Genova",
                    "location": {
                        "settlement": "Genoa",
                        "country": "Italy"
                    }
                },
                "email": "campi@dima.unige.it"
            },
            {
                "first": "\u00b7",
                "middle": [
                    "F"
                ],
                "last": "Marchetti",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universit\u00e0 di Padova",
                    "location": {
                        "settlement": "Padua",
                        "country": "Italy"
                    }
                },
                "email": ""
            },
            {
                "first": "\u00b7",
                "middle": [
                    "E"
                ],
                "last": "Perracchione",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universit\u00e0 di Genova",
                    "location": {
                        "settlement": "Genoa",
                        "country": "Italy"
                    }
                },
                "email": "perracchione@dima.unige.it"
            },
            {
                "first": "Robert",
                "middle": [],
                "last": "Schaback",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "E",
                "middle": [],
                "last": "Perracchione",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "F",
                "middle": [],
                "last": "Marchetti",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "We investigate the use of the so-called variably scaled kernels (VSKs) for learning tasks, with a particular focus on support vector machine (SVM) classifiers and kernel regression networks (KRNs). Concerning the kernels used to train the models, under appropriate assumptions, the VSKs turn out to be more expressive and more stable than the standard ones. Numerical experiments and applications to breast cancer and coronavirus disease 2019 (COVID-19) data support our claims. For the practical implementation of the VSK setting, we need to select a suitable scaling function. To this aim, we propose different choices, including for SVMs a probabilistic approach based on the naive Bayes (NB) classifier. For the classification task, we also numerically show that the VSKs inspire an alternative scheme to the sometimes computationally demanding feature extraction procedures.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In the context of approximation theory, the variably scaled kernels (VSKs) were introduced in 2015 by [6] . The basic idea behind them is to map the initial set of examples via a scaling function and construct an augmented approximation space. Our main contribution consists in linking the VSKs to the field of machine learning, as the VSKs have a long-known equivalent in pattern analysis. Precisely, many methods based on feature augmentation, as, e.g., zero padding and feature replication [9, 21, 27] , fall into the general VSK setting that we are going to investigate.",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 105,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 493,
                    "end": 496,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 497,
                    "end": 500,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 501,
                    "end": 504,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Focusing on kernel learning methods and specifically on KRNs and SVMs (see, e.g., [16, 42] ), we give a very general formulation of feature augmentation schemes via VSKs. In doing so, we drive our attention towards the Gaussian and linear kernels, being truly popular for learning issues. We provide theoretical results concerning their practical implementation and expressiveness [13] and we further analyze the spectrum of the kernel matrices constructed via VSKs. This study reveals the effectiveness of the proposed approach especially for the Gaussian kernel; indeed, the condition number of the VSK kernel matrix is less than or equal to the condition number of the matrix constructed via the standard kernel. This fact turns out to be relevant for KRNs, where one may require to compute the inverse of the kernel matrix, which is usually affected by severe ill-conditioning. Moreover, for the selection of the scaling function of the KRN-VSK, one can refer to the available literature in the context of approximation theory [10, 36] . Indeed, the scaling function might be selected so that it mimics the samples and this might lead to an improvement in terms of accuracy and/or stability (see, e.g., [6, 10, 11] ). Here, in particular, we propose to use a non-linear fitting of the function itself as augmented feature.",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 86,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 87,
                    "end": 90,
                    "text": "42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 381,
                    "end": 385,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1036,
                    "end": 1039,
                    "text": "36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1211,
                    "end": 1214,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1215,
                    "end": 1218,
                    "text": "11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "While for the KRN-VSK we can refer to some available literature for selecting the scaling function, for SVM-VSK, we consider a probabilistic solution. More precisely, focusing on binary classification problems, we first note that the VSK setting induces new feature maps and spaces that depend on the scaling function associated to the VSK. For being competitive with the accuracy of the classical SVMs, as well as with other common classifiers, we have to select a suitable scaling function for the VSKs. To this aim, we remark that the SVM is characterized by a geometric point of view. Nevertheless, methods based on probability distributions, as the NB classifiers, might outperform SVM. For that reason, many efforts are devoted to investigate which classifier performs better and under which conditions; for a general overview refer, e.g., to [7, 31, 47] . In this work, we thus fuse SVM and NB classifiers by means of VSKs, so that the mixed approach takes into account the probabilistic features of the NB algorithm and classifies geometrically with SVM. Moreover, we conclude the paper by presenting a feature extraction algorithm that is inspired by the VSK framework and that might be considered in place of other feature extraction schemes; refer, e.g., to [19, 45] .",
            "cite_spans": [
                {
                    "start": 849,
                    "end": 852,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 853,
                    "end": 856,
                    "text": "31,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 857,
                    "end": 860,
                    "text": "47]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 1269,
                    "end": 1273,
                    "text": "[19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1274,
                    "end": 1277,
                    "text": "45]",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The paper is organized as follows. In Section 2, we briefly review the use of kernels in machine learning literature. In Section 3, we investigate the VSKs for two learning methods, specifically SVM and KRNs. Then, in Sections 4 and 5, we drive our attention towards the Gaussian and linear VSKs as well as towards the problem of selecting the scaling function. Section 6 is devoted to numerical experiments with both toy models and a real dataset. In Section 7, we present a feature extraction algorithm whose underlying idea is derived from the study of the variably scaled setting. The last section deals with conclusions and work in progress.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We consider a learning problem with training examples",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "where x i \u2208 \u03a9 \u2286 R n and y i \u2208 R. For the particular case of the classification setting, we fix y i \u2208 {\u22121, +1}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "For both SVMs and KRNs, we drive our attention towards (strictly) positive definite kernels \u03ba : \u03a9 \u00d7 \u03a9 \u2212\u2192 R, where \u03a9 is a bounded set, that can be decomposed via the Mercer's Theorem as explained below (see, e.g., Theorem 2.2. [15] p. 107 or [26] ).",
            "cite_spans": [
                {
                    "start": 226,
                    "end": 230,
                    "text": "[15]",
                    "ref_id": null
                },
                {
                    "start": 241,
                    "end": 245,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "then the kernel can be expressed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "where {\u03bb k } k\u22650 are the (non-negative) eigenvalues and {\u03c1 k } k\u22650 are the (L 2orthonormal) eigenfunctions of the operator T : L 2 (\u03a9) \u2212\u2192 L 2 (\u03a9), given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "Moreover, such expansion is absolutely and uniformly convergent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "We point out that many relevant kernels, e.g., cases where \u03a9 is unbounded or non-measurable, do not fall into the above Mercer decomposition. Thus, on one side, taking only Mercer kernels might be restrictive. On the other side, it provides the adequate background for our purposes and it offers an easy way to introduce feature maps and spaces. Indeed, for such kernels that admit a Mercer expansion (also called valid kernels according to the definition given by [42] ), it is worth to note that we can interpret the series representation in terms of an inner product in the so-called feature space F , which is a Hilbert space. Indeed,",
            "cite_spans": [
                {
                    "start": 465,
                    "end": 469,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "where \u03a6 : \u03a9 \u2212\u2192 F is a feature map. For a given kernel, the feature map and space are not unique. A possible solution is the one of taking the map \u03a6(x) = \u03ba(\u00b7, x), which is linked to the characterization of F as a reproducing kernel Hilbert space; see [16, 42] for further details.",
            "cite_spans": [
                {
                    "start": 250,
                    "end": 254,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 255,
                    "end": 258,
                    "text": "42]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "In the classification context, many studies are devoted to investigate and measure the complexity of a chosen model, such as the so-called VC dimension [44] and the empirical Rademacher complexity [4] . The complexity of a method is usually referred to as capacity or expressiveness. Indeed, complex models have the capability to perform complex tasks, by determining elaborated decision functions, and thus to express sophisticated links between the data. In any case, the capacity of a method needs to be tailored to the considered task, in order to avoid overfitting; for a general overview, we refer, e.g., the reader to [39] .",
            "cite_spans": [
                {
                    "start": 152,
                    "end": 156,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 197,
                    "end": 200,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 625,
                    "end": 629,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "To better investigate the concept of expressiveness in the kernel setting, we introduce the kernel matrix K constructed via the dataset \u039e = {x 1 , . . . , x N } \u2286 \u03a9, i.e., the matrix of entries",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "where \u03ba is a (strictly) positive definite kernel. Note that if \u03ba is a strictly positive definite kernel then K is positive definite, while it is positive semi-definite if \u03ba is a positive definite kernel.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "The expressiveness of a kernel-based model is related to the number of dichotomies achievable by a linear separator in the feature space. Moreover, concerning the rank of the kernel matrix, we have the following result [ As capacity measure dedicated to the kernel setting, we consider the spectral ratio that has been introduced in [13] . It is defined as",
            "cite_spans": [
                {
                    "start": 219,
                    "end": 220,
                    "text": "[",
                    "ref_id": null
                },
                {
                    "start": 333,
                    "end": 337,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Remark 1"
        },
        {
            "text": "According to the following definition (see [13, Definition 1, p. 8] ), such quantity is an expressiveness measure for kernels. As a remark, we also point out that it is connected to the empirical Rademacher complexity [13, Theorem 4, p. 9].",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 47,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 48,
                    "end": 61,
                    "text": "Definition 1,",
                    "ref_id": null
                },
                {
                    "start": 62,
                    "end": 67,
                    "text": "p. 8]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Remark 1"
        },
        {
            "text": "Definition 1 Let \u03ba i , \u03ba j : \u03a9 \u00d7 \u03a9 \u2212\u2192 R, be two (strictly) positive definite kernels. We say that \u03ba j is more specific (or more expressive) than \u03ba i whenever for any dataset",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1"
        },
        {
            "text": "where K i and K j are the kernel matrices on \u039e obtained via \u03ba i and \u03ba j , respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1"
        },
        {
            "text": "Remark 2 Technically the Definition 1, which is taken by [13] , states that \u03ba j is more specific (or more expressive) than \u03ba i also when the equality in (2.3) holds true. In the latter case, we should use the term \"equally or more specific than.\" To take a common notation with the native definition, we simply use the term \"more expressive\" also for the trivial case.",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 61,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Remark 1"
        },
        {
            "text": "The spectral ratio being an expressiveness measure, it is related to the rank of the kernel matrix (see also Remark 1), indeed",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1"
        },
        {
            "text": "We conclude this brief review on kernels for machine learning by pointing out that the kernel matrices introduced above might suffer from severe ill-conditioning. In order to partially overcome instability issues in the approximation framework, a possible solution comes from the use of VSKs (see below for their definition), which have been recently introduced in [6] ; refer also to [10, 11] . We point out that in Definition 2, we present a multidimensional extension of the scaling function \u03c8, which has been introduced as a real-valued function in the previous literature [6] .",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 368,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 385,
                    "end": 389,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 390,
                    "end": 393,
                    "text": "11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 577,
                    "end": 580,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Remark 1"
        },
        {
            "text": "When dealing with Mercer's kernels, the construction of a VSK as in Definition 2 provides a valid kernel. We now extend this general setting to work with KRNs and SVMs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1"
        },
        {
            "text": "To have a clear theoretical framework, we investigate the use of VSKs as a feature augmentation algorithm, where new features are added to the original dataset in order to possibly increase the performances of learning schemes. According to Definition 2, we define a function \u03a8 : \u03a9 \u2212\u2192\u03a9 as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Learning with VSKs"
        },
        {
            "text": "The function \u03a8 extends the data vector x \u2208 \u03a9, including m features that depend on the original ones. The VSK kernel defined in (2.4) is a valid kernel, as it corresponds to an inner product in the associated feature space F \u03a8 (see [42, Proposition 3.22, p. 75] ). Moreover, it induces a new feature map \u0398 : \u03a9 \u2212\u2192 F \u03a8 so that",
            "cite_spans": [
                {
                    "start": 231,
                    "end": 235,
                    "text": "[42,",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 236,
                    "end": 253,
                    "text": "Proposition 3.22,",
                    "ref_id": null
                },
                {
                    "start": 254,
                    "end": 260,
                    "text": "p. 75]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Learning with VSKs"
        },
        {
            "text": "Referring to (2.1), because of [6, Theorem 3.1], the spaces F \u03a8 and the classical feature space F , associated to \u03ba :\u03a9 \u00d7\u03a9 \u2212\u2192 R and induced by the feature map \u03a5 :\u03a9 \u2212\u2192 F , are isometric; see also [10, Proposition 2.3] . We now investigate the use of the VSKs for both SVMs and KRNs.",
            "cite_spans": [
                {
                    "start": 194,
                    "end": 198,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 199,
                    "end": 215,
                    "text": "Proposition 2.3]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Learning with VSKs"
        },
        {
            "text": "In this section, we present the VSK setting in the SVM algorithm. For this general overview, we also refer the reader to [16, 42] . We take \u039e = {x i , i = 1, . . . , N} \u2286 \u03a9, where \u03a9 \u2286 R n . The associate function values are so that y i \u2208 {\u22121, +1}, i = 1, . . . , N. Indeed, for the binary classification problem via VSKs, we need to find a predictor, i.e., a decision function s \u03a8 : \u03a9 \u2212\u2192 {\u22121, +1}, that assigns appropriate labels, i.e.,\u1ef9 i \u2208 {\u22121, +1}, to other unknown",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 125,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 126,
                    "end": 129,
                    "text": "42]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "SVM-VSK"
        },
        {
            "text": "we define a non-linear SVM classifier that makes use of VSKs via the following decision function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SVM-VSK"
        },
        {
            "text": "Observe that in the computation of b any given index i so that 0 < \u03b1 i < \u03b6 can be used. However, to make b uniquely defined and for stability purposes, it is computed via an average over all such candidates. The equation of the SVM decision function s \u03a8 : \u03a9 \u2212\u2192 {\u22121, +1}, i.e., w and b as in equation (3.1) and (3.2), is then found by imposing the Karush Kuhn Tucker conditions (see, e.g., [29] ) and thanks to (3.1), for x \u2208 \u03a9, it reads as follows:",
            "cite_spans": [
                {
                    "start": 389,
                    "end": 393,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "SVM-VSK"
        },
        {
            "text": "If one uses the standard kernel \u03ba : \u03a9 \u00d7 \u03a9 \u2212\u2192 R, then we recover the classical SVM setting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SVM-VSK"
        },
        {
            "text": "As a second test case for the use of VSKs in the machine learning context, we investigate regression networks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SVM-VSK"
        },
        {
            "text": "Since here KRNs are used for regression/interpolation tasks, given distinct data",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KRN-VSK"
        },
        {
            "text": "Concerning supervised learning networks, the simplest strategy consists in learning the trend between inputs and outputs via a predictor s \u03a8 : \u03a9 \u2212\u2192 R which is a linear combination of some basis functions, in this case VSKs. For a general overview on KRNs, we refer the reader to [16, 30] .",
            "cite_spans": [
                {
                    "start": 279,
                    "end": 283,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 284,
                    "end": 287,
                    "text": "30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "KRN-VSK"
        },
        {
            "text": "We keep the general framework of KRNs and we adapt them to the use of VSKs. Here, we focus on kernels with centers at locations Z = {z i , i = 1, . . . , M} \u2286 \u03a9; and thus, our KRN-VSK predictor s \u03a8 : \u03a9 \u2212\u2192 R is of the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KRN-VSK"
        },
        {
            "text": "for (strictly) positive definite kernels \u03ba \u03a8 : \u03a9 \u00d7 \u03a9 \u2212\u2192 R and for some real coefficients c 1 , . . . , c M .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KRN-VSK"
        },
        {
            "text": "For KRN-VSK, we compute c = (c 1 , . . . , c M ) \u2208 R M via the following minimization problem [15] ",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 98,
                    "text": "[15]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "KRN-VSK"
        },
        {
            "text": "where \u03bd \u2208 R + is a regularization parameter.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KRN-VSK"
        },
        {
            "text": "In the following, we may take the set of kernel centers Z \u2261 \u039e . In that case, the kernel matrix K \u03a8 of entries",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KRN-VSK"
        },
        {
            "text": "is square. Furthermore, if a strictly positive definite kernel as the Gaussian function is used, then the matrix is non-singular. Therefore, we may look at the special setting for which \u03bd = 0. In that case, the solution can be found as c = (K \u03a8 ) \u22121 y, where y = (y 1 , . . . , y N ) and c = (c 1 , . . . , c N ) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "KRN-VSK"
        },
        {
            "text": "In general, computing the inverse of the kernel matrix K might lead to serious instability issues due to the typical ill-conditioning of the kernel matrix. This problem may be somehow overcome by selecting a safe shape parameter \u03b3 , formally introduced below, and/or by using stable bases; refer, e.g., to [20, 24, 34] . In the incoming sections, we will point out that the use of VSKs might reduce the usual ill-conditioning of the kernel matrices.",
            "cite_spans": [
                {
                    "start": 306,
                    "end": 310,
                    "text": "[20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 311,
                    "end": 314,
                    "text": "24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 315,
                    "end": 318,
                    "text": "34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "KRN-VSK"
        },
        {
            "text": "In this section, we focus on specific kernels providing the practical implementation of the variably scaled setting. Furthermore, we also study the expressiveness and the conditioning induced by the VSKs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gaussian and linear VSKs"
        },
        {
            "text": "Radial kernels are truly common. They are kernels for whom there exists a radial basis function (RBF) \u03d5 : R + \u2212\u2192 R, where R + := [0, \u221e), and (possibly) a shape parameter \u03b3 > 0 such that, for all x, y \u2208 \u03a9,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gaussian kernel"
        },
        {
            "text": "Among all radial kernels, we remark that the Gaussian is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gaussian kernel"
        },
        {
            "text": "We now discuss its practical implementation in the variably scaled setting. We point out that the Gaussian kernel is strictly positive definite; and thus, its associated kernel matrix turns out to be positive definite, provided that the data are distinct; see, e.g., [16] .",
            "cite_spans": [
                {
                    "start": 267,
                    "end": 271,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Gaussian kernel"
        },
        {
            "text": "Throughout this section, we take N data points",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Practical implementation for the Gaussian VSK"
        },
        {
            "text": "The Gaussian VSK matrix can be seen as a Hadamard product; indeed, we have the following result.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Practical implementation for the Gaussian VSK"
        },
        {
            "text": ". . , N, and \u2022 denotes the Hadamard matrix product.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Practical implementation for the Gaussian VSK"
        },
        {
            "text": "Proof For x, y \u2208 \u03a9, we have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Practical implementation for the Gaussian VSK"
        },
        {
            "text": "Therefore, the entries of the VSK matrix built on \u039e = {x i , i = 1, . . . , N} are given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Practical implementation for the Gaussian VSK"
        },
        {
            "text": "and thus",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Practical implementation for the Gaussian VSK"
        },
        {
            "text": "About the Hadamard product, we report here a result that can be traced back to 1911 by Schur [40] . It will be helpful in what follows; refer also to [ ",
            "cite_spans": [
                {
                    "start": 93,
                    "end": 97,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 150,
                    "end": 151,
                    "text": "[",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Practical implementation for the Gaussian VSK"
        },
        {
            "text": "This result allows us to infer about the spectrum of the kernel matrix (see [12] ) and to show that with the Gaussian VSK we gain both in terms of stability and expressiveness of the kernel.",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 80,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Practical implementation for the Gaussian VSK"
        },
        {
            "text": "We now give upper and lower bounds for the Frobenius norm \u00b7 F of the kernel matrix K in terms of its variably scaled setting. This turns out to be helpful when comparing the spectral ratio of the two matrices (K and K \u03a8 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the Gaussian VSK"
        },
        {
            "text": "Proof Being the RBF \u03d5 : R + \u2212\u2192 R associated to the Gaussian kernel \u03ba nonincreasing, for x, y \u2208 \u03a9, we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the Gaussian VSK"
        },
        {
            "text": "which in particular implies that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the Gaussian VSK"
        },
        {
            "text": "From this theorem, we can easily infer on the spectral ratio in the VSK setting. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the Gaussian VSK"
        },
        {
            "text": "where \u03d5 : R + \u2212\u2192 R is the RBF associated to the Gaussian kernel \u03ba : \u03a9 \u00d7\u03a9 \u2212\u2192 R. Taking into account Theorem 5, we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the Gaussian VSK"
        },
        {
            "text": "Remark 3 The concept of expressiveness when the equality in (4.2) holds true has already been clarified in Remark 2. We further point out that the equality is satisfied, for instance, in the trivial case for which \u03c8(x) \u2261 0, for all x \u2208 \u03a9.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the Gaussian VSK"
        },
        {
            "text": "On one side, the fact that the Gaussian VSK is more expressive than the standard one tells us that the VSK-based learning might be able to deal with more complex tasks. In the next subsection, we focus on the stability of the kernel matrix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the Gaussian VSK"
        },
        {
            "text": "The smallest eigenvalue of a positive definite kernel matrix is of course linked to the ill-conditioning. Moreover, given \u039e = {x i , i = 1, . . . , N} \u2286 \u03a9, the stability is also related to the separation distance",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectrum of the Gaussian VSK"
        },
        {
            "text": "which only depends on the data. As shown in, e.g., [6] , we have that",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 54,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Spectrum of the Gaussian VSK"
        },
        {
            "text": "is the separation distance in the VSK setting. This gives the intuition of the fact that the VSKs might lead to possible improvements in terms of stability [6] . Indeed, in general, it is well-known that the smallest eigenvalue of the kernel matrix is related to the separation distance, meaning that the ill-conditioning usually grows as the separation distance decreases; refer, e.g., to [28] , where the authors make use of a result from [3] on the eigenvalues of distance matrices. These facts are the fruits on many studies on the so-called trade-off or uncertainty principle [37, 38] , which could be summarized in a conflict between accuracy and stability. As already mentioned, the VSKs are helpful for improving the stability, especially in view of the following property. We also refer the reader to [43, ",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 159,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 390,
                    "end": 394,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 441,
                    "end": 444,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 581,
                    "end": 585,
                    "text": "[37,",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 586,
                    "end": 589,
                    "text": "38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 810,
                    "end": 814,
                    "text": "[43,",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Spectrum of the Gaussian VSK"
        },
        {
            "text": "Proof First note that, since in this case the matrix is positive definite, the condition number can be computed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectrum of the Gaussian VSK"
        },
        {
            "text": "Moreover, from Theorem 4 and since the RBF \u03d5 : R + \u2212\u2192 R associated to the Gaussian kernel \u03ba :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectrum of the Gaussian VSK"
        },
        {
            "text": "This result turns out to be meaningful especially for the KRN-VSK approach. As a second case study, we now consider the linear kernel, which is truly popular for classification tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectrum of the Gaussian VSK"
        },
        {
            "text": "For x, y \u2208 \u03a9, the linear kernel \u03ba : \u03a9 \u00d7 \u03a9 \u2212\u2192 R is given by \u03ba(x, y) = x y.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The linear VSK"
        },
        {
            "text": "As for the Gaussian kernel, its implementation in the variably scaled setting turns out to be trivial. We remark that the linear kernel is positive definite; and thus, its associated kernel matrix turns out to be positive",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The linear VSK"
        },
        {
            "text": "The linear VSK can be written as sum of matrices; indeed, we have the following result.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Practical implementation for the linear VSK"
        },
        {
            "text": "Let \u039e = {x i , i = 1, . . . , N} \u2286 \u03a9 be a set of data points. Let \u03c8 : \u03a9 \u2212\u2192 \u039b be the scaling function for the VSK setting. Let \u03ba : \u03a9 \u00d7 \u03a9 \u2212\u2192 R be the linear kernel. Then, the VSK matrix constructed on \u039e via \u03ba \u03a8 : \u03a9 \u00d7 \u03a9 \u2212\u2192 R is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 6"
        },
        {
            "text": "Proof For x, y \u2208 \u03a9 we have that:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 6"
        },
        {
            "text": "and thus the kernel matrix is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 6"
        },
        {
            "text": "We now drive our attention towards the expressiveness of the linear VSK.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 6"
        },
        {
            "text": "Depending on the function \u03c8, we might have that the linear VSK is less expressive than the standard linear kernel; indeed, we have the following proposition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the linear VSK"
        },
        {
            "text": ". . , N} \u2286 \u03a9 be a set of data points. Let \u03c8 : \u03a9 \u2212\u2192 \u039b be the scaling function for the VSK setting. Let \u03ba : \u03a9 \u00d7 \u03a9 \u2212\u2192 R be the linear kernel. Let us suppose that the associated kernel matrix K is non-negative, i.e., so that all the entries of K are non-negative. Given the VSK matrix",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the linear VSK"
        },
        {
            "text": "Proof Under our assumptions, if \u03c8 : \u03a9 \u2212\u2192 \u039b is so that K \u03c8 is non-negative, we have that tr(K) tr(K \u03a8 ) \u2264 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the linear VSK"
        },
        {
            "text": "Moreover, since we suppose K to be non-negative, we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the linear VSK"
        },
        {
            "text": "Finally, taking into account the definition of the spectral ratio, the statement follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the linear VSK"
        },
        {
            "text": "Note that the requirements of Proposition 2 are satisfied, e.g., if \u03a9 \u2286 R n + and \u039b \u2286 R m + .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral ratio for the linear VSK"
        },
        {
            "text": "Being Gramian matrices, K \u03a8 and K \u03c8 are positive semi-definite. Concerning the minimum eigenvalue of the VSK matrix K \u03a8 , by virtue of Weyl's inequality (see, e.g., [5, Section III.2, p. 62]), we obtain that:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Minimum eigenvalue of the linear VSK matrix"
        },
        {
            "text": "As for the Gaussian kernel, one can make many different choices for the function \u03c8. Some of them are discussed in the next section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Minimum eigenvalue of the linear VSK matrix"
        },
        {
            "text": "In this section, we provided some theoretical findings concerning the expressiveness of VSKs in terms of the spectral ratio, without taking into account the tuning of the shape parameter \u03b3 (see (4.1)), which is considered fixed. While such a theoretical investigation concerns a relevant topic in the theory of machine learning, as we pointed out in Section 2, the spectral ratio represents a poor choice for model selection in practical applications. Indeed, in view of the maximization of a certain score (e.g., accuracy, AUC, f1-score), it is convenient to perform a classical tuning of the model parameters (e.g., SVM, KRN) instead of analyzing its capacity via the spectral ratio as it is.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 4"
        },
        {
            "text": "In the framework of approximation theory, as well as for KRNs, the choice of the scaling function can be guided by some characteristics concerning the data distribution or the underlying function that needs to be reconstructed (see, e.g., [35, 36] ). In the classification setting, the VSKs can be seen as feature augmentation methods. More precisely, our aim is to adopt this strategy to encode possible a priori information in the kernel. Let us take N data points \u039e = {x i , i = 1, . . . , N} \u2286 \u03a9, where \u03a9 \u2286 R n and consider a subset \u039b \u2286 R m , we now propose some techniques to define the scaling function of the VSK framework.",
            "cite_spans": [
                {
                    "start": 239,
                    "end": 243,
                    "text": "[35,",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 244,
                    "end": 247,
                    "text": "36]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Choices for the scaling function"
        },
        {
            "text": "Depending on the task and on the available knowledge, different choices for the scaling function could be taken into account. Here, we construct the scaling function \u03c8 : \u03a9 \u2212\u2192 \u039b as follows. Given the dataset",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for SVM-VSK"
        },
        {
            "text": "we introduce the classes C 1 and C 2 , associated to the labels y = \u22121 and y = +1, respectively. Letx = (x 1 , . . . ,x n ) be a new example that we need to classify. Treating the features as mutually independent, the NB classifier (see, e.g., [1, 25] ) computes",
            "cite_spans": [
                {
                    "start": 244,
                    "end": 247,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 248,
                    "end": 251,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Scaling function for SVM-VSK"
        },
        {
            "text": "The likelihood n i=1 P (x i |C j ) and the prior P (C j ) are typically estimated from the dataset \u03a3. In other cases, especially when the dataset is not too large, they could be obtained as a priori knowledge, for example by consulting the literature.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for SVM-VSK"
        },
        {
            "text": "In For x \u2208 \u03a9, since P 2 (x) = 1 \u2212 P 1 (x) and P 1 (x) are not independent, we observe that it is sufficient to consider one of the two probabilities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for SVM-VSK"
        },
        {
            "text": "Concerning the effectiveness of this scaling function \u03a8 : \u03a9 \u2212\u2192\u03a9 for the Gaussian VSK, we refer to the notation introduced in Theorem 3 and we point out that,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for SVM-VSK"
        },
        {
            "text": "We observe that if P 1 (x i ) \u2248 P 1 (x j ), then K \u03c8 ij \u2248 1 and so K \u03a8 ij \u2248 K ij . Considering instead the linear VSK \u03ba \u03a8 : \u03a9 \u00d7 \u03a9 \u2212\u2192 R described in Section 4.2, we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for SVM-VSK"
        },
        {
            "text": "We remark that, according to to Proposition 2, with the linear VSK we construct kernels that might be less expressive than the standard ones.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for SVM-VSK"
        },
        {
            "text": "For both kernels, this means that the matrices change according to our a priori knowledge on the dataset, leading to a different, possibly easier, learning task for SVM.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for SVM-VSK"
        },
        {
            "text": "Here, we take again N distinct data \u039e = {x i , i = 1, . . . , N} \u2286 \u03a9, where \u03a9 \u2286 R n , and the associated measurements y i \u2208 R, i = 1, . . . , N, and consider a subset \u039b \u2286 R m . We now investigate some ideas to define the scaling function for KRNs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for KRN-VSK"
        },
        {
            "text": "Therefore, concerning the choice of the scaling function \u03c8 : \u03a9 \u2212\u2192 \u039b, we suppose to know the trend of data, which can be modelled via a specific class of functions, i.e., a model M : \u03a9 \u00d7 R l \u2212\u2192 R depending on x \u2208 \u03a9, and on l parameters \u03b2 = (\u03b2 1 , . . . , \u03b2 l ). To determine \u03b2, we compute:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for KRN-VSK"
        },
        {
            "text": "Then, one possible solution to define the function \u03c8 : \u03a9 \u2212\u2192 \u039b is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for KRN-VSK"
        },
        {
            "text": "Of course, this gives a recipe for the selection of the scaling function which is not unique.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scaling function for KRN-VSK"
        },
        {
            "text": "All the performed experiments have been carried out in PYTHON, using also the scientific module scikit-learn [32] , on a Intel(R) Core(TM) i7 CPU 4712MQ 2.13 GHz processor.",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 113,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Numerical tests for SVM-VSK and KRN-VSK"
        },
        {
            "text": "In the following, we consider different toy datasets of various sizes, with precise probability information concerning the features' distributions, and we compare our SVM-VSK approach with standard SVM and NB classifiers. A freely available software can be downloaded at https://github.com/emmaA89/SVM-VSK. Moreover, in the validation and in the test steps, we evaluate the performance of the considered methods by means of the f 1 -score, weighted with respect to the classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests for SVM-VSK"
        },
        {
            "text": "We remind that the f 1 -score is defined as the harmonic mean between precision and recall. More precisely, given the number of true positive (TP), false positive (FP), and false negative (FN) cases,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests for SVM-VSK"
        },
        {
            "text": "where precision = TP TP + FP and recall = TP TP + FN .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests for SVM-VSK"
        },
        {
            "text": "We proceed by constructing 12 toy datasets that differ in terms of number of features and examples. We now fix n = 64. Letting \u03a9 \u2286 R n , they are extracted from the dataset",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests for SVM-VSK"
        },
        {
            "text": "where the two classes C 1 and C 2 , associated to the labels y = \u22121 and y = +1 respectively, are exactly balanced. The construction of such a dataset is explained in the following steps.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests for SVM-VSK"
        },
        {
            "text": "1. Each class C j , j = 1, 2, is characterized by two vectors \u03bc j = \u03bc 1 j , . . . , \u03bc n j , \u03c3 j = \u03c3 1 j , . . . , \u03c3 n j .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests for SVM-VSK"
        },
        {
            "text": "More precisely, let us denote by U (a, b) a univariate uniform random distribution on the interval (a, b) \u2286 R and by p \u223c U (a, b) a sample from such distribution. Then, \u03bc k j and \u03c3 k j , k = 1, . . . , n, j = 1, 2 are determined as follows: N (\u03bc, \u03c3 ) the univariate normal distribution with mean \u03bc and standard deviation \u03c3 . Let x i = x 1 i , . . . , x n i be an example in \u03a9 belonging to a class C j , j = 1, 2. The elements x k i of x i \u2208 \u03a9, k = 1, . . . , n, are then randomly generated as samples of N \u03bc k j , \u03c3 k j + 1 = N \u03bc k j , \u03c3 k j + N (0, 1), where N (0, 1) is Gaussian white noise. 3. Finally, the data are normalized between [0, 1].",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 121,
                    "end": 129,
                    "text": "U (a, b)",
                    "ref_id": null
                },
                {
                    "start": 241,
                    "end": 250,
                    "text": "N (\u03bc, \u03c3 )",
                    "ref_id": null
                }
            ],
            "section": "Tests for SVM-VSK"
        },
        {
            "text": "From the so-constructed \u0393 , let n k \u2208 {2, 4, 16, 64} and let x i,n k be the element x i \u2208 \u0393 reduced in dimensions to n k randomly pre-selected features (that are the same for all x i \u2208 \u0393 ). We extract the datasets \u0393 \u03be = {(x i,n k , y i ), i = 1, . . . , N q , y i \u2208 {\u22121, +1}}, with N q \u2208 {100, 500, 2500}, \u03be = (N q , n k ), and preserving the balance among the two classes. More precisely, fixed N q , we randomly select N q indices from the set {1, . . . , 5000}. Moreover, since all combinations examples-features are taken into account, we obtain 12 different datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "We denote by"
        },
        {
            "text": "In the following description, we fix one of the extracted datasets \u0393 \u03be for some value of N q and n k . We divide such a dataset in a training set \u03a3 \u03be and a test set T \u03be . These sets are so that card(\u03a3 \u03be ) \u2248 2card(T \u03be ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "We denote by"
        },
        {
            "text": "In this experiment, we suppose to have a priori information and to encode it in the SVM-VSK method by means of the NB algorithm. More precisely, the NB classifier is trained considering both \u03a3 \u03be and\u0393 \u03be , which is defined as the dataset containing the examples of \u0393 , whose number of features has been reduced to n k , that are not in \u0393 \u03be , i.e.,\u0393 \u03be := \u0393 (5000,n k ) \\ \u0393 \u03be . Therefore, in this test, we compare the performances on T \u03be of the three methods constructed as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "We denote by"
        },
        {
            "text": "1. The NB classifier, which is trained on\u0393 \u03be \u222a \u03a3 \u03be . Given x = (x 1 , . . . , x n k ), we adopt the Gaussian likelihood [33] P (",
            "cite_spans": [
                {
                    "start": 120,
                    "end": 124,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "We denote by"
        },
        {
            "text": "for i = 1, . . . , n k , j = 1, 2. 2. The standard SVM method, which is trained on \u03a3 \u03be . 3. The SVM-VSK classifier, which is trained on \u03a3 \u03be and whose scaling map \u03c8 :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "We denote by"
        },
        {
            "text": "\u03a9 \u2212\u2192 \u039b, constructed as explained in Section 5, considers the probabilistic outcomes of the NB classifier.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "We denote by"
        },
        {
            "text": "In order to tune the SVM hyperparameters \u03b6 and \u03b3 , the latter in case of RBF kernel, we consider a 5-fold cross-validation on \u03a3 \u03be .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "We denote by"
        },
        {
            "text": "We carry out the test for each dataset \u0393 \u03be and we show the obtained results in Fig. 1 . The proposed SVM-VSK algorithm is competitive with the best among SVM and NB methods, slightly outperforming both in some cases.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 79,
                    "end": 85,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "We denote by"
        },
        {
            "text": "For the Gaussian kernel, we numerically verify Corollary 1 by reporting in Table 1 the spectral ratios related to the matrices K and K \u03a8 , obtained from the training sets \u03a3 \u03be with N q = 100, 500, 2500, and n k = 2. The results numerically confirm what was theoretically predicted, i.e., the Gaussian VSK is more expressive than the standard one for a fixed shape parameter.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 75,
                    "end": 82,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "We denote by"
        },
        {
            "text": "Moreover, for the linear kernel, we are in the hypothesis of Proposition 2. The quantities involved in that proposition are reported in Table 2 . The results support what was theoretically observed.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 136,
                    "end": 143,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "We denote by"
        },
        {
            "text": "Here, we refer the reader to [16, Program 18.1, p. 340 ] for a detailed software that deals with KRNs. As an example for KRNs, we focus on the Italian data of the 2020 COVID-19 pandemic. The task we consider consists in learning the time series, i.e., \u03a9 \u2286 R, of people that in Italy were hospitalized as intensive care unit (ICU) patients from 24 February 2020 to 26 April 2020. The dataset, provided by the \"Dipartimento della Protezione Civile,\" is available at https://github.com/pcm-dpc/COVID-19/tree/ master/dati-andamento-nazionale.",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 34,
                    "end": 47,
                    "text": "Program 18.1,",
                    "ref_id": null
                },
                {
                    "start": 48,
                    "end": 54,
                    "text": "p. 340",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Tests for KRN-VSK"
        },
        {
            "text": "The dataset \u0393 consists of 63 samples and it is divided as follows. The first 58 days define the training set \u03a3, i.e., they are used to construct the regression model, which is then tested on the last t = 5 days,x i , i = 1, . . . , t. Referring to Section 3.2 we take the set of kernel centers Z as the set of available data in \u039e and we construct the model using the Gaussian kernel defined in (4.1). Moreover, the feature augmentation strategy outlined in ( In addition, we encode into the kernel also other available data. Precisely, thinking of time series, one usually disposes of other available and dependent data sampled at the same locations which can be used as additional features (see, e.g., [8, 41] ). In this direction, we take into account the total number of COVID19 infected (included death and recovered people), the daily number of new infected and the total number Table 1 The spectral ratios of the matrices K and K \u03a8 related to the normalized training sets \u03a3 \u03be , varying N q = 100, 500, 2500. We set n k = 2 and we considered a Gaussian kernel with \u03b3 = 1 of infected (excluded death and recovered people). Of course, this selection of the scaling function means that we are adding a priori knowledge to the selected time series. Therefore, the scaling function \u03c8 is so that \u03c8 :",
            "cite_spans": [
                {
                    "start": 703,
                    "end": 706,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 707,
                    "end": 710,
                    "text": "41]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [
                {
                    "start": 457,
                    "end": 458,
                    "text": "(",
                    "ref_id": null
                },
                {
                    "start": 884,
                    "end": 891,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Tests for KRN-VSK"
        },
        {
            "text": "To analyze the performances of the variably scaled setting, we take the Gaussian kernel and we compute the condition number of the kernel matrix and the rounded mean error (RME). Let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests for KRN-VSK"
        },
        {
            "text": "be the mean error, where A is a decision function as defined in Section 3.2 obtained via classical or variably scaled kernels. Since hospitalized patients are involved in the dynamics we consider, as accuracy indicator, the RME defined as RME = ME , if ME \u2212 ME \u2264 0.5, ME , if ME \u2212 ME > 0.5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests for KRN-VSK"
        },
        {
            "text": "In the first experiments, we set the parameter \u03bd = 0. We remark that for regression networks the selection of the shape parameter plays a crucial role. Therefore, to make a fair comparison between classical and VSK regression networks, we report the condition numbers and the RME for 200 values of the shape parameter \u03b3 in the interval [0. 5, 20] . The results are reported in Fig. 2 . We observe that the computation carried out via VSKs is characterized by a lower condition number of the kernel matrix, as theoretically observed in Proposition 1. For such experiment, this directly reflects on the accuracy of the computation, meaning that the safe interval for the shape parameter \u03b3 is larger than for the classical method (see Fig. 2 , right).",
            "cite_spans": [
                {
                    "start": 340,
                    "end": 342,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 343,
                    "end": 346,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 377,
                    "end": 383,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 732,
                    "end": 738,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Tests for KRN-VSK"
        },
        {
            "text": "In Fig. 3 , we report two graphical results corresponding to \u03bd = 0 and \u03bd = 1e \u2212 04, left and right respectively. In both cases we take the optimal shape parameter \u03b3 * , meaning that it leads to the smallest RME, in the same framework of Fig. 2 (right) . The associated RME is shown in Table 3 . We note that the VSK setting outperforms the classical method for \u03bd = 0, while for \u03bd = 1e \u2212 04 the two approximations are comparable.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 9,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 237,
                    "end": 251,
                    "text": "Fig. 2 (right)",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 285,
                    "end": 292,
                    "text": "Table 3",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Tests for KRN-VSK"
        },
        {
            "text": "In this section, we propose a feature extraction method directly inspired by the presented variably scaled setting, which can be used as an alternative to other possible expensive feature extraction algorithms. To this aim, we consider the Wisconsin Breast Cancer Database [22, 23] , which consists of 699 instances described by 9 features, extracted from a digitized image of a fine needle aspirate of a breast mass. The task consists in predicting if the mass is benign or malignant. From the original dataset, we exclude 16 instances that present missing values. The two classes are not equally distributed, presenting 444 benign instances and 239 malignant instances.",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 277,
                    "text": "[22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 278,
                    "end": 281,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "A VSK-like feature extraction algorithm"
        },
        {
            "text": "At first, we divide the dataset into a training set, consisting of 226 benign and 116 malignant cases, and a test set, which is composed of 218 benign and 123 malignant cases. Then, taking the hyperparameter grids adopted in Section 6.1, we compare the performances on the test set of the following four methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A VSK-like feature extraction algorithm"
        },
        {
            "text": "1. A NB classifier with Gaussian likelihood. 2. A standard SVM classifier, whose hyperparameters \u03b6 and \u03b3 (in the Gaussian case) are validated by means of 5-fold cross-validation on the training set. 3. A SVM classifier constructed after a feature selection process, as explained in what follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A VSK-like feature extraction algorithm"
        },
        {
            "text": "Analyzing the resulting weights of the SVM classifier (in the linear case), we can rank the features by their influence in the classification; see, e.g., [17] . Then, we choose then more relevant features, here we fixn = 2, and we consequently reduce our training and test sets by restricting to the two most relevant features. Finally, we take both linear and Gaussian kernels, we train a SVM classifier via 5-fold cross-validation on the reduced training set and we evaluate the results on the reduced test set.",
            "cite_spans": [
                {
                    "start": 154,
                    "end": 158,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "A VSK-like feature extraction algorithm"
        },
        {
            "text": "We denote this method with SVM-Selection (SVM-S). 4. A SVM classifier constructed after a VSK-like feature extraction process, as described in the following lines. We randomly selectn \u2212 1 features (heren = 2). The training set restricted to the remaining 8 features is used to train a Gaussian NB classifier. Reduced training and test sets are obtained by juxtaposing the previously selectedn \u2212 1 features to the probabilistic output of the NB classifier. Then, we take both linear and Gaussian kernels, we train a SVM classifier via 5-fold cross-validation on the reduced training set and we evaluate the results on the reduced test set. We denote this method with SVM-Extraction (SVM-E). We point out that both SVM-S and SVM-E consider reduced training and test sets that are characterized by the same number of featuresn. Moreover, the SVM-E presents some advantages in terms of computational complexity with respect to SVM-S, since training an auxiliary NB classifier to perform feature extraction is cheaper than training a SVM classifier to carry out the feature selection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A VSK-like feature extraction algorithm"
        },
        {
            "text": "In Table 4 , we present the results obtained considering the SVM, NB, and SVM-S methods. In Table 5 , we report the results concerning the SVM-E algorithm. For completeness, we vary the randomly selected feature, taking into account all the possibilities.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "text": "Table 4",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 92,
                    "end": 99,
                    "text": "Table 5",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "A VSK-like feature extraction algorithm"
        },
        {
            "text": "We observe that the best score is achieved by the SVM-E algorithm. Moreover for this dataset, we point out that such a method prefers the Gaussian kernel with respect to the linear one, while the standard SVM and SVM-S obtain better classification scores when the linear kernel is considered.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A VSK-like feature extraction algorithm"
        },
        {
            "text": "We investigated the link between VSKs and feature augmentation strategies. In doing so, we tailored the VSKs for SVM and KRNs. The proposed methods turn out to be flexible and easy to implement. For KRNs, the use of VSKs takes advantage of being stable and for classification of merging the probabilistic features of NB and the geometric ones of SVM. This results in effective algorithms that can be used for many tasks. Applications to real datasets show the effectiveness of our approach.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and future work"
        },
        {
            "text": "Work in progress consists in extending this concept for support vector regression and as well as for greedy methods [2, 46] .",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 119,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 120,
                    "end": 123,
                    "text": "46]",
                    "ref_id": "BIBREF45"
                }
            ],
            "ref_spans": [],
            "section": "Conclusions and future work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Data Classification: Algorithms and Applications",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Aggarwal",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Learning with subsampled kernel-based methods: Environmental and financial applications",
            "authors": [
                {
                    "first": "Aminian",
                    "middle": [],
                    "last": "Shahrokhabadi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Neisy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Perracchione",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Polato",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Dolomites Res. Notes Approx",
            "volume": "12",
            "issn": "",
            "pages": "17--27",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Eigenvalues of Euclidean distance matrices",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ball",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "J. Approx. Theory",
            "volume": "68",
            "issn": "",
            "pages": "74--82",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Rademacher and Gaussian complexities: risk bounds and structural results",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "L"
                    ],
                    "last": "Bartlett",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mendelson",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "J. Mach. Learn. Res",
            "volume": "3",
            "issn": "",
            "pages": "463--482",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Matrix Analysis",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bhatia",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Interpolation with variably scaled kernels",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bozzini",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lenarduzzi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rossini",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schaback",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IMA J. Numer. Anal",
            "volume": "35",
            "issn": "",
            "pages": "199--219",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Landslide susceptibility assessment in Vietnam using support vector machines, decision tree and Na\u00efve Bayes models",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "T"
                    ],
                    "last": "Bui",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pradhan",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Lofman",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Revhaug",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Math. Probl. Eng",
            "volume": "",
            "issn": "",
            "pages": "1--26",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Smoothing exponential-polynomial splines for multiexponential decay data",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Campagna",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Conti",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Cuomo",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Dolomites Res. Notes Approx",
            "volume": "12",
            "issn": "",
            "pages": "86--100",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Frustratingly easy domain adaptation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Daum\u00e9",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Association for computational linguistics (ACL)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Shape-Driven Interpolation with Discontinuous Kernels: Error Analysis, Edge Extraction and Applications in MPI",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "De Marchi",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Erb",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Marchetti",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Perracchione",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rossini",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "SIAM J. Sci. Comput",
            "volume": "42",
            "issn": "",
            "pages": "472--491",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Jumping with variably scaled discontinuous kernels (VSDKs)",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "De Marchi",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Marchetti",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Perracchione",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "BIT Num. Math",
            "volume": "60",
            "issn": "",
            "pages": "441--463",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Improved estimates for condition numbers of radial basis function interpolation matrices",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Diederichs",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Iske",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "J. Approx. Theory",
            "volume": "238",
            "issn": "",
            "pages": "38--51",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Learning deep kernels in the space of dot product polynomials",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Donini",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Aiolli",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Mach. Learn",
            "volume": "106",
            "issn": "",
            "pages": "1245--1269",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "The spectrum of kernel random matrices",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "El Karoui",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Ann. Statist",
            "volume": "38",
            "issn": "",
            "pages": "1--50",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Kernel-based Approximation Methods Using Matlab",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Fasshauer",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Mccourt",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "World Scientific",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Kernel PCA for novelty detection",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hoffmann",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Pattern Recogn",
            "volume": "40",
            "issn": "",
            "pages": "863--874",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Bounds on the spectral radius of a Hadamard product of nonnegative or positive semidefinite matrices",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Horn",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Electron J. Linear Algebra",
            "volume": "20",
            "issn": "",
            "pages": "90--94",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Face recognition using kernel principal component analysis",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "I"
                    ],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Jung",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "J"
                    ],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "IEEE Signal Proc. Lett",
            "volume": "9",
            "issn": "",
            "pages": "40--42",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Theoretical and computational aspects of multivariate interpolation with increasingly flat radial basis functions",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Larsson",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Fornberg",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Comput Math. Appl",
            "volume": "49",
            "issn": "",
            "pages": "103--130",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Learning with augmented features for supervised and semisupervised heterogeneous domain adaptation",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "W"
                    ],
                    "last": "Tsang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Pattern. Anal. Mach. Intell",
            "volume": "36",
            "issn": "",
            "pages": "1134--1148",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Wisconsin breast cancer database, UCI machine learning repository",
            "authors": [
                {
                    "first": "O",
                    "middle": [
                        "L"
                    ],
                    "last": "Mangasarian",
                    "suffix": ""
                },
                {
                    "first": "Nick",
                    "middle": [],
                    "last": "Street",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wolberg",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "H"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 1991,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Cancer diagnosis via linear programming",
            "authors": [
                {
                    "first": "O",
                    "middle": [
                        "L"
                    ],
                    "last": "Mangasarian",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "H"
                    ],
                    "last": "Wolberg",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "SIAM News",
            "volume": "106",
            "issn": "",
            "pages": "1--18",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "The extension of Rippa's algorithm beyond LOOCV",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Marchetti",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Appl. Math. Lett",
            "volume": "120",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Automatic indexing: An experimental inquiry",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "E"
                    ],
                    "last": "Maron",
                    "suffix": ""
                }
            ],
            "year": 1961,
            "venue": "J ACM",
            "volume": "8",
            "issn": "",
            "pages": "404--417",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Functions of positive and negative type and their connection with the theory of integral equations",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Mercer",
                    "suffix": ""
                }
            ],
            "year": 1909,
            "venue": "Phil. Trans. Royal Society",
            "volume": "209",
            "issn": "",
            "pages": "415--446",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Cyclic prefixing or zero padding for wireless multicarrier transmissions?",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Muquet",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "B"
                    ],
                    "last": "Giannakis",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "De Courville",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Duhamel",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "IEEE Trans. Commun",
            "volume": "50",
            "issn": "12",
            "pages": "2136--2148",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Norm estimates for the inverses of a general class of scattered-data radialfunction interpolation matrices",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "J"
                    ],
                    "last": "Narcowich",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Ward",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "J. Approx. Theory",
            "volume": "69",
            "issn": "",
            "pages": "84--109",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Numerical Optimization",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Nocedal",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Wright",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Introduction to radial basis function networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "J L"
                    ],
                    "last": "Orr",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Thumbs up? Sentiment Classification Using Machine Learning Techniques",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vaithyanathan",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Proc. of EMNLP",
            "volume": "",
            "issn": "",
            "pages": "79--86",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Scikit-learn: Machine learning in python",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pedregosa",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Varoquaux",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gramfort",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Michel",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Thirion",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Grisel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Blondel",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Prettenhofer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Weiss",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Dubourg",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Vanderplas",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Passos",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cournapeau",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Brucher",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Perrot",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Duchesnay",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J. Mach. Learn. Res",
            "volume": "12",
            "issn": "",
            "pages": "2825--2830",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Naive Bayes classification of uncertain data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "D"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kao",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cheung",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proc. 9th IEEE Int. Conf. Data Mining (ICDM)",
            "volume": "",
            "issn": "",
            "pages": "944--949",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "An algorithm for selecting a good value for the parameter c in radial basis function interpolation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Rippa",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Adv. Comput. Math",
            "volume": "11",
            "issn": "",
            "pages": "193--210",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Edge detection methods based on RBF interpolation",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Romani",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rossini",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Schenone",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "J. Comput. Appl. Math",
            "volume": "349",
            "issn": "",
            "pages": "532--547",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Interpolating functions with gradient discontinuities via variably scaled kernels",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rossini",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Dolom. Res. Notes Approx",
            "volume": "11",
            "issn": "",
            "pages": "3--14",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Error estimates and condition numbers for radial basis function interpolation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schaback",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Adv. Comput. Math",
            "volume": "3",
            "issn": "",
            "pages": "251--264",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Multivariate interpolation and approximation by translates of a basis function",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schaback",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Approximation Theory VIII: Approximation and Interpolation",
            "volume": "",
            "issn": "",
            "pages": "491--514",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Bemerkungen zur Theorie der beschr\u00e4nkten Bilinearformen mit unendlich vielen Ver\u00e4nderlichen",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schur",
                    "suffix": ""
                }
            ],
            "year": 1911,
            "venue": "J. Reine Angew. Math",
            "volume": "140",
            "issn": "",
            "pages": "1--28",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "A simple PSA-based computational approach predicts the timing of cancer relapse in prostatectomized patients",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Stura",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gabriele",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Guiot",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Cancer Res",
            "volume": "76",
            "issn": "",
            "pages": "4941--4947",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Kernel Methods for Pattern Analysis",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shawe-Taylor",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Cristianini",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Hadamard products and multivariate statistical analysis",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "P H"
                    ],
                    "last": "Styan",
                    "suffix": ""
                }
            ],
            "year": 1973,
            "venue": "Linear Algebra Appl",
            "volume": "6",
            "issn": "",
            "pages": "217--240",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "On the uniform convergence of relative frequencies of events to their probabilities",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "N"
                    ],
                    "last": "Vapnik",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "Y"
                    ],
                    "last": "Chervonenkis",
                    "suffix": ""
                }
            ],
            "year": 1971,
            "venue": "Theory Probab. Appl",
            "volume": "16",
            "issn": "",
            "pages": "264--280",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Large scale image annotation: Learning to rank with joint wordimage embeddings",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Usunier",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Mach. Learn",
            "volume": "81",
            "issn": "",
            "pages": "21--35",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "A vectorial kernel orthogonal greedy algorithm",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wirtz",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Haasdonk",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Dolomites Res. Notes Approx",
            "volume": "6",
            "issn": "",
            "pages": "83--100",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Question classification using support vector machines",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "S"
                    ],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "26--32",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "be a continuous (strictly) positive definite kernel. Given a scaling function \u03c8 : \u03a9 \u2212\u2192 \u039b, a variably scaled kernel \u03ba \u03a8 : \u03a9 \u00d7 \u03a9 \u2212\u2192 R is defined as \u03ba \u03a8 (x, y) := \u03ba((x, \u03c8(x)), (y, \u03c8(y))), (2.4) for x, y \u2208 \u03a9.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Let \u039e = {x i , i = 1, . . . , N} \u2286 \u03a9 be a set of data points. Let \u03c8 : \u03a9 \u2212\u2192 \u039b be the scaling function for the VSK setting. Let \u03ba : \u03a9 \u00d7 \u03a9 \u2212\u2192 R be the Gaussian kernel, then the VSK kernel \u03ba \u03a8 : \u03a9 \u00d7 \u03a9 \u2212\u2192 R is more expressive than \u03ba.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Corollary 3.1]. For a given matrix M, we focus on the 2-condition number defined as cond(M) = ||M|| 2 ||M \u22121 || 2 . Proposition 1 Let \u039e = {x i , i = 1, . . . , N} \u2286 \u03a9 be a set of distinct data. Let \u03c8 : \u03a9 \u2212\u2192 \u039b be the scaling function for the VSK setting. Let \u03ba : \u03a9 \u00d7 \u03a9 \u2212\u2192 R be the Gaussian kernel. Given the VSK matrix",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "this view, for the SVM-VSK, we propose the scaling map \u03a8 : \u03a9 \u2212\u2192\u03a9 defined by \u03a8 (x) := (x, P 1 (x)), and the kernel \u03ba \u03a8 : \u03a9 \u00d7 \u03a9 \u2212\u2192 R \u03ba \u03a8 (x, y) := \u03ba (\u03a8 (x), \u03a8 (y)) .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The hyperparameters are validated by taking \u03b6 \u2208 {2 \u22126 , 2 \u22125 , . . . , 2 6 }, \u03b3 \u2208 {10 \u22126 , 10 \u22125 , . . . , 10 2 }.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "The f 1 -score of the experiments performed on various datasets using the linear (lin.) and Gaussian kernel (RBF). The considered number of examples and features are displayed on the top",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "5.1) is carried out considering M : \u03a9 \u00d7 R \u2212\u2192 R given by M(x, \u03b2) = e \u2212\u03b2|x\u2212p| , wherep = 42 is the peak of the considered time series. The model M is constructed on \u03a3.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Left: The condition numbers for different values of the shape parameter of the classical kernel and VSK matrix denoted by red triangles and blue dots, respectively. Right: The RME for different values of the shape parameter of the classical KRN and KRN-VSK methods denoted by red triangles and blue dots, respectively. Both plots are in semi-logarithmic scale and obtained by considering the normalized dataset",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "The ICU patients' curves reconstructed via KRN and KRN-VSK denoted by red triangles and blue dots, respectively. We fix \u03bd = 0 and \u03bd = 1e \u2212 04, left and right respectively. The true solution is plotted with a black solid line",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "14, Lemma A.5] and [18, Lemma 2.1]. Theorem 4 If E and M \u2208 R N\u00d7N are positive definite matrices, denoting by \u03bb min and \u03bb max the smallest and largest eigenvalue of a matrix, we have that",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "The ratios of the norms involved in Proposition 2 obtained via the linear kernel. The matrices K and K \u03a8 are related to the normalized training sets \u03a3 \u03be , varying N q = 100, 500, 2500, and with n k = 2",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "The RME for the optimal shape parameter by using KRN and KRN-VSK in reconstructing the ICU curves",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "The f 1 -score for the Wisconsin Breast Cancer Database via the SVM, NB, and SVM-S methods",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "The f 1 -score for the Wisconsin Breast Cancer Database via the SVM-E method",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements We sincerely thank the reviewers for helping us to significantly improve the manuscript. This research has been accomplished within Rete ITaliana di Approssimazione (RITA) and partially funded by GNCS-IN\u03b4AM, by the European Union's Horizon 2020 research and innovation programme ERA-PLANET, grant agreement no. 689443, via the GEOEssential project and by the ASI -INAF grant \"Artificial Intelligence for the analysis of solar FLARES data (AI-FLARES).\"",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        },
        {
            "text": "Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}
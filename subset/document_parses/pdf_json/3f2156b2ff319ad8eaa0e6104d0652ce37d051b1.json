{
    "paper_id": "3f2156b2ff319ad8eaa0e6104d0652ce37d051b1",
    "metadata": {
        "title": "Deep learning for identification and face, gender, expression recognition under constraints",
        "authors": [
            {
                "first": "Ahmad",
                "middle": [
                    "B"
                ],
                "last": "Hassanat",
                "suffix": "",
                "affiliation": {},
                "email": "hasanat@mutah.edu.jo"
            },
            {
                "first": "Abeer",
                "middle": [],
                "last": "Albustanji",
                "suffix": "",
                "affiliation": {},
                "email": "abeeralbustanji95@gmail.com"
            },
            {
                "first": "Ahmad",
                "middle": [
                    "S"
                ],
                "last": "Tarawneh",
                "suffix": "",
                "affiliation": {},
                "email": "ahmad.trwh@gmail.com"
            },
            {
                "first": "Malek",
                "middle": [],
                "last": "Alrashidi",
                "suffix": "",
                "affiliation": {},
                "email": "mqalrashidi@ut.edu.sa"
            },
            {
                "first": "Hani",
                "middle": [],
                "last": "Alharbi",
                "suffix": "",
                "affiliation": {},
                "email": "hani.almoamari@iu.edu.sa"
            },
            {
                "first": "Mohammed",
                "middle": [],
                "last": "Alanazi",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Mansoor",
                "middle": [],
                "last": "Alghamdi",
                "suffix": "",
                "affiliation": {},
                "email": "malghamdi@ut.edu.sa"
            },
            {
                "first": "Ibrahim",
                "middle": [
                    "S"
                ],
                "last": "Alkhazi",
                "suffix": "",
                "affiliation": {},
                "email": "i.alkhazi@ut.edu.sa"
            },
            {
                "first": "V",
                "middle": [
                    "B"
                ],
                "last": "Surya Prasath",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Biometric recognition based on the full face is an extensive research area. However, using only partially visible faces, such as in the case of veiledpersons, is a challenging task. Deep convolutional neural network (CNN) is used in this work to extract the features from veiled-person face images. We found that the sixth and the seventh fully connected layers, FC6 and FC7 respectively, in the structure of the VGG19 network provide robust features with each of these two layers containing 4096 features. The main objective of this work is to test the ability of deep learning based automated computer system to identify not only persons, but also to perform recognition of gender, age, and facial expressions such as eye smile. Our experimental results indicate that we obtain high accuracy for all the tasks. The best recorded accuracy values are up to 99.95% for identifying persons, 99.9% for gender recognition, 99.9% for age recognition and 80.9% for facial expression (eye smile) recognition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "A wide variety of systems require reliable solid individual acknowledgment plans to either confirm or determine the identity of an individual requesting their services. The purpose of such schemes is to guarantee that the rendered services are accessed only by a legitimate user and no one else [Jain et al., 2004] . By using biometrics, it is possible to establish an individual's identity or confirm based on \"who she/he is\", rather than \"what she/he has\" (e.g., a token, key) or \"remembers\" (e.g., PIN) [Delac and Grgic, 2004 , Hassanat et al., 2017a , Hassanat, 2018 , Al-Shamaileh et al., 2019 . Systems based on what users remember and not based on what they own such as passwords are easy to penetrate by guessing or by a brute force dictionary attack and they are easy to be forgotten, unlike those that use biometrics that are hard to be lost or forgotten. Biometrics provide convenient authentications (as users are no longer required to remember complex passwords) while preserving a sufficiently high degree of security [Jain et al., 2004] .",
            "cite_spans": [
                {
                    "start": 295,
                    "end": 314,
                    "text": "[Jain et al., 2004]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 506,
                    "end": 528,
                    "text": "[Delac and Grgic, 2004",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 529,
                    "end": 553,
                    "text": ", Hassanat et al., 2017a",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 554,
                    "end": 570,
                    "text": ", Hassanat, 2018",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 571,
                    "end": 598,
                    "text": ", Al-Shamaileh et al., 2019",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1032,
                    "end": 1051,
                    "text": "[Jain et al., 2004]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The use of biometrics is the statute of verifying a person's identity by imaging or measuring unique characteristics of that person. Typical phases of biometric include the acquisition of the data, so called enrollment, the feature extraction (of a template based on the data), the comparison and the storage. The biometric characteristic can be physiological such as a face, iris texture of the iris, fingerprints, hand geometry, DNA and fingerprints, it can behavioral, such as handwriting, keystroke, and way of walking [Delbaere et al., 2014 , Hassanat, 2011 . Face recognition plays a major role in biometrics, the face is a global feature of human beings, and the method of acquiring face images is non-intrusive and can be captured at a distance [Zhao et al., 2003] . Face recognition keeps being an active topic in computer vision research [Ahonen et al., 2004] . There are several biometric systems but among the six known biometric attributes deemed by [Heitmeyer, 2000] in a Machine Readable Travel Documents (MRTD) system facial features record the highest compatibility, such as security system, enrollment, machine requirements, renewal and public perception.",
            "cite_spans": [
                {
                    "start": 523,
                    "end": 545,
                    "text": "[Delbaere et al., 2014",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 546,
                    "end": 562,
                    "text": ", Hassanat, 2011",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 753,
                    "end": 772,
                    "text": "[Zhao et al., 2003]",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 848,
                    "end": 869,
                    "text": "[Ahonen et al., 2004]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 963,
                    "end": 980,
                    "text": "[Heitmeyer, 2000]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Face recognition is a function that human achieves effectively, even under unfavorable conditions like facial changes due to aging or poor lighting. This simple function of our brains has become a real challenge in the recent computer vision. Face recognition is of a functional importance in numerous applications, such as authentication in security systems such as bank machines or computers. Several methods to solve the problem of facial recognition have been studied. The greater part of them has concentrated on frontal face pictures, profile pictures, or geometric areas, (for example, of eyes, noses, and mouths) [Sato et al., 1998 ,Moreno et al., 2016 . Face recognition when the full face is visible is an extensive research area with accurate results. However, using only parts of faces (as in veiled-persons) is a challenging task. Partial face recognition (PFR) has turned into a rising issue with expanding requirements of identification from CCTV cameras and vision systems in cell phones, robots, and Smart home accessories. It is therefore important to recognize the arbitrary facial patch or face sample covered to enhance the intelligence of these systems [Moreno et al., 2013 ,Hu et al., 2013 . Despite the significance of the human face for social communication and person identification, some people in the population insist on veiling their faces for certain cultural and other purposes, such as soldiers on the battlefield, contagious disease (such as Coronavirus Disease -COVID19), and muslim women who get dressed Niqab (cloth cover the face except the eye area). Thus, there is a need for computer applications that can identify a veiled-face for many reasons for example: facilitating the identification process of muslim women, who get dressed Niqab, to identify them without asking them to reveal the whole face, in cases where some men wear Niqab for the purpose of hiding identity when doing criminal activities, to identify veiled-terrorists and other security reasons.",
            "cite_spans": [
                {
                    "start": 621,
                    "end": 639,
                    "text": "[Sato et al., 1998",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 640,
                    "end": 660,
                    "text": ",Moreno et al., 2016",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 1175,
                    "end": 1195,
                    "text": "[Moreno et al., 2013",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 1196,
                    "end": 1212,
                    "text": ",Hu et al., 2013",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Automatic gender recognition is now linked to the extent to which it is used in many programs and devices, especially because of the growth of social networking sites on the Internet and social media [Dhomne et al., 2018] . Gender Recognition was begun with the problem in psychophysical studies to gender classification of the human face; it focuses on the types of perception in perceiving human visual processing and recognizing related features that can be used to distinguish between female and male individuals. Some investigations have demonstrated that the inconsistency between a female face and male face can be utilized viably to improvise the result of face recognition software in biometrics devices [Dhomne et al., 2018] . Gender classification has always been an active area of research in the field of computer vision and artificial intelligence. The methodologies used to classify gender are extensively walk-based, body-based and face-based. The vast majority of the researcher's center on gender classification using faces images. But advances in computer vision and machine learning made it possible to do the same using whole body images and the images containing only partial information. Some of the important problems that researchers are still facing are facial differences such as blockage, expression changes, lighting changes, and the high-dimensional feature space [Liew et al., 2016] . The traditional approach utilized in face-based gender recognition usually involves Multiple stages starting from image acquisition and processing and then reducing the dimensions and then extracting the features and finally classifying them. In addition to the need for prior knowledge in the field of application in order to determine the best advantage extracted. The performance of the recognition system is very dependent on the type of classifier chosen, which is in turn dependent on the feature extraction method used. It is hard to have a classifier that combines best with the selected feature extractor such that the best classification performance is accomplished. And any changes in the problem domain require redesigning the entire system [Liew et al., 2016] .",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 221,
                    "text": "[Dhomne et al., 2018]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 713,
                    "end": 734,
                    "text": "[Dhomne et al., 2018]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1394,
                    "end": 1413,
                    "text": "[Liew et al., 2016]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 2169,
                    "end": 2188,
                    "text": "[Liew et al., 2016]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Gender Recognition"
        },
        {
            "text": "In recent years, many applications have been developed using face recognition, such as identification systems and security systems. But many researchers have proposed ageclassification systems based on facial images. For more than 20 years, gender classification has been one of the most research topics in this area [Ueki et al., 2006] . The classification of age using the face is one of the tasks in the vision of human and computer, which is the basis for many applications such as forensics or social media [Rothe et al., 2018] . Human age is one of the most important personal traits which can be deduced directly through different patterns appear on the face that has recently been used in biometrics, cosmetology and security control, but it is still a difficult and interesting field. It is known that there are some general changes when people get older in age from childhood to adulthood such as the size of the face and the shape of the eye, nose, mouth, eyebrows, and lips [Padme and Desai, 2015] . Age and gender play key roles in social interactions, but they still face some challenges, notably extreme blur (low-resolution), expressions, occlusions, etc [Levi and Hassner, 2015] .",
            "cite_spans": [
                {
                    "start": 317,
                    "end": 336,
                    "text": "[Ueki et al., 2006]",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 512,
                    "end": 532,
                    "text": "[Rothe et al., 2018]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 986,
                    "end": 1009,
                    "text": "[Padme and Desai, 2015]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 1171,
                    "end": 1195,
                    "text": "[Levi and Hassner, 2015]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Age Recognition"
        },
        {
            "text": "Facial behavior is one of the most important signs of sensing the human emotions and intention of people. Based on recent developments in human-centered computing, the automated system for accurate and reliable facial expressions has emerging applications such as remote education, interactive and entertainment games, intelligent transportation systems, etc. [Liu et al., 2014] . Facial expression recognition (FER) has remained a challenging and interesting problem [Mollahosseini et al., 2016] . Thus deep neural networks [LeCun et al., 2015] have increasingly been supported to learn representations of discrimination for FER [Li and Deng, 2020] . To the best of our knowledge, part of the face was not used to distinguish facial expressions using traditional or modern methods such as deep learning [Hassanat et al., 2017a ,Hassanat, 2018 ,Al-Shamaileh et al., 2019 ,Tarawneh et al., 019b, Tarawneh et al., 2018 .",
            "cite_spans": [
                {
                    "start": 360,
                    "end": 378,
                    "text": "[Liu et al., 2014]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 468,
                    "end": 496,
                    "text": "[Mollahosseini et al., 2016]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 525,
                    "end": 545,
                    "text": "[LeCun et al., 2015]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 630,
                    "end": 649,
                    "text": "[Li and Deng, 2020]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 804,
                    "end": 827,
                    "text": "[Hassanat et al., 2017a",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 828,
                    "end": 843,
                    "text": ",Hassanat, 2018",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 844,
                    "end": 870,
                    "text": ",Al-Shamaileh et al., 2019",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 871,
                    "end": 916,
                    "text": ",Tarawneh et al., 019b, Tarawneh et al., 2018",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Facial Expression Recognition"
        },
        {
            "text": "During the collection of veiled-persons image database (VPI), we had trouble taking pictures of veiled-persons for personal reasons, especially males, many of whom refused to wear veils. However, we were able to collect pictures of 150 people of both genders with 14 pictures each, under different circumstances such as lighting, with a view to realism during taking pictures. We used this new dataset for testing facial expression recognition which is more challenging than a straightforward full frontal face views considered by other methods of the past.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Facial Expression Recognition"
        },
        {
            "text": "Face recognition plays a major role in biometrics, however some people in the population prefer to veiling their faces for cultural and other purposes such as soldiers on the battlefield, contagious disease -Hepatitis and recent Coronavirus -COVID19 pandemic mask mandates [Aseeri et al., 2020] , and muslim women who get dressed in Niqab. Thus, there is a need for automatic computer assisted applications that can identify a veiled-face using machine learning models:",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 294,
                    "text": "[Aseeri et al., 2020]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "\u2022 Facilitating the identification process of muslim women who get dressed Niqab, to identify them without asking them to reveal the whole face.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "\u2022 In cases where persons attempt to hide their identity for potential criminal activities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "\u2022 To identify veiled-terrorists.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "\u2022 Other security reasons.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "In addition, our work has several contributions that tackle challenging face, gender, expression identification problems, and can be summarized as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "\u2022 Creating a new veiled-persons image (VPI) database, containing veiled-face images of 150 persons (male and female) from different age groups in the range of 8 to 78 years. The total number of images is 2100.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "\u2022 Besides identifying a veiled-person, several other application are tested using the new dataset including veiled-person gender recognition, veiled-person age recognition, and veiled-facial expression recognition just from the revealed part of the face.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "\u2022 Unlike the traditional methods which used hand crafted-based features extraction methods to solve the aforementioned problems, we used a popular deep pretrained architecture, VGG, to extract high-level features from different layers of the VGG model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "\u2022 The performance of the different systems has been tested and compared using several machine learning algorithms, namely, k-nearest neighbors (kNN), random forest (RF), Na\u00efve Bayes (NB), BayesNet (BN), and artificial neural network (ANN).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "\u2022 Each of the proposed systems is tested under different dimensionality reduction parentage (99%, 97%, and 95%) using the principle component analysis algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Contributions"
        },
        {
            "text": "The reminder of the work is organized as follows. In Section 2, we review the most recent works in the field of deep learning for face recognition, including from partially covered face images. This includes a detailed analysis of how each method works and what results are achieved with a description of the data sets used in each method. In Section 3, we propose a new method for veiled-face classification and recognition of gender, age and the facial expression. We detail how the new method works, its advantages and disadvantages, and how the accuracy of the veiled-face classification using deep learning model can be increased. The specific database, veiled-persons image (VPI), that we have created for the purpose of this research work is described in detail, including the number of images, people, naming system, and capturing camera specifications. Detailed experimental results are given in Section 4 base on the method studied here including comparisons with related models. Finally, conclusion and future works are presented in Section 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Work Structure"
        },
        {
            "text": "Face recognition has become one of the most researched topics in computer vision and biometrics since the 1970s. More recently, traditional face recognition methods have been superseded by deep learning methods based on CNNs [Trigueros et al., 2018] . Facial recognition has been developed significantly through the emergence of deep learning. Deep neural networks have recently achieved major success in object recognition because of their remarkable learning potential. Therefore, this has been an incentive in verifying its effectiveness in facial recognition, which many researchers have done [Sun et al., 2015] . Though face recognition systems have broad applications, they are vulnerable to attack. Thus, the presentation attack detection (PAD) method is required to improve the security of face recognition systems. To beat the restrictions of previously suggested PAD methods [Nguyen et al., 2018] have proposed a new PAD method that used hybrid features of handcrafted and deep features extracted from the images by the visible-light camera sensor. Their proposed method uses the multilevel local binary pattern (MLBP) to extract skin features from facial images, and CNN to extract deep image features to distinguish between real and offensive facial images, which have stronger discrimination ability than the single image features. They also used the support vector machines (SVM) in order to classify the features of the image to the real and attack classes, the classification error was significantly reduced to 0.456% on a CASIA database. proposed a model combining the fully convolutional network (FCN), with Sparse Representation Classification (SRC) to propose a new approach to partial face recognition, called Dynamic Feature Matching (DFM), to treat partial facial images regardless of size and without needed to align the face. Their proposed DFM method has achieved remarkable accuracy with high efficiency on various partial face databases, including LFW, YTF, and CASIA-NIR-Distance databases in comparison with state-of-the-art Partial Face Recognition (PFR) methods. [Farfade et al., 2015] have proposed a method based on deep learning, called Deep Dense Face Detector (DDFD), which is able to detect faces in a wide range of orientations using a single model. have proposed a new method for face detection using deep learning to improve the state-of-the-art faster RCNN framework for generic object detection, by combining a number of strategies, including feature concatenation, hard negative mining, multi-scale training, model pre-training, and proper calibration of key parameters. Their proposed scheme outperformed some of the state-of-the-art face detection methods compared, making it the best model in terms of ROC curves comparing to the other methods tested on the FDDB benchmark. In addition, [Sun et al., 2014] have proposed a method to learn a set of high-level feature representations through deep learning, referred to as Deep hidden IDentity features (DeepID), for face verification. The features are built on top of the feature extraction hierarchy of deep CNN and are summarized from multiscale mid-level features. By representing a large number of different identities with a small number of hidden variables, highly compact and discriminative features are acquired. And therefore, achieved 97.45% face verification accuracy.",
            "cite_spans": [
                {
                    "start": 225,
                    "end": 249,
                    "text": "[Trigueros et al., 2018]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 597,
                    "end": 615,
                    "text": "[Sun et al., 2015]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 885,
                    "end": 906,
                    "text": "[Nguyen et al., 2018]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 2095,
                    "end": 2117,
                    "text": "[Farfade et al., 2015]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 2834,
                    "end": 2852,
                    "text": "[Sun et al., 2014]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [],
            "section": "Literature Review"
        },
        {
            "text": "In a related work, [Sun et al., 2015] proposed two deep neural networks architectures, where they reused the basic elements of GoogLeNet and VGG net for face recognition, the resulting network DeepID3 was rebuilt from inception layers and stacked convolution proposed in GoogLeNet and VGG net to make it appropriate for face recognition. Supervisory signs for identity verification were added to each of the intermediate and final feature extraction layers throughout the training. Their method achieved 99.53% facial verification accuracy on the faces scanned in life wild database (LFW). [Hassanat et al., 2017b] proposed a method to verify the ability of a computer system to identify a veiled-person from the revealed part of the face alone, employing some simple geometric and texture features, a new veiled-person image database (VPI) was created for the purpose of their study using a mobile phone camera. The images were taken for 100 different individuals (both male and female) from different age groups in the range of 13 to 50 years, over two sessions, The total number of images is 1200, this method achieved 88.63% to 97.22% person identification accuracy. In their proposed system, two methods were implemented to extract distinct features from VPI, geometric features; such as edges, and texture features; such as mean, variance, skewness. Before extracting the features, the captured images were resized to 25% of the original size with the aim of speeding the process.",
            "cite_spans": [
                {
                    "start": 19,
                    "end": 37,
                    "text": "[Sun et al., 2015]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 590,
                    "end": 614,
                    "text": "[Hassanat et al., 2017b]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Literature Review"
        },
        {
            "text": "[ Hu et al., 2013] have introduced a new method to identify persons from their partial face images using the distance of an instance-to-class, which is based on representation of the local features. Their experiments were conducted on two sets of widely used data: LFW dataset and AR dataset. The method achieved up to 98% accuracy. [Mahbub et al., 2016] proposed a face detection method based on part of the face. This method was designed to detect the partially cut and covered faces that were captured using a mobile phone camera for continuous authentication. The performance of the face detector was evaluated on the Active Authentication dataset (AA-01), achieving high accuracy compared to some other methods. [Teo et al., 2007] used parts of frontal face images such as eye, nose, and mouth for personal authentication, the frontal human eye images were generated from Essex dataset with 153 subjects, the partial face images were tested with non-negative matrix factorization (NMF), local NMF (LNMF) and spatially confined NMF (SFNMF), Their experimental results showed that the LNMF performed better achieving 95.12% recognition rate. [Akbar et al., 2015] proposed an arithmetical model for face recognition, where he investigated several feature extraction methods such as discrete wavelet transform (DWT), discrete sine transform (DST), local binary patterns (LBP) and local phase quantization (LPQ), with several classifiers such as SVM, probabilistic neural network (PNN) and k-nearest neighbors (kNN). The hybrid feature vector of DWT and DST achieved the best performance of 92.1% accuracy using SVM. [Ranjan et al., 2017 ] proposed a multi-task deep learning method called HyperFace for simultaneously detecting faces, localizing landmarks, estimating head pose and identifying gender using deep convolution neural networks (DCNNs). Their proposed method fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features, this allowed the method to capture both global and local features of the targeted face image, which achieved high results. [Chen et al., 2018] proposed a face detection model named adversarial occlusion-aware face detector (AOFD) to address the issue of face occlusions. Their proposed model is able to find faces with a few exposed face features with extremely high confidence and maintain high accuracy for even detectable faces. A deep adversarial network was used in their proposed model to generate face samples with occlusions from mask generator. AOFD has achieved superior performance for face detection and masked face detection up to 97.88% by training on benchmark dataset for general face detection such as FDDB.",
            "cite_spans": [
                {
                    "start": 2,
                    "end": 18,
                    "text": "Hu et al., 2013]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 333,
                    "end": 354,
                    "text": "[Mahbub et al., 2016]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 717,
                    "end": 735,
                    "text": "[Teo et al., 2007]",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 1145,
                    "end": 1165,
                    "text": "[Akbar et al., 2015]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1617,
                    "end": 1637,
                    "text": "[Ranjan et al., 2017",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 2141,
                    "end": 2160,
                    "text": "[Chen et al., 2018]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Literature Review"
        },
        {
            "text": "In [Huang et al., 2020] proposed a new loss function, which is called Adaptive Curriculum Learning loss (CurricularFace). To help prioritizing the training of the hard examples, the proposed loss function claims to offer better training strategy for face recognition, as shown by the experiments conducted on common datasets.",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 23,
                    "text": "[Huang et al., 2020]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Literature Review"
        },
        {
            "text": "In addition, [Shi et al., 2020] proposed a face recognition framework called URFace. The proposed framework claimed to be able to recognise wide variety of faces under reallife conditions, including low resolution, occlusion and head pose. The proposed method rely on a new augmentation method, which works by using multiple sub-embeddings in order to make the training process smoother. The proposed framework recorded a state-ofthe-art results on challenging datasets. More information can be found in a recent survey of occluded and unoccluded Face Recognition [Xu, 2021] .",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 31,
                    "text": "[Shi et al., 2020]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 564,
                    "end": 574,
                    "text": "[Xu, 2021]",
                    "ref_id": "BIBREF63"
                }
            ],
            "ref_spans": [],
            "section": "Literature Review"
        },
        {
            "text": "DCNN has become a generic approach to gender recognition achieving many successes [Dhomne et al., 2018] were the first to use the VGGNet to predict gender with celebrity face image database. One of the methods used in this work was the transfer learning, where a pre-trained DCNN achieved improved the performance up to 7% and 4.5% compared to the other gender recognition methods compared. In an earlier work [Tivive and Bouzerdoum, 2006] were the first to use the CNN to develop a system for automatic gender recognition, which can detect faces in arbitrary size images, and then recognize gender. This system was tested on two different image databases, achieving 97.2% recognition rate on the FERET database, and 85.7% recognition rate on images collected from the Web and the BioID face image database. [Liew et al., 2016] proposed an optimized CNN architecture for real-time gender classification based on facial images. The number of processing layers was reduced in CNN to only four by fusing the convolutional and sub sampling layers, cross-correlation was applied in the processing layers instead of convolution. The performance of their proposed CNN model was evaluated on two publicly available face databases, achieving 98.75% recognition rate on SUMS, and 99.38% on AT&T. Hassanat et al., 2017b] presented a model to measure the ability of the computer system to gender classification a veiled-person from the revealed part of the face alone depending on the Geometric features. Their proposed method achieved a success rate of 99.41% for gender classification. [Ueki et al., 2006] proposed a two-phased approach based on two-dimensional linear discriminant analysis (2DLDA) and LDA (2DLDA+LDA) to classify the age group using facial images under different lighting conditions. WIT-DB was created using images from about 5500 different Japanese subjects (about 2500 females and about 3000 males) with 1 to 14 images of each subject. Most facial expressions in these images are normal except for smiles in some images. Their method achieved accuracy rates of 46.3% for age groups within 5 years, 67.8% for age groups within 10 years, and 78.1% for age groups within 15 years. [Levi and Hassner, 2015] have proposed a simple convolutional network architecture, reducing the number of parameters and opportunities for processing and used when the amount of learning data is limited to estimate age and gender. Their proposed network was evaluated on the newly released audience face images, the evaluation results showed high age recognition rate compared to some other methods. The recent work of [Rothe et al., 2018] used the CNNs of the VGG16 structure that was previously trained on ImageNet for image classification, their proposed form was named the Deep EXpectation (DEX), which relies on strong facial alignment to estimate real and apparent age. Their proposed method was evaluated on the IMDB-WIKI database, the largest public dataset to date with age and gender annotations, showing state-of-the-art results. [Gonz\u00e1lez-Briones et al., 2018] proposed a hybrid structure that included CNN, which aimed to extract features from input images; and extreme learning machine (ELM). The hybrid method is employed for gender and age classification. The proposed method was evaluated on two popular face databases; namely, MORPHII and Adience benchmarks to classify inputs into four age groups, Children, Youth, Adults and Elderly, the best success rate achieved when using Fisher-faces with Gabor filter.",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 103,
                    "text": "[Dhomne et al., 2018]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 410,
                    "end": 439,
                    "text": "[Tivive and Bouzerdoum, 2006]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 808,
                    "end": 827,
                    "text": "[Liew et al., 2016]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 1286,
                    "end": 1309,
                    "text": "Hassanat et al., 2017b]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1576,
                    "end": 1595,
                    "text": "[Ueki et al., 2006]",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 2189,
                    "end": 2213,
                    "text": "[Levi and Hassner, 2015]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 2609,
                    "end": 2629,
                    "text": "[Rothe et al., 2018]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 3031,
                    "end": 3062,
                    "text": "[Gonz\u00e1lez-Briones et al., 2018]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Gender Recognition"
        },
        {
            "text": "Facial expressions are not used only to express our feelings, but also to provide important communication signals during social interaction, such as our level of interest. It is reported that the facial expressions have a significant effect on the listener; about 55 percent of the impact of spoken words depends on facial expressions and eye movements of the speaker [Ghosh and Bandyopadhyay, 2015] . There are many obstacles make facial expressions difficult to be recognized, such as occlusion, various head poses, low image resolution, and lighting conditions [Luo et al., 2016] . Having been a challenging task, facial expression recognition has attracted many researchers to work on the entire face, however, a few have worked on part of the face, since partial facial expression recognition is more challenging task.",
            "cite_spans": [
                {
                    "start": 368,
                    "end": 399,
                    "text": "[Ghosh and Bandyopadhyay, 2015]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 564,
                    "end": 582,
                    "text": "[Luo et al., 2016]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Facial Expression Recognition"
        },
        {
            "text": "[ Mollahosseini et al., 2016] have worked on the whole face; proposing a new deep neural network architecture to automatically identify facial expressions. Their proposed network consists of two convolutional layers, each followed by max pooling and then four Inception layers. The network is a single-component structure that takes the facial images as an input and classifies them to any of the six basic expressions (anger, disgust, fear, happiness, sadness, and surprise). Their proposed method was tested on seven facial expression databases, namely, DISFA, FERA, SFEW, FER2013, MultiPIE, MMI, and CK. The results of the proposed architecture were better than that of the traditional convolutional neural networks in terms of accuracy and training time. [Yu and Zhang, 2015] proposed a DCNN based facial expression recognition method. The proposed method includes a face detection module assembled from three state-of-the-art face detectors, followed by a classification module assembled from multiple DCNN. Their proposed method was tested on the FER and SFEW data sets. The highest accuracy achieved was 61.29% on the SFEW dataset. [Kotsia et al., 2008] have worked on partial facial expression recognition, attempting to find which part of the face (lower, upper, left or right) provides more discriminant information about each facial expression. They found that the lower part of the face (mouth region) provide more information about anger, fear, happiness and sadness, while information about disgust and surprise are well preserved by the upper part of the face (eyes region). In this work, where veiled-faces are used, all parts of the face are occluded, except for the eyes region, from which we attempt to recognize smiles, age and gender, in addition to identifying the person. This makes our work unique, more challenging and important. To the best of our knowledge, this is the first paper to work on veiled-faces solving the four aforementioned problems altogether in one paper, one exception is our previous work [Hassanat et al., 2017b] . However, our previous work was based on a smaller veiled-faces database, which does not contain age and smile information in addition to using the traditional hand-crafted features. While this work is based on the use of the deep features and a larger and wealthier veiled-faces database. Table 1 shows how the proposed work is compares from the existing works in terms of various recognition tasks.",
            "cite_spans": [
                {
                    "start": 2,
                    "end": 29,
                    "text": "Mollahosseini et al., 2016]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 759,
                    "end": 779,
                    "text": "[Yu and Zhang, 2015]",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 1139,
                    "end": 1160,
                    "text": "[Kotsia et al., 2008]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 2034,
                    "end": 2058,
                    "text": "[Hassanat et al., 2017b]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [
                {
                    "start": 2350,
                    "end": 2357,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Facial Expression Recognition"
        },
        {
            "text": "The main objective of this work is to test the ability of an automated computer system to identify persons, their genders, their ages, and their facial expression, all from veiled-face ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Veiled-Persons Identification (VPI-New) Database"
        },
        {
            "text": "Sample images from VPI-New database with the same subject in each row, the colors of the veils were black and white, and the total number of images is 2100 images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Veiled-Persons Identification (VPI-New) Database"
        },
        {
            "text": "images, employing one of the most common deep learning methods. Most of the standard database of face recognition consists of the whole face images, front or face profile, except for the VPI-Old database, which is created by [Hassanat et al., 2017b] . However, we found some limitations in this database, such as the relatively small number of images, which allows for ineffective training using deep learning, moreover, the images of the VPI-Old database does not contain facial expressions, such as eye-smile. Therefore, and for the purposes of this study, we have created a new database to satisfy the main objective of this paper. VPI-New is a new database created to identify persons, classify gender, age, and facial expressions (eye-smile) from veiled-face images. The images were taken while the subject was sitting on a chair, the camera was too close to the subject, the camera was moving a bit after each shot so that the images were taken from more than one angle and more than a degree of widening the eye. What follows shows the design guidelines we followed while creating the VPI-New database:",
            "cite_spans": [
                {
                    "start": 225,
                    "end": 249,
                    "text": "[Hassanat et al., 2017b]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Veiled-Persons Identification (VPI-New) Database"
        },
        {
            "text": "-A HUAWEI P9 Lite mobile phone camera (13-megapixel, 3120 \u00d7 4160) was used to capture the veiled-face images of 150 subjects (41 male and 109 female) from different age groups in the range of 8 to 78 years. The images were taken in two different sessions (sessions 1 and 2) with seven pictures per session for the same subject. In each session, two images were taken in smiling mode and five images were taken in the normal mode. The total number of images of VPI-New database is 2100.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Veiled-Persons Identification (VPI-New) Database"
        },
        {
            "text": "-This VPI-New is designed primarily for the purpose of this study; namely, identifying persons, distinguishing between male and female, age recognition, and facial expression; namely, eye-smile.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Veiled-Persons Identification (VPI-New) Database"
        },
        {
            "text": "-The Images were taken in different offices and under uncontrolled lighting conditions (office environment).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Veiled-Persons Identification (VPI-New) Database"
        },
        {
            "text": "-Distances when taking pictures are not restricted, the distance between the camera and the veiled-person may vary in the range (from 30 to 50 cm).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Veiled-Persons Identification (VPI-New) Database"
        },
        {
            "text": "-All images were resized to 224 \u00d7 224 to fit the input layer in the VGG19 architecture before extracting features.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Veiled-Persons Identification (VPI-New) Database"
        },
        {
            "text": "The file name starts with the session number (session 1 -S1 or session -S2), then (P) followed by the subject's number, then the subject gender (male -M or female -F), which is followed by the their age (in years), then the image number within each session ranging from 1 to 7, and finally facial expressions (Normal -N or Smile -S). Table 2 provides some examples of the naming system of the image files of our VPI-New database. Figure 1 shows a sample of images from our VPI-New database. Note that each row shown in the figure is dedicated to a specific subject. These images were captured in different sessions and with both facial expressions (normal and eye-smile), under different conditions such as the difference in light, the widening of the eyes, the varied areas of the exposed part of the face, and type of Niqab used.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 334,
                    "end": 341,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 430,
                    "end": 438,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Veiled-Persons Identification (VPI-New) Database"
        },
        {
            "text": "Conversion of the input image into a set of features is called feature extraction. In computer vision, the feature extraction process is a special form of dimensionality reduction, so that is the input data will be converted into a reduced representation set of features (called feature vector). The main goal of feature extraction is to obtain the most important information from the raw data and represent that information in a lower dimensionality space [Kumar and Bhatia, 2014] . In order to extract features, we used Pre-trained VGG-19 (a typical architecture is presented in Figure 2 ), which was pre-trained on the ImageNet database [Deng et al., 2009] . The obtained deep features were classified by some WEKA classifiers [Witten et al., 2011] .",
            "cite_spans": [
                {
                    "start": 457,
                    "end": 481,
                    "text": "[Kumar and Bhatia, 2014]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 640,
                    "end": 659,
                    "text": "[Deng et al., 2009]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 730,
                    "end": 751,
                    "text": "[Witten et al., 2011]",
                    "ref_id": "BIBREF62"
                }
            ],
            "ref_spans": [
                {
                    "start": 581,
                    "end": 589,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Proposed Approach"
        },
        {
            "text": "It is worth noting the following characters of the extraction and classification tasks considered in this paper:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Approach"
        },
        {
            "text": "\u2022 For the purpose of extracting features and classifying them to identify persons from their veiled-face images, the database is divided into 150 folders, each contains 14 images of a different subject. Figure 3 The flow diagram of the proposed framework \u2022 In the case of age classification, the subjects were categorized into four categories -Children, Youth, Adults, and Elderly. Hence, the database is divided into two folders, each contains the images of a certain age category, similar to [Gonz\u00e1lez-Briones et al., 2018] , however they are slightly different in the number of subjects belonging to the last two categories, this is due to the lack of elderly people who volunteered to give their images. The number of subjects of the first class (Children which represents the age groups below 18 years) is 36, the number of subjects of the second class (Youth which represents the age groups of 19 to 30 years) is 75, the number of subjects of the third class (Adults which represents the age groups of 31 to 50 years) is 33, and the number of subjects of the fourth class (Elderly which represents the age groups of 51 to 77 years) is 6.",
            "cite_spans": [
                {
                    "start": 494,
                    "end": 525,
                    "text": "[Gonz\u00e1lez-Briones et al., 2018]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 203,
                    "end": 211,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "Proposed Approach"
        },
        {
            "text": "\u2022 In the case of the classification of facial expressions, the files were divided into two files -Eye-Smile, and Normal. The number of images of the first class (Normal) is 1500 images and the number of images of the second class (Eye-Smile) is 600 images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Approach"
        },
        {
            "text": "In order to obtain deep features, we used the fully connected layer (FC6) and fully connected layer (FC7) of the VGG19 model. Algorithm 1 illustrates the steps of extracting these deep features to identify persons, and recognize gender, age and facial expression. Figure 3 shows a visual diagram of the proposed framework. VGGNet is one of the most preferred choices for extracting features from images [Simonyan and Zisserman, 2014] . Typically, VGGNet consists of 16 layers or 19 layers. Since VGG19 is deeper, we have used it to extract our features after resizing the images to 224 \u00d7 224 to fit the input layer.",
            "cite_spans": [
                {
                    "start": 403,
                    "end": 433,
                    "text": "[Simonyan and Zisserman, 2014]",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [
                {
                    "start": 264,
                    "end": 272,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "Proposed Approach"
        },
        {
            "text": "The output of Algorithm 1 is two feature vector for each image, obtained from each layer (FC6 and FC7), each contains 4096 features. The resultant feature vectors of all the database is used for training and testing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Approach"
        },
        {
            "text": "Due to the large size of the extracted features (2100 \u00d7 4097), it is recommended to use some kind of dimensionality reduction [Hassanat et al., 018b, Tarawneh et al., 2019] . Here, we used the Principal Component Analysis (PCA) for dimensionality reduction with three different percentages of data variance, namely 99%, 97% and 95%. Since we expect that the number of the resultant principal components will be reduced as the percentage of the data variance is decreased, however keeping less data might significantly affects the classification results. The resultant PCA features are used for the four classification tasks, Since there is no a priori knowledge about which layer (FC6 or FC7) being the most representative, we propose merging both layers. For merging we investigated 3 different methods: the arithmetic mean, minimum and maximum of the two features vectors as shown in Algorithm 2. Here, the inputs are the feature vectors (FC6 and FC7) and the outputs are 3 merged features vectors based on the following equations:",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 172,
                    "text": "[Hassanat et al., 018b, Tarawneh et al., 2019]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Proposed Approach"
        },
        {
            "text": "The dimensions of the resulting feature vector FFV remains the same after merging both of (FC6 and FC7), therefore, we applied the PCA For dimensionality of each of the three merged vectors. Table 4 shows the number of features for each classification tasks after applying the three percentages of PCA on the merged feature vectors. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 191,
                    "end": 198,
                    "text": "Table 4",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Proposed Approach"
        },
        {
            "text": "We used 10-fold cross-validation to evaluate the proposed model. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and testing. Our methods were evaluated using a set of machine learning classifiers; namely, k-nearest neighbors (kNN), random forest (RF), Na\u00efve Bayes (NB), BayesNet (BN), and artificial neural network (ANN); mainly for identifying persons and recognizing gender, age and facial expression (eye-smil) all from veiled-face images. PCA is used for dimensionality reduction, and merging both FC6 and FC7 is used for obtaining more representative deep features.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results and Discussion"
        },
        {
            "text": "Initially, we evaluated the ability of a VGG19 to identify persons from VPI-New dataset by applying a number of classifiers on the both of the raw deep features vectors (FC6 and FC7) extracted by VGG19. As can be seen from Table 5 the deep features obtained from FC6 allow for higher accuracy of identifying persons than those obtained from FC7, where the highest accuracy was 99.5238% recorded by the 1NN classifier. Normally, the dimensionality of the deep features is high, that why we used the PCA, which significantly reduces the dimensions while preserving important information, and speeds up the identification process. Reducing the dimensionality allows for using other classifiers such as the ANN, which needs unacceptable training time on the high dimensional deep features. It is also interesting to note that the accuracy of the classifiers applied on the features vector extracted from FC6 showed a relative decrease (in general) when applying PCA. Unlike the FC7, which allows the classifiers to benefit form applying PCA in most cases. Similarly, the highest obtained accuracy is 99.9524% recorded by the ANN on the smaller set of features, i.e. after applying the PCA preserving 95% of the data variance. The reason behind this performance might be due to removing a large number of poor (redundant) deep features. Because of the fluctuating performance of FC6 and FC7, we opt for merging these deep features together to get a new feature vector based on both, using either of arithmetic mean, maximum or minimum of FC6 and FC7.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 223,
                    "end": 230,
                    "text": "Table 5",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Results of Veiled-Face Recognition"
        },
        {
            "text": "We compared each merging method as shown in Table 6 , the best way to do the merge is the arithmetic mean, where the highest identification accuracy was recorded by most of the classifiers used. However, the highest accuracy obtained by merging was 99.4286% Table 6 The accuracy of identifying persons from VPI-New dataset after applying three merge methods and after applying PCA with 99%, 97%, 95% data variance, data in (%). recorded by the 1NN classifier, which is slightly less than that recorded on the raw deep features by the same classifier. However, using PCA after merging, particularly, merging by the arithmetic mean, allowed for better results in general, reaching up to 99.9524% in more cases than without merging.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 44,
                    "end": 51,
                    "text": "Table 6",
                    "ref_id": null
                },
                {
                    "start": 258,
                    "end": 265,
                    "text": "Table 6",
                    "ref_id": null
                }
            ],
            "section": "Results of Veiled-Face Recognition"
        },
        {
            "text": "In this section we tested the ability of a VGG19 to gender recognition from VPI-New dataset. A number of classifiers have been applied on the raw feature vectors extracted by the VGG19. Table 7 shows the accuracy of gender recognition from VPI-New database. The highest accuracy of gender recognition obtained was 99.8095%, which is recorded by the 1NN classifier. The results also suggest that the deep features obtained from FC6 are more representative than those obtained from FC7. As shown in Table7, the highest accuracy of gender recognition obtained after applying PCA of 99% is 99.8095%, which is recorded by the 1NN classifier, this is always expected since preserving more data variance allows for more information to feed the learning algorithm. However, what is more interesting is to get better results using less information, e.g. PCA preserving 97% or even 95% of data variance recorded higher classification accuracy. Table 9 The confusion matrix resulting from applying 3NN classifier after applying PCA95%, and the total accuracy is 0.999048.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 186,
                    "end": 193,
                    "text": "Table 7",
                    "ref_id": null
                },
                {
                    "start": 934,
                    "end": 941,
                    "text": "Table 9",
                    "ref_id": null
                }
            ],
            "section": "Results of Veiled-Face Gender Recognition"
        },
        {
            "text": "-Female Male Female 1539 1 Male 1 559 Table 8 shows the accuracy of gender recognition of each of the three feature vectors resulting from merging FC6 and FC7. As can be seen from this table, the merging methods produced no better classification. However, the accuracy of gender recognition from the merged deep features has increased after applying PCA, but it is still slightly less than the maximum accuracy obtained, except for the 3NN with (again) PCA 95%, which recorded the same (99.9048%) using the mean merger. Table 9 shows the structure of the confusion matrix where TP refers to true positive, FN refers to a false negative, FP refers to false positive and TN refers to true negative. The confusion matrix resulting from applying 3NN classifier after applying PCA by preserving 95% of data variance. Note that there were two classes -Female, Male.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 38,
                    "end": 45,
                    "text": "Table 8",
                    "ref_id": "TABREF8"
                },
                {
                    "start": 520,
                    "end": 527,
                    "text": "Table 9",
                    "ref_id": null
                }
            ],
            "section": "Results of Veiled-Face Gender Recognition"
        },
        {
            "text": "We applied a number of classifiers on the raw deep features extracted by the VGG19 for age recognition from VPI-New dataset. Table 10 shows the accuracy of age recognition. The highest age recognition accuracy obtained was 99.9524%, which is recorded by the 1NN classifier on the FC6 deep features. As can be seen from the results in Table 10 , the performance of the features obtained from layer FC6 for the purpose of age recognition, is more representative than that of layer FC7.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 125,
                    "end": 133,
                    "text": "Table 10",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 334,
                    "end": 342,
                    "text": "Table 10",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Results of Veiled-Face Age Recognition"
        },
        {
            "text": "The most interesting note in Table 10 , is the zero error rate, which is obtained by the 1NN classifier, after applying the PCA 95% on the deep features obtained from FC6. What makes this result special is that the classifier used only 137 features out of 4096 features, which are obtained by the PCA after preserving 95% of data variance. And this, surprisingly, We further merged both FC6 and FC7, using the arithmetic mean, maximum and minimum for the purpose of age recognition. Table 11 shows the accuracy of age recognition for each of the three feature vectors, which were resulted from the three merging processes. As can be seen from the same table, all of the merged features have outperformed the results of the raw deep features of FC7, as well as some of the results of the raw deep features of FC6. The highest age recognition result (99.9524%) obtained after applying PCA 95% on the merged features of the arithmetic mean, this high recognition result, again, fosters the ability of merging the deep features, particularly, when using the arithmetic mean. Table 12 shows the confusion matrix, which is resulted from applying 1NN classifier on the PCA 95% of the arithmetic mean merged deep features, where we have four classes -Children, Youth, Adults and Elderly. Table 13 shows the accuracy of facial expression (eye-smile) recognition, since the eyesmile is the only facial expression recorded in the VPI-new database. As can be seen from Table 14 shows the accuracy of the facial expression (eye smile) recognition after each merging method. As expected, and similar to the previous problems, we can easily see that the eye smile recognition has slightly improved when using the merged deep features, with no significant preference to any merging method. Table 15 shows the confusion matrix, which is resulted from applying the ANN classifier on the PCA 95% of minimum merged deep features, where we have two classes -Smile, Normal. Most of the previous results show that the use of PCA has improved the identification/recognition tasks. This is also supported by [Tarawneh et al., 2020a , Gan et al., 2015 , Linge and Pawar, 2014 , whose experiments in face recognition have shown improvement in the performance of classification using PCA and neural network. Our results also show that the use of the deep features in general, and those obtained from FC6, in particular, allowed for better recognition/identification. This is supported by the results of [Kataoka et al., 2015] , whose results have shown that the fully connected layers tend to perform better for object recognition tasks.",
            "cite_spans": [
                {
                    "start": 2083,
                    "end": 2106,
                    "text": "[Tarawneh et al., 2020a",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 2107,
                    "end": 2125,
                    "text": ", Gan et al., 2015",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 2126,
                    "end": 2149,
                    "text": ", Linge and Pawar, 2014",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 2475,
                    "end": 2497,
                    "text": "[Kataoka et al., 2015]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [
                {
                    "start": 29,
                    "end": 37,
                    "text": "Table 10",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 483,
                    "end": 491,
                    "text": "Table 11",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1071,
                    "end": 1079,
                    "text": "Table 12",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1280,
                    "end": 1288,
                    "text": "Table 13",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1457,
                    "end": 1465,
                    "text": "Table 14",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1774,
                    "end": 1782,
                    "text": "Table 15",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Results of Veiled-Face Age Recognition"
        },
        {
            "text": "Sometimes, the accuracy measures is not enough to show the real performance of a machine learning method [Olanrewaju et al., 2017] ; particularly when applied to an imbalanced dataset [Ghatasheh et al., 2020 , Tarawneh et al., 2020b , Abu Alfeilat et al., 2019 . Although our dataset of the deep features for persons identification is balanced; having 14 images for each subject/class, we have the other three datasets, which contain the deep features for gender, age, or smile recognition is not fairly balanced as can be seen from the confusion matrices in Tables 9, 12 and 15 respectively. Therefore, we opt for more metrics, namely, F-Measure, ROC Area, and PRC Area. However, instead of repeating all the previous results, we opt for the results of deep features merged using the mean method, as being the best performer, and we used the PCA preserving 99% of the data for the same reason. Table 16 shows the results with these metrics. As can be seen from Table 16 , and similar to the previous accuracy results, the high values of F-Measure, ROC Area, and PRC Area indicate the robustness of the extracted PCA features from the deep features obtained by mean-merging FC6 and FC7.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 130,
                    "text": "[Olanrewaju et al., 2017]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 184,
                    "end": 207,
                    "text": "[Ghatasheh et al., 2020",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 208,
                    "end": 232,
                    "text": ", Tarawneh et al., 2020b",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 233,
                    "end": 260,
                    "text": ", Abu Alfeilat et al., 2019",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 895,
                    "end": 903,
                    "text": "Table 16",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 962,
                    "end": 970,
                    "text": "Table 16",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Result of Veiled-Facial Expression Recognition"
        },
        {
            "text": "To the best of our knowledge, no methods have been proposed to identify persons and gender recognition from veiled-face images, except for [Hassanat et al., 2017b] , who used handcraft features on the VPI-Old database. Their highest identification rate was 97.55%, and 99.41% for gender recognition. Both VPI-Old and new databases are different, therefore, and for the purpose of a valid comparison, we extracted the deep features using the same VGG19.As can bee seen from Table 17 , the use of the deep features has improved the identification/recognition rates significantly. [Hassanat et al., 2017b] 97.22% 99.41% -- [Teo et al., 2007] 95.12% --- [Hu et al., 2013] 98% --- [Liew et al., 2016] -99.38% -- [Sun et al., 2015] 99.53% --- [Lian and Lu, 2006 Although it is not a good practice to compare the proposed approach to other methods that used other databases, e.g. Table 18 , at least, such a comparison gives a hint about the goodness of the deep features, when used for our problems. Knowing that the deep CNNs have recently achieved excellent performance in a wide range of image classification tasks [Yu and Zhang, 2015] , unlike traditional machine learning methods, where features are hand-crafted [Mollahosseini et al., 2016] .",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 163,
                    "text": "[Hassanat et al., 2017b]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 578,
                    "end": 602,
                    "text": "[Hassanat et al., 2017b]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 620,
                    "end": 638,
                    "text": "[Teo et al., 2007]",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 650,
                    "end": 667,
                    "text": "[Hu et al., 2013]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 676,
                    "end": 695,
                    "text": "[Liew et al., 2016]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 707,
                    "end": 725,
                    "text": "[Sun et al., 2015]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 737,
                    "end": 755,
                    "text": "[Lian and Lu, 2006",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 1112,
                    "end": 1132,
                    "text": "[Yu and Zhang, 2015]",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 1212,
                    "end": 1240,
                    "text": "[Mollahosseini et al., 2016]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [
                {
                    "start": 473,
                    "end": 481,
                    "text": "Table 17",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 873,
                    "end": 881,
                    "text": "Table 18",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Results comparison to some related work"
        },
        {
            "text": "In this paper, we proposed a new approach based on deep features to identify persons, and recognize genders, ages, and a facial expression (eye smile), all from veiled-faces image database, which was created for the purpose of this study. The proposed approach is basically based on extracting deep features from both FC6 and FC7 layers of the VGG19, merging these features and then reducing the dimensionality using PCA. We investigated a number of merging methods with different classifiers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "Our results obtained on an in-house veiled-faces image database show that it is possible to identify persons, and recognize genders, ages, and a facial expression (eye smile), all from veiled-faces, the use of deep features is a good choice for our tasks, and the PCA of the deep features extracts more distinctive features for better identification/recognition rates in most cases, with a smaller number of features, and hence allows for faster learning/testing. In addition, we found that the 1NN and ANN classifiers (in general) were the best classifiers to be used for our tasks in terms of accuracy. We also found that merging deep features (FC6 and FC7), particularly, when using the arithmetic mean, provides better identification/recognition results in most cases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "The limitation of this work is linked to the in-house image database used, since the images of the frontal veiled-faces where taken from a short distance, in an office environment, we used this for simplicity, so as not to be forced to use face detection, and handling complex backgrounds, etc. more than about the missing information posed by the usage of veils. A more realistic veiled-face image database (in the wild) is needed to further prove the feasibility of the proposed approach and to be used in practice. Our future will focus on collecting such a complex database, in addition to investigating more deep learning methods that can handle complex backgrounds robustly.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Effects of distance measure choice on k-nearest neighbor classifier performance: a review",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "A"
                    ],
                    "last": "Abu Alfeilat",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Lasassmeh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Tarawneh",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "B"
                    ],
                    "last": "Alhasanat",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "S"
                    ],
                    "last": "Salman",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "S"
                    ],
                    "last": "Prasath",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Big data",
            "volume": "7",
            "issn": "4",
            "pages": "221--248",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Face recognition with local binary patterns. European Conference on Computer Vision",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ahonen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hadid",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pietik\u00e4inen",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "3021",
            "issn": "",
            "pages": "469--481",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Face recognition using hybrid feature space in conjunction with support vector machine",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Akbar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ahmad",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hayat",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Journal of Applied Environmental and Biological Sciences",
            "volume": "5",
            "issn": "7",
            "pages": "28--36",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "New online/offline text-dependent arabic handwriting dataset for writer authentication and identification",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "Z"
                    ],
                    "last": "Al-Shamaileh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Tarawneh",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "S"
                    ],
                    "last": "Rahman",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Celik",
                    "suffix": ""
                },
                {
                    "first": "Jawthari",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "10th International Conference on Information and Communication Systems (ICICS)",
            "volume": "",
            "issn": "",
            "pages": "116--121",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Modelling-based simulator for forecasting the spread of covid-19: A case study of saudi arabia",
            "authors": [],
            "year": null,
            "venue": "International Journal of Computer Science and Network Security (IJCSNS)",
            "volume": "20",
            "issn": "10",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Adversarial occlusion-aware face detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)",
            "volume": "",
            "issn": "",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "A new algorithm for age recognition from facial images",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Dehshibi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bastanfard",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Signal Processing",
            "volume": "90",
            "issn": "8",
            "pages": "2431--2444",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A survey of biometric recognition methods",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Delac",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Grgic",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "46th International Symposium on Electronics in Marine (ELMAR)",
            "volume": "",
            "issn": "",
            "pages": "184--193",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Optical biometric security element",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "X"
                    ],
                    "last": "Delbaere",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Seiberle",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Studer",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "US Patent",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Imagenet: A large-scale hierarchical image database",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "L.-J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Fei-Fei",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "248--255",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Gender recognition through face using deep learning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dhomne",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Bhan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Procedia Computer Science",
            "volume": "132",
            "issn": "",
            "pages": "2--10",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Multi-view face detection using deep convolutional neural networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Farfade",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Saberian",
                    "suffix": ""
                },
                {
                    "first": "L.-J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "5th ACM on International Conference on Multimedia Retrieval",
            "volume": "",
            "issn": "",
            "pages": "643--650",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "A pca-based convolutional network",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhong",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1505.03703"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Cost-sensitive ensemble methods for bankruptcy prediction in a highly imbalanced data distribution: a real case from the spanish market",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ghatasheh",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Faris",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Abukhurma",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "A"
                    ],
                    "last": "Castillo",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Al-Madi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Mora",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ala",
                    "suffix": ""
                },
                {
                    "first": "A.-Z",
                    "middle": [],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Progress in Artificial Intelligence",
            "volume": "9",
            "issn": "4",
            "pages": "361--375",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "A method for face recognition from facial expression",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ghosh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "K"
                    ],
                    "last": "Bandyopadhyay",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Methods",
            "volume": "5",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A multiagent system for the classification of gender and age from images",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gonz\u00e1lez-Briones",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Villarrubia",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "De Paz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Corchado",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Computer Vision and Image Understanding",
            "volume": "172",
            "issn": "",
            "pages": "98--106",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Visual speech recognition. Speech and Language Technologies",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "1",
            "issn": "",
            "pages": "279--303",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "On identifying terrorists using their victory signs",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Data Science Journal",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Victory sign biometrie for terrorists identification: Preliminary results",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Btoush",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Abbadi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "M"
                    ],
                    "last": "Al-Mahadeen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Al-Awadi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "I"
                    ],
                    "last": "Mseidein",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Almseden",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Tarawneh",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "B"
                    ],
                    "last": "Alhasanat",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "S"
                    ],
                    "last": "Prasath",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "8th International Conference on Information and Communication Systems (ICICS)",
            "volume": "",
            "issn": "",
            "pages": "182--187",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Magnetic energy-based feature extraction for low-quality fingerprint images. Signal",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "S"
                    ],
                    "last": "Prasath",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Al-Kasassbeh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Tarawneh",
                    "suffix": ""
                },
                {
                    "first": "Al-Shamailh",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Image and Video Processing",
            "volume": "12",
            "issn": "8",
            "pages": "1471--1478",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Classification and gender recognition from veiled-faces",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "S"
                    ],
                    "last": "Prasath",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "M"
                    ],
                    "last": "Al-Mahadeen",
                    "suffix": ""
                },
                {
                    "first": "Alhasanat",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "M M"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Journal of Biometrics",
            "volume": "9",
            "issn": "4",
            "pages": "347--364",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Recognizing partial biometric patterns",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Wang",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1810.07399"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Biometric identification promises fast and secure processing of airline passengers",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Heitmeyer",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "ICAO journal",
            "volume": "55",
            "issn": "9",
            "pages": "10--11",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Robust partial face recognition using instance-to-class distance",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Y.-P",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Visual Communications and Image Processing (VCIP)",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Curricularface: adaptive curriculum learning loss for deep face recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tai",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "5901--5910",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "An introduction to biometric recognition ieee transactions on circuits and systems for video technology",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "K"
                    ],
                    "last": "Jain",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ross",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Prabhakar",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Special Issue on Image-and Video-Based Biometrics",
            "volume": "14",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Feature evaluation of deep convolutional neural networks for object recognition and detection",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kataoka",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Iwata",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Satoh",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1509.07627"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "An analysis of facial expression recognition under partial facial image occlusion",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kotsia",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Buciu",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Pitas",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Image and Vision Computing",
            "volume": "26",
            "issn": "7",
            "pages": "1052--1067",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "A detailed review of feature extraction in image processing systems",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "K"
                    ],
                    "last": "Bhatia",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Fourth International Conference on Advanced Computing and Communication Technologies",
            "volume": "",
            "issn": "",
            "pages": "5--12",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Deep learning. nature",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "521",
            "issn": "",
            "pages": "436--444",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Age and gender classification using convolutional neural networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Levi",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hassner",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops",
            "volume": "",
            "issn": "",
            "pages": "34--42",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Deep facial expression recognition: A survey",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Transactions on Affective Computing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Multi-view gender classification using local binary patterns and support vector machines",
            "authors": [
                {
                    "first": "H.-C",
                    "middle": [],
                    "last": "Lian",
                    "suffix": ""
                },
                {
                    "first": "B.-L",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "International Symposium on Neural Networks",
            "volume": "",
            "issn": "",
            "pages": "202--209",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Gender classification: a convolutional neural network approach",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Liew",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "K"
                    ],
                    "last": "Hani",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Radzi",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bakhteri",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Turkish Journal of Electrical Engineering and Computer Sciences",
            "volume": "24",
            "issn": "3",
            "pages": "1248--1264",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Neural network based face recognition using pca",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Linge",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pawar",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International Journal of Computer Science and Information Technologies",
            "volume": "5",
            "issn": "3",
            "pages": "4011--4014",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Facial expression recognition via a boosted deep belief network",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tong",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1805--1812",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Spontaneous smile recognition for interest detection",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Chinese Conference on Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "119--130",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Partial face detection for continuous authentication",
            "authors": [
                {
                    "first": "U",
                    "middle": [],
                    "last": "Mahbub",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "M"
                    ],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chandra",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Barbello",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chellappa",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE International Conference on Image Processing (ICIP)",
            "volume": "",
            "issn": "",
            "pages": "2991--2995",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Going deeper in facial expression recognition using deep neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mollahosseini",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Mahoor",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Winter Conference on Applications of Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Robust periocular recognition by fusing local to holistic sparse representations",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Moreno",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "S"
                    ],
                    "last": "Prasath",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Proen\u00e7a",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "6th International Conference on Security of Information and Networks",
            "volume": "",
            "issn": "",
            "pages": "160--164",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Robust periocular recognition by fusing sparse representations of color and geometry information",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Moreno",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "S"
                    ],
                    "last": "Prasath",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Santos",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Proen\u00e7a",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Journal of Signal Processing Systems",
            "volume": "82",
            "issn": "3",
            "pages": "403--417",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Combining deep and handcrafted image features for presentation attack detection in face recognition systems using visible-light camera sensors",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "T"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "D"
                    ],
                    "last": "Pham",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "R"
                    ],
                    "last": "Baek",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "R"
                    ],
                    "last": "Park",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Sensors",
            "volume": "18",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Enhancement web proxy cache performance using wrapper feature selection methods with nb and j48",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "F"
                    ],
                    "last": "Olanrewaju",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "W"
                    ],
                    "last": "Azman",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IOP Conference Series: Materials Science and Engineering",
            "volume": "260",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Estimation of age from face images",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Padme",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Desai",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Journal of Science and Research",
            "volume": "",
            "issn": "12",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ranjan",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "M"
                    ],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chellappa",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "41",
            "issn": "1",
            "pages": "121--135",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Deep expectation of real and apparent age from a single image without facial landmarks",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Rothe",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Timofte",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Journal of Computer Vision",
            "volume": "126",
            "issn": "2",
            "pages": "144--157",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Partial face recognition using radial basis function networks",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sato",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Aggarwal",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition",
            "volume": "",
            "issn": "",
            "pages": "288--293",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Towards universal representation learning for deep face recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sohn",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chandraker",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "K"
                    ],
                    "last": "Jain",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "6817--6826",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Very deep convolutional networks for large-scale image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1409.1556"
                ]
            }
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Face detection using deep learning: An improved faster RCNN approach",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "C"
                    ],
                    "last": "Hoi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Neurocomputing",
            "volume": "299",
            "issn": "",
            "pages": "42--50",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Deepid3: Face recognition with very deep neural networks",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1502.00873"
                ]
            }
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Deep learning face representation from predicting 10,000 classes",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1891--1898",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Detailed investigation of deep features with sparse representation and dimensionality reduction in cbir: A comparative study",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Tarawneh",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Celik",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chetverikov",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "24",
            "issn": "",
            "pages": "47--68",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Stability and reduction of statistical features for image classification and retrieval: Preliminary results",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Tarawneh",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chetverikov",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Verma",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "9th International Conference on Information and Communication Systems (ICICS)",
            "volume": "",
            "issn": "",
            "pages": "117--121",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Smotefuna: Synthetic minority over-sampling technique based on furthest neighbour algorithm",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Tarawneh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Almohammadi",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chetverikov",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bellinger",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Access",
            "volume": "8",
            "issn": "",
            "pages": "59069--59082",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Deep face image retrieval: A comparative study with dictionary learning",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Tarawneh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Celik",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chetverikov",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "S"
                    ],
                    "last": "Rahman",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Verma",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "10th International Conference on Information and Communication Systems (ICICS)",
            "volume": "",
            "issn": "",
            "pages": "185--192",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Invoice classification using deep features and machine learning techniques",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Tarawneh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Hassanat",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chetverikov",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Lendak",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Verma",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT)",
            "volume": "",
            "issn": "",
            "pages": "855--859",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "A study on partial face recognition of eye region",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Teo",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "F"
                    ],
                    "last": "Neo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B J"
                    ],
                    "last": "Teoh",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "2007 International Conference on Machine Vision",
            "volume": "",
            "issn": "",
            "pages": "46--49",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "A gender recognition system using shunting inhibitory convolutional neural networks",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "H C"
                    ],
                    "last": "Tivive",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bouzerdoum",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "The 2006 IEEE International Joint Conference on Neural Network Proceedings",
            "volume": "",
            "issn": "",
            "pages": "5336--5341",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Face recognition: From traditional to deep learning methods",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "S"
                    ],
                    "last": "Trigueros",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hartnett",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1811.00116"
                ]
            }
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "Subspace-based age-group classification using facial images under various lighting conditions",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ueki",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hayashida",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kobayashi",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "7th International Conference on Automatic Face and Gesture Recognition (FGR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Data Mining: Practical Machine Learning Tools and Techniques: Practical Machine Learning Tools and Techniques",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Witten",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Frank",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hall",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "Survey of occluded and unoccluded face recognition",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Advances in Computer, Communication and Computational Sciences",
            "volume": "",
            "issn": "",
            "pages": "1001--1014",
            "other_ids": {}
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "Image based static facial expression recognition with multiple deep network learning",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ACM on International Conference on Multimodal Interaction",
            "volume": "",
            "issn": "",
            "pages": "435--442",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "Face recognition: A literature survey",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chellappa",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Phillips",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rosenfeld",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Typical architecture of the VGG model[Simonyan and Zisserman, 2014].",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Recognition tasks tackled by our proposed work compared to other related works.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Naming system used in our VPI-New database.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "The number of the deep features before and after applying the PCA.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "The number of features after the PCA applied on the feature vector resulting from merging FC6 and FC7",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "shows the number of features obtained after applying the three percentages of data variance. Note that this is done on both layers FC6 and FC7.Algorithm 2 Min, Max, Mean Merge with FC6 and FC7 layer features Input: Feature vector of FC6 and Feature vector of FC7 Output: Three Final feature vectors, one from each method (Min, Max and Mean), FFVmin,",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "The accuracy of identifying persons from VPI-New dataset on raw deep features and after applying PCA with 99%, 97%, 95% data variance, data in (%).",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "The accuracy of gender recognition from VPI-New dataset after applying three merge",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "The accuracy of age recognition from raw features and after applying PCA with 99%,",
            "latex": null,
            "type": "table"
        },
        "TABREF10": {
            "text": "The confusion matrix of the 1NN classifier on PCA 95% of the arithmetic mean merged features.",
            "latex": null,
            "type": "table"
        },
        "TABREF11": {
            "text": "The accuracy of facial expression (eye smile) recognition from VPI-New dataset on the raw deep features and after applying PCA with 99%, 97%, 95% data variance, data in (%).Table 14The accuracy of facial expression (eye smile) recognition from VPI-New dataset after merging and applying PCA by 99%, 97%, 95% data variance, data in (%).the same table, the highest accuracy (80%) is recorded by the ANN on FC6 after applying PCA, and in general, FC6 provides more distinctive deep features than that of FC7. Similar to the previous problems, we merged both FC6 and FC7 deep features, using the aforementioned merging methods.",
            "latex": null,
            "type": "table"
        },
        "TABREF12": {
            "text": "The confusion matrix of eye smile recognition by the ANN classifier of PCA 95% on the minimum merged deep features.",
            "latex": null,
            "type": "table"
        },
        "TABREF13": {
            "text": "Results of person identification, gender recognition, smile recognition and gender recognition of mean-merged deep features with PCA preserving 99% of data.1NNANN F-Measure ROC Area PRC Area F-Measure ROC Area PRC Area",
            "latex": null,
            "type": "table"
        },
        "TABREF14": {
            "text": "The accuracy of identifying persons and gender recognition from VPI-Old dataset[Hassanat et al., 2017b] using deep features, data in (%).Table 18Comparison of our proposed approach to other methods.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The third author (A. S. T.) would like to acknowledge Tempus Public Foundation for sponsoring his Ph.D. program. His work is also supported by the Hungarian Government and co-financed by the European Social Fund, under the project EFOP-3.6.3-VEKOP-16-2017-00001 (Talent Management in Autonomous Vehicle Control Technologies.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgment"
        }
    ]
}
{
    "paper_id": "2e8cc81fcd95033759674c2f0453c7b55332dec7",
    "metadata": {
        "title": "A New Algorithm for Tessellated Kernel Learning",
        "authors": [
            {
                "first": "Brendon",
                "middle": [
                    "K"
                ],
                "last": "Colbert",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Arizona State University Tempe",
                    "location": {
                        "postCode": "85287, 85287",
                        "region": "AZ, AZ"
                    }
                },
                "email": "brendon.colbert@asu.edu"
            },
            {
                "first": "Matthew",
                "middle": [
                    "M"
                ],
                "last": "Peet",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Arizona State University Tempe",
                    "location": {
                        "postCode": "85287, 85287",
                        "region": "AZ, AZ"
                    }
                },
                "email": "mpeet@asu.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "The accuracy and complexity of machine learning algorithms based on kernel optimization are limited by the set of kernels over which they are able to optimize. An ideal set of kernels should: admit a linear parameterization (for tractability); be dense in the set of all kernels (for robustness); be universal (for accuracy). The recently proposed Tesselated Kernels (TKs) is currently the only known class which meets all three criteria. However, previous algorithms for optimizing TKs were limited to classification and relied on Semidefinite Programming (SDP) -limiting them to relatively small datasets. By contrast, the 2-step algorithm proposed here scales to 10,000 data points and extends to the regression problem. Furthermore, when applied to benchmark data, the algorithm demonstrates significant improvement in performance over Neural Nets and SimpleMKL with similar computation time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "In this paper, we extend the TK framework proposed in [4] to the problem of regression. The KL problem in regression has been studied using SDP in [16, 15] and Quadratic Programming (QP) in e.g. [17, 10] . However, neither of these previous works considered a set of kernels with both the tractability and the density property. By generalizing the Tessellated KL framework proposed in [4] 33rd",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 151,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 152,
                    "end": 155,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 195,
                    "end": 199,
                    "text": "[17,",
                    "ref_id": null
                },
                {
                    "start": 200,
                    "end": 203,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 385,
                    "end": 388,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Kernel methods for classification and regression (and Support Vector Machines (SVMs) in particular) require selection of a kernel. Kernel Learning (KL) algorithms such as those found in [23, 21, 24] automate this task by finding the kernel, k \u2208 K which optimizes an achievable metric such as the soft margin (for classification). The set of kernels, k \u2208 K, over which the algorithm can optimize, however, strongly influences the performance and robustness of the resulting classifier or predictor.",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 190,
                    "text": "[23,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 191,
                    "end": 194,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 195,
                    "end": 198,
                    "text": "24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To understand how the choice of K influences performance and robustness, three properties were proposed in [4] to characterize the set K -tractability, density, and universality. Specifically, K is tractable if K is convex (or, preferably, a linear variety) -implying the KL problem is solvable using, e.g. [17, 10, 12, 16, 9] . The set K has the density property if, for any > 0 and any positive kernel, k * there exists a k \u2208 K where k \u2212 k * \u2264 . The density property implies the kernel will perform well on untrained data (robustness or generalizability). The set K has the universal property if any k \u2208 K is universal -ensuring the classifier/predictor will perform arbitrarily well on large sets of training data.",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 110,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 307,
                    "end": 311,
                    "text": "[17,",
                    "ref_id": null
                },
                {
                    "start": 312,
                    "end": 315,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 316,
                    "end": 319,
                    "text": "12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 320,
                    "end": 323,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 324,
                    "end": 326,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In [4] , the Tessellated Kernels (TKs) were shown to have all 3 properties, the first known such class of kernels. This work was based on a general framework for using positive matrices to parameterize positive kernels (as opposed to positive kernel matrices as in [12, 16, 15] ). Unfortunately, however, the algorithms proposed in [4] were either based on SemiDefinite Programming (SDP) (thereby limiting the amount of training data) or used a randomized linear basis for the kernels (implying loss of density). Thus, while the algorithms in [4] outperformed all other methods (including deep learning) as measured by Test Set Accuracy (TSA), the computation times were not competitive. Furthermore, the results in [4] did not encompass the problem of regression.",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 6,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 265,
                    "end": 269,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 270,
                    "end": 273,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 274,
                    "end": 277,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 332,
                    "end": 335,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 543,
                    "end": 546,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 716,
                    "end": 719,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "to the regression problem, we demonstrate significant increases in performance, as measured by Mean Square Error (MSE), and when compared to the results in [17, 10, 16] .",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 160,
                    "text": "[17,",
                    "ref_id": null
                },
                {
                    "start": 161,
                    "end": 164,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 165,
                    "end": 168,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In addition, we show that the SDP-based algorithm [4] for classification, and extended here to regression, can be decomposed into primal and dual sub-problems, OP T _A and OP T _P -similar to the approach taken in [17, 10] . Furthermore, we show that OP T _P (an SDP) admits an analytic solution using the Singular Value Decomposition (SVD) -an approach which allows us to consider higher dimensional feature spaces and more complex TKs. In addition, OP T _A is a convex QP and may be solved efficiently with achieved complexity which scales as O(m 2.16 ) where m is the number of data points. We use a two-step algorithm on OP T _A and OP T _P and show that termination at OP T _A = OP T _P is equivalent to global optimality. The resulting algorithm, then, does not require the use of SDP and, when applied to several standard test cases, is shown to retain the favorable TSA of [4] for classification, while offering improved MSE for regression, and competitive computation times as compared to other KL and deep learning algorithms.",
            "cite_spans": [
                {
                    "start": 50,
                    "end": 53,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 214,
                    "end": 218,
                    "text": "[17,",
                    "ref_id": null
                },
                {
                    "start": 219,
                    "end": 222,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 881,
                    "end": 884,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Consider a generalized representation of the KL problem, which encompasses both classification and regression where (using the representor theorem [19] ) the learned function is of the form",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 151,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "An Ideal Set of Kernels for KL in Classification and Regression"
        },
        {
            "text": "Here",
            "cite_spans": [],
            "ref_spans": [],
            "section": "An Ideal Set of Kernels for KL in Classification and Regression"
        },
        {
            "text": "and l(f \u03b1,k , b) yi,xi is the loss function and is defined for SVM binary classification and SVM regression as l c (f \u03b1,k , b) yi,xi and l r (f \u03b1,k , b) yi,xi , respectively, where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "An Ideal Set of Kernels for KL in Classification and Regression"
        },
        {
            "text": "The properties of the classifier/predictor, f \u03b1,k , resulting from Optimization Problem 1 will depend on the properties of the set K, which is presumed to be a subset of the convex cone of all positive kernels. To understand how K influences the tractability of the optimization problem and the resulting fit, we consider three properties of the set, K.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "An Ideal Set of Kernels for KL in Classification and Regression"
        },
        {
            "text": "We say a set of kernel functions, K, is tractable if it can be represented using a countable basis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tractability"
        },
        {
            "text": "Note the G i (x, y) need not be positive kernel functions. The tractable property is required for the KL problem to be tractable using algorithms for convex optimization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tractability"
        },
        {
            "text": "Universal kernel functions always have positive definite (full rank) kernel matrices, implying that for arbitrary data",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Universality"
        },
        {
            "text": ", such that f (x j ) = y j for all j = 1, .., m. Conversely, if a kernel is not universal, then exists a data set {x i , y i } m i=1 such that for any \u03b1 \u2208 R m , there exists some j \u2208 {1, \u00b7 \u00b7 \u00b7 , m} such that f (y j ) = m i=1 \u03b1 i k(x i , x j ). This ensures that SVMs using universal kernels can always benefit from additional training data, whereas non-universal kernels may saturate.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Universality"
        },
        {
            "text": "Definition 2. A kernel k : X \u00d7 X \u2192 R is said to be universal on the compact metric space X if it is continuous and there exists an inner-product space W and feature map, \u03a6 : X \u2192 W such that k(x, y) = \u03a6(x), \u03a6(y) W and where the unique Reproducing Kernel Hilbert Space (RKHS),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Universality"
        },
        {
            "text": "The following definition extends the universal property to a set of kernels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Universality"
        },
        {
            "text": "Definition 3. A set of kernel functions K has the universal property if every kernel function k \u2208 K is universal.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Universality"
        },
        {
            "text": "The third property is density which distinguishes the TK class from other sets of kernel functions with the universal property. For instance consider a set containing a single Gaussian kernel functionwhich is clearly not ideal for kernel learning. The set containing a single Gaussian is tractable (it has only one element) and every member of the set is universal. However, it is not dense.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density"
        },
        {
            "text": "Considering SVM for classification, the KL problem determines the kernel k \u2208 K for which we may obtain the maximum separation in the kernel-associated feature space. Increasing this separation distance makes the resulting classifier more robust (generalizable) [2] . The density property, then, ensures that the resulting KL algorithm will be maximally robust (generalizable) in the sense of separation distance.",
            "cite_spans": [
                {
                    "start": 261,
                    "end": 264,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Density"
        },
        {
            "text": "Likewise, considering SVMs for regression, the KL problem finds the kernel k \u2208 K which permits the \"flattest\" [20] function in feature space. In this case, the density property ensures that the resulting KL algorithm will be maximally robust (generalizable) in the sense of flatness.",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 114,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Density"
        },
        {
            "text": "These arguments motivate the following definition of the pointwise density property. Definition 4. The set of kernels K is said to be pointwise dense if for any positive kernel, k * , any set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density"
        },
        {
            "text": "Here we define a framework for constructing classes of tractable positive kernel functions and illustrate this approach on the class of General Polynomial Kernels. Proposition 5. Let N be any bounded measurable function N : X \u00d7 Y \u2192 R q and P \u2208 R q\u00d7q be a positive semidefinite matrix P \u2265 0. Then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A General Framework for Representation of Tractable Kernel Sets"
        },
        {
            "text": "The proof for Proposition (5) may be found in [4] . Lemma 6. Let N be any bounded measurable function N : X \u00d7 Y \u2192 R q on compact X and Y . Then the set of kernel functions",
            "cite_spans": [
                {
                    "start": 46,
                    "end": 49,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "A General Framework for Representation of Tractable Kernel Sets"
        },
        {
            "text": "For a given N , the map P \u2192 k is linear. Specifically,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A General Framework for Representation of Tractable Kernel Sets"
        },
        {
            "text": "and thus by Definition 1 K is tractable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A General Framework for Representation of Tractable Kernel Sets"
        },
        {
            "text": "In Subsection 3.1 we apply this framework to obtain Generalized Polynomial Kernels. In Subsection 4.1, we use the framework to obtain the TK class.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A General Framework for Representation of Tractable Kernel Sets"
        },
        {
            "text": "The class of General Polynomial Kernels (GPKs) is defined as the set of all polynomials, each of which is a positive kernel.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Class of General Polynomial Kernels is Tractable"
        },
        {
            "text": "The GPK class is not universal, but is tractable, as per the following lemma. Lemma 7. K P is tractable. Proof. Let Z d : R n \u2192 R q be the vector of monomials of degree d or less. From [4] , we have that a polynomial k of degree 2d is a positive polynomial kernel if and only if there exists some P \u2265 0 such that k(x, y) = Z d (x) T P Z d (y). Now for any finite-dimensional subset of K P , let d be the maximum degree over this subset and define N (z, y) = Z d (y). Then Lemma 6 implies that K P is tractable.",
            "cite_spans": [
                {
                    "start": 185,
                    "end": 188,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "The Class of General Polynomial Kernels is Tractable"
        },
        {
            "text": "This lemma implies that a representation of the form of Equation (2) is necessary and sufficient for a GPK to be positive. For convenience, we denote the set of GPK kernels of degree d or less as follows [18] .",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 208,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "The Class of General Polynomial Kernels is Tractable"
        },
        {
            "text": "In this section, we define the class of TK kernels and show it is tractable, dense, and universal.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tessellated Kernels: Tractable, Dense and Universal"
        },
        {
            "text": "Again, let Z d : R n \u00d7 R n \u2192 R q be the vector of monomials of degree d. Define I, the indicator function for the positive orthant, and the following choice of N : R n \u00d7 R n \u2192 R 2q as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tessellated Kernels"
        },
        {
            "text": "where z \u2265 0 means z i \u2265 0 for all i. We now define the set of TK kernels for a < b \u2208 R n as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tessellated Kernels"
        },
        {
            "text": "Kernels in the TK class are \"Tessellated\" in the sense that each datapoint defines a vertex which bisects each dimension of the domain of the resulting classifier/predictor -resulting in a tessellated partition of the feature space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tessellated Kernels"
        },
        {
            "text": "The class of TK kernels is prima facie in the form of Eqn. (3) in Lemma 6 and hence is tractable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Set of TK Kernels is Tractable"
        },
        {
            "text": "However, we will expand on this result by specifying the basis for the set of TK kernels, which will then be used in Section 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Set of TK Kernels is Tractable"
        },
        {
            "text": "Corollary 8. Suppose that for a < b \u2208 R n , and d \u2208 N. We define the finite set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Set of TK Kernels is Tractable"
        },
        {
            "text": "where 1 \u2208 N n is the vector of ones, p * : R 2n \u2192 R n is defined elementwise as p * (x, y) i = max{x i , y i }, and T : R n \u00d7 R n \u00d7 N n \u2192 R is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Set of TK Kernels is Tractable"
        },
        {
            "text": "The proof of Corollary 8 can be found in [4] .",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 44,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "The Set of TK Kernels is Tractable"
        },
        {
            "text": "The density property differentiates the set of TK kernels from other sets of kernel functions (e.g. a linear combination of Gaussian kernels of fixed bandwidths).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The TK Class is Dense"
        },
        {
            "text": "From [4] we have that the set of TK kernels satisfies the pointwise density property.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 8,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "The TK Class is Dense"
        },
        {
            "text": "Theorem 9. For any kernel matrix K * and any finite set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The TK Class is Dense"
        },
        {
            "text": "In [4] an analytical solution, K * , was found for the optimal trace-constrained kernel matrix that maximized the separation distance between two classes of points in the feature space. It was shown in this work that when {y i } has an equal number of positive and negative labels, K * contains an equal number of positive and negative elements -illustrating the importance of using kernels which are not pointwise positive (Gaussians are pointwise positive).",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 6,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "The TK Class is Dense"
        },
        {
            "text": "Polynomial Degree (d) To illustrate the density property, then, we show how optimal GPK and TK kernels yield kernel matrices which approximate the analytic solution, K * , of the optimal kernel matrix problem for a given set of data {x i } and labels {y i }, while Gaussian kernels do not. Specifically, we consider the following optimization problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The TK Class is Dense"
        },
        {
            "text": "In these problems, the sets K will be: K \u03b3 G -the sum of N Gaussians with bandwidths \u03b3 i ; K d P -the GPKs of degree d; and K d T -the TK kernels of degree d. More precisely, for bandwidths \u03b3 \u2208 R N , we",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The TK Class is Dense"
        },
        {
            "text": "Consider a spiral data set with 20 samples, using equal numbers of positive and negative labels. Fig. 1 shows the achieved objective value of Problem (7) for K \u03b3 G , K d P , and K d T as a function of the number of bandwidths (top x axis -N in K \u03b3 G ), polynomial degree (bottom x axis -d in K d P , and K d T ). The x-axes of the plots are scaled to show equal numbers of decision variables. As expected, the case K = K G \u03b3 saturates with an objective value significantly larger than the lower bound. The cases K = K d P and K = K d T , meanwhile have almost no error at degree d = 7.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 97,
                    "end": 103,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "The TK Class is Dense"
        },
        {
            "text": "Finally we discuss the universality property of the class of TK kernels which ensures that every TK function can fit the training data well.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "TK Kernels are Universal"
        },
        {
            "text": "The following theorem from [4] shows that any TK kernel with P > 0 is necessarily universal. Theorem 10. Suppose k is as defined in Eqn. (2) for some P > 0, d \u2208 N and N as defined in Eqn. (6) . Then k is universal for a < b \u2208 R n .",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 30,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 188,
                    "end": 191,
                    "text": "(6)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "TK Kernels are Universal"
        },
        {
            "text": "This theorem implies that even if we use the subset of TK kernels defined by d = 0, this subset is still universal.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "TK Kernels are Universal"
        },
        {
            "text": "In this section, we express the KL optimization problem for both classification and regression and break this optimization problem into two sub-problems which allow us to express the problem in primal and dual form. For convenience, we define the feasible sets for the sub-problems as X := {P \u2208 R q\u00d7q : trace(P ) = q, P > 0}",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A New Algorithm for KL in Classification and Regression using TKs"
        },
        {
            "text": "while the unique parts of the objective are",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The common part of the objective is"
        },
        {
            "text": "Then the KL optimization problem (OP T ) for TK kernels ( being elementwise multiplication) is as follows for classification and regression, respectively. and strong duality holds (p * \u2212 d * = 0) if X and Y are both convex and one is compact, \u03c6(\u00b7, \u03b1) is convex for every \u03b1 \u2208 Y and \u03c6(P, \u00b7) is concave for every P \u2208 X , and the function \u03c6 is continuous [7] . Hence if OP T _A(P ) = OP T _P (\u03b1), then OP T _A(P ) = OP T _A(P * ) = OP T _P (\u03b1 * ) = OP T _P (\u03b1) and hence P and \u03b1 solve OP T _A and OP T _P , respectively. We propose Algorithm 1 as a two-step iterative algorithm for solving Optimization Problem (10).",
            "cite_spans": [
                {
                    "start": 351,
                    "end": 354,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "The common part of the objective is"
        },
        {
            "text": "For a given P > 0, OP T _A(P ) is a Quadratic Program (QP). General purpose QP solvers as applied to this problem have a worst-case complexity which scales as O(m 3 ) [25] where m is the number of data points. This computational complexity may be improved, however, by noting that the problem formulation is compatible with the representation defined in [3] for QPs derived from SVM. In this case, the algorithm in LibSVM [3] can reduce the computational burden somewhat. This improved performance is illustrated in Figure 3 where we observe the achieved complexity scales as O(m 2.1 ). Note that for the 2-step algorithm proposed in this manuscript, solving the QP in OP T _A(P ) is significantly slower that solving the Singular Value Decomposition (SVD) required for OP T _P (\u03b1), which is defined in the following subsection. However, the achieved complexity of O(m 2.1 ) is also significantly faster than solving the large SDP, as described in [12] , [16] , and [4] . This complexity comparison will be further discussed in Section 6. (c) Numerical complexity analysis of TKL for classification versus q. O(\u03b1, P ) := min",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 171,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 354,
                    "end": 357,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 422,
                    "end": 425,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 948,
                    "end": 952,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 955,
                    "end": 959,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 966,
                    "end": 969,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [
                {
                    "start": 516,
                    "end": 524,
                    "text": "Figure 3",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Solving OP T _A(P ) Algorithm 1 Two Step TKL"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solving OP T _A(P ) Algorithm 1 Two Step TKL"
        },
        {
            "text": "and g, t and h can be found in Corollary 8.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solving OP T _A(P ) Algorithm 1 Two Step TKL"
        },
        {
            "text": "The following theorem gives an analytic solution for OP T _P using the SVD. Theorem 12. Let C = V \u03a3V T be the SVD of symmetric C \u2208 R q\u00d7q and v be the right singular vector corresponding to the minimum singular value of C. Then P * = qvv T solves OP T _P . Proof. Recall OP T _P has the form min P \u2208R q\u00d7q trace(C T P ) s.t. P \u2265 0, trace(P ) = q. Denote the minimum singular value of C as \u03c3 min (C). Then for any feasible P \u2208 X , by [8] we have trace(C T P ) \u2265 \u03c3 min (C)trace(P ) = \u03c3 min (C)q. Now consider P = qvv T \u2208 R q\u00d7q . P is feasible since P \u2265 0, and trace(P ) = q. Furthermore,",
            "cite_spans": [
                {
                    "start": 431,
                    "end": 434,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Solving OP T _A(P ) Algorithm 1 Two Step TKL"
        },
        {
            "text": "as desired.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solving OP T _A(P ) Algorithm 1 Two Step TKL"
        },
        {
            "text": "Note that the size of the SVD problem in OP T _P (\u03b1) is q 2 , which increases with the number of features, which is typically relatively small. As a result, we observe that the OP T _P step of Algorithm 1 is typically less computationally intense than the OP T _A step.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solving OP T _A(P ) Algorithm 1 Two Step TKL"
        },
        {
            "text": "We consider the computational complexity of Algorithm 1. If we define the number of data points used to learn the TK kernel function as m and the size of P as q \u00d7 q, then we find experimentally that the complexity of Algorithm 1 scales as approximately O(m 2.16 q 2.23 ) for classification and O(m 2.24 q 3.59 ) for regression as can be seen in Fig. 3 . These results are lower with respect to m than the value of O(m 2.6 q 1.9 ) reported in [4] for binary classification. The values for classification and regression are both estimated using the data set: Combined Cycle Power Plant (CCPP) in [22, 11] , containing 4 features and m = 9568 samples. In the case of classification, labels with value greater than or equal to the median of the were relabeled as 1, and those less than the median were relabeled as \u22121. Note that to study scalability in q, we varied the number of features in the dataset -thereby incrementing the size of the matrix P \u2208 R q\u00d7q .",
            "cite_spans": [
                {
                    "start": 442,
                    "end": 445,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 594,
                    "end": 598,
                    "text": "[22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 599,
                    "end": 602,
                    "text": "11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 345,
                    "end": 351,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Complexity and Scalability of the New TK Kernel Learning Algorithm"
        },
        {
            "text": "Aside from improved scalability, the overall time required for Algorithm 1 is significantly reduced when compared with the algorithm in [4] , improving by two orders of magnitude in some cases. This is illustrated for classification using four data sets in Table 1 . This improved complexity is likely due to the lower overhead associated with QP and the SVD. Table 1 : We report the mean computation time (in seconds), along with standard deviation, for 30 trials comparing the SDP algorithm in [4] and the new TKL algorithm on several data sets. All tests are run on a computer with an Intel i7-5960X CPU at 3.00 GHz with 128 Gb of RAM.",
            "cite_spans": [
                {
                    "start": 136,
                    "end": 139,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 496,
                    "end": 499,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [
                {
                    "start": 257,
                    "end": 264,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 360,
                    "end": 367,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Complexity and Scalability of the New TK Kernel Learning Algorithm"
        },
        {
            "text": "Liver [6] Cancer [13] Heart [6] Pima [ 7 Accuracy of the New TK Kernel Learning Algorithm for Regression",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 9,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 17,
                    "end": 21,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 28,
                    "end": 31,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 37,
                    "end": 38,
                    "text": "[",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "As expected, for classification, the accuracy of the new TK kernel learning algorithm (TKL) is identical to the analysis in [4] .",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 127,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "For regression, we evaluate the accuracy of TKL when compared to other state of the art machine learning algorithms. Because the set of TK kernels is dense, for classification (as shown in [4] ), TKL outperforms all existing algorithm with respect to TSA. For regression, the appropriate metric is Mean Square Error (MSE). The algorithms used in our comparison are as follows.",
            "cite_spans": [
                {
                    "start": 189,
                    "end": 192,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "[TKL] Algorithm 1 with d = 1, = .1 and we scale the data so that x i \u2208 [0, 1] n , and then select [a, b] = [0 \u2212 \u03b4, 1 + \u03b4] n , where \u03b4 > 0 and C are chosen by 5-fold cross-validation;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "[SimpleMKL] We use SimpleMKL [17] with a standard selection of Gaussian and polynomial kernels with bandwidths arbitrarily chosen between .5 and 10 and polynomial degrees one through three -yielding approximately 13(n + 1) kernels. We set = .1 as in TKL and C is chosen by 5-fold cross-validation;",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "text": "[17]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "[Neural Net] We use a 3 layer neural network with 50 hidden layers using MATLABs (feedforwardnet) implementation and stopped learning after the error in a validation set decreased sequentially 50 times.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "In Table 2 , we see the average MSE on the test set for these three approaches as applied to randomly selected regression benchmark data sets where n is the dimension of the data, m is the number of training data and m t is the number of testing data points. In all cases except Forest, [TKL] had both a lower (or comparable) computation time and MSE than both SimpleMKL and Neural Net. In all cases, the MSE for TKL was significantly lower -illustrating the importance of the density property.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "text": "Table 2",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Method"
        },
        {
            "text": "To further illustrate the importance of density property and the TKL framework for practical regression problems, we used elevation data from [1] to learn a TK kernel and associated SVM predictor representing the surface of the Grand Canyon in Arizona. This data set is particularly challenging due to the variety of geographical features. The result of the TKL algorithm can be seen in Figure 2 (d).",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 145,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 387,
                    "end": 395,
                    "text": "Figure 2",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Method"
        },
        {
            "text": "We have extended the TK kernel learning framework to regression problems and proposed a faster algorithm for TK kernel learning which can be used for both classification and regression. The set of TK kernels is tractable, dense, and universal -implying that KL algorithms based on TK kernels are more robust -resulting in higher TSA for classification and lower MSE for regression. These three properties, combined with the improved computational complexity of the new algorithm, has resulted in a kernel learning framework which achieves both lower MSE and computation time when compared to both SimpleMKL and neural networks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "While machine learning algorithm have become very accurate in recent years, they perform poorly when faced with changes in the underlying process. As evidenced by Covid19, predictive models based on ML algorithms can be brittle [14] . The density property of the TK class ensures that the models generated using the algorithms described in this manuscript will be more robust to such changes in environment. Naturally, however, over-reliance on predictive models, without understanding of the process, can lead to negative outcomes, even if the models are robust.",
            "cite_spans": [
                {
                    "start": 228,
                    "end": 232,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Broader Impacts"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Global bathymetry and elevation data at 30 arc seconds resolution: Srtm30_plus",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "J"
                    ],
                    "last": "Becker",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "T"
                    ],
                    "last": "Sandwell",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "H F"
                    ],
                    "last": "Smith",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Braud",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Binder",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "L"
                    ],
                    "last": "Depner",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Fabre",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Factor",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ingalls",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Marine Geodesy",
            "volume": "32",
            "issn": "4",
            "pages": "355--371",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Hands-On Machine Learning with R",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Boehmke",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "M"
                    ],
                    "last": "Greenwell",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "LIBSVM: A library for support vector machines",
            "authors": [
                {
                    "first": "C-C",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "C-J",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "ACM Transactions on Intelligent Systems and Technology",
            "volume": "2",
            "issn": "",
            "pages": "27--28",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "A convex parametrization of a new class of universal kernel functions",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "K"
                    ],
                    "last": "Colbert",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Peet",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Machine Learning Research",
            "volume": "21",
            "issn": "45",
            "pages": "1--29",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "A data mining approach to predict forest fires using meteorological data",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cortez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Morais",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "UCI machine learning repository",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dua",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Graff",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Minimax theorems",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                }
            ],
            "year": 1953,
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "volume": "39",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Inequalities for the trace of matrix product",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "A"
                    ],
                    "last": "Loparo",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "IEEE Transactions on Automatic Control",
            "volume": "39",
            "issn": "12",
            "pages": "2489--2490",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Multiple kernel learning algorithms",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "G\u00f6nen",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Alpayd\u0131n",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Journal of Machine Learning Research",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "SPF-GMKL: generalized multiple kernel learning with a million kernels",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jain",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vishwanathan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Varma",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Local and global learning methods for predicting power of a combined gas & steam turbine",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kaya",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "T\u00fcfekci",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "S"
                    ],
                    "last": "G\u00fcrgen",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the international conference on emerging trends in computer and electronics engineering icetcee",
            "volume": "",
            "issn": "",
            "pages": "13--18",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Learning the kernel matrix with semidefinite programming",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lanckriet",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Cristianini",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bartlett",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "El"
                    ],
                    "last": "Ghaoui",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Journal of Machine Learning Research",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Pattern recognition via linear programming: Theory and application to medical diagnosis",
            "authors": [
                {
                    "first": "O",
                    "middle": [
                        "L"
                    ],
                    "last": "Mangasarian",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Setiono",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "H"
                    ],
                    "last": "Wolberg",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "AI isn't magical and won't help you reopen your business",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Mims",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Wall Street Journal",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Learning the kernel matrix for superresolution",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ni",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "IEEE Workshop on Multimedia Signal Processing",
            "volume": "",
            "issn": "",
            "pages": "441--446",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Multiple kernel learning for support vector regression",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Qiu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lane",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Convex Modeling with Priors",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Recht",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "A generalized representer theorem",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Herbrich",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "International conference on computational learning theory",
            "volume": "",
            "issn": "",
            "pages": "416--426",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "A tutorial on support vector regression",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Statistics and computing",
            "volume": "14",
            "issn": "3",
            "pages": "199--222",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "The shogun machine learning toolbox",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sonnenburg",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "R\u00e4tsch",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Henschel",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Widmer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Behr",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zien",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "De",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bona",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Binder",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Gehl",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Franc",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Journal of Machine Learning Research",
            "volume": "11",
            "issn": "60",
            "pages": "1799--1802",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "T\u00fcfekci",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International Journal of Electrical Power & Energy Systems",
            "volume": "60",
            "issn": "",
            "pages": "126--140",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Simple and efficient multiple kernel learning by group lasso",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "King",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Lyu",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 27th international conference on machine learning",
            "volume": "",
            "issn": "",
            "pages": "1175--1182",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Efficient sparse generalized multiple kernel learning",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "King",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Lyu",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE Transactions on neural networks",
            "volume": "22",
            "issn": "3",
            "pages": "433--446",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "An extension of karmarkar's projective algorithm for convex quadratic programming. Mathematical programming",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Tse",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "",
            "volume": "44",
            "issn": "",
            "pages": "157--179",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The achieved objective ( K \u2212 K * \u221e ) of Optimization Problem 7 for TK and GPK of degree d; and m Gaussians with bandwidths in [.01, 10]. The number of bandwidths is selected so that the number of decision variables match in the Gaussian and TK cases.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "y, P ) + \u03ba c (\u03b1), and OP T := min P \u2208X max \u03b1\u2208Yr O(\u03b1, P ) + \u03ba r (\u03b1). Primal Formulation: We can formulate the primal problem (OP T P ) as OP T P = min P \u2208X max \u03b1\u2208Y O(\u03b1, P ) + \u03ba r (\u03b1) or O(\u03b1 y, P ) + \u03ba c (\u03b1) = min P \u2208X OP T _A(P ) (9) where for classification and regression, respectively, OP T _A(P ) := max \u03b1\u2208Yc O(\u03b1 y, P ) + \u03ba c (\u03b1), and OP T _A(P ) := max \u03b1\u2208Yr O(\u03b1, P ) + \u03ba r (\u03b1).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "kernel trained on the elevation data in (b). The Gaussian predictor poorly represents the sharp edge at the north and south rim.(d) Predictor from Algorithm 1 trained on the elevation data in (b). The TK predictor accurately represent the north and south rims of the canyon.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Subfigure (a) shows an 3D representation of the section of the Grand Canyon to be fitted. In (b) we plot elevation data of this section of the Grand Canyon. In (c) we plot the predictor for a hand-tuned Gaussian kernel. In (d) we plot the predictor from Algorithm 1 where d = 2.Dual Formulation: Alternatively, we have the dual formulation (OP T D ).OP T D = max \u03b1\u2208Y OP T _P (\u03b1)(10)where Y = Y c for classification and Y = Y r regression. Likewise, for classification and regression, respectively, OP T _P (\u03b1) := min P \u2208X O(\u03b1 y, P ) + \u03ba c (\u03b1) and OP T _P (\u03b1) := min P \u2208X O(\u03b1, P ) + \u03ba r (\u03b1). Lemma 11. For \u03b1 \u2208 Y, P \u2208 X , OP T _A(P ) = OP T _P (\u03b1) if and only if: {\u03b1, P } solve OP T ; P solves OP T P ; and \u03b1 solves OP T D . Proof. For any minmax optimization problem with objective function \u03c6, we have d * = max \u03b1\u2208Y min P \u2208X \u03c6(P, \u03b1) \u2264 min P \u2208X max \u03b1\u2208Y \u03c6(P, \u03b1) = p * ,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "In our case, these conditions hold for both classification and regression where \u03c6(P, \u03b1) = O(\u03b1, P ) + \u03ba r (\u03b1) or O(\u03b1 y, P ) + \u03ba c (\u03b1). Hence if \u03b1 * solves OP T _P and P * solves OP T _A, then {\u03b1 * , P * } solves OP T and OP T _P (\u03b1 * ) = max \u03b1\u2208Y OP T _P (\u03b1) = min P \u2208X OP T _A(P ) = OP T _A(P * ). Conversely, suppose \u03b1 \u2208 Y, P \u2208 X , then OP T _P (\u03b1) \u2264 max \u03b1\u2208Y OP T _P (\u03b1) = OP T _P (\u03b1 * ) = OP T _A(P * ) = min P \u2208X OP T _A(P ) \u2264 OP T _A(P ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "In (a) and (b) we plot log scale plots of the time taken to optimize TKL as the number of inputs change for P \u2208 R q\u00d7q . The line of best linear fit is plotted as a dotted line. In (c) and (d) we plot log scale plots of the time taken to optimize TKL as the value of q changes for four different values of m.5.2 Solving OP T _P (\u03b1)For a given \u03b1, OP T _P (\u03b1) is an SDP. Fortunately, however, this SDP is structured so as to admit an analytic solution using the SVD. To solve OP T _P (\u03b1) we minimize O(\u03b1, P ) from Eq. (8) which, as per Corollary 8, is linear in P and can be formulated as OP T _P (\u03b1) := min P \u2208R q\u00d7q trace(P )=q P >0",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Mean Square Error comparison for algorithms [TKL], [SimpleMKL] and [Neural Net]. In the data set column m is the number of points in the training data set and n is the number of features. All tests are run on a computer with an Intel i7-5960X CPU at 3.00 GHz with 128 Gb of RAM.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
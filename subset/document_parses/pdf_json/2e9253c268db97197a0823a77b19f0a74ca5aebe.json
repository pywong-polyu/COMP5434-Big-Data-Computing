{"paper_id": "2e9253c268db97197a0823a77b19f0a74ca5aebe", "metadata": {"title": "Demand Forecasting of Short Life Cycle Products Using Data Mining Techniques", "authors": [{"first": "Ashraf", "middle": ["A"], "last": "Afifi", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of the West of England", "location": {"settlement": "Bristol", "country": "UK"}}, "email": "ashraf.afifi@uwe.ac.uk"}]}, "abstract": [{"text": "Products with short life cycles are becoming increasingly common in many industries due to higher levels of competition, shorter product development time and increased product diversity. Accurate demand forecasting of such products is crucial as it plays an important role in driving efficient business operations and achieving a sustainable competitive advantage. Traditional forecasting methods are inappropriate for this type of products due to the highly uncertain and volatile demand and the lack of historical sales data. It is therefore critical to develop different forecasting methods to analyse the demand trend of these products. This paper proposes a new data mining approach based on the incremental k-means clustering algorithm and the RULES-6 rule induction classifier for forecasting the demand of short life cycle products. The performance of the proposed approach is evaluated using real data from one of the leading Egyptian companies in IT ecommerce and retail business, and results show that it has the capability to accurately forecast demand trends of new products with no historical sales data.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Forecasting is an important and necessary tool that helps managers make decisions about future resourcing of their organisations. It plays a pivotal role in the effective planning of various operations in an organisation including production, inventory, budget, sales, personnel and facilities. Accurate forecasts can lead to significant cost savings, reduced inventory levels, improved customer satisfaction and increased competitiveness.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "The fast pace of new product introduction continually drives product life cycles shorter. Products with life cycles of few weeks to few months are very common in fashion (e.g., toys and clothing) and high technology (e.g., mobile phones, computers and consumer electronics) retail industries. A typical demand pattern for such products is characterised by rapid growth, maturity, and decline phases [1] . The demand is also highly uncertain and volatile, particularly in the introduction stage. An additional problem is the inadequacy of historical data due to the short period of sales. In case of new products, there is complete unavailability of any previous data related to the sales of such products. These characteristics make it difficult to forecast the demand of short life cycle products.", "cite_spans": [{"start": 399, "end": 402, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Introduction"}, {"text": "Traditionally, demand forecasting is accomplished by statistical methods such as moving averages, exponential smoothing, Bayesian analysis, regression models, Holt-Winters and Box-Jenkins methods, and autoregressive integrated moving average (ARIMA) models [2] . Statistical methods are popular because of their simplicity and fast speed, and they provide satisfactory results in many forecasting applications. However, these methods are not designed for application in the short life cycle environment, especially most of them require large historical data to ensure accurate estimation of their parameters and they are limited to linear relations [3, 4] .", "cite_spans": [{"start": 257, "end": 260, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 649, "end": 652, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 653, "end": 655, "text": "4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Introduction"}, {"text": "Artificial intelligence (AI) methods such as neural networks, evolutionary algorithms, support vector machines, and fuzzy inference systems are widely used in forecasting activities [5, 6] . AI methods cope well with complexity and uncertainty, and they have better forecasting accuracy compared to statistical methods. However, they usually require a long computational time which makes them less appealing to the fast changing market of fashion and high technology products [7, 8] .", "cite_spans": [{"start": 182, "end": 185, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 186, "end": 188, "text": "6]", "ref_id": "BIBREF5"}, {"start": 476, "end": 479, "text": "[7,", "ref_id": "BIBREF6"}, {"start": 480, "end": 482, "text": "8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Introduction"}, {"text": "Recently, various hybrid methods such as neural fuzzy systems are proposed in the literature to enhance demand forecasting [9, 10] . Hybrid methods utilise the strengths of different models to form a new forecasting method. They can learn complex relations in an uncertain environment and many of them are considered to be more accurate and efficient than the pure statistical and AI models [11] .", "cite_spans": [{"start": 123, "end": 126, "text": "[9,", "ref_id": "BIBREF8"}, {"start": 127, "end": 130, "text": "10]", "ref_id": "BIBREF9"}, {"start": 391, "end": 395, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Introduction"}, {"text": "In an attempt to cope with the lack of historical data of short life cycle products, some references use the data of similar products for which sufficient history is available to forecast the demand of new products [12] [13] [14] [15] [16] . For example, a hybrid method based on the k-means clustering technique and a decision tree classifier was developed to estimate the demand of textile fashion products [12] . A similar approach that uses the self-organizing map and the neural networks techniques was introduced in [13] . This paper reports on a new forecasting method using alternative data mining techniques to improve demand forecasting in the context of a large retail company. In particular, the incremental kmeans clustering technique [17] and the RULES-6 classification rule learning algorithm [18] are employed. These methods are simple, effective and computationally efficient, which make them powerful and practical tools for retail sales forecasting.", "cite_spans": [{"start": 215, "end": 219, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 220, "end": 224, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 225, "end": 229, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 230, "end": 234, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 235, "end": 239, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 409, "end": 413, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 522, "end": 526, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 748, "end": 752, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 808, "end": 812, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Introduction"}, {"text": "The outline of this paper is as follow. Section 2 reviews the basic concepts of data mining with particular focus on clustering and rule induction methods. Section 3 presents the proposed forecasting approach. Section 4 reports and analyses the experimental results obtained with a real retail data. Section 5 concludes the paper and provides suggestions for future work.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Data mining, or knowledge discovery in databases (KDD), aims at processing large data into knowledge bases for better decision making. It has been successfully used in many applications to uncover hidden patterns and predict future trends and behaviours. There are three main steps in data mining, namely, data preparation, data modelling, and post processing and model evaluation [19] [20] [21] . This section gives a brief description of the data modelling techniques employed in this research.", "cite_spans": [{"start": 381, "end": 385, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 386, "end": 390, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 391, "end": 395, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Overview of Data Mining"}, {"text": "Clustering techniques are concerned with partitioning of data sets into several homogeneous clusters. These techniques assign a large number of data objects to a relatively small number of groups so that data objects in a group share the same properties while, in different groups, they are dissimilar. Many clustering techniques have been proposed over the years from different research disciplines [22, 23] . K-means is one of the best known and commonly used clustering algorithms. The algorithm forms k clusters that are represented by the mean value of the data points belonging to each of them. This is an iterative process that searches for a division of data objects into k clusters to minimise the sum of Euclidean distances between each object and its closest cluster centre.", "cite_spans": [{"start": 400, "end": 404, "text": "[22,", "ref_id": "BIBREF21"}, {"start": 405, "end": 408, "text": "23]", "ref_id": "BIBREF22"}], "ref_spans": [], "section": "Clustering Techniques"}, {"text": "The k-means algorithm is relatively scalable and efficient in clustering large data sets because its computational complexity grows linearly with the number of data objects. However, it is sensitive to the initial selection of cluster centres and requires the number of clusters k to be specified before the clustering process starts. The algorithm has been improved to address many of its deficiencies [17, 24, 25] . In particular, a new version called incremental k-means was introduced to reduce the dependence of the k-means algorithm on the initialisation of cluster centres [17] . To validate the robustness of the new algorithm it has been tested on a number of artificial and real datasets. The results showed clearly that incremental k-means consistently outperforms the original algorithm [17] . Therefore, this algorithm is applied in this research to search for interesting and natural clusters in the retail data.", "cite_spans": [{"start": 403, "end": 407, "text": "[17,", "ref_id": "BIBREF16"}, {"start": 408, "end": 411, "text": "24,", "ref_id": "BIBREF23"}, {"start": 412, "end": 415, "text": "25]", "ref_id": "BIBREF24"}, {"start": 580, "end": 584, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 799, "end": 803, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Clustering Techniques"}, {"text": "The incremental k-means algorithm is summarised in Fig. 1 . The algorithm starts initially with one cluster with the number of clusters k being incremented by 1 at each step thereafter. With each increase of k, a new cluster centre is inserted into the cluster with the highest distortion, and the objects are reassigned to clusters until the centres stop \"moving\". The process is repeated until k reaches the specified number of clusters.", "cite_spans": [], "ref_spans": [{"start": 51, "end": 57, "text": "Fig. 1", "ref_id": "FIGREF0"}], "section": "Clustering Techniques"}, {"text": "Classification learning employs a set of pre-categorised data objects to develop a model that can be used to classify new data objects from the same population or to provide a better understanding of the data objects' characteristics. Among the various classification learning techniques developed, inductive learning may be the most commonly used in real world applications [26] . The inductive learning techniques are relatively fast compared to other techniques. Another advantage is that they are simpler and the models that they generate are easier to understand.", "cite_spans": [{"start": 375, "end": 379, "text": "[26]", "ref_id": "BIBREF25"}], "ref_spans": [], "section": "Classification Learning Techniques"}, {"text": "Assign k = 1.", "cite_spans": [], "ref_spans": [], "section": "Classification Learning Techniques"}, {"text": "Phase 1: Normal training", "cite_spans": [], "ref_spans": [], "section": "Classification Learning Techniques"}, {"text": "Step 1: If k = 1, choose an arbitrary point for a cluster centre. If k > 1, insert the centre of the new cluster in the cluster with the greatest distortion.", "cite_spans": [], "ref_spans": [], "section": "Classification Learning Techniques"}, {"text": "Step 2: Assign each object in the training set to the closest cluster and update its centre.", "cite_spans": [], "ref_spans": [], "section": "Classification Learning Techniques"}, {"text": "Step 3: If the cluster centre does not move, go to Phase 2.", "cite_spans": [], "ref_spans": [], "section": "Classification Learning Techniques"}, {"text": "Else, go to Phase 1, Step 2.", "cite_spans": [], "ref_spans": [], "section": "Classification Learning Techniques"}, {"text": "If k is smaller than a specified value, increase k by 1 and go to Phase 1 -Step 1.", "cite_spans": [], "ref_spans": [], "section": "Phase 2: Increasing the number of clusters"}, {"text": "Else, stop. In RULES-6, an attribute-value pair constitutes a condition. If the number of attributes is N a , a rule may contain between one and N a conditions, each of which must be a different attribute-value pair. Only conjunction of conditions is permitted in a rule and hence the attributes must all be different if the rule comprises more than one condition.", "cite_spans": [], "ref_spans": [], "section": "Phase 2: Increasing the number of clusters"}, {"text": "RULES-6 works in an iterative fashion. In each iteration, it takes a \"seed\" example not covered by previously created rules to form a new rule. Having found a rule, RULES-6 marks those examples that are covered by it and appends the new rule to its rule set. The algorithm stops when all examples in the training set are covered. In order to avoid the overfitting problem and to generate simple rules, the rules are pruned with a post pruning strategy [27] . A simplified description of RULES-6 is given in Fig. 2 .", "cite_spans": [{"start": 452, "end": 456, "text": "[27]", "ref_id": "BIBREF26"}], "ref_spans": [{"start": 507, "end": 513, "text": "Fig. 2", "ref_id": "FIGREF1"}], "section": "Phase 2: Increasing the number of clusters"}, {"text": "To form a rule, RULES-6 performs a general-to-specific beam search for a rule that optimises a given quality criterion. It starts with the most general rule and specialises it in steps considering only conditions extractable from the selected seed example. The aim of specialisation is to construct a rule that covers as many examples from the target class and as few examples from the other classes as possible, while ensuring that the seed example remains covered. As a consequence, simpler rules that are not consistent, but are more accurate for unseen data, can be learned. RULES-6 uses effective searchspace pruning rules to avoid useless specialisations and to terminate a non-productive search during rule construction. This substantially increases the efficiency of the learning process. It deals with continuous-value attributes using a pre-processing discretisation method. With this method, the range of each attribute is split into a number of smaller intervals that are then regarded as nominal values. A detailed description of the process by which RULES-6 induces a rule can be found in [18] .", "cite_spans": [{"start": 1103, "end": 1107, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Phase 2: Increasing the number of clusters"}, {"text": "In this paper, it is proposed to forecast the demand of short life cycle products by applying data mining techniques. This forecasting approach includes the following steps (Fig. 3 ):", "cite_spans": [], "ref_spans": [{"start": 173, "end": 180, "text": "(Fig. 3", "ref_id": null}], "section": "Proposed Forecasting Approach"}, {"text": "1. Preparing and transforming the available sales data into a format suitable for data mining techniques. 2. Using the incremental k-means algorithm to identify the sales profiles of historical products by grouping together the related time series of the products sales. 3. Applying the RULES-6 algorithm to build a classification model that links the descriptive attributes of the historical products and the sales profiles of the discovered groups in Step 2. 4. Using the classification model generated in Step 3 to forecast the sales profiles of the new products based on their descriptive attributes. This section discusses the techniques applied in this research to realise the steps of the proposed approach.", "cite_spans": [], "ref_spans": [], "section": "Proposed Forecasting Approach"}, {"text": "The data used in this research was obtained from a large retail Egyptian company called 2B that has 37 branches all over Egypt. The company sells a wide range of high technology products (e.g., laptops, mobiles, tablets, gaming, networking, cables, software, laptop and mobile accessories) that have a short life cycle ranging from a few weeks to a few months. Two sets of data were extracted. The first contains 507 products corresponding to year 2015 and it is used for the learning process. The second is composed of 253 products for year 2016 and it is used for testing. The data is available weekly and covers a period of 26 weeks. The selected descriptive attributes of the products are the price, the starting date of the sales and the life span of each product.", "cite_spans": [], "ref_spans": [], "section": "Data Preparation"}, {"text": "High quality data is a prerequisite for applying any data mining technique [28] . Prior to data modelling, the data needs to be prepared. The objective at this stage is two-fold: to convert the data into a format required by the data mining algorithms and also to expose as much information as possible to data modelling. To prepare the available data for further analysis the following pre-processing steps are implemented.", "cite_spans": [{"start": 75, "end": 79, "text": "[28]", "ref_id": "BIBREF27"}], "ref_spans": [], "section": "Data Preparation"}, {"text": "1. Aggregate the sales for each product into weekly time buckets in order to cope with high uncertainty and volatility in sales patterns. 2. Filter the data by removing the negative sales numbers for customer returns. 3. Normalise the sales volume of all products during the sales period to enable comparison between the time series of products having different sales volumes. The normalised sales at period i, y i , is computed by dividing the sale at period i, x i , by the sum of sales during the sales period: y i = x i / n k=1 x k x k where n is the number of periods. 4 . Normalise the life span of all products to 26 weeks to allow comparison between the time series of products having different life spans. Normalisation of the life span is computed using homothetic transformation [12] . 5. Organise the time series sales and descriptive attributes of the products into a data object format that is suitable for data mining algorithms (examples of the structure of the training and validation data sets created in this way are shown in Fig. 3 ).", "cite_spans": [{"start": 574, "end": 575, "text": "4", "ref_id": "BIBREF3"}, {"start": 790, "end": 794, "text": "[12]", "ref_id": "BIBREF11"}], "ref_spans": [{"start": 1045, "end": 1051, "text": "Fig. 3", "ref_id": null}], "section": "Data Preparation"}, {"text": "This section discusses the data mining techniques that were used to analyse the retail data. The analysis is performed in two stages. First, the time series sales of historical products are grouped into several homogeneous clusters. Second, the sales profiles of the new products are predicted by mapping them to the clusters using the descriptive attributes.", "cite_spans": [], "ref_spans": [], "section": "Data Modelling"}, {"text": "The incremental k-means clustering algorithm is applied on the pre-processed retail data to discover groups of products with similar time series sales. Each cluster centre defines a sales profile characterising the sales behaviour of the products belonging to the cluster. The distortion error was used to evaluate the clustering results [17] , with a lower value of this measure indicative of better quality of clustering. The incremental k-means algorithm requires users to specify a number of parameters, namely, the number of clusters and the termination conditions for stopping the clustering process. To find a satisfactory clustering result, a number of iterations were conducted where the algorithm was executed with different values of k, the number of clusters, and the k value producing the lowest distortion error was selected [17] . In this work, k values in the range of 2 to \u221a n, where n is the size of the training data set [29] , were considered and the optimal value was found to be 13. The clustering process could be stopped by specifying termination conditions such as a predefined number of iterations and the percentage reduction of the distortion errors in one iteration being smaller than a given value \u03b5. In this work, these two termination criteria were used. In particular, the maximum number of iterations was set to 50 and \u03b5 to 10 \u22127 to stop the search process when one of these conditions is satisfied.", "cite_spans": [{"start": 338, "end": 342, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 839, "end": 843, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 940, "end": 944, "text": "[29]", "ref_id": "BIBREF28"}], "ref_spans": [], "section": "Sales Profiles Identification for Historical Products."}, {"text": "After clustering the time series data of the historical products, labels such as SP 1 , SP 2 , \u2026, SP l , \u2026, SP q representing the sales profiles of the formed groups, are assigned to the discovered clusters. A training set for the classification algorithm is then constructed using the descriptive attributes and the clusters labels. Each product is considered as a data point in the training set and it is described by the descriptive attributes as inputs and the label of the cluster to which it belongs is taken as its output.", "cite_spans": [], "ref_spans": [], "section": "Sales Profiles Prediction for New Products."}, {"text": "The RULES-6 algorithm is used to extract if-then rules from the training data set. These rules provide a comprehensive insight into the data and are used to predict the sales profiles of the new products based on their descriptive attributes. RULES-6 has a number of parameters whose values determine the quality of the induced rule sets. In this research, the default settings were used [18] .", "cite_spans": [{"start": 388, "end": 392, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Sales Profiles Prediction for New Products."}, {"text": "The incremental k-means algorithm was applied to the training time series data and thirteen distinct clusters were created. Figure 4 shows four examples of the clustering results that have the maximum distortion errors. As can be seen in the figure, the incremental k-means clustering procedure is effective in producing accurate groups of products with similar time series sales. The sales behaviour of the products belonging to the different clusters are accurately described by the associated sales profiles (indicated in bold). It can then be concluded that a robust identification of the sales profiles has been achieved. ", "cite_spans": [], "ref_spans": [{"start": 124, "end": 132, "text": "Figure 4", "ref_id": "FIGREF2"}], "section": "Clustering Results"}, {"text": "The RULES-6 algorithm was applied on the classification training data created from the descriptive attributes and the discovered sales profiles. Table 1 illustrates the produced set of rules to describe the training data. It is clear from the figure that the number of rules generated is significantly lower than the number of data points in the training set. Also, the number of features describing each rule is drastically reduced. The generated rule set is a compressed description of the training data that could be used to predict the sales profiles of new products. To evaluate its performance, the RULES-6 algorithm was applied on the validation data to predict the sales profiles of the new products. Figure 5 shows a comparison between the actual sales of the new products and the sales profiles predicted by the RULES-6 algorithm. As can be seen in the figure, RULES-6 assigns the correct profiles to most products. The actual sales associated with profile SP9 of cluster 9 are quite different in some weeks. Also, the actual sales accompanied with profile SP8 of cluster 8 are relatively similar but are slightly different from the predicted sales profile. This could arise from the failure of the descriptive attributes used to build the rule set to explain the sales behaviour of these products.", "cite_spans": [], "ref_spans": [{"start": 145, "end": 152, "text": "Table 1", "ref_id": "TABREF1"}, {"start": 709, "end": 717, "text": "Figure 5", "ref_id": "FIGREF4"}], "section": "Rule Induction Results"}, {"text": "To further test its performance, the prediction accuracy of the RULES-6 algorithm was compared with that of five different well-known classifiers. The classification methods are the OneR algorithm [30] , the Na\u00efve Bayes method [31] , the k-nearest-neighbours classifier [32] , the RIPPER rule induction classifier [33] , and the C4.5 decision tree [34] . These methods are representative of the different classification techniques and widely used in other comparative studies. Each of the classification methods was first applied on the classification training data and the generated classification models were then tested on the validation data to predict the sales profiles of the new products. The default parameters of the tested methods were used. The forecasting errors of the predicted sales profiles were analysed using three of the most popular measures, namely, Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and Root Mean Squared Error (RMSE) [35] .", "cite_spans": [{"start": 197, "end": 201, "text": "[30]", "ref_id": "BIBREF29"}, {"start": 227, "end": 231, "text": "[31]", "ref_id": "BIBREF30"}, {"start": 270, "end": 274, "text": "[32]", "ref_id": "BIBREF31"}, {"start": 314, "end": 318, "text": "[33]", "ref_id": "BIBREF32"}, {"start": 348, "end": 352, "text": "[34]", "ref_id": "BIBREF33"}, {"start": 973, "end": 977, "text": "[35]", "ref_id": "BIBREF34"}], "ref_spans": [], "section": "Rule Induction Results"}, {"text": "The forecasting error values of the predicted sales profiles for the different classification methods are given in Table 2 . As shown in the table, RULES-6 is the most accurate algorithm. It achieved the smallest error values for the three measures and the values were lower by 39.2% for the MAE, 21.7% for the MAPE and 43.4% for the RMSE compared to the next best performing classifier. It could therefore be concluded that the proposed RULES-6 algorithm gives the best performance among the six methods tested. ", "cite_spans": [], "ref_spans": [{"start": 115, "end": 122, "text": "Table 2", "ref_id": "TABREF2"}], "section": "Rule Induction Results"}, {"text": "The constraints of the retail market make the forecasting of products sales very challenging. This paper has proposed a forecasting approach based on the incremental k-means clustering algorithm and the RULES-6 rule induction classifier to estimate the sales profiles of new products. The performance of the proposed forecasting procedure has been tested on real sales data of a large retail Egyptian company specialised in selling and distribution of high tech products. The incremental k-means algorithm employed in this study has proved to be effective in discovering interesting groupings of historical products, from which general descriptions can be derived by applying the RULES-6 algorithm. From the clusters and their descriptions, sales profiles for new products could be predicted easily. However, some inaccurate profiles could result from the inadequacy of the chosen descriptive attributes to discriminate the sales behaviours of all new products. The RULES-6 algorithm has also been tested against five other state-of-the-art classifiers and attained more accurate forecasts. These results suggest that the combined application of the incremental k-means and RULES-6 techniques is a valid approach for estimating the sales profiles of new products in retail businesses where historical sales data are not available. Further research could be conducted to apply the proposed forecasting procedure to other short life cycle products and to compare the RULES-6 algorithm with other classifiers such as decision trees, neural networks, or genetic algorithms. Also, considering the various sources of uncertainty that arise in the retail market, it would be interesting to use fuzzy learning techniques which may help to produce more accurate forecasts.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and Future Work"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "A survey on short life cycle time series forecasting", "authors": [{"first": "S", "middle": [], "last": "Kadam", "suffix": ""}, {"first": "D", "middle": [], "last": "Apte", "suffix": ""}], "year": 2015, "venue": "Int. J. Appl. Innov. Eng. Manag", "volume": "4", "issn": "5", "pages": "445--449", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Principles of Forecasting -A Handbook for Researchers and Practitioners", "authors": [{"first": "J", "middle": ["S"], "last": "Armstrong", "suffix": ""}], "year": 2001, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "A seasonal discrete grey forecasting model for fashion retailing", "authors": [{"first": "M", "middle": [], "last": "Xia", "suffix": ""}, {"first": "W", "middle": ["K"], "last": "Wong", "suffix": ""}], "year": 2014, "venue": "Knowl.-Based Syst", "volume": "57", "issn": "", "pages": "119--126", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "A survey on retail sales forecasting and prediction in fashion markets", "authors": [{"first": "S", "middle": [], "last": "Beheshti-Kashia", "suffix": ""}, {"first": "H", "middle": ["R"], "last": "Karimic", "suffix": ""}, {"first": "K.-D", "middle": [], "last": "Thobenb", "suffix": ""}, {"first": "M", "middle": [], "last": "L\u00fctjenb", "suffix": ""}, {"first": "M", "middle": [], "last": "Teuckeb", "suffix": ""}], "year": 2015, "venue": "Syst. Sci. Control Eng", "volume": "3", "issn": "", "pages": "154--161", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Demand forecasting in the fashion industry a review", "authors": [{"first": "L", "middle": [], "last": "Giustiniano", "suffix": ""}, {"first": "M", "middle": ["E"], "last": "Nenni", "suffix": ""}, {"first": "L", "middle": [], "last": "Pirolo", "suffix": ""}], "year": 2013, "venue": "Int. J. Eng. Bus. Manag", "volume": "5", "issn": "37", "pages": "", "other_ids": {"DOI": ["10.5772/56840"]}}, "BIBREF5": {"ref_id": "b5", "title": "Sales forecasting for fashion retailing service industry: a review", "authors": [{"first": "N", "middle": [], "last": "Liu", "suffix": ""}, {"first": "S", "middle": [], "last": "Ren", "suffix": ""}, {"first": "T.-M", "middle": [], "last": "Choi", "suffix": ""}, {"first": "C.-L", "middle": [], "last": "Hui", "suffix": ""}, {"first": "S.-F", "middle": [], "last": "Ng", "suffix": ""}], "year": 2013, "venue": "Math. Probl. Eng", "volume": "4", "issn": "", "pages": "1--9", "other_ids": {"DOI": ["10.1155/2013/738675"]}}, "BIBREF6": {"ref_id": "b6", "title": "Fast fashion sales forecasting with limited data and time", "authors": [{"first": "T", "middle": ["M"], "last": "Choi", "suffix": ""}, {"first": "C.-L", "middle": [], "last": "Hui", "suffix": ""}, {"first": "N", "middle": [], "last": "Liu", "suffix": ""}, {"first": "S.-F", "middle": [], "last": "Ng", "suffix": ""}, {"first": "Y", "middle": [], "last": "Yu", "suffix": ""}], "year": 2014, "venue": "Decis. Support Syst", "volume": "59", "issn": "", "pages": "84--92", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Adaptive and hybrid forecasting models-a review", "authors": [{"first": "C", "middle": ["H"], "last": "Fajardo-Toro", "suffix": ""}, {"first": "J", "middle": [], "last": "Mula", "suffix": ""}, {"first": "R", "middle": [], "last": "Poler", "suffix": ""}, {"first": "\u00c1", "middle": [], "last": "Ortiz", "suffix": ""}, {"first": "C", "middle": [], "last": "Andr\u00e9s Romano", "suffix": ""}, {"first": "R", "middle": [], "last": "Poler", "suffix": ""}, {"first": "", "middle": [], "last": "Garc\u00eda-Sabater", "suffix": ""}], "year": 2019, "venue": "Engineering Digital Transformation. LNMIE", "volume": "", "issn": "", "pages": "315--322", "other_ids": {"DOI": ["10.1007/978-3-319-96005-0_38"]}}, "BIBREF8": {"ref_id": "b8", "title": "A hybrid SARIMA wavelet transform method for sales forecasting", "authors": [{"first": "T.-M", "middle": [], "last": "Choi", "suffix": ""}, {"first": "Y", "middle": [], "last": "Yu", "suffix": ""}, {"first": "K.-F", "middle": [], "last": "Au", "suffix": ""}], "year": 2011, "venue": "Decis. Support Syst", "volume": "51", "issn": "", "pages": "130--140", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Sales forecasts in clothing industry: the key success factor of the supply chain management", "authors": [{"first": "S", "middle": [], "last": "Thomassey", "suffix": ""}], "year": 2012, "venue": "Int. J. Prod. Econ", "volume": "128", "issn": "", "pages": "470--483", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Intelligent Fashion Forecasting Systems: Models and Applications", "authors": [{"first": "T.-M", "middle": [], "last": "Choi", "suffix": ""}, {"first": "C.-L", "middle": [], "last": "Hui", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1007/978-3-642-39869-8"]}}, "BIBREF11": {"ref_id": "b11", "title": "A hybrid sales forecasting system based on clustering and decision trees", "authors": [{"first": "S", "middle": [], "last": "Thomassey", "suffix": ""}, {"first": "A", "middle": [], "last": "Fiordaliso", "suffix": ""}], "year": 2006, "venue": "Decis. Support Syst", "volume": "42", "issn": "1", "pages": "408--421", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "A neural clustering and classification system for sales forecasting of new apparel items", "authors": [{"first": "S", "middle": [], "last": "Thomassey", "suffix": ""}, {"first": "M", "middle": [], "last": "Happiette", "suffix": ""}], "year": 2007, "venue": "Appl. Soft Comput", "volume": "7", "issn": "4", "pages": "1177--1187", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Improving short-term demand forecasting for short-lifecycle consumer products with data mining techniques", "authors": [{"first": "D", "middle": [], "last": "Maa\u00df", "suffix": ""}, {"first": "M", "middle": [], "last": "Spruit", "suffix": ""}, {"first": "P", "middle": [], "last": "De Waal", "suffix": ""}], "year": 2014, "venue": "Decis. Anal", "volume": "1", "issn": "", "pages": "1--17", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Demand forecasting procedure for short life-cycle products with an actual food processing enterprise", "authors": [{"first": "R", "middle": [], "last": "Gaku", "suffix": ""}], "year": 2014, "venue": "Int. J. Comput. Intell. Syst", "volume": "7", "issn": "", "pages": "85--92", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Analogue-based demand forecasting of short life-cycle products: a regression approach and a comprehensive assessment", "authors": [{"first": "M", "middle": [], "last": "Basallo", "suffix": ""}, {"first": "J", "middle": [], "last": "Rodr\u00edguez-Sarasty", "suffix": ""}, {"first": "H", "middle": [], "last": "Benitez-Restrepo", "suffix": ""}], "year": 2016, "venue": "Int. J. Prod. Res", "volume": "55", "issn": "", "pages": "1--15", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "An incremental k-means algorithm", "authors": [{"first": "D", "middle": ["T"], "last": "Pham", "suffix": ""}, {"first": "S", "middle": ["S"], "last": "Dimov", "suffix": ""}, {"first": "C", "middle": ["D"], "last": "Nguyen", "suffix": ""}], "year": 2004, "venue": "Proc. Inst. Mech. Eng. Part C: J. Mech. Eng. Sci", "volume": "218", "issn": "7", "pages": "783--795", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "RULES-6: a simple rule induction algorithm for handling large data sets", "authors": [{"first": "D", "middle": ["T"], "last": "Pham", "suffix": ""}, {"first": "A", "middle": ["A"], "last": "Afify", "suffix": ""}], "year": 2005, "venue": "Proc. Inst. Mech. Eng. Part C: J. Mech. Eng. Sci", "volume": "219", "issn": "10", "pages": "1119--1137", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations", "authors": [{"first": "I", "middle": ["H"], "last": "Witten", "suffix": ""}, {"first": "E", "middle": [], "last": "Frank", "suffix": ""}], "year": 2000, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Data Mining Concepts and Techniques", "authors": [{"first": "J", "middle": [], "last": "Han", "suffix": ""}, {"first": "M", "middle": [], "last": "Kamber", "suffix": ""}], "year": 2001, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Handbook of Data Mining and Knowledge Discovery", "authors": [{"first": "W", "middle": [], "last": "Kl\u00f6sgen", "suffix": ""}, {"first": "J", "middle": ["M"], "last": "\u017bytkow", "suffix": ""}], "year": 2002, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "Finding Groups in Data: An Introduction to Cluster Analysis", "authors": [{"first": "L", "middle": [], "last": "Kaufman", "suffix": ""}, {"first": "P", "middle": ["J"], "last": "Rousseeuw", "suffix": ""}], "year": 2005, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "Clustering techniques and their applications in engineering", "authors": [{"first": "D", "middle": ["T"], "last": "Pham", "suffix": ""}, {"first": "A", "middle": ["A"], "last": "Afify", "suffix": ""}], "year": 2007, "venue": "Proc. Inst. Mech. Eng. Part C: J. Mech. Eng. Sci", "volume": "221", "issn": "11", "pages": "1445--1459", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "A two-phase k-means algorithm for large datasets", "authors": [{"first": "D", "middle": ["T"], "last": "Pham", "suffix": ""}, {"first": "S", "middle": ["S"], "last": "Dimov", "suffix": ""}, {"first": "C", "middle": ["D"], "last": "Nguyen", "suffix": ""}], "year": 2004, "venue": "Proc. Inst. Mech. Eng. Part C: J. Mech. Eng. Sci", "volume": "218", "issn": "10", "pages": "1269--1273", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "Selection of k in k-means clustering", "authors": [{"first": "D", "middle": ["T"], "last": "Pham", "suffix": ""}, {"first": "S", "middle": ["S"], "last": "Dimov", "suffix": ""}, {"first": "C", "middle": ["D"], "last": "Nguyen", "suffix": ""}], "year": 2005, "venue": "Proc. Inst. Mech. Eng. Part C: J. Mech. Eng. Sci", "volume": "215", "issn": "1", "pages": "103--119", "other_ids": {}}, "BIBREF25": {"ref_id": "b25", "title": "Machine-learning techniques and their applications in manufacturing", "authors": [{"first": "D", "middle": ["T"], "last": "Pham", "suffix": ""}, {"first": "A", "middle": ["A"], "last": "Afify", "suffix": ""}], "year": 2005, "venue": "Proc. Inst. Mech. Eng. Part B: J. Eng. Manuf", "volume": "219", "issn": "5", "pages": "395--412", "other_ids": {}}, "BIBREF26": {"ref_id": "b26", "title": "A new minimum description length based pruning technique for rule induction algorithms", "authors": [{"first": "D", "middle": ["T"], "last": "Pham", "suffix": ""}, {"first": "A", "middle": ["A"], "last": "Afify", "suffix": ""}], "year": 2008, "venue": "Proc. Inst. Mech. Eng. Part C: J. Mech. Eng. Sci", "volume": "222", "issn": "7", "pages": "1339--1352", "other_ids": {}}, "BIBREF27": {"ref_id": "b27", "title": "Data Preparation for Data Mining", "authors": [{"first": "D", "middle": [], "last": "Pyle", "suffix": ""}], "year": 1999, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF28": {"ref_id": "b28", "title": "Clustering of the self-organizing map", "authors": [{"first": "J", "middle": [], "last": "Vesanto", "suffix": ""}, {"first": "E", "middle": [], "last": "Alhoniemi", "suffix": ""}], "year": 2000, "venue": "IEEE Trans. Neural Netw", "volume": "11", "issn": "3", "pages": "586--600", "other_ids": {}}, "BIBREF29": {"ref_id": "b29", "title": "Very simple classification rules perform well on most commonly used datasets", "authors": [{"first": "R", "middle": ["C"], "last": "Holte", "suffix": ""}], "year": 1993, "venue": "Mach. Learn", "volume": "11", "issn": "", "pages": "63--91", "other_ids": {}}, "BIBREF30": {"ref_id": "b30", "title": "An analysis of Bayesian classifiers", "authors": [{"first": "P", "middle": [], "last": "Langley", "suffix": ""}, {"first": "W", "middle": [], "last": "Iba", "suffix": ""}, {"first": "K", "middle": [], "last": "Thompson", "suffix": ""}], "year": 1992, "venue": "Proceedings of the 10th National Conference on Artificial Intelligence", "volume": "", "issn": "", "pages": "223--228", "other_ids": {}}, "BIBREF31": {"ref_id": "b31", "title": "Tolerating noisy, irrelevant, and novel attributes in instance-based learning algorithms", "authors": [{"first": "D", "middle": [], "last": "Aha", "suffix": ""}], "year": 1992, "venue": "Int. J. Man Mach. Stud", "volume": "36", "issn": "2", "pages": "267--287", "other_ids": {}}, "BIBREF32": {"ref_id": "b32", "title": "Fast effective rule induction", "authors": [{"first": "W", "middle": ["W"], "last": "Cohen", "suffix": ""}], "year": 1995, "venue": "Proceedings of the 12th International Conference on Machine Learning", "volume": "", "issn": "", "pages": "115--123", "other_ids": {}}, "BIBREF33": {"ref_id": "b33", "title": "C4.5: Programs for Machine Learning", "authors": [{"first": "J", "middle": ["R"], "last": "Quinlan", "suffix": ""}], "year": 1993, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF34": {"ref_id": "b34", "title": "Error measures for generalizing about forecasting methods: empirical comparisons", "authors": [{"first": "J", "middle": ["S"], "last": "Armstrong", "suffix": ""}, {"first": "F", "middle": [], "last": "Collopy", "suffix": ""}], "year": 1992, "venue": "Int. J. Forecast", "volume": "8", "issn": "", "pages": "69--80", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "A pseudo-code description of the incremental k-means algorithm[17].In this study, a simple inductive learning algorithm called RULES-6 (RULe Extraction System -Version 6)[18] is used. RULES-6 extracts a set of classification rules from a collection of examples, each belonging to one of a number of given classes. The examples together with their associated classes constitute the set of training examples from which the algorithm generates the rules. Every example is described in terms of a fixed set of attributes, each with its own set of possible values.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "A simplified description of RULES-6[18].", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Examples of the sales profiles created by the incremental k-means clustering algorithm.", "latex": null, "type": "figure"}, "FIGREF3": {"text": "IF (Sales start date >= 14) THEN Sales profile = SP2 R04: IF (\u00a3101 <= Price <= \u00a3147) AND (Life span >= 22) THEN Sales profile = SP3 R05: IF (Price <= \u00a398) AND (Sales start date <= 2) THEN Sales profile = SP7 R06: IF (Price <= \u00a399) AND (Sales start date <= 11) THEN Sales profile = SP11 R07: IF (Life span <= 11) THEN Sales profile = SP6 R08: IF (Life span = 25) THEN Sales profile = SP9 R09: IF (Price <= \u00a392) THEN Sales profile = SP10 R10: IF (Price >= \u00a3300) THEN Sales profile = SP12 R11: IF (\u00a3200 <= Price <= \u00a3249) THEN Sales profile = SP5 R12: IF (Price >= \u00a3250) THEN Sales profile = SP13 R13: IF (Sales start date = 1) AND (11 < Life span <= 14) THEN Sales profile = SP8", "latex": null, "type": "figure"}, "FIGREF4": {"text": "Comparison between the actual sales of new products and the sales profiles predicted by RULES-6.", "latex": null, "type": "figure"}, "TABREF1": {"text": "Rules derived by RULES-6 from the retail data.", "latex": null, "type": "table"}, "TABREF2": {"text": "Forecasting errors of the predicted sales profiles for the tested classification methods.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>MAE </td><td>MAPE\n</td><td>RMSE\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>(%)\n</td><td>\u00a0</td></tr><tr><td>OneR </td><td>70.7 </td><td>5.2 </td><td>84.3\n</td></tr><tr><td>Na\u00efve Bayes </td><td>63.2 </td><td>4.5 </td><td>78.9\n</td></tr><tr><td>k-nearest-neighbours </td><td>51.3 </td><td>3.7 </td><td>66.8\n</td></tr><tr><td>RIPPER </td><td>16.4 </td><td>2.9 </td><td>21.1\n</td></tr><tr><td>C4.5 </td><td>9.7 </td><td>2.3 </td><td>12.9\n</td></tr><tr><td>Proposed RULES-6 </td><td>5.9 </td><td>1.8 </td><td>7.3\n</td></tr></table></body></html>"}}, "back_matter": [{"text": "The author wishes to thank the University of the West of England for providing a good environment, facilities and financial means to complete this paper.", "cite_spans": [], "ref_spans": [], "section": "Acknowledgement."}]}
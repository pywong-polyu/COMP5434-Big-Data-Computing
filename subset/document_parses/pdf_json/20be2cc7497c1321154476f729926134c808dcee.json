{
    "paper_id": "20be2cc7497c1321154476f729926134c808dcee",
    "metadata": {
        "title": "3D Point-of-Intention Determination Using a Multimodal Fusion of Hand Pointing and Eye Gaze for a 3D Display",
        "authors": [
            {
                "first": "Suparat",
                "middle": [],
                "last": "Yeamkuan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "King Mongkut's University of Technology Thonburi",
                    "location": {
                        "postCode": "10140",
                        "settlement": "Bangkok",
                        "country": "Thailand"
                    }
                },
                "email": ""
            },
            {
                "first": "Kosin",
                "middle": [],
                "last": "Chamnongthai",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "King Mongkut's University of Technology Thonburi",
                    "location": {
                        "postCode": "10140",
                        "settlement": "Bangkok",
                        "country": "Thailand"
                    }
                },
                "email": ""
            },
            {
                "first": "Stefano",
                "middle": [],
                "last": "Berretti",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "King Mongkut's University of Technology Thonburi",
                    "location": {
                        "postCode": "10140",
                        "settlement": "Bangkok",
                        "country": "Thailand"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [],
    "body_text": [
        {
            "text": "Basically, intention is the philosophical ability of the mind to form representations, and it is a logical concept for doing something with a goal [1] . Humans communicate intention to others for informing what they desire based on their decision making. If a machine can detect human intention, it may be able to serve the human properly and safely. For instance, elderly individuals and patients feel it convenient to command against facilities in smart homes [2] . Collaborative robots might work safer with human workers in factories [3, 4] . Medical facilities in hospitals would become more intelligent in assisting medical doctors to cure patients [5, 6] . Vehicles would be prevented safety in the case that drivers do not concentrate on driving, and facilities in driverless vehicles serve passengers in being more friendly and convenient. Therefore, human intention detection is regarded as a key function for the development of intelligent machines. In fact, it is difficult to understand human intention, which is originally kept in the mind. Although humans express their intention in some ways, such as speech, touch, gestures, and so on, to communicate to others, it is difficult for machines that are based on current technologies to detect and understand the human intention. Intention detection can be functionally analyzed into a couple of functions that are mode switching between intention and non-intention and intention target detection.",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 150,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 462,
                    "end": 465,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 538,
                    "end": 541,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 542,
                    "end": 544,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 655,
                    "end": 658,
                    "text": "[5,",
                    "ref_id": null
                },
                {
                    "start": 659,
                    "end": 661,
                    "text": "6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In order to implement the required functions, approaches that are based on touch, speech, and gestures have recently been proposed and developed. Currently, the touch approach is popularly used in mobile devices, which are used in cases of short distances between humans and machines [7] . For instance, touch screen-based interactions of car infotainment [8] , smart home operation panels [9, 10] , etc. Although methods in the touch approach are not smooth and friendly as speech communication between human and human, the methods are available and reliable in limited distance. As methods available for farther distance, the speech approach seems to work similarly to natural human-tohuman commands and it may be preferred by users. For instance, speech activation for chosen this approach. For instance, Q. De Smedt et al. [20] presented skeleton-based hand gesture recognition in a dynamic system and then compared it with methods utilizing depth information. The results showed that the overall accuracy of skeleton-based methods was better than that of depth-based methods. B. I. Ahmad et al. [21] proposed a hand pointing gesture with a Leap Motion sensor as the hand tracker to determine items that were selected by a user on a 2D screen. The method worked well for 2D displays. B. Daniel [22] presented an evaluation of Leap Motion as a contact-free hand pointing for gesture-based human machine interaction. The method using hand and fingertip positions is detected in coordinates relative to the center of the controller in coordinate system. The results show good accuracy declared at the sub-millimeter scale based on the analysis by Fitts' law. S. S. Das [23] proposed a method of estimating precise pointing directions using depth data with a Kinect depth sensor. The method effectively estimated pointing directions. All of the mentioned related works used a vector of hand pointing for the determination of POI, so that the methods could deal excellently with POI on 2D screen. The crossing point between the hand pointing vector and the plane of a 2D screen was assumed to be the POI. However, these might not be appropriate for POI detection in a 3D space, since the detection of POI in a 3D space geometrically required the crossing point of at least a couple of vectors. Therefore, another vector should be fused with the hand pointing one, and multimodal fusion between hand pointing and others became challenging for the 3D-POI-detection system development.",
            "cite_spans": [
                {
                    "start": 284,
                    "end": 287,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 356,
                    "end": 359,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 390,
                    "end": 393,
                    "text": "[9,",
                    "ref_id": null
                },
                {
                    "start": 394,
                    "end": 397,
                    "text": "10]",
                    "ref_id": null
                },
                {
                    "start": 827,
                    "end": 831,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "text": "[21]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1298,
                    "end": 1302,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1670,
                    "end": 1674,
                    "text": "[23]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Researchers of related works have fused hand gesture, speech, and eye gaze with hand pointing as a multimodal fusion [24] for POI detection on a 2D screen. The multimodal fusion of hand pointing and hand gesture [25] played roles of intention expression, such as pointer on the screen, click timing, rotation, and so on. The method used the natural behavior of humans as an advantage, but users have to learn command formats in advance. In the group of multimodal fusion of hand pointing and speech [26] , the speech, which normally was regarded as friendly for users, was conveniently used as command, but the speech was sensitive with noise and crosstalk as the drawback. On the other hand, the methods [16, [27] [28] [29] in the group of multimodal fusion between a hand pointing and eye gaze are considered to be a useful tool for elderly persons, patients, and disabled persons, and the crossing point between hand pointing and eye gaze vectors would be workable for POI detection, not only on a 2D screen, but also in a 3D space. Research works in the group of multimodal fusion between a hand pointing and eye gaze are as follows.",
            "cite_spans": [
                {
                    "start": 117,
                    "end": 121,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 212,
                    "end": 216,
                    "text": "[25]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 499,
                    "end": 503,
                    "text": "[26]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 705,
                    "end": 709,
                    "text": "[16,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 710,
                    "end": 714,
                    "text": "[27]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 715,
                    "end": 719,
                    "text": "[28]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 720,
                    "end": 724,
                    "text": "[29]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "N. Chuan and A. Sivaji [16] presented a method for combining eye gaze and hand tracking for the pointer in a human-machine interface. In this method, the hand is tracked by the Kinect depth sensor in terms of data voxels, and the AOI of the eye gaze that is sensed by the eye tracker is then matched with the hand pointing direction on a 2D screen. P. Gowdham and B. Pradipta [27] presented a system utilizing the combination of eye gaze and finger tracking for a projected display in automotive and military aviation environments. The method worked well in applications for touch buttons on 2D screens. B. I. Ahmad et al. [28] performed experiments in real situations and confirmed the possibility of combining gestures and eye-gaze trackers for intention expression with 2D displays in vehicles. The results of the measurements were also reported. F. Roider and T. Gross [29] proposed the integration of eye gaze and pointing gestures while driving for a 2D virtual screen. The method was shown to be feasible for applications on 2D virtual screens.",
            "cite_spans": [
                {
                    "start": 23,
                    "end": 27,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 376,
                    "end": 380,
                    "text": "[27]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 623,
                    "end": 627,
                    "text": "[28]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 873,
                    "end": 877,
                    "text": "[29]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To conclude, regarding the research and development of combinations of hand pointing and eye gaze, researchers have contributed to the tracking of human intention to touch a button at remote distances based on physical 2D screens as well as virtual 2D screens. Because the research was intended to solve problems for 2D screens, the touched button was ultimately estimated in a 2D space. These methods could be applied to problems of 2D screens by matching the hand pointing direction with the AOI of the eye gaze.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Suppose that there exist 3D coordinates (X,Y,Z) originating at an origin point (0, 0, 0), as shown in Figure 1 , and a 3D POI is located at any position in the 3D coordinates. In conventional methods using the hand pointing and eye gaze [16, 27, 29] , if the 3D POI is located on a display screen (A), both the vectors of hand pointing ( \u2212 \u2192 H ) and eye gaze ( \u2212 \u2192 E ) may ideally be combined at the 3D POI located at point A on the display screen. The position of the 3D POI on the display screen in this case is mathematically measured because the depth of the display screen is known. If the 3D POI is located somewhere in the 3D space and does not exist on the display screen, for example, both of the vectors of the hand pointing and eye gaze may point to point D, which is assumed to be the 3D POI. However, conventional methods determine the points of the hand pointing (A) and eye gaze (B) on the display screen, and then draw an AOI and find the middle point (C) at the closest distance as the POI. Although errors occur according to these methods, they can be used in many applications that require users to choose areas on a display screen; however, they are not suitable for 3D virtual and real display screens in which the depths are not known. Recently, an excellent tool, called an eye tracker [30, 31] , emerged, which can detect the eye pupils and create a ray representing the eye gaze that is drawn from the middle point (E c ) between the pupils to a point on a fixed display screen. The display screen is studied for use as a virtual screen through calibration in advance, and the vector drawn from the middle point of both pupils passing through a point on the virtual screen has been shown to be able to be used as an eye gaze vector in 3D space [32] . Ref. [33] proposed translating the head by a short distance to find another eye gaze vector and using the crossing point of the two eye gaze vectors as the 3D POI. The constraint of the method is that users have to move their heads slightly. However, users are not allowed to move their heads, even slightly, in some environments, such as cockpits, for safety reasons. Therefore, a 3D POI detection method without head movement is also required. If the eye gaze is used as a vector passing through a 3D POI, and hand pointing, which is already considered to be a powerful action for intention expression [34, 35] , is fused with the eye gaze to find a crossing point, the position of the 3D POI can be geometrically obtained, as follows:",
            "cite_spans": [
                {
                    "start": 237,
                    "end": 241,
                    "text": "[16,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 242,
                    "end": 245,
                    "text": "27,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 246,
                    "end": 249,
                    "text": "29]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1309,
                    "end": 1313,
                    "text": "[30,",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 1314,
                    "end": 1317,
                    "text": "31]",
                    "ref_id": null
                },
                {
                    "start": 1769,
                    "end": 1773,
                    "text": "[32]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 1781,
                    "end": 1785,
                    "text": "[33]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 2380,
                    "end": 2384,
                    "text": "[34,",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 2385,
                    "end": 2388,
                    "text": "35]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [
                {
                    "start": 102,
                    "end": 110,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Analysis of 3D POI"
        },
        {
            "text": "where, H represents the tip of the index finger.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis of 3D POI"
        },
        {
            "text": "Pointing in a single-hand gesture has been shown to be an effective way of human intention expression, according to [34, 36] . When a single hand gestures by pointing to a point in 3D coordinates [37] , it can be detected as an intention regarding all the points. If there is more than one candidate point on the pointing line (e.g., C, D, etc.), as shown in Figure 2a , it is difficult to determine the intended point. In this case, the human eye gaze assists in confirming the intended position, as introduced in [38] . Therefore, our paper proposes using the fusion of a pointing hand gesture and the eye gaze for intended point determination. A crossing point (D in Figure 2a ) between the hand pointing line and eye gaze line, which represents a human intention, can be mathematically obtained from 3D coordinates in an ideal case, as shown in Equation (1) . However, human eyes always perform saccades between fixation points [18] , and sensor devices may not be able to address this in real situations, so it is quite difficult to correctly detect an eye gaze and then an intention point. Human line-of-sight rays may move within an approximate range of 5-10 degrees due to saccades, according to research works in psychological experiments [39, 40] . Therefore, this paper sets the maximum cornea movement range covering saccades, which is called the SOI, as shown in Figure 2b . In order to determine the SOI, while the light ray \u2212 \u2192 E making a right angle with the straight-line between both eyes and passing through a fixation point (D) is assumed to cross the hand pointing vector at the maximum distance, line-of-sight rays that are assumed to move utmost 10 degrees will saccade in a circle range. This circle range is exactly the SOI, and the radius (r) of the SOI can be obtained by the following equation:",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 120,
                    "text": "[34,",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 121,
                    "end": 124,
                    "text": "36]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 196,
                    "end": 200,
                    "text": "[37]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 515,
                    "end": 519,
                    "text": "[38]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 858,
                    "end": 861,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 932,
                    "end": 936,
                    "text": "[18]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "text": "[39,",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1253,
                    "end": 1256,
                    "text": "40]",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [
                {
                    "start": 359,
                    "end": 368,
                    "text": "Figure 2a",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 670,
                    "end": 679,
                    "text": "Figure 2a",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1376,
                    "end": 1385,
                    "text": "Figure 2b",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Fusion of Hand Pointing and Eye Gaze as Human Intention"
        },
        {
            "text": "where,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusion of Hand Pointing and Eye Gaze as Human Intention"
        },
        {
            "text": "The crossing point between the light ray \u2212 \u2192 E and hand pointing \u2212 \u2192 H should be in the SOI because the SOI is assumed to cover the maximum cornea movement range. In reality, both of the mentioned vectors sometimes approach close without crossing, and the closest point should be considered as the crossing point or intended point under the condition of SOI. Therefore, the closest point is guaranteed to be the intended point by the following equation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusion of Hand Pointing and Eye Gaze as Human Intention"
        },
        {
            "text": "where,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusion of Hand Pointing and Eye Gaze as Human Intention"
        },
        {
            "text": "In order to implement an automatic system, it is absolutely necessary to define the mode of intention. If we define the intention point by the crossing point between the pointing hand and eye gaze, as mentioned earlier, and the intention mode may be switched on by finding this crossing point. However, eye gaze errors usually occur due to saccades and sensor devices, and a crossing point that should be found may not be detected, even though a passenger is displaying an intention. This paper considers errors related to the saccades of both the left and right eyes because the human eye naturally has limitations of movement and sight within a range, called the visual field [41] , as shown in Figure 2b . The visual field is enlarged as a cylinder surrounding the eye gaze vector, and its diameter is similar to the range between the two eyes, which is the SOI. In this paper, the occurrence of a close point between the SOI and a hand pointing line is considered to indicate a change in the intention mode.",
            "cite_spans": [
                {
                    "start": 678,
                    "end": 682,
                    "text": "[41]",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [
                {
                    "start": 697,
                    "end": 706,
                    "text": "Figure 2b",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Fusion of Hand Pointing and Eye Gaze as Human Intention"
        },
        {
            "text": "When a human watches a 3D POI (D), as shown in Figure 1 , a vector of eye gaze ( \u2212 \u2192 E ) is theoretically drawn from the center (E c ) of both the left and right pupils to the POI (D). The eye tracker [30] that was used in this paper normally provides the positions of the left and right pupils. These positions are then used in order to calculate the center (E c ) of both pupils, and a 2D POG (A), which is a eye gaze point on the virtual board, is used with the mentioned center point (E c ) to create the vector of the eye gaze ( \u2212 \u2192 E ). In this paper, the system origin is initially established at the center of the eye tracker, and the positions of the extracted left and right pupils are located at the coordinates E r (\u2212X r , Y c , \u2212Z) and E l (X l , Y c , \u2212Z), respectively, as shown in Figure 3 . Humans normally blink their eyes [42] and cause the eye tracker to sometimes lose track of the pupil positions, as seen in the observation results of the left and right eyes over time. The duration of this tracking loss is called the pupil disappearance duration in this paper. In addition to the constant saccades of human eyes, it is found that the pupil positions sometimes disappear in the data series of pupil positions in the time domain. Therefore, the saccades and blinks of the human eye cause vibrations and outstanding peaks, respectively, as shown in Figure 4a . W. Pichitwong and K. Chamnongthai [33] take the average to determine the representative position of the pupil and, therefore, errors occur due to pupil disappearance. If we take derivatives between consecutive time frames, the differentiation results detect peaks that represent the starting and ending points of a blink or pupil disappearance duration, as shown by (AB and CD) duration in Figure 4b . These pupil disappearance durations are detected and ignored in the method proposed in this paper, and the original signal representing the pupil position becomes an analog signal including only the vibration of saccades, as shown in Figure 4c . Therefore, the average position of the signal is improved as compared with that in the conventional method [33] . ",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 205,
                    "text": "[30]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 841,
                    "end": 845,
                    "text": "[42]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "text": "[33]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 2138,
                    "end": 2142,
                    "text": "[33]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [
                {
                    "start": 47,
                    "end": 55,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 797,
                    "end": 805,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1371,
                    "end": 1380,
                    "text": "Figure 4a",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1773,
                    "end": 1782,
                    "text": "Figure 4b",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 2019,
                    "end": 2028,
                    "text": "Figure 4c",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Eye Gaze Vector"
        },
        {
            "text": "A hand shape can basically be represented based on finger joints as a feature [20] , a gesture of hand pointing can be established as a hand shape pattern in a still image, and humans naturally pause their hand posture in a pointing pattern position for a period of time during pointing [43] . This paper analyzes the hand pointing mode, considering two parts as necessary conditions: static finger gestures and the hand posture during pointing. The hand pointing mode is used to switch the mode of the intention estimation. Suppose that a fingertip and four joints of each finger can be detected by a sensor [44] , as shown in Figure 5a , which are T1 \u2212 4, I1 \u2212 4, M1 \u2212 4, R1 \u2212 4, and P1 \u2212 4. Physiologically, the first joints of all fingers (T1, I1, M1, R1, and P1) are fixed without any movement, and a line passing those joints can be assumed to be a baseline for considering the movement of other finger joints. Regarding the first condition of static finger gestures, while the thumb is free as the don't-care term, the index finger absolutely needs to be a straight line in the pointing gesture, and the pinky, ring, and middle fingers have to be bent inward. Although these three bent fingers are slightly varied in vertical direction in reality, the bent fingers that represent hand pointing can be concluded in some patterns, which will be discussed in detail in the next subsection. If we set up rectangles to cover all of the finger joints and tips, with the tips and joints being located in the center of the rectangle, a template consisting of rectangular cells can be obtained, as shown in Figure 5b . In some cases in which the fingers are bent inward lower than the baseline, other cells (P0, R0, M0, and I0) are established to cover some possible patterns of the fingertips of bent fingers. Finally, if we informatively assign the logical \"1\" as a hand and \"0\" as no hand in the cells of the matrix, the pattern of the hand pointing template is obtained, as shown in Figure 5c .",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 82,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 287,
                    "end": 291,
                    "text": "[43]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 609,
                    "end": 613,
                    "text": "[44]",
                    "ref_id": "BIBREF47"
                }
            ],
            "ref_spans": [
                {
                    "start": 628,
                    "end": 637,
                    "text": "Figure 5a",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1605,
                    "end": 1614,
                    "text": "Figure 5b",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1985,
                    "end": 1994,
                    "text": "Figure 5c",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Definition of Hand Pointing Shapes"
        },
        {
            "text": "Originally, hand and finger are sensed by a Leap Motion sensor in the unit of hand pointing detection, and the coordinates of 15 finger joints and five fingertips are obtained. These coordinates are input data for the determination of hand pointing. Naturally, hand and fingers are freely moved and located in any postures, and coordinate patterns of hand pointing would be varied according to the hand postures. Although hand pointing patterns are fixed in a finite number, the number of hand pointing patterns may be increased according to the hand postures and this may disable the pattern matching. The coordinates of finger joints and fingertips would be first rotated to the original angle before the pattern matching process in order to solve the problem. For instance, coordinates of hand pointing and rotated hand pointing, as shown in Figure 6a ,b, respectively, are converted into a 3D binary pattern, as shown in Figure 6d ,e, respectively. Obviously, the 3D binary patterns of both coordinate examples are different due to the rotated posture. Thus, the coordinates of finger joints and fingertips should be rotated to the origin posture in the first step. Moreover, when a user performs a hand pointing gesture many times, the hand pointing gestures may exactly be different, due to slight changes of bending pinky, ring, and middle fingers, as shown in Figure 6a ,c. Apparently, these slight changes may not affect the meaning of hand pointing, but 3D binary patterns are differentiated, as shown by the parts that are surrounded by red dash lines presented in Figure 6d ,f. However, it can be observed that coordinates in Y axis of middle, ring, and pinky fingers are largely different when compared with the ones in X and Z axes, as shown by red dash-line rectangles in shown Figure 6a ,c. If only coordinates of X and Z axes are converted into a 2D binary pattern, then both of coordinates, as shown in Figure 6a ,c, would become the same pattern, as shown in Figure 6g . This means that 2D information of finger joints and fingertips can absorb the slight changes of those pinky, ring, and middle fingers, and many 3D binary patterns of a hand pointing gesture can be unified into a 2D binary pattern. Therefore, the 3D coordinates of finger joints and fingertips should be projected into 2D ones before finding a binary pattern. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 845,
                    "end": 854,
                    "text": "Figure 6a",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 925,
                    "end": 934,
                    "text": "Figure 6d",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1368,
                    "end": 1377,
                    "text": "Figure 6a",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1576,
                    "end": 1585,
                    "text": "Figure 6d",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1793,
                    "end": 1802,
                    "text": "Figure 6a",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1921,
                    "end": 1930,
                    "text": "Figure 6a",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1978,
                    "end": 1987,
                    "text": "Figure 6g",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Hand Pointing Angle and Coordinate"
        },
        {
            "text": "With our basic concept of hand pointing described above, the gestures of all five fingers should be digitally analyzed into one of the possible patterns for hand pointing. These possible hand patterns are used as templates to classify hand pointing. While the thumb is logically regarded as a don't-care term, the index finger then has to be a straight line as the first condition, and the remaining three fingers, the middle, ring, and pinky, fingers physically have to be bent inward as the second condition. In bending the middle, ring, and pinky fingers, these fingers are not always in the same positions, even for the same person. This means that there exists more than one possible pattern of bending for these three fingers when a hand intentionally points. By using a sensor, such as the Leap Motion sensor [45] , which can digitally sense the positions of a fingertip and three finger joints of each finger, the patterns of a fingertip and three finger joints of these three fingers in a bent position after 3D-to-2D projection can be grouped into eight patterns in terms of probability and physiology, as shown in Figure 7 , in which all of the joints of these three fingers are located below the middle of the index finger. This means that all four joins of the middle, ring, and pinky fingers have to be physically located below joint \"2\" of the index finger. When the index finger (I) behaves as a straight line and all four joints are located in four individual cells in the vertical direction, all three fingers (M, R, and P) are located based on I1 and the cell below I1, as shown in PT 1 , and each of the three fingers is shifted to one upper cell from left to right, as shown in PT 2 , PT 3 , and PT 4 . Upon shifting all of the bent fingers up by one cell to I2, PT 5 to PT 8 show the hand pointing with three bent fingers located below I2. All four joints of the three fingers (M, R, and P) are located at the same level as I2 and I1, as shown in PT 5 . Each of the three fingers, \"middle, ring, or pinky\", in PT 5 is shifted down one cell to form the other three patterns of PT 6 , PT 7 , and PT 8 , respectively. These are the possible patterns of hand pointing that are used to recognize and confirm the hand pointing shape in this paper.",
            "cite_spans": [
                {
                    "start": 816,
                    "end": 820,
                    "text": "[45]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [
                {
                    "start": 1125,
                    "end": 1133,
                    "text": "Figure 7",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Hand Pointing Pattern"
        },
        {
            "text": "On the other hand, the hand posture must pause for a time in the second condition of the hand pointing mode. If we analyze the movement of finger joints in a video sequence, then the finger joints in a pointing pattern and other patterns are shown in the video sequence, as shown in the example presented in Figure 8a . If we simply differentiate consecutive frames, the results may indicate differences during non-pointing, transition, and approaching durations and no difference during pause durations, as shown in Figure 8b . Therefore, this paper proposes identifying the candidates of the hand pointing mode by detecting the first frame of the pause duration, performing pattern matching with the first frame and each of the eight trained patterns, and confirming that the hand pointing vector is located in the SOI. When the hand pointing mode is confirmed, the closest point between the eye gaze and hand pointing vectors is then obtained as the POI. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 308,
                    "end": 317,
                    "text": "Figure 8a",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 517,
                    "end": 526,
                    "text": "Figure 8b",
                    "ref_id": "FIGREF9"
                }
            ],
            "section": "Hand Pointing Pattern"
        },
        {
            "text": "While facilities in living and working environment, such as home, hospital, factory, and so on, tend to be more smart, convenient, and friendly for users, vehicles, such as cars, airplanes, and ships, are becoming increasingly advanced due to their related technologies. Currently, it is obvious that our living and working environments will be changed to be conveniently controlled by users in remote places, and cars will become auto-driving, without requiring a driver, and will also be much more capable of communicating with and performing services for passengers. One of the keys to communicate and interface with users is to know the intentions of all of the users. Some users may not be free to move to the control switches in the living and working environments, and some passengers may not sit in the front of the passenger space. Therefore, they will not be able to touch a touchscreen or push buttons on the front console; thus, 3D virtual screens may become effective for interfacing with passengers, as shown by the scenario depicted in Figure 9 . Our proposed system is described in two parts, which are the hardware and software units, below. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1051,
                    "end": 1059,
                    "text": "Figure 9",
                    "ref_id": "FIGREF10"
                }
            ],
            "section": "System Overview"
        },
        {
            "text": "In our system overview, the hardware system consists of an eye tracker [30] , the Leap Motion sensor [45] , memory, processor, and input/output units, as shown in Figure 10 . The Leap Motion sensor, which can sense hand gestures, is installed in front of the user. In this paper, the range covered by the Leap Motion sensor is called the hand tracking area. In addition, an eye tracker is set up in front of the user, so that it can detect the user's eyes, and the range that is covered by the eye tracker is called the eye tracking area in this paper. The origins of the coordinates of the eye tracker (X, Y, Z) and Leap Motion sensor (X h , Y h , Z h ) are located in the center of each sensor and, in practice, the coordinates of the hand tracking area are initially calibrated to the coordinates of the eye tracking sensor. In testing, the eye gaze ( \u2212 \u2192 E ) and hand pointing ( \u2212 \u2192 H ) directions are detected by the eye tracking sensor and Leap Motion sensor, respectively. The 3D POI is geometrically found at the crossing point between the hand-pointing and eye gaze directions.",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[30]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 101,
                    "end": 105,
                    "text": "[45]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [
                {
                    "start": 163,
                    "end": 172,
                    "text": "Figure 10",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Hardware Unit"
        },
        {
            "text": "Based on the established hardware, our software unit starts with the Leap Motion input process, which senses the finger joint positions with the Leap Motion sensor, as shown in Figure 11 . The unit then projects the 3D finger joint-position data to 2D data in the 3D-to-2D hand projection process and detects hand pointing in the hand-pointing determination process. Sections 5.2 and 5.3, respectively, describe these processes. In the case in which the hand is not determined to be in pointing mode, the system does not process further and it goes back to the Leap Motion input process. If the system decides that the hand is in pointing mode, the eye gaze is input from the eye tracker in the eye tracking sensor input process, the eye gaze vector is determined in the process of determination of the eye gaze vector, and the SOI is calculated in the SOI calculation process, which are processes introduced in Sections 5.4 and 5.5, respectively. At this time, if the vector of hand pointing intersects the SOI, then the system is confirmed to be in the pointing mode. The system then finds the closest 3D point between the vectors of eye gaze and hand pointing, which is assumed to be the POI. The details of the closest-point calculation are reported in Section 5.6. Otherwise, the system returns to start the loop again, with new finger joint positions being obtained by the Leap Motion sensor. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 177,
                    "end": 186,
                    "text": "Figure 11",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Software Unit"
        },
        {
            "text": "The calibration and processes of the flowchart shown in Figure 11 are realized, as follows, in order to implement our basic concept of the proposed 3D POI determination method using multimodal fusion of the eye gaze and hand pointing for a 3D display.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 56,
                    "end": 65,
                    "text": "Figure 11",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Proposed Methods"
        },
        {
            "text": "There are three coordinate systems: those of the eye tracker (X, Y, Z), hand tracker (X h , Y h , Z h ), and virtual board (x b , y b ), as shown in Figure 12 . In practice, users have to calibrate these three coordinate systems to find connection the virtual board and hand tracker coordinates with the coordinates of the eye tracking sensor in advance. The eye tracking sensor is assumed to obtain the coordinates of the left and right pupil centers and the 2D eye gaze position on the virtual board. In the calibration, a board with some points that represent the priors of the measured 3D position (C 1 -C 5 ) on the eye tracker coordinates is temporarily installed, and the 2D coordinates of those five points on the board [46] (C 1 -C 5 ) are the outputs from the eye tracking sensors when an examiner looks at those five points. The conversion between the 2D virtual board and 3D eye tracking sensor coordinates is performed with the following equations:",
            "cite_spans": [
                {
                    "start": 728,
                    "end": 732,
                    "text": "[46]",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [
                {
                    "start": 149,
                    "end": 158,
                    "text": "Figure 12",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Calibration"
        },
        {
            "text": "where, N is a fixed distant value with real-world units (millimeters), and the resulting value R is the ratio of the converted eye gaze position in terms of pixels to millimeters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Calibration"
        },
        {
            "text": "where, G is the position (x, y) of the eye gaze in units of pixels. The resulting value J is the real-world value according to the ratio R and it is offset by C 5 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Calibration"
        },
        {
            "text": "Calibration plane Subsequently, the distances between the origins of the hand tracker and eye tracker (H z and H x ) are manually measured, so that the transformation between the hand tracker and eye tracker coordinates can be obtained. During the testing state, the board that is used in calibrations is removed, and this is called the \"virtual board\" in this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Eye tracking sensor"
        },
        {
            "text": "As another necessary calibration, the hand pointing template has to be established in advance. Because the hand and finger sizes of each user are different, the cell sizes have to be measured by a hand sensor, and the hand-pointing template has to be determined. The ranges between neighboring joints and tips (I L2\u22124 , M L2\u22124 , R L2\u22124 , and P L2\u22124 ) are measured according to coordinates from a sensor, and rectangular cells are assumed to be established to cover all fingertips and joints, as shown in Figure 13 . The sizes of the template cells (I T1\u22124 , M T1\u22124 , R T1\u22124 , and P T1\u22124 ) can be simply obtained by the following equations:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 504,
                    "end": 513,
                    "text": "Figure 13",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Eye tracking sensor"
        },
        {
            "text": "where, F = {I, M, R, P}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Eye tracking sensor"
        },
        {
            "text": "Wrist (W) On the other hand, the ranges between neighboring finger joints on the baseline (W 1 -W 4 ) are measured according to the coordinates from the hand motion, and the width of the template cells (Iw, Mw, Rw, and Pw) can be calculated by the following equations:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Eye tracking sensor"
        },
        {
            "text": "Finally, the bottom cells of those four fingers (I0, M0, R0, and P0) are originally for some bent fingers that possibly cross over the baseline, such that their sizes are assumed to be the same as those on the baseline (I1, M1, R1, and P1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Eye tracking sensor"
        },
        {
            "text": "In the viewpoint of the Leap Motion sensor that is installed below the hand, finger joints are sometimes considered to be occluded in the palm or other fingers. This is likely to cause errors in finger joint extraction. The view angle of the hand should be facing the sensor at the appropriate angle to obtain the finger joint positions and convert them to a template. Therefore, it is required to rotate [47] the hand (X, Y, Z) at the wrist to the desired angle with the following equation:",
            "cite_spans": [
                {
                    "start": 405,
                    "end": 409,
                    "text": "[47]",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "3D-To-2D Hand Projection"
        },
        {
            "text": "The finger joints of the five fingers of the hand, as shown in Figure 14a , are then rotated to the desired angle, as shown in Figure 14b . Now, the hand angle is ready to detect all of the finger joints, assuming no occlusion, and this is a good angle at which to project [48] the 3D position (X , Y , Z ) onto the 2D (X , Z ) position by the following Equation (19) , as shown in Figure 14c . ",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 277,
                    "text": "[48]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 363,
                    "end": 367,
                    "text": "(19)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 63,
                    "end": 73,
                    "text": "Figure 14a",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 127,
                    "end": 137,
                    "text": "Figure 14b",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 382,
                    "end": 392,
                    "text": "Figure 14c",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "3D-To-2D Hand Projection"
        },
        {
            "text": "The 2D positions (X , Z ) of five joints have been projected from 3D finger joint positions by the previous process of 3D-to-2D hand projection. In order to determine the 2D projected hand shape, which is either hand pointing or non-pointing, two conditions are used in this paper: (1) the index finger has to be a straight line and (2) the middle, ring, and pinky fingers have to be bent inward [34] . In the first step, the four joints of the index (I1, I2, I3, and I4) are geometrically checked to confirm a straight line. The Hough transform [49] is used in this paper to check whether the three line parts segments (the lines drawn between I1 and I2, I2 and I3, and I3 and I4) are either the same straight lines or different straight lines. As shown in Figure 15a , the three line segments (lines drawn between I1 and I2, I2 and I3, and I3 and I4) that are represented in red are converted by the Hough transform to become three points in the Hough coordinates (\u03b8, \u03b3), as shown in Figure 15b . In the ideal case, in which all three lines are on the same straight line, all of the straight lines are converted to the same point, which is located at point H(90, \u03b3).",
            "cite_spans": [
                {
                    "start": 396,
                    "end": 400,
                    "text": "[34]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 546,
                    "end": 550,
                    "text": "[49]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [
                {
                    "start": 758,
                    "end": 768,
                    "text": "Figure 15a",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 986,
                    "end": 996,
                    "text": "Figure 15b",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Hand Pointing Determination"
        },
        {
            "text": "In practical cases, the index finger might be slightly bent and, physiologically, the human index finger can bend up to 30 degrees at most [50] , so this paper proposes training the maximum acceptable angle range, which is called the pointing intention range, from some samples in advance. The trained pointing intention range must be within the limit of 30 degrees for the upper and lower angles above and below from the target line. In the case in which the human intends to point to a 3D POI target, the three lines mentioned are expected to be geometrically located between two upper and lower acceptable-error range vectors, R u and R l , which define the so-called pointing intention range, as shown in Figure 15c .",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 143,
                    "text": "[50]",
                    "ref_id": "BIBREF54"
                }
            ],
            "ref_spans": [
                {
                    "start": 709,
                    "end": 719,
                    "text": "Figure 15c",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Hand Pointing Determination"
        },
        {
            "text": "In order to convert this range into Hough coordinates, the pointing intention range converted by the following Equation (20) is used; it determines the pink rectangle shown in Figure 15d . If the human has no intention to point to a 3D POI, not all of the vectors associated with the index finger may be geometrically located in the pointing intention range, as shown in Figure 15e , and at least one of the points representing the three straight lines mentioned has to be outside the rectangle of the pointing intention range, as shown in Figure 15f . Algorithm 2 conducts the straight-line check for the index finger. if \u03b8 u < \u03b8 p \u03b8 l and \u03b3 u < \u03b3 p \u03b3 l then 7: set index finger straight 8: If the index finger is confirmed to be a straight line as the first necessary condition, then the hand pointing template is applied to the 2D projected hand. Suppose that the coordinates of the fingertips and joints are obtained as input data, as shown in Figure 16a . The data are matched with the template for hand pointing determination. Practically, the template is separated into a one-dimensional (1D) matrix for each finger, and each matrix is applied based on the baseline of the finger, as shown in Figure 16b . All of the cells are checked for the existence of tips and joints, with \"1\" and \"0\" used for existence and non-existence, respectively, to create a matrix, as shown in Figure 16c . The matrix is then used to perform pattern matching with the eight trained templates of the hand pointing. Template matching for hand pointing determination (HP) is logically performed with the eight templates (PT1-PT8), according to the following equations:",
            "cite_spans": [
                {
                    "start": 660,
                    "end": 662,
                    "text": "7:",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 176,
                    "end": 186,
                    "text": "Figure 15d",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 371,
                    "end": 381,
                    "text": "Figure 15e",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 540,
                    "end": 550,
                    "text": "Figure 15f",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 948,
                    "end": 958,
                    "text": "Figure 16a",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1200,
                    "end": 1210,
                    "text": "Figure 16b",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1381,
                    "end": 1391,
                    "text": "Figure 16c",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Hand Pointing Determination"
        },
        {
            "text": "HP = 0; Non matching 1; Matching (22) If one of the eight templates is matched with the hand shape, the output of the equation logically results in \"1\", which means that the hand is pointing, as shown in the equation above. In the cases in which the hand shape is not matched with any of the eight templates, the logical result is \"0\", which indicates non-pointing. Algorithm 3 concludes the pattern matching of hand pointing. In order to estimate the hand pointing direction in the final process of the subroutine, the index is physiologically defined to be a straight line on the straight-line drawn between I1 and I4 [51] , as shown in Figure 17 . In practice, the index finger, which is intentionally pointed in a straight line directly toward a target, may not always form a real straight line, but makes slight angles at the joints. Therefore, this paper establishes an acceptable range, called the pointing intention range, for absorbing small acceptable errors in the index gesture and determines the index pointing vector according to the average direction between the two vectors formed by I1 and I2 and by I1 and I4, as shown by the dashed line presented in Figure 17 . The index is assumed to be intentionally pointing to a target if the index pointing vector is located within the pointing intention range. The pointing intention range is initially established by training on samples of pointing human hands, and this paper practically uses 20 degrees for the upper and lower angles above and below as the range.",
            "cite_spans": [
                {
                    "start": 33,
                    "end": 37,
                    "text": "(22)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 620,
                    "end": 624,
                    "text": "[51]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [
                {
                    "start": 639,
                    "end": 648,
                    "text": "Figure 17",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1169,
                    "end": 1178,
                    "text": "Figure 17",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Hand Pointing Determination"
        },
        {
            "text": "H = \u2212 \u2192 I u + \u2212 \u2192 I l(23)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2212 \u2192"
        },
        {
            "text": "Based on the concepts shown in Section 3.2, this paper proposes determining the pupil disappearance duration (AB and CD) and deleting it to correct the analog signal representing the pupil positions (P t ). In implementing this concept, the analog signal representing the pupil positions, as shown in Figure 4a , is calculated according to the threshold value (th 1 ) to detect the pupil disappearance duration as shown in Figure 18 , that is defined by the following equation:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 301,
                    "end": 310,
                    "text": "Figure 4a",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 423,
                    "end": 432,
                    "text": "Figure 18",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Determination of the Eye Gaze Vector"
        },
        {
            "text": "where F and N represent the parameters of the threshold value and the number of time frames, respectively. In thresholding, the differentiation of consecutive time frames (D p ), which is performed by Equation (25), is binarized in order to find the starting and ending points of the pupil disappearance durations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Determination of the Eye Gaze Vector"
        },
        {
            "text": "where, t = 1, 2, . . . , N. The differentiation time frames with a logical \"1\" are regarded as the starting and ending points of the pupil disappearance duration, and the signal components during this duration should be deleted for correction. The corrected signal components that represent the pupil positions are found as the average pupil positions. Subsequently, left and right representative pupil positions are both used to find the center point (E c ), and the center point is used with the 2D POG obtained from the eye tracking sensor to draw a straight line, which is regarded as the vector of the eye gaze. Figure 18 . Thresholding for pupil-disappearance duration determination.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 617,
                    "end": 626,
                    "text": "Figure 18",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Determination of the Eye Gaze Vector"
        },
        {
            "text": "Human line-of-sight rays may move within an approximate range of 5-10 degrees due to saccades, according to the research using works in psychological experiments [39, 40] . Therefore, this paper sets the maximum cornea movement range to cover covering saccades, which is called the SOI, as shown in Figure 2b . When the eye tracker senses both pupil centers, the light ray making a right angle with the straight-line between both eyes and passing through a point on the virtual board may be used to determine the visual field based on the aforementioned maximum cornea movement.",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 166,
                    "text": "[39,",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 167,
                    "end": 170,
                    "text": "40]",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [
                {
                    "start": 299,
                    "end": 308,
                    "text": "Figure 2b",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Soi Calculation"
        },
        {
            "text": "V u = r cos(\u03b8 e ), r sin(\u03b8 e ) (26) \u2212 \u2192 V l = r cos(\u03b8 e ), r sin(\u03b8 e )(27)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2212 \u2192"
        },
        {
            "text": "When pointing at a target, a user normally moves the current hand shape to the hand pointing shape, and then pauses for a time, as shown by the example presented in Figure 19 . At first, the sensor may sense nothing, and the detected hand velocity may be zero, as shown in the beginning of the graph in Figure 19 , when the hand disappears in the hand tracking area. As a hand sensor, the Leap Motion sensor senses a hand whenever a hand appears in the hand tracking area. The hand velocity may sharply increase when a hand appears in the hand tracking area (A), and the hand takes the shape of pointing; this is called the transforming state. When the hand shape transforms, and has almost reached the hand pointing shape (P), the hand velocity rapidly decreases; this is called the approaching state. Subsequenyly, the hand enters a mode of adjusting the hand shape (B), which is called the fine-tuning state, and the hand velocity fluctuates slightly until it reaches a nearly zero level. The hand in the pointing shape may be paused for a while during the dwell time. The best time to find the target of hand pointing is the starting point of the dwell time (C), as observed.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 165,
                    "end": 174,
                    "text": "Figure 19",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 303,
                    "end": 312,
                    "text": "Figure 19",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Dwell Time and Intersection Detection"
        },
        {
            "text": "To detect the dwell time, we should start by finding the first non-zero point (A) and then perform smoothing to delete local spikes in the signal, as shown in Algorithm 4. Subsequently, the slopes of all parts of the signal are calculated; positive and negative slopes representing the transforming and approaching states, respectively, are found; and the end of the approaching state or beginning of the fine-tuning state (B) is detected. To find the starting point of the dwell time or the end of the fine-tuning state, ideally, it is possible to find the first zero point. However, there is noise in reality, and the hand velocity in the dwell time is not absolutely zero or even close to zero. Practically, the noise should be suppressed by thresholding, and the first zero point then becomes the starting point of the dwell time. The thresholding value should be initially set by rate comparison to the average of the signal from B to D in pretesting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dwell Time and Intersection Detection"
        },
        {
            "text": "Input: data frame from Leap Motion as L 1: set i = 0 2: while V i = 0 do 3: i = i + 1 4: end while\u2212\u2192 A 5: calculate smoothing (i \u2192 n) 6: calculate slope finding (-) and (+) \u2212\u2192 B 7: compute thresholding \u2212\u2192 suppress noise 8: compute find zero A line representing hand pointing can logically be categorized into one of three cases, Cases 1-3, as shown in Figure 20 . In Case 1, the straight-line representing the hand pointing direction intersects the visual field at a point behind zero, while the intersected point is located in front of the zero point in Case 3. Case 2 shows the hand pointing direction, which is a ray parallel with the edge of the SOI. When Case 3, which is a suspected candidate for the pointing intention, is detected, there needs be a pause, called the \"dwell time\", according to a study of human intention [52, 53] . The pause time should be initially measured by pretesting with examiners, and it is thereafter used to check the dwell time for intention determination.",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 75,
                    "text": "3:",
                    "ref_id": null
                },
                {
                    "start": 829,
                    "end": 833,
                    "text": "[52,",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 834,
                    "end": 837,
                    "text": "53]",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [
                {
                    "start": 352,
                    "end": 361,
                    "text": "Figure 20",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Algorithm 4 Algorithm for dwell time detection."
        },
        {
            "text": "When an intersection is detected during the dwell time, the closest point [54, 55] between the hand pointing vector and eye gaze is calculated (28)-(32).",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 78,
                    "text": "[54,",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 79,
                    "end": 82,
                    "text": "55]",
                    "ref_id": "BIBREF59"
                }
            ],
            "ref_spans": [],
            "section": "Algorithm 4 Algorithm for dwell time detection."
        },
        {
            "text": "where E c is the middle point between the left and right pupil positions. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2212 \u2192"
        },
        {
            "text": "In the implementation of the system based on the basic concept, users should realize some limitations to prevent errors. Those limitations are explained, as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Limitation of Proposed Method"
        },
        {
            "text": "The proposed POI determination method basically uses a light ray vector making a right angle with the straight line between both eyes and passing through a fixation point on the virtual board to assist determination of POI by finding a crossing point with the vector of hand pointing. The head angle is regarded to sensitively influence the right angle of the mentioned right ray vector, and it may induce errors in POI detection. Users should raise awareness of the angle of the participant head, and fix the head in calibration, training, and testing processes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Head Fixation"
        },
        {
            "text": "Normally, every person has different sizes of hands and fingers. The hand template and patterns are calibrated and determined based on the sizes of the participant hand and fingers, as shown in Figures 5 and 6 . This means that a hand template and hand patterns belonging to a person cannot be used for others. Users should be cautious of this matter and try to set the hand template and patterns for each person. In addition, the shape of the index finger during hand pointing also depends on a person in which all of the participants may perform different gestures of the index finger. In practice, these should be trained for each person and then tested based on the trained data of the same person to avoid errors.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 194,
                    "end": 209,
                    "text": "Figures 5 and 6",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Hand Size Measurement and Hand Pointing Training"
        },
        {
            "text": "Our proposed POI determination method basically deals with a POI in a 3D space. Sometimes POI is located behind other objects. If an object (C) obstructs the vector of hand pointing, this method can find the POI, as shown in Figure 21 . However, in the case that another object (A) occludes the eye gaze of the participant, the proposed method actually cannot find the POI. This kind of occlusion on the eye gaze becomes the limitation of our proposed method. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 225,
                    "end": 234,
                    "text": "Figure 21",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Poi Occlusion against Eye Gaze"
        },
        {
            "text": "According to psychologist experiments [53] , a person may pause the hand pointing gesture within approximately 350-600 milliseconds. The POI determination system should complete all of the processes within the mentioned duration to prevent errors of the hand pointing vector and POI position. This is exactly a condition for the system design and development, and the time condition is regarded as another limitation.",
            "cite_spans": [
                {
                    "start": 38,
                    "end": 42,
                    "text": "[53]",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [],
            "section": "Available Processing Time"
        },
        {
            "text": "Human eyes naturally saccade in different range. The eye saccade range is used in this proposed method in order to determine the SOI based on the maximum distance of the POI. In our experiments, the average saccade range that is based on statistical data of many people is used to set the angle of the SOI cone. Precisely, saccade should be measured personally, and the measured angle of saccade range at the maximum distance of POI should be exactly used in the testing process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Size of SOI"
        },
        {
            "text": "In order to evaluate the performance of the proposed 3D POI determination using multimodal fusion of the hand pointing and eye gaze, experiments with 10 participants, five males and five females, were performed based on the case study specifications that are shown in Table 1 . An eye tracking sensor and hand tracking sensor that detect both pupils and all joint positions of the hand, respectively, were used as sensors, and a currently popular computer, operating system, and software were selected as the basic experimental system. The experimental equipment was conceptually designed in the layout that is shown in Figure 22 . In the equipment, nine poles were installed on the left-hand side or right-hand side of the examiner, depending on whether they were left or right-handed, as shown in Figure 23 . The system origin was set at the center of the eye tracking sensor, so that the measurement results of the eye tracker, which are the 2D coordinates of the virtual board and the 3D coordinates of the hand joints, were calibrated to the same origin. Calibrations of the eye tracker were initially performed by a virtual board that was made from a piece of paper. Figure 24a ,b show the layout of the virtual board and its photo. In the calibrations for converting points on the virtual board (2D) to points on the 3D coordinates, in which the origin is located in the center of the eye tracker, a person who is trained puts his/her chin on the fixed head stand to stabilize the head for several seconds, as shown in Figure 25a . The obtained coordinates (X, Y) of the five points on the virtual board and the measured 3D coordinates (X, Y, Z) of those five points on the board are used to fix the required parameters for the perspective transform, as shown in Figure 24b . These parameters and the perspective transform were applied in the calibrations that are mentioned in Section 5.1, and experiments on the multimodal fusion of eye and hand tracking were then performed, as shown in Figure 25b . Before testing, all of the participants were advised and trained the usage of experimental instruments until their ability was confirmed.The testing processes were performed in the same manner as those in the training state. Since hand size and hand pointing shape may differ among participants, each participant was calibrated and tested based on the individual trained data. Figure 25b shows examples of the processing of the hand-pointing pattern determination, index-finger straight detection, and eye gaze positions on nine poles in the experiments. The experimental results of the hand pointing mode are shown for a number of different testing samples presented in Table 2 . Table 3 shows the probability of handpointing pattern matching in each pattern obtained from our experiments. An evaluation of the proposed method for 3D coordination measurement was performed, and Table 4 shows the results. These measurement accuracies were compared with the results of the conventional methods in 2D, as shown in Table 5 . ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 268,
                    "end": 275,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 620,
                    "end": 629,
                    "text": "Figure 22",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 799,
                    "end": 808,
                    "text": "Figure 23",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1173,
                    "end": 1183,
                    "text": "Figure 24a",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1526,
                    "end": 1536,
                    "text": "Figure 25a",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1770,
                    "end": 1780,
                    "text": "Figure 24b",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1997,
                    "end": 2007,
                    "text": "Figure 25b",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 2387,
                    "end": 2397,
                    "text": "Figure 25b",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 2681,
                    "end": 2688,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 2691,
                    "end": 2698,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 2889,
                    "end": 2896,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 3023,
                    "end": 3030,
                    "text": "Table 5",
                    "ref_id": null
                }
            ],
            "section": "Experiments and Results"
        },
        {
            "text": "In order to express an intended 3D POI to others, humans normally point to the POI by hand pointing, or hand pointing with eye gaze. Hand pointing has been studied as the best priority to reveal intention on a POI based on visual estimation. Although methods of POG detection using eye gaze proved to be excellent ways to detect a point of gaze, the detected points of gaze or POGs are not necessary to be POI. To determine intention of human based on gestures, hand pointing is absolutely like a switch to trigger the intention. The authors of this paper have improved the decision of hand pointing vectors as compared with the conventional methods [22, 23] for POI detection on 2D screen, as shown by Euclidean distance errors presented in Table 5 .",
            "cite_spans": [
                {
                    "start": 650,
                    "end": 654,
                    "text": "[22,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 655,
                    "end": 658,
                    "text": "23]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [
                {
                    "start": 742,
                    "end": 749,
                    "text": "Table 5",
                    "ref_id": null
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "In the case of expressing an intended a POI in 3D space, another vector is required to find a crossing point, which can be either hand pointing with eye gaze or hand pointing with head direction [56] . Originally, it was proven that eye gaze is more precise than head direction, so that this paper proposed selecting a multimodal combination of hand pointing with eye gaze to find 3D POIs. The experimental results shown in Table 4 proved that performance of the combination of hand pointing and eye gaze is workable for 3D POI determination. Moreover, the proposed method can deal not only 3D, but also 2D as accuracy comparison with conventional methods for 2D POI, as shown in Table 5 .",
            "cite_spans": [
                {
                    "start": 195,
                    "end": 199,
                    "text": "[56]",
                    "ref_id": "BIBREF60"
                }
            ],
            "ref_spans": [
                {
                    "start": 424,
                    "end": 431,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 680,
                    "end": 687,
                    "text": "Table 5",
                    "ref_id": null
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "In future vehicles that are autonomously navigated without human drivers, passengers may feel free to work or enjoy activities in the passenger area, and a virtual human-machine interface that applies 3D POIs may play a larger role as a smart infrastructure in the cabin. Moreover, some counter services in the future may apply 3D POIs to decrease the number of human workers. These environments are examples in which 3D POIs will become increasingly essential, and users in these environments may not normally want to move their head and face to express intentions, because it is not a typical human method. Hand pointing is basically accepted as a good and accurate way for humans to show intention, but finding another straight line to exactly determine a POI on the line of hand pointing is required. This paper proposes the utilization of the eye gaze, which can be detected as another straight-line for POI determination in 3D space. In practice, there are many cases in which the two straight lines never cross in the 3D space due to human eye saccades and device errors and, thus, a point needs to be determined as the crossing point.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Therefore, this paper proposes the determination of the crossing point according to the closest distance between the two straight lines. The experimental results presented in Tables 4 and 5 show that our proposed method achieved an accuracy greater than 90% within a POI distance of one meter.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 175,
                    "end": 189,
                    "text": "Tables 4 and 5",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "In the error analysis, the errors of 3D POI determination are divided into two groups: errors in hand pointing determination representing intention and errors in multimodalfusion-based 3D POI determination. In the first group of hand-pointing determination errors, it is divided into two factors that are straight index finger detection and template matching. As shown in Table 2 , the accuracy of the first and second factors reach 95.53% and 100%, respectively. The errors of the straight index finger detection can be considered and analyzed in finger bending as the cause. The yellow index finger comes up over the thresholding range of the dash line, as shown by an example in Figure 28a . It is also obvious in the Hough domain that the yellow points representing straight index fingers, which are over the range, are allocated outside the pointing intention range, as shown in Figure 28b . Although this is actually close to the border of straight index finger detection, it is counted as out of the range based on the threshold. The thresholding value has been set to ensure the intention of hand pointing so that some located close to the border may not be picked up. In order to solve this problem, the training of hand pointing should be trained well enough to ensure the pointing ability. Originally, the index finger should be paused during the pausing period, but it vibrated slightly, as shown in Figure 29a . This caused an error in the position of the index finger, as shown in Figure 29b . In the Hough coordinates, some points were located outside of the rectangle representing the pointing intention range. In fact, the pointing intention range was initially determined based on the average data of training samples, and it is regarded as the average among a group of people. Sometimes, this is not suitable for all users. The pointing intention range should be trained individually for each user tso solve this problem. In the second cause of index finger vibration, the human index finger sometimes vibrates slightly, and its range normally varies, depending on the person. Our paper solves this problem by taking the average position as the pointing vector, which is not guaranteed to represent the real pointing vector. The pointing characteristics that are regarded as depending on a person may be reconsidered, and users may individually retrain the system in the training state.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 372,
                    "end": 379,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 682,
                    "end": 692,
                    "text": "Figure 28a",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 884,
                    "end": 894,
                    "text": "Figure 28b",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1412,
                    "end": 1422,
                    "text": "Figure 29a",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1495,
                    "end": 1505,
                    "text": "Figure 29b",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "For the second error group of multimodal-fusion-based 3D POI determination according to the experimental results presented in Table 4 , the eye gaze is ideally concentrated on a target of 3D POG. In reality, eye saccades always occur, and they cause errors in eye gaze tracking, as shown in Figure 30b . In the time domain, the eye gaze vibrates around the POI, as shown in Figure 30a , and these become errors that are caused by saccades within a range, as shown in Figure 30b . This should be considered statistically in solving this problem in future work. In addition, the hand pattern matching performed well in the experiments, as shown by the results presented in Table 2 , because all of the participants were trained appropriately before the experiments. Users are recommended to be trained before testing to avoid errors in hand pattern matching in real applications. Finally, the proposed method of 3D POI detection while using a multimodal fusion of hand pointing and eye gaze was shown to be able to find an acceptable 3D POI and it may often contribute to the virtual world in the future. As a trade-off in development of 3D POI detection system, users may obtain an excellent function of 3D POI detection, while users have to invest costs of some instruments such as eye tracker, Leap Motion sensor, processor, memory, and so on. Although these devices currently become much lower cost with more excellent functions, users should consider the total investment cost in the view point of worth paying. Moreover, excellent functions, which can help elderly, patient, and disabled people, may be traded off with complexity of the developed system. This method obviously utilizes two sensors, which are an eye tracker and a Leap Motion sensor, in implementation. Although the system has good accuracy and enables 3D POI determination, the investment cost and complexity of the system are regarded as trade-offs of the proposed method.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 126,
                    "end": 133,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 291,
                    "end": 301,
                    "text": "Figure 30b",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 374,
                    "end": 384,
                    "text": "Figure 30a",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 467,
                    "end": 477,
                    "text": "Figure 30b",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 671,
                    "end": 678,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "Finding a 3D POI, which plays an important role in the development of smart humanmachine interfaces in virtual worlds, requires reliability in terms of both accuracy and real-time operation in practice. In some special applications, such as in a cockpit, users are not allowed to move their head, face, or body due to safety reasons; therefore, the fusion of the eye gaze and hand pointing is an effective and human-friendly method of determining a 3D POI. Because the eye gaze has recently been shown to be reliably detected in determining a POG on a 2D screen, the straight-line that is drawn from the pupil to the POG on the screen can be extended to a 3D POI, although 3D POI determination still requires another straight line that crosses the straight-line extended in space. Hand pointing, which is considered to be a natural way for humans to indicate a 3D POI, can be used to address this issue. However, if the hand moves freely in the 3D space, then a hand pointing mode showing human intention should be defined to avoid intention errors. This paper proposes determining the hand pointing mode by first determining the hand pause duration and then comparing hand gestures that have straight index fingers and bent middle, ring, and pinky fingers. Subsequently, the hand pointing vector drawn along the straight index finger is confirmed to be within the SOI, and the 3D POI is finally determined based on the crossing point or closest distance between the two vectors of hand pointing and the eye gaze. Experiments that were performed with 10 participants show that the proposed method can measure 3D POIs located at a distance of one meter with an average distance error of approximately 5.25%. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "The authors declare no conflict of interest.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conflicts of Interest:"
        },
        {
            "text": "The following abbreviations are used in this manuscript: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abbreviations"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Inference of Human Intentions in Context Aware Systems",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Oyama",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mitra",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Handbook of Research on Ambient Intelligence and Smart Environments",
            "volume": "",
            "issn": "",
            "pages": "376--391",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Sensor Technology for Smart Homes",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "O&apos;grady",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Banos",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Sensors",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/s20247046"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "On Inferring Intentions in Shared Tasks for Industrial Collaborative Robots",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Olivares-Alarcos",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Foix",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Alenya",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/electronics8111306"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Improving Safety in Collaborative Robot Tasks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mandal",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sukhwani",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Jetley",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sarkar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 IEEE 17th International Conference on Industrial Informatics (INDIN)",
            "volume": "",
            "issn": "",
            "pages": "470--477",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Robotics Utilization for Healthcare Digitization in Global COVID-19 Management",
            "authors": [
                {
                    "first": "Z",
                    "middle": [
                        "H"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Siddique",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "W"
                    ],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Int. J. Environ. Res. Public Health",
            "volume": "17",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/ijerph17113819"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Usability Evaluation of the Touch Screen User Interface Design",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "Y"
                    ],
                    "last": "Hsiao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "J"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "International Conference on Human Interface and the Management of Information",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "In-Car Touch Screen Interaction; Comparing Standard, Finger-Specific and Multi-Finger Interaction",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Colley",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Vayrynen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hakkila",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 4th International Symposium on Pervasive Displays (PerDis '15)",
            "volume": "",
            "issn": "",
            "pages": "131--137",
            "other_ids": {
                "DOI": [
                    "10.1145/2757710.2757724"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Control of Smart Home Operations Using Natural Language Processing, Voice Recognition and IoT Technologies in a Multi-Tier Architecture",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Alexakis",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Panagiotakis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fragkakis",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Markakis",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Vassilakis",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/designs3030032"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Speech recognition results for voice-controlled assistive applications",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Caranica",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Cucu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Burileanu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Portet",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Vacher",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)",
            "volume": "",
            "issn": "",
            "pages": "6--9",
            "other_ids": {
                "DOI": [
                    "10.1109/SPED.2017.7990438"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "An In-Car Speech Recognition System for Disabled Drivers",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ivanecky",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mehlhase",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Text, Speech and Dialogue",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A roadmap for natural language processing research in information systems",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Thomas",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 50th Hawaii International Conference on System Sciences",
            "volume": "",
            "issn": "",
            "pages": "4--7",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Collaborative Systems for Smart Environments: Trends and Challenges. IFIP Advances in Information and Communication Technology",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "Luis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hamideh",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "PRO-VE, IFIP Advances in Information and Communication Technology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-662-44745-1_1"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Combining eye gaze and hand tracking for pointer control in HCI: Developing a more robust and accurate interaction system for pointer positioning and clicking",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Chuan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sivaji",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 2012 IEEE Colloquium on Humanities, Science and Engineering (CHUSER)",
            "volume": "",
            "issn": "",
            "pages": "172--176",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Saccade Target Selection During Visual Search",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Findlay",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Vis. Res",
            "volume": "37",
            "issn": "",
            "pages": "617--631",
            "other_ids": {
                "DOI": [
                    "10.1016/S0042-6989(96)00218-0"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Models of Horizontal Eye Movements, Part II: A 3rd Order Linear Saccade Model",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Enderle",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Synth. Lect. Biomed. Eng",
            "volume": "5",
            "issn": "",
            "pages": "1--159",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Are Pointing Gestures Induced by Communicative Intention?",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Nowikow",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Skeleton-based dynamic hand gesture recognition",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Smedt",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wannous",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Vandeborre",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Intent Inference for Hand Pointing Gesture-Based Interactions in Vehicles",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "I"
                    ],
                    "last": "Ahmad",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "K"
                    ],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Langdon",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hardy",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Skrypchuk",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Cybern",
            "volume": "46",
            "issn": "",
            "pages": "878--889",
            "other_ids": {
                "DOI": [
                    "10.1109/TCYB.2015.2417053"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Evaluation of the Leap Motion Controller as a New Contact-Free Pointing Device",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Bachmann",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Weichert",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Rinkenauer",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Sensors",
            "volume": "15",
            "issn": "",
            "pages": "214--233",
            "other_ids": {
                "DOI": [
                    "10.3390/s150100214"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Precise pointing direction estimation using depth data",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Das",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)",
            "volume": "",
            "issn": "",
            "pages": "202--207",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Oviatt",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "R"
                    ],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Potamianos",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "In Association for Computing Machinery and Morgan and Claypool",
            "volume": "14",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1145/3015783"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Handed Mid-Air Gestural HCI: Point + Command. In Human-Computer Interaction. Interaction Modalities and Techniques",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schwaller",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Brunner",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lalanne",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "HCI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "VGPN: Voice-Guided Pointing Robot Navigation for Humans",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Hall",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE International Conference on Robotics and Biomimetics (ROBIO)",
            "volume": "",
            "issn": "",
            "pages": "12--15",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Eye Gaze Controlled Projected Display in Automotive and Military Aviation Environments",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Prabhakar",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Biswas",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Multimodal Technol. Interact",
            "volume": "2",
            "issn": "1",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/mti2010001"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "How does eye-gaze relate to gesture movement in an automotive pointing task?",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "I"
                    ],
                    "last": "Ahmad",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "M"
                    ],
                    "last": "Langdon",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Skrypchuk",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Godsill",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Advances in Human Aspects of Transportation",
            "volume": "",
            "issn": "",
            "pages": "423--434",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "I see your point: Integrating gaze to enhance pointing gesture accuracy while driving",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Roider",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Gross",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, ser. AutomotiveUI '18",
            "volume": "",
            "issn": "",
            "pages": "351--358",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Tobii Eye Trcking 4c",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "3-d gaze estimation by stereo gaze direction",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Pichitwong",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chamnongthai",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 13th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)",
            "volume": "",
            "issn": "",
            "pages": "1--4",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "An eye-tracker-based 3d point-of-gaze estimation method using head movement",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Pichitwong",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chamnongthai",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Access",
            "volume": "7",
            "issn": "",
            "pages": "86--99",
            "other_ids": {
                "DOI": [
                    "10.1109/ACCESS.2019.2929195"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Pointing: Where Language, Culture, and Cognition Meet",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gullberg",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Prediction of user's grasping intentions based on eye-hand coordination",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Miguel",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Xavier",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems",
            "volume": "",
            "issn": "",
            "pages": "18--22",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Why do the eyes prefer the index finger? Simultaneous recording of eye and hand movements during precision grasping",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cavina-Pratesi",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hesse",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "J. Vis",
            "volume": "13",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1167/13.5.15"
                ]
            }
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Pointing and reference reconsidered",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lucking",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pfeiffer",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Rieser",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "J. Pragmat",
            "volume": "77",
            "issn": "",
            "pages": "56--79",
            "other_ids": {
                "DOI": [
                    "10.1016/j.pragma.2014.12.013"
                ]
            }
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Eyepointing: A gaze based selection technique",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schweigert",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Schwind",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mayer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proc. Mensch Comput",
            "volume": "",
            "issn": "",
            "pages": "719--723",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Human Factors Design Guide for Acquisition of Commercial-off-the-Shelf Subsystems, Non-Developmental Items, and Developmental Systems; United States Department of Transportation",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "M D"
                    ],
                    "last": "Wagner Dan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Joseph",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Human factors of visual and cognitive performance in driving",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "S"
                    ],
                    "last": "Young",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Ergonomics",
            "volume": "53",
            "issn": "",
            "pages": "444--445",
            "other_ids": {
                "DOI": [
                    "10.1080/00140130903494785"
                ]
            }
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Visual Field in Encyclopedia of Neuroscience",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Strasburger",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Poppel",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Comparative study of the blinking time between young adult and adult video display terminal users in indoor environment",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Mara",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Arthur",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fernando",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Newton Kara",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Arq. Bras. Oftalmol",
            "volume": "72",
            "issn": "",
            "pages": "682--686",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Weakly supervised learning with multi-stream cnn-lstm-hmms to discover sequential parallelism in sign language videos",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Koller",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Camgoz",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ney",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bowden",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "42",
            "issn": "",
            "pages": "2306--2320",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2019.2911077"
                ]
            }
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Introducing the Skeletal Tracking Model",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Leap Motion Sensor",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Geometry-based camera calibration using five-point correspondences from a single image",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Circuits Syst. Video Technol",
            "volume": "27",
            "issn": "",
            "pages": "2555--2566",
            "other_ids": {
                "DOI": [
                    "10.1109/TCSVT.2016.2595319"
                ]
            }
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Rotations and rotation matrices",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "R"
                    ],
                    "last": "Evans",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Acta Crystallogr. Sect. Biol. Crystallogr",
            "volume": "57",
            "issn": "",
            "pages": "1355--1359",
            "other_ids": {
                "DOI": [
                    "10.1107/S0907444901012410"
                ]
            }
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Mathematics of Computing the 2D Coordinates of a 3D Point",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Line Segment Detection with Hough Transform Based on Minimum Entropy",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "S"
                    ],
                    "last": "Shin",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Image and Video Technology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "The anatomy and mechanics of the human hand",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Taylor",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Schwarz",
                    "suffix": ""
                }
            ],
            "year": 1955,
            "venue": "Artif. Limbs",
            "volume": "2",
            "issn": "",
            "pages": "22--35",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Multi-finger coordination in healthy subjects and stroke patients: A mathematical modelling approach",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ilaria",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Johanna",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Maurizio",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J. Neuroeng. Rehabil",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Command without a click: Dwell time typing by mouse and gaze selections",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Nsen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Johansen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hansen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Itoh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mashino",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Dwell-based pointing in applications of human computer interaction",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "Tomfelde",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the Human-Computer Interaction-INTERACT 2007",
            "volume": "",
            "issn": "",
            "pages": "560--573",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Nearest approaches to multiple lines in n-dimensional space",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Bancroft",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Crewes Res. Rep",
            "volume": "22",
            "issn": "",
            "pages": "1--17",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Generation of 3D microstructure model for discontinuously reinforced composite by modified random sequential absorption method",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Gokhale",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "J. Eng. Mater. Technol",
            "volume": "138",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1115/1.4032152"
                ]
            }
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Understanding Human Behaviors Based on Eye-Head-Hand Coordination; Biologically Motivated Computer Vision. BMCV",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ballard",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Lecture Notes in Computer Science",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Three-dimensional point-of-intention (3D POI) determined through the eye gaze and hand pointing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Human intention determined through the eye gaze and hand pointing: (a) Human intention through the eye gaze and hand pointing and (b) space of interest (SOI) and hand pointing during identification of intention.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Pupil positions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Deletion of pupil disappearance duration: (a) Pupil disappearance duration, (b) Differentiation of pupil position, and (c) Modified of pupil position.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Definition of hand and template: (a) Finger joints and skeleton, (b) Template, and (c) Matrix.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Cont.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Comparison of a hand pointing pattern in different postures: (a) Hand pointing in origin posture with coordinates, (b) Hand pointing in rotated posture, (c) Hand pointing with slight changes, ring, and pinky finger, (d) 3D binary pattern of (a), (e) 3D binary pattern of (b), (f) 3D binary pattern of (c), and (g) Two-dimensional (2D) binary patterns of (a,c)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Hand pointing patterns: (a) Pattern PT 1 , (b) Pattern PT 2 , (c) Pattern PT 3 , (d) Pattern PT 4 , (e) Pattern PT 5 , (f) Pattern PT 6 , (g) Pattern PT 7 , and (h) Pattern PT 8 .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Pause duration and transition: (a) A sequence of finger joint form, (b) Differences of consecutive frames.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Scenario of a human-machine interface in a vehicle.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Overview system of multimodal of eye gaze and hand pointing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Flowchart of proposed method.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "System calibration.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Template range determination.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "3D-to-2D hand projection: (a) 3D hand skeleton detection, (b) Rotation, and (c) 2D projection. Algorithm 1 depicts the algorithm of 3D-to-2D hand projection. Algorithm 1 Algorithm for 3D-to-2D hand projection. Input: 3D finger joint positions J Input: 3D wrist position W 1: compute J = J \u2212 W 2: for i each column do 3: for j each joint do 4: compute rotation of J[i][j](X , Y , Z ) 5: compute projection of J[i][j](X , Y , Z ) \u2212\u2192 J[i][j](X , Z )",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Index fingers with Hough transform: (a) Straight index finger, (b) Straight index finger in Hough domain, (c) Bending index finger considered as straight, (d) Bending index finger in Hough domain, (e) Straight index finger with failed determination, and (f) Failed straight index finger in Hough domain. Algorithm 2 Algorithm for straight index finger detection. Input: range of \u03b8 as \u03b8 u and \u03b8 l Input: range of \u03b3 as \u03b3 u and \u03b3 l Input: index finger joints as I = [I1, I2, I3, I4] 1: for i each joint of index finger do 2:for j = 0 to 180 do3:    compute I[i](Z, Y) as the Hough transform of each j degree \u2212\u2192 H t (\u03b3, \u03b8) intersection of H t (\u03b3, \u03b8) and \u03b3 0 \u2212\u2192 (\u03b3 p , \u03b8 p ) 6:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Template matching: (a) Hand skeleton projection, (b) Template matching and (c) Matrix of binary pattern.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Algorithm for pattern matching.Input: PT1, PT2, PT3, PT4, PT5, PT6, PT7, PT8 Input: hand pattern as T at current frame1:  for i each pattern of templates do2:    calculate HP = T \u2227 (PT1 \u2228 PT2 \u2228 ..",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "Hand-pointing direction estimation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "Thresholding for finding pause as pointing duration.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "Crossing point determination using closest point principle.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "Intended point detection when eye gaze is obstructed by an object.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "Experimental equipment model.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF25": {
            "text": "A photograph of an experimental equipment.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF26": {
            "text": "Virtual board: (a) Layout of points on a virtual board, (b) Photograph of a virtual board with a head fixing stand. Photographs of head position fixed in the experiments: (a) Front view, and (b) Side view.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF27": {
            "text": "b show an example of the experimental results of hand-pointing patterning matching and straightness detection of the index finger, respectively.Figure 27shows the experimental results of eye gaze detection against nine poles. Example of process of hand pointing determination: (a) Hand pointing determination and (b) Index finger straight detection.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF28": {
            "text": "Trajectories of eye gaze at the intended points of nine poles.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF29": {
            "text": "Hand error during pointing mode: (a) Index finger over range during pointing mode and (b) Error caused from over range during pointing mode.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF30": {
            "text": "Error of straight index finger determination caused by vibration during hand pointing: (a) Index finger vibration, and (b) Index finger position.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF31": {
            "text": "Error caused by saccade: (a) Eye gazes in time domain, and (b) Eye gazes in Hough domain.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF32": {
            "text": "Contributions: Conceptualization, S.Y. and K.C.; Methodology, S.Y. and K.C.; Software, S.Y.; Validation, S.Y. and K.C.; Formal analysis, K.C.; Investigation, K.C.; Resources, S.Y.; Data curation, S.Y.; writing-original draft preparation, K.C.; writing-review and editing, S.Y. and K.C.; Visualization, S.Y.; supervision, K.C.; project administration, K.C.; Funding acquisition, S.Y. All authors have read and agreed to the published version of the manuscript.Funding: The research project is financially supported by Petchra Pra Jom Klao Research Scholarship (Grant No. 24/2561) from King Mongkut's University of Technology Thonburi in Bangkok, Thailand. Institutional Review Board Statement: Not applicable. Informed Consent Statement: Informed consent was obtained from all subjects involved in the study. Data Availability Statement: Not applicable.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "System configuration and experimental setup.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Hand pointing determination accuracy.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Appearance probability of hand patterns.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Euclidean distance error of the proposed method in 3D spaces.Table 5. Comparison of POI detection errors between conventional and proposed methods in 2D and 3D spaces.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "POG Two-Dimensional Point of Gaze 3D POG Three-Dimensional Point of Gaze SOI Space of interest AOI Area of Interest",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
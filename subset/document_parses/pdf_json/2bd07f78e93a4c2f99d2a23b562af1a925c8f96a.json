{"paper_id": "2bd07f78e93a4c2f99d2a23b562af1a925c8f96a", "metadata": {"title": "Competitive Online Quantile Regression", "authors": [{"first": "Raisa", "middle": [], "last": "Dzhamtyrova", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of London", "location": {"settlement": "Egham", "country": "UK"}}, "email": "raisa.dzhamtyrova.2015@live.rhul.ac.uk"}, {"first": "(", "middle": ["B"], "last": "", "suffix": "", "affiliation": {}, "email": ""}, {"first": "Yuri", "middle": [], "last": "Kalnishkan", "suffix": "", "affiliation": {"laboratory": "", "institution": "University of London", "location": {"settlement": "Egham", "country": "UK"}}, "email": ""}]}, "abstract": [{"text": "Interval prediction often provides more useful information compared to a simple point forecast. For example, in renewable energy forecasting, while the initial focus has been on deterministic predictions, the uncertainty observed in energy generation raises an interest in producing probabilistic forecasts. One aims to provide prediction intervals so that outcomes lie in the interval with a given probability. Therefore, the problem of estimating the quantiles of a variable arises. The contribution of our paper is two-fold. First, we propose to apply the framework of prediction with expert advice for the prediction of quantiles. Second, we propose a new competitive online algorithm Weak Aggregating Algorithm for Quantile Regression (WAAQR) and prove a theoretical bound on the cumulative loss of the proposed strategy. The theoretical bound ensures that WAAQR is asymptotically as good as any quantile regression. In addition, we provide an empirical survey where we apply both methods to the problem of probability forecasting of wind and solar powers and show that they provide good results compared to other predictive models.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Probabilistic forecasting attracts an increasing attention in sports, finance, weather and energy fields. While an initial focus has been on deterministic forecasting, probabilistic prediction provides a more useful information which is essential for optimal planning and management in these fields. Probabilistic forecasts serve to quantify the uncertainty in a prediction, and they are an essential ingredient of optimal decision making [4] . An overview of the state of the art methods and scoring rules in probabilistic forecasting can be found in [4] . Quantile regression is one of the methods which models a quantile of the response variable conditional on the explanatory variables [6] .", "cite_spans": [{"start": 439, "end": 442, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 552, "end": 555, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 690, "end": 693, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Introduction"}, {"text": "Due to its ability to provide interval predictions, quantile regression found its niche in the renewable energy forecasting area. Wind power is one of the fastest growing renewable energy sources [3] . As there is no efficient way to store wind power, producing accurate wind power forecasts are essential for reliable operation of wind turbines.", "cite_spans": [{"start": 196, "end": 199, "text": "[3]", "ref_id": "BIBREF2"}], "ref_spans": [], "section": "Introduction"}, {"text": "Due to the uncertainty in wind power generation, there have been studies for improving the reliability of power forecasts to ensure the balance between supply and demand at electricity market. Quantile regression has been extensively used to produce wind power quantile forecasts, using a variety of explanatory variables such as wind speed, temperature and atmospheric pressure [7] .", "cite_spans": [{"start": 379, "end": 382, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Introduction"}, {"text": "The Global Energy Forecasting Competition 2014 showed that combining predictions of several regressors can produce better results compared to a single model. It is shown in [9] that a voted ensemble of several quantile predictors could produce good results in probabilistic solar and wind power forecasting. In [1] the analogue ensemble technique is applied for prediction of solar power which slightly outperforms the quantile regression model.", "cite_spans": [{"start": 173, "end": 176, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 311, "end": 314, "text": "[1]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Introduction"}, {"text": "In this paper we apply a different approach to combine predictions of several models based on the method of online prediction with expert advice. Contrary to batch mode, where the algorithm is trained on training set and gives predictions on test set, in online setting we learn as soon as new observations become available. One may wonder why not to use predictions of only one best expert from the beginning and ignore predictions of others. First, sometimes we cannot have enough data to identify the best expert from the start. Second, good performance in the past does not necessary lead to a good performance in the future. In addition, previous research shows that combining predictions of multiple regressors often produce better results compared to a single model [11] .", "cite_spans": [{"start": 773, "end": 777, "text": "[11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Introduction"}, {"text": "We consider the adversarial setting, where no stochastic assumptions are made about the data generating process. Our approach is based on Weak Aggregating Algorithm (WAA) which was first introduced in [5] . The WAA works as follows: we assign initial weights to experts and at each step the weights of experts are updated according to their performance. The approach is similar to the Bayesian method, where the prediction is the average over all models based on the likelihood of the available data. The WAA gives a guarantee ensuring that the learner's loss is as small as best expert's loss up to an additive term of the form C \u221a T , where T is the number of steps and C is some constant. It is possible to apply WAA to combine predictions of an infinite pool of experts. In [8] WAA was applied to the multi-period, distribution-free perishable inventory problem, and it was shown that the asymptotic average performance of the proposed method was as good as any time-dependent stocking rule up to an additive term of the form C \u221a T ln T . The WAA was proposed as an alternative to the Aggregating Algorithm (AA), which was first introduced in [12] . The AA gives a guarantee ensuring that the learner's loss is as small as best expert's loss up to a constant in case of finitely many experts. The AA provides better theoretical guarantees, however it works with mixable loss functions, and it is not applicable in our task. An interesting application of the method of prediction with expert advice for the Brier loss function in forecasting of football outcomes can be found in [14] ; it was shown that the proposed strategy that follows AA is as good as any bookmaker. Aggregating Algorithm for Regression (AAR) which competes with any expert from an infinite pool of linear regressions under the square loss was proposed in [13] .", "cite_spans": [{"start": 201, "end": 204, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 778, "end": 781, "text": "[8]", "ref_id": "BIBREF7"}, {"start": 1147, "end": 1151, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 1582, "end": 1586, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 1830, "end": 1834, "text": "[13]", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "Introduction"}, {"text": "The contribution of our paper is two-fold. First, as a proof of concept, we apply WAA to a finite pool of experts to show that this method is applicable for this problem.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "As our experts we pick several models that provide quantile forecasts and then combine their predictions using WAA. To the best of our knowledge prediction with expert advice was not applied before for the prediction of quantiles. Second, we propose a new competitive online algorithm Weak Aggregating Algorithm for Quantile Regression (WAAQR), which is as good as any quantile regression up to an additive term of the form C \u221a T ln T . For this purpose, we apply WAA to an infinite pool of quantile regressions. While the bound for the finite case can be straightforwardly applied to finite or countable sets of experts, every case of a continuous pool needs to be dealt with separately. We listed above a few results for different specific pools of experts, however there is no generic procedure for deriving a theoretical bound for the cumulative loss of the algorithm. WAAQR can be implemented by using Markov chain Monte Carlo (MCMC) method in a way which is similar to the algorithm introduced in [15] , where AAR was applied to generalised linear regression class of function for making a prediction in a fixed interval. We derive a theoretical bound on the cumulative loss of our algorithm which is approximate (in the number of MCMC steps). MCMC is only a method for evaluating the integral and it can be replaced by a different numerical method. Theoretical convergence of the Metropolis-Hastings method in this case follows from Theorems 1 and 3 in [10] . Estimating the convergence speed is more difficult. With the experiments provided we show that by tuning parameters online, our algorithm moves fast to the area of high values of the probability function and gives a good approximation of the prediction.", "cite_spans": [{"start": 1003, "end": 1007, "text": "[15]", "ref_id": "BIBREF14"}, {"start": 1460, "end": 1464, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Introduction"}, {"text": "We apply both methods to the problem of probabilistic forecasting of wind and solar power. Experimental results show a good performance of both methods. WAA applied to a finite set of models performs close or better than the retrospectively best model, whereas WAAQR outperforms the best quantile regression model that was trained on the historical data.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "In the framework of prediction with expert advice we need to specify a game which contains three components: a space of outcomes \u03a9, a decision space \u0393 , and a loss function \u03bb : \u03a9 \u00d7 \u0393 \u2192 R. We consider a game with the space of outcomes \u03a9 = [A, B] and decision space \u0393 = R, and as a loss function we take the pinball loss for q \u2208 (0, 1)", "cite_spans": [], "ref_spans": [], "section": "Framework"}, {"text": "This loss function is appropriate for quantile regression because on average it is minimized by the q-th quantile. Namely, if Y is a real-valued random variable with a cumulative distribution function", "cite_spans": [], "ref_spans": [], "section": "Framework"}, {"text": "In many tasks predicted outcomes are bounded. For example, wind and solar power cannot reach infinity. Therefore, it is possible to have a sensible estimate for the outcome space \u03a9 based on the historical information.", "cite_spans": [], "ref_spans": [], "section": "Framework"}, {"text": "Learner works according to the following protocol:", "cite_spans": [], "ref_spans": [], "section": "Framework"}, {"text": "for t = 1, 2, . . . nature announces signal x t \u2286 R n learner outputs prediction \u03b3 t \u2208 \u0393 nature announces outcome y t \u2208 \u03a9 learner suffers loss \u03bb(y t , \u03b3 t ) end for", "cite_spans": [], "ref_spans": [], "section": "Protocol 1"}, {"text": "The cumulative loss of the learner at the step T is:", "cite_spans": [], "ref_spans": [], "section": "Protocol 1"}, {"text": "We want to find a strategy which is capable of competing in terms of cumulative loss with all prediction strategies E \u03b8 , \u03b8 \u2208 R n (called experts) from a given pool, which output \u03be t (\u03b8) at step t. In a finite case we denote experts E i , i = 1, . . . , N.", "cite_spans": [], "ref_spans": [], "section": "Protocol 1"}, {"text": "Let us denote L \u03b8 T the cumulative loss of expert E \u03b8 at the step T :", "cite_spans": [], "ref_spans": [], "section": "Protocol 1"}, {"text": "In the framework of prediction with expert advice we have access to experts' predictions at each time step and the learner has to make a prediction based on experts' past performance. We use an approach based on the WAA since a pinball loss function \u03bb(y, \u03b3) is convex in \u03b3. The WAA maintains experts' weights P t (d\u03b8), t = 1, . . . , T . After each step t the WAA updates the weights of the experts according to their losses:", "cite_spans": [], "ref_spans": [], "section": "Weak Aggregating Algorithm"}, {"text": "where P 0 (d\u03b8) is the initial weights of experts and c is a positive parameter. Experts that suffer large losses will have smaller weights and less influence on futher predictions.", "cite_spans": [], "ref_spans": [], "section": "Weak Aggregating Algorithm"}, {"text": "The prediction of WAA is a weighted average of the experts' predictions:", "cite_spans": [], "ref_spans": [], "section": "Weak Aggregating Algorithm"}, {"text": "where P * t\u22121 (d\u03b8) are normalized weights:", "cite_spans": [], "ref_spans": [], "section": "Weak Aggregating Algorithm"}, {"text": "where \u0398 is a parameter space, i.e. \u03b8 \u2208 \u0398.", "cite_spans": [], "ref_spans": [], "section": "Weak Aggregating Algorithm"}, {"text": "In a finite case, an integral in (5) is replaced by a weighted sum of experts' predictions \u03be t (i), i = 1, . . . , N.", "cite_spans": [], "ref_spans": [], "section": "Weak Aggregating Algorithm"}, {"text": "In particular, when there are finitely many experts E i , i = 1, . . . , N for bounded games the following lemma holds. [5] ). For every L > 0, every game \u03a9, \u0393, \u03bb such that |\u03a9| < +\u221e with \u03bb(y, \u03b3) \u2264 L for all y \u2208 \u03a9 and \u03b3 \u2208 \u0393 and every N = 1, 2, . . . for every merging strategy for N experts that follows the WAA with initial weights", "cite_spans": [{"start": 120, "end": 123, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "Weak Aggregating Algorithm"}, {"text": "is guaranteed for every T = 1, 2, . . . and every i = 1, 2, . . . , N.", "cite_spans": [], "ref_spans": [], "section": "Lemma 1 (Lemma 11 in"}, {"text": "After taking equal initial weights p 1 = p 2 = \u00b7 \u00b7 \u00b7 = p N = 1/N in the WAA, the additive term reduces to (cL 2 + (ln N )/c) \u221a T . When c = \u221a ln N/L, this expression reaches its minimum. The following corollary shows that the WAA allows us to obtain additive terms of the form C \u221a T .", "cite_spans": [], "ref_spans": [], "section": "Lemma 1 (Lemma 11 in"}, {"text": "Applying Lemma 1 for an infinite number of experts and taking a positive constant c = 1, we get the following Lemma. [8] ). Let \u03bb(y, \u03b3) \u2264 L for all y \u2208 \u03a9 and \u03b3 \u2208 \u0393 . The WAA guarantees that, for all T", "cite_spans": [{"start": 117, "end": 120, "text": "[8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Corollary 1 (Corollary 14 in [5]). Under the conditions of Lemma 1, there is a merging strategy such that the bound"}, {"text": "L T \u2264 \u221a T \u2212 ln \u0398 exp \u2212 L \u03b8 T \u221a T P 0 (d\u03b8) + L 2 .", "cite_spans": [], "ref_spans": [], "section": "Lemma 2 (Lemma 2 in"}, {"text": "In this section we formulate the theoretical bounds of our algorithm. We want to find a strategy which is capable of competing in terms of cumulative loss with all prediction strategies E \u03b8 , \u03b8 \u2208 \u0398 = R n , which at step t output:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "where x t is a signal at time t. The cumulative loss of expert E \u03b8 is defined in (3).", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "and \u03b3 \u2208 \u0393 . There exists a prediction strategy for Learner such that for every positive integer T , every sequence of outcomes of length T , and every \u03b8 \u2208 R n with initial distribution of parameters", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "the cumulative loss L T of Learner satisfies", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "The theorem states that the algorithm predicts as well as the best quantile regression, defined in (6), up to an additive regret of the order \u221a T ln T . The choice of the regularisation parameter a is important as it affects the behaviour of the theoretical bound of our algorithm. Large parameters of regularisation increase the bound by an additive term \u221a T a \u03b8 1 , however the regret term has a smaller growth rate as time increases. As the maximum time T is usually not known in advance, the regularisation parameter a cannot be optimised, and its choice depends on the particular task. We discuss the choice of the parameter a in Sect. 6.2.", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "Proof. We consider that outcomes come from the interval [A, B], and it is known in advance. Let us define the truncated expert\u1ebc \u03b8 which at step t outputs:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "Let us denoteL \u03b8 T the cumulative loss of expert\u1ebc \u03b8 at the step T :", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "We apply WAA for truncated experts\u1ebc \u03b8 . As experts\u1ebc \u03b8 output predictions inside the interval [A, B], and predictions of WAA is a weighted average of experts' predictions (5), then each \u03b3 t lies in the interval [A, B]. We can bound the maximum loss at each time step:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "Applying Lemma 2 for initial distribution (7) and putting the bound on the loss in (10) we obtain:", "cite_spans": [{"start": 42, "end": 45, "text": "(7)", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "whereJ", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "For all \u03b8, \u03b8 0 \u2208 R n we have:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "Analogously, we have:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "By multiplying inequality (13) by (1 \u2212 q), inequality (14) by q and summing them, we have:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "The cumulative loss of truncated expert\u1ebc \u03b8 cannot exceed the cumulative loss of non-truncated expert E \u03b8 for all \u03b8 \u2208 R n : L \u03b8 T \u2264 L \u03b8 T . By dividing (15) by \u221a T and adding a \u03b8 1 to both parts, we have:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "We evaluate the integral:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "By putting this expression in (11) we obtain the theoretical bound.", "cite_spans": [{"start": 30, "end": 34, "text": "(11)", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "Note that even though we apply WAA for truncated experts (8), we achieve the theoretical bound for prediction strategy that competes with a class of experts (6).", "cite_spans": [], "ref_spans": [], "section": "Theoretical Bounds for WAAQR"}, {"text": "A prediction of WAA (5) can be re-written as follows:", "cite_spans": [], "ref_spans": [], "section": "Prediction Strategy"}, {"text": "where", "cite_spans": [], "ref_spans": [], "section": "Prediction Strategy"}, {"text": "and Z is the normalising constant ensuring that \u0398 w * T (\u03b8)d\u03b8 = 1. Integral (16) is a Bayesian mixture, where function \u03be T (\u03b8) needs to be integrated with respect to the normalized distribution w * T (\u03b8). It is possible to avoid the calculation of normalising constant Z as it is a computationally inefficient operation, and integrate function \u03be T (\u03b8) from the unnormalized distribution w T (\u03b8). In order to calculate the integral (16), it is possible to use MCMC algorithms. A good introduction of MCMC for Machine Learning is in [2] .", "cite_spans": [{"start": 531, "end": 534, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Prediction Strategy"}, {"text": "We will use Metropolis-Hastings algorithm for sampling parameters \u03b8 from the posterior distribution P. As a proposal distribution we choose Gaussian distribution N (0, \u03c3 2 ) with some chosen parameter \u03c3. We start with some initial parameter \u03b8 0 and at each step m we update:", "cite_spans": [], "ref_spans": [], "section": "Prediction Strategy"}, {"text": "where M is a maximum number of iterations in MCMC method.", "cite_spans": [], "ref_spans": [], "section": "Prediction Strategy"}, {"text": "The update parameter \u03b8 m at step m is accepted with probability min 1, fP (\u03b8 m ) fP (\u03b8 m\u22121 ) , where f P (\u03b8) is the density function for the distribution P at point \u03b8. At each step by accepting and rejecting the updates of parameters \u03b8 we move closer to the maximum of the density function. At the beginning it is common to use a 'burn-in' stage when the integral is not calculated till we will reach the area of high values of the density function f P . Thus, we perform integration only from the area with high density of P. Some values of \u03b8 are accepted even when the calculated probability is less than 1, it allows the algorithm to move away from local minimum of the density function. Because we are interested only in the ratio of density functions of generated parameters, we can generate new parameters \u03b8 from the unnormalized posterior distribution w T (\u03b8) and avoid the weights normalization at each step which is more computationally efficient.", "cite_spans": [], "ref_spans": [], "section": "Prediction Strategy"}, {"text": "At time t = 0 the algorithm starts with the initial estimate of the parameters \u03b8 0 = 0. At each iteration t > 0 we start with parameter \u03b8 M t\u22121 calculated at the previous step t \u2212 1. It allows the algorithm to converge faster to the correct location of the main mass of the distribution.", "cite_spans": [], "ref_spans": [], "section": "Prediction Strategy"}, {"text": "Parameters: number M > 0 of MCMC iterations, standard deviation \u03c3 > 0, regularization coefficient a > 0 initialize \u03b8 M 0 := 0 \u2208 R n define w 0 (\u03b8) := exp(\u2212a \u03b8 1 ) for t = 1, 2, . . . do \u03b3 t := 0 define w t (\u03b8) by (17) read x t \u2208 R n initialize \u03b8 0 t = \u03b8 M t\u22121 for m = 1, 2, . . . , M do \u03b8 * := \u03b8 m\u22121 t + N (0, \u03c3 2 I) flip a coin with success probability min 1, w t\u22121 (\u03b8 * )/w t\u22121 (\u03b8 m\u22121 t ) if success then \u03b8 m t := \u03b8 * else \u03b8 m t := \u03b8 m t\u22121 end if \u03b3 t := \u03b3 t +\u03be t (\u03b8 m t ) end for output predictions \u03b3 t = \u03b3 t /M end for", "cite_spans": [], "ref_spans": [], "section": "WAAQR"}, {"text": "In this section we apply WAA and WAAQR for prediction of wind and solar power and compare their performance with other predictive models. The data set is downloaded from Open Power System Data which provides free and open data platform for power system modelling. The platform contains hourly measurements of geographically aggregated weather data across Europe and time-series of wind and solar power. Our training data are measurements in Austria from January to December 2015, test set contains data from January to July 2016. 1", "cite_spans": [], "ref_spans": [], "section": "Experiments"}, {"text": "We apply WAA for three models: Quantile Regression (QR), Quantile Random Forests (QRF), Gradient Boosting Decision Trees (GBDT). These models were used in GEF-Com 2014 energy forecasting competition on the final leaderboard [9] . In this paper the authors argue that using multiple regressors is often better than using only one, and therefore combine multiple model outputs. They noted that voting was found to be particularly useful for averaging the quantile forecasts of different models.", "cite_spans": [{"start": 224, "end": 227, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "WAA"}, {"text": "We propose an alternative approach to combine different models' predictions by using WAA. We work according to Protocol 1: at each step t before seeing outcome y t , we output our prediction \u03b3 t according to (5) . After observing outcome y t , we update experts' weights according to (4) .", "cite_spans": [{"start": 208, "end": 211, "text": "(5)", "ref_id": "BIBREF4"}, {"start": 284, "end": 287, "text": "(4)", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "WAA"}, {"text": "To build models for wind power forecasting we use wind speed and temperature as explanatory variables. These variables have been extensively used to produce wind power quantile forecasts [7] . We train three models QR, QRF and GBDT on training data set, and then apply WAA using forecasts of these models on test data set. We start with equal initial weights of each model and then update their weights according to their current performance. We estimate the constant of WAA c = 0.01 using information about maximum losses on training set. Figure 1 shows weights of each model for different quantiles depending on the current time step. We can see from the graph that for most of quantiles GBDT obtains the largest weights which indicates that it suffers smaller losses compared to other models. However, it changes for q = 0.95, where the largest weights are acquired by QR. It shows that sometimes we can not use the past information to evaluate the best model. The retrospectively best model can perform worse in the future as an underlying nature of data generating can change. In addition, different models can perform better on different quantiles. Table 1 illustrates total losses of QR, QRF, GBDT, WAA and Average methods, where Average is a simple average of QR, QRF and GBDT. For the prediction of wind power, for q = 0.25 and q = 0.50 the total loss of WAA is slightly higher than the total loss of GBDT, whereas for q = 0.75 and q = 0.95 WAA has the smallest loss. In most cases, WAA outperforms Average method.", "cite_spans": [{"start": 187, "end": 190, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [{"start": 540, "end": 548, "text": "Figure 1", "ref_id": null}, {"start": 1155, "end": 1162, "text": "Table 1", "ref_id": "TABREF0"}], "section": "WAA"}, {"text": "We perform similar experiments for prediction of solar power. We choose measurements of direct and diffuse radiations to be our explanatory variables. In a similar way, QR, QRF and GBDT are trained on training set, and WAA is applied on test data. Figure 2 illustrates weights of models depending on the current step. Opposite to the previous experiments, GBDT has smaller weights compared to other models for q = 0.25 and q = 0.50. However, for q = 0.75 and q = 0.95 weights of experts become very close to each other. Therefore, predictions of WAA should become close to Average method. Table 1 shows total losses of the methods. For q = 0.25 and q = 0.5 both QR and QRF have small losses compared to GBDT, and WAA follows their predictions. However, for q = 0.75 and q = 0.95 it is not clear which model performs better, and predictions of WAA almost coincide with Average method. It again illustrates that the retrospectively best model could change with time, and one should be cautious about choosing the single retrospectively best model for future forecasts. ", "cite_spans": [], "ref_spans": [{"start": 248, "end": 256, "text": "Figure 2", "ref_id": "FIGREF0"}, {"start": 589, "end": 596, "text": "Table 1", "ref_id": "TABREF0"}], "section": "WAA"}, {"text": "In this section we demonstrate the performance of our algorithm for prediction of wind power and compare it with quantile regression model. We train QR on training data set, and apply WAAQR on test set. First, we use training set to choose the parameters of our algorithm. Table 2 illustrates the acceptance ratio of new sampling parameters of our algorithm for q = 0.5. Increasing values of \u03c3 results in decreasing acceptance ratios of new sampling parameters \u03b8. With large values of \u03c3 we move faster to the area of high values of density function while smaller values of \u03c3 can lead to more expensive computations as our algorithm would require more iterations to find the optimal parameters. Figure 3 illustrates logarithm of parameters likelihood w(\u03b8) defined in (17) for a = 0.1 and \u03c3 = 0.5 and 3.0. We can see from the graphs that for \u03c3 = 3.0 the algorithm reaches maximum value of log-likelihood after around 800 iterations while for \u03c3 = 0.5 it still tries to find maximum value after 1500 iterations. Table 2 shows the total losses of WAAQR for different parameters a and \u03c3. We can see that choosing the right parameters is very important as it notably affects the performance of WAAQR. It is important to keep track of acceptance ratio of the algorithm, as high acceptance ratio means that we move too slowly and need more iterations and larger 'burn-in' period to find the optimal parameters. Now we compare performances of our algorithm and QR. We choose the parameters of WAAQR to be the number of iterations M = 1500, 'burn-in' stage M 0 = 300, regularization parameter a = 0.1, and standard deviation \u03c3 = 3. Note that even though we use the prior knowledge to choose the parameters of WAAQR, we start with initial \u03b8 0 = 0 and train our algorithm only on the test set. Figure 4 illustrates a difference between cumulative losses of QR and WAAQR. If the difference is greater than zero, our algorithm shows better results compared to QR. For q = 0.25 WAAQR shows better performance at the beginning, but after around 1000 iterations its performance becomes worse, and by the end of the period cumulative losses of QR and WAAQR are almost the same. We observe a different picture for q = 0.5 and q = 0.75: most of the time a difference between cumulative losses is positive, which indicates that WAAQR performs better than QR. Figure 5 shows predictions of WAAQR and QR with [25%, 75%] confidence interval for the first and last 100 steps. We can see from the graph, that initially predictions of WAAQR are very different from predictions of QR. However, by the end of period, predictions of both methods become very close to each other.", "cite_spans": [], "ref_spans": [{"start": 273, "end": 280, "text": "Table 2", "ref_id": "TABREF1"}, {"start": 694, "end": 702, "text": "Figure 3", "ref_id": "FIGREF1"}, {"start": 1008, "end": 1015, "text": "Table 2", "ref_id": "TABREF1"}, {"start": 1781, "end": 1789, "text": "Figure 4", "ref_id": null}, {"start": 2337, "end": 2345, "text": "Figure 5", "ref_id": null}], "section": "WAAQR"}, {"text": "One of the disadvantages of WAAQR is that it might perform much worse with non-optimal input parameters of regularization a and standard deviation \u03c3. If no prior knowledge is available, one can start with some reasonable values of input parameters and keep track of the acceptance ratio of new generated \u03b8. If the acceptance ratio is too high it might indicate that the algorithm moves too slowly to the area of high values of the probability function of \u03b8, and standard deviation \u03c3 should be increased. Another option is to take very large number of steps and larger 'burn-in' period. ", "cite_spans": [], "ref_spans": [], "section": "WAAQR"}, {"text": "We proposed two ways of applying the framework of prediction with expert advice to the problem of probabilistic forecasting of renewable energy. The first approach is to apply WAA with a finite number of models and combine their predictions by updating weights of each model online based on their performance. Experimental results show that WAA performs close or better than the best model in terms of cumulative pinball loss function. It also outperforms the simple average of predictions of models. With this approach we show that it is reasonable to apply WAA for the prediction of quantiles. Second, we propose a new competitive online algorithm WAAQR which combines predictions of an infinite pool of quantile regressions. We derive the theoretical bound which guarantees that WAAQR asymptotically performs as well as any quantile regression up to an additive term of the form C \u221a T ln T . Experimental results show that WAAQR can outperform the best quantile regression model that was trained on the historical data.", "cite_spans": [], "ref_spans": [], "section": "Conclusions"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "An analog ensemble for shortterm probabilistic solar power forecast", "authors": [{"first": "S", "middle": [], "last": "Alessandrini", "suffix": ""}, {"first": "L", "middle": [], "last": "Delle Monache", "suffix": ""}, {"first": "S", "middle": [], "last": "Sperati", "suffix": ""}, {"first": "G", "middle": [], "last": "Cervone", "suffix": ""}], "year": 2015, "venue": "Appl. Energy", "volume": "157", "issn": "", "pages": "95--110", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "An introduction to MCMC for machine learning", "authors": [{"first": "C", "middle": [], "last": "Andrieu", "suffix": ""}, {"first": "N", "middle": [], "last": "De Freitas", "suffix": ""}, {"first": "A", "middle": [], "last": "Doucet", "suffix": ""}, {"first": "M", "middle": ["I"], "last": "Jordan", "suffix": ""}], "year": 2003, "venue": "Mach. Learn. J", "volume": "50", "issn": "", "pages": "5--43", "other_ids": {"DOI": ["10.1023/A:1020281327116"]}}, "BIBREF2": {"ref_id": "b2", "title": "Energy storage and its use with intermittent renewable energy", "authors": [{"first": "J", "middle": ["P"], "last": "Barton", "suffix": ""}, {"first": "D", "middle": ["G"], "last": "Infield", "suffix": ""}], "year": 2004, "venue": "IEEE Trans. Energy Convers", "volume": "19", "issn": "", "pages": "441--448", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Probabilistic forecasting", "authors": [{"first": "T", "middle": [], "last": "Gneiting", "suffix": ""}, {"first": "M", "middle": [], "last": "Katzfuss", "suffix": ""}], "year": 2014, "venue": "Ann. Rev. Stat. Appl", "volume": "1", "issn": "", "pages": "125--151", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "The weak aggregating algorithm and weak mixability", "authors": [{"first": "Y", "middle": [], "last": "Kalnishkan", "suffix": ""}, {"first": "M", "middle": [], "last": "Vyugin", "suffix": ""}], "year": 2008, "venue": "J. Comput. Syst. Sci", "volume": "74", "issn": "", "pages": "1228--1244", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Quantile Regression", "authors": [{"first": "R", "middle": [], "last": "Koenker", "suffix": ""}], "year": 2005, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Regression quantiles", "authors": [{"first": "R", "middle": [], "last": "Koenker", "suffix": ""}, {"first": "G", "middle": [], "last": "Bassett", "suffix": ""}], "year": 1978, "venue": "Econometrica", "volume": "46", "issn": "", "pages": "33--50", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Weak aggregating algorithm for the distribution-free perishableinventory problem", "authors": [{"first": "T", "middle": [], "last": "Levina", "suffix": ""}, {"first": "Y", "middle": [], "last": "Levin", "suffix": ""}, {"first": "J", "middle": [], "last": "Mcgill", "suffix": ""}, {"first": "M", "middle": [], "last": "Nediak", "suffix": ""}, {"first": "V", "middle": [], "last": "Vovk", "suffix": ""}], "year": 2010, "venue": "Oper. Res. Lett", "volume": "38", "issn": "", "pages": "516--521", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "GEFCom2014: probabilistic solar and wind power forecasting using ageneralized additive tree ensemble approach", "authors": [{"first": "G", "middle": ["I"], "last": "Nagya", "suffix": ""}, {"first": "G", "middle": [], "last": "Barta", "suffix": ""}, {"first": "S", "middle": [], "last": "Kazia", "suffix": ""}, {"first": "G", "middle": [], "last": "Borbelyb", "suffix": ""}, {"first": "G", "middle": [], "last": "Simon", "suffix": ""}], "year": 2016, "venue": "Int. J. Forecast", "volume": "32", "issn": "", "pages": "1087--1093", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Simple conditions for the convergence of the Gibbs sampler and Metropolis-Hastings algorithms", "authors": [{"first": "G", "middle": ["O"], "last": "Roberts", "suffix": ""}, {"first": "A", "middle": ["F M"], "last": "Smith", "suffix": ""}], "year": 1994, "venue": "Stoch. Processes Appl", "volume": "49", "issn": "", "pages": "207--216", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Ensemble-based classifiers", "authors": [{"first": "L", "middle": [], "last": "Rokach", "suffix": ""}], "year": 2010, "venue": "Artif. Intell. Rev", "volume": "33", "issn": "", "pages": "1--39", "other_ids": {"DOI": ["10.1007/s10462-009-9124-7"]}}, "BIBREF11": {"ref_id": "b11", "title": "Aggregating strategies", "authors": [{"first": "V", "middle": [], "last": "Vovk", "suffix": ""}], "year": 1990, "venue": "Proceedings of the 3rd Annual Workshop on Computational Learning Theory", "volume": "", "issn": "", "pages": "371--383", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Competitive on-line statistics", "authors": [{"first": "V", "middle": [], "last": "Vovk", "suffix": ""}], "year": 2001, "venue": "Int. Stat. Rev", "volume": "69", "issn": "2", "pages": "213--248", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Prediction with expert advice for the Brier game", "authors": [{"first": "V", "middle": [], "last": "Vovk", "suffix": ""}, {"first": "F", "middle": [], "last": "Zhdanov", "suffix": ""}], "year": 2009, "venue": "J. Mach. Learn. Res", "volume": "10", "issn": "", "pages": "2445--2471", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Competitive online generalized linear regression under square loss", "authors": [{"first": "F", "middle": [], "last": "Zhdanov", "suffix": ""}, {"first": "V", "middle": [], "last": "Vovk", "suffix": ""}, {"first": "J", "middle": ["L"], "last": "Balc\u00e1zar", "suffix": ""}, {"first": "F", "middle": [], "last": "Bonchi", "suffix": ""}, {"first": "A", "middle": [], "last": "Gionis", "suffix": ""}], "year": 2010, "venue": "ECML PKDD 2010", "volume": "6323", "issn": "", "pages": "531--546", "other_ids": {"DOI": ["10.1007/978-3-642-15939-8_34"]}}}, "ref_entries": {"FIGREF0": {"text": "Weights update for solar power", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Log-likelihood of parameters for a = 0.1.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Cumulative loss difference between QR and WAAQR Predictions with [25%, 75%] confidence interval for WAAQR and QR", "latex": null, "type": "figure"}, "TABREF0": {"text": "Total", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>wind\n</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>solar\n</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>q </td><td>QRF </td><td>GBDT </td><td>QR </td><td>Average </td><td>WAA\n</td><td>q </td><td>QRF GBDT </td><td>QR </td><td>Average WAA\n</td></tr><tr><td>0.25 </td><td>538.5 </td><td>491.2 </td><td>516.6 </td><td>500.3 </td><td>493.0\n</td><td>0.25 </td><td>48.6 </td><td>98.3 </td><td>53.1 </td><td>63.8 </td><td>50.1\n</td></tr><tr><td>0.5 </td><td>757.0 </td><td>707.5 </td><td>730.7 </td><td>714.0 </td><td>709.0\n</td><td>0.5 </td><td>70.5 </td><td>110.7 </td><td>68.8 </td><td>79.1 </td><td>69.2\n</td></tr><tr><td>0.75 </td><td>668.3 </td><td>610.7 </td><td>633.9 </td><td>616.6 </td><td>610.1\n</td><td>0.75 </td><td>63.5 </td><td>67.6 </td><td>59.3 </td><td>58.7 </td><td>58.0\n</td></tr><tr><td>0.95 </td><td>270.5 </td><td>222.1 </td><td>217.5 </td><td>216.0 </td><td>211.0\n</td><td>0.95 </td><td>29.2 </td><td>26.1 </td><td>23.2 </td><td>21.0 </td><td>20.8\n</td></tr></table></body></html>"}, "TABREF1": {"text": "Acceptance ratio (AR) and total losses of WAAQR on training set", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>\u00a0</td><td>AR\n</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>Loss\n</td><td>\u00a0</td><td>\u00a0</td></tr><tr><td>a \\ \u03c3 </td><td>0.5 </td><td>1.0 </td><td>2.0 </td><td>3.0\na </td><td>\\ \u03c3 </td><td>0.5 </td><td>1.0 </td><td>2.0 </td><td>3.0\n</td></tr><tr><td>0.1 </td><td>0.533 </td><td>0.550 </td><td>0.482 </td><td>0.375\n</td><td>0.1 </td><td>1821.8 </td><td>823.5 </td><td>216.3 </td><td>28.8\n</td></tr><tr><td>0.3 </td><td>0.554 </td><td>0.545 </td><td>0.516 </td><td>0.371\n</td><td>0.3 </td><td>1806.2 </td><td>844.9 </td><td>265.3 </td><td>62.7\n</td></tr><tr><td>0.5 </td><td>0.549 </td><td>0.542 </td><td>0.510 </td><td>0.352\n</td><td>0.5 </td><td>1815.7 </td><td>878.5 </td><td>272.7 </td><td>92.1\n</td></tr><tr><td>1.0 </td><td>0.548 </td><td>0.538 </td><td>0.502 </td><td>0.343\n</td><td>1.0 </td><td>1810.4 </td><td>877.5 </td><td>379.3 </td><td>116.9\n</td></tr></table></body></html>"}}, "back_matter": []}
{
    "paper_id": "db6d6bde546670094d0356d740a666bfd0eaca91",
    "metadata": {
        "title": "Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis",
        "authors": [
            {
                "first": "Jiamin",
                "middle": [],
                "last": "Liang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Engineering Laboratory for Medical Ultrasound",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Xin",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Engineering Laboratory for Medical Ultrasound",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Yuhao",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Engineering Laboratory for Medical Ultrasound",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Haoming",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Shuangchi",
                "middle": [],
                "last": "He",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Engineering Laboratory for Medical Ultrasound",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Xindi",
                "middle": [],
                "last": "Hu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Engineering Laboratory for Medical Ultrasound",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Zejian",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Engineering Laboratory for Medical Ultrasound",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Wufeng",
                "middle": [],
                "last": "Xue",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Engineering Laboratory for Medical Ultrasound",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Jun",
                "middle": [],
                "last": "Cheng",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Engineering Laboratory for Medical Ultrasound",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": "chengjun583@qq.comjuncheng"
            },
            {
                "first": "Dong",
                "middle": [],
                "last": "Ni",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Engineering Laboratory for Medical Ultrasound",
                    "institution": "Shenzhen University",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": "nidong@szu.edu.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "A B S T R A C T Ultrasound (US) imaging is widely used for anatomical structure inspection in clinical diagnosis. The training of new sonographers and deep learning based algorithms for US image analysis usually requires a large amount of data. However, obtaining and labeling large-scale US imaging data are not easy tasks, especially for diseases with low incidence. Realistic US image synthesis can alleviate this problem to a great extent. In this paper, we propose a generative adversarial network (GAN) based image synthesis framework. Our main contributions include: 1) we present the first work that can synthesize realistic B-mode US images with high-resolution and customized texture editing features; 2) to enhance structural details of generated images, we propose to introduce auxiliary sketch guidance into a conditional GAN. We superpose the edge sketch onto the object mask and use the composite mask as the network input; 3) to generate highresolution US images, we adopt a progressive training strategy to gradually generate high-resolution images from low-resolution images. In addition, a feature loss is proposed to minimize the difference of high-level features between the generated and real images, which further improves the quality of generated images; 4) the proposed US image synthesis method is quite universal and can also be generalized to the US images of other anatomical structures besides the three ones tested in our study (lung, hip joint, and ovary); 5) extensive experiments on three large US image datasets are conducted to validate our method. Ablation studies, customized texture editing, user studies, and segmentation tests demonstrate promising results of our method in synthesizing realistic US images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Ultrasound (US) imaging is prevalent in routine clinical examinations because of its relatively low cost, real-time imaging capability and avoidance of radiation exposure (Kutter et al., 2009; Alessandrini et al., 2015) . During US diagnosis, sonographers first manually operate an imaging equipment to produce arXiv:2204.06929v3 [eess.IV] 25 May 2022 images required for diagnosis, and then review and analyze the images to find abnormalities (Doi, 2007) . This process relies heavily on sonographers' knowledge and experience. It usually takes a long time for novices to acquire operating and diagnostic skills. This is even truer when diagnosing rare diseases, due to the lack of training on real data (Mattausch et al., 2017) .",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 192,
                    "text": "(Kutter et al., 2009;",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 193,
                    "end": 219,
                    "text": "Alessandrini et al., 2015)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 444,
                    "end": 455,
                    "text": "(Doi, 2007)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 705,
                    "end": 729,
                    "text": "(Mattausch et al., 2017)",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In recent years, we have witnessed considerable progress in computational medical image analysis for the detection, diagnosis, and treatment of diseases (Cheng et al., 2020) . Compared with medical image interpretation by human experts, automated analysis is more efficient, objective, and does not suffer from inter-observer variations (Cheng et al., 2016) . In the stream of applying machine learning, especially deep learning, to data analysis, large-scale datasets and annotations lie at the heart of its success to accomplish target tasks. For example, the Ima-geNet database, designed for visual object recognition, contains more than one million annotated images. However, in medical applications, usually only a very limited number of images are available, and annotations require expert knowledge about the data and task. Therefore, the lack of large-scale datasets and annotations remains a major obstacle hindering the successful application of deep learning algorithms to medical images Gao et al., 2019) .",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 173,
                    "text": "(Cheng et al., 2020)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 337,
                    "end": 357,
                    "text": "(Cheng et al., 2016)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 999,
                    "end": 1016,
                    "text": "Gao et al., 2019)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Researchers have been trying to circumvent this obstacle via data augmentation. The most common method is affine transformation, including translation, rotation, and scaling. This technique simply modifies original images to expand the dataset for model training. Although the sample size can be remarkably increased in this way, only little additional information is introduced into the dataset, due to the small content changes (e.g. rotating an image by an angle) (Frid-Adar et al., 2018; Salehi et al., 2020) . In this regard, there is an urgent need for a new data augmentation method that can enrich the dataset with more variability, so that the model trained on a small dataset can also generalize well on unseen data (Yi et al., 2019) .",
            "cite_spans": [
                {
                    "start": 467,
                    "end": 491,
                    "text": "(Frid-Adar et al., 2018;",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 492,
                    "end": 512,
                    "text": "Salehi et al., 2020)",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 726,
                    "end": 743,
                    "text": "(Yi et al., 2019)",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Image synthesis is a new and more sophisticated augmentation method. It can be classified into physics-based and learning-based methods. Well-known US simulation packages such as Field II (Jensen, 1997 (Jensen, , 2004 and k-Wave (Treeby and Cox, 2010) can be used to simulate B-mode ultrasound images, though they are not only designed for image synthesis. k-Wave is designed for the time-domain simulation of propagating acoustic waves and can account for both linear and nonlinear wave propagation, while Field II is a linear ultrasound simulation tool. Ram\u00edrez et al. (2004) proposed a physical model for simulating intravascular US (IVUS) images. Burger et al. (2012) built deformable mesh models from CT volumes to fulfill real-time simulation by GPU. Cardiac US sequences were simulated based on an electromechanical model (Prakosa et al., 2012) and a warping strategy (Zhou et al., 2017) . Although these methods follow US imaging principles, their computational complexity is often high due to the modeling of wave propagation process (Hu et al., 2017) . They are very timeconsuming, especially for generating high-resolution images. Moreover, their performance may be affected by the quality of pre-built models, which are often essential and difficult to construct.",
            "cite_spans": [
                {
                    "start": 188,
                    "end": 201,
                    "text": "(Jensen, 1997",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 202,
                    "end": 217,
                    "text": "(Jensen, , 2004",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 229,
                    "end": 251,
                    "text": "(Treeby and Cox, 2010)",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 556,
                    "end": 577,
                    "text": "Ram\u00edrez et al. (2004)",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 651,
                    "end": 671,
                    "text": "Burger et al. (2012)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 829,
                    "end": 851,
                    "text": "(Prakosa et al., 2012)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 875,
                    "end": 894,
                    "text": "(Zhou et al., 2017)",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 1043,
                    "end": 1060,
                    "text": "(Hu et al., 2017)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the past few years, deep learning based synthetic methods have gained more and more interest. Among them, the generative adversarial network (GAN) (Goodfellow et al., 2014) is the most promising approach. Fujioka et al. (2019) employed a deep convolutional GAN (DCGAN) (Radford et al., 2015) to synthesize breast US images without additional constrains. Hu et al. (2017) first proposed a novel spatially-conditioned GAN based on conditional GANs (cGANs) (Mirza and Osindero, 2014) to synthesize US images from fetal phantoms. The proposed architecture can improve the training stability by taking pixel coordinates as conditioning input. Tom et al. (2018) introduced a multi-stage method including two different cGANs to transform tissue maps into synthetic IVUS images. Although the cascaded cGANs are hard to train, this system is the first one to use tissue labels as conditioning input to enhance the training stability.",
            "cite_spans": [
                {
                    "start": 150,
                    "end": 175,
                    "text": "(Goodfellow et al., 2014)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 208,
                    "end": 229,
                    "text": "Fujioka et al. (2019)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 272,
                    "end": 294,
                    "text": "(Radford et al., 2015)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 357,
                    "end": 373,
                    "text": "Hu et al. (2017)",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 457,
                    "end": 483,
                    "text": "(Mirza and Osindero, 2014)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 641,
                    "end": 658,
                    "text": "Tom et al. (2018)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Although cGAN (Mirza and Osindero, 2014; Isola et al., 2017) is effective and enables the user-controlled image generation, the synthesized images often have low resolution and checkerboard artifacts. To make the structural details of generated images more realistic, auxiliary guidance information, such as the sketch and edge of the background, was introduced (Shin et al., 2018; Zhang et al., 2019) . However, it is still challenging to synthesize high-resolution images. Due to the more details in high-resolution images, the discriminator can easily recognize the differences between generated and real images, which may lead to the vanishing gradient problem and make the training difficult. Additionally, training such model is memoryintensive, which limits using a large batch size to improve training stability.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 40,
                    "text": "(Mirza and Osindero, 2014;",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 41,
                    "end": 60,
                    "text": "Isola et al., 2017)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 362,
                    "end": 381,
                    "text": "(Shin et al., 2018;",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 382,
                    "end": 401,
                    "text": "Zhang et al., 2019)",
                    "ref_id": "BIBREF42"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To address above issues, we devise a novel sketch guided and progressive growing GAN (spGAN) to synthesize US images. The main contributions of our work include:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "1. To the best of our knowledge, this is the first work that can synthesize realistic B-mode US images with highresolution and customized texture editing features. A software tool and a video demo of our method are available at GitHub (https://github.com/Carmenliang/UI synthesis). 2. To enhance the fidelity of synthesized structure details, we propose to introduce auxiliary sketch guidance into a cGAN. Specifically, we superpose the edge sketch onto the object mask and use the composite mask as the network input. Customized editing of the edge sketch and object mask makes our method quite flexible in generating different US images for training new sonographers and augmenting data in deep learning models. 3. To generate high-resolution US images, we adopt a progressive training strategy (Karras et al., 2017) to gradually generate high-resolution images from low-resolution images. In addition, a feature loss (FL) is proposed to minimize the difference of high-level features between the generated and real images, which further improves the quality of generated images. 4. The proposed US image synthesis method is quite universal and can also be generalized to the US images of other Fig. 1 . Examples of the US images and the corresponding label maps for three different datasets. On the left are the annotated label maps in pseudo colors, and on the right are the real US images. The red arrows indicate the non-target background regions in US images, which are difficult to synthesize realistically due to the lack of background information in the label maps.",
            "cite_spans": [
                {
                    "start": 797,
                    "end": 818,
                    "text": "(Karras et al., 2017)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [
                {
                    "start": 1197,
                    "end": 1203,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "anatomical structures besides the three ones tested in our study (lung, hip joint, and ovary). 5. Extensive experiments on three large US image datasets are conducted to validate the efficacy of our method, including ablation studies, customized texture editing, user studies, and segmentation comparison between real and synthesized images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Some preliminary results of this study have been published in the ISBI 2020 conference (Liang et al., 2020) . In this paper, we make substantial extensions in the following two aspects. 1) A new regularization term is introduced into the loss function to make the generated images and real images alike in terms of high-level features, which can successfully remove the artifacts present in our previous study and other advanced GANbased synthesis methods. 2) Besides the ovary dataset used in our previous study, we collect two additional large datasets of US images (COVID-19 and infant hip joint) to further validate our method. 3) We add segmentation experiments to demonstrate the efficacy of our method as a data augmentation approach. Compared with traditional data augmentation such as image translation and rotation, our GAN-based augmentation method can provide greater variability with editable operations and therefore has great potential to improve performance. 4) We add extensive ablation studies to verify the effectiveness of each component of our method. 5) We investigate the effect of three key parameters on the performance of our method. 6) We release our US image synthesis tool to a public repository (https://github.com/Carmenliang/UI synthesis), which can be readily used by other researchers.",
            "cite_spans": [
                {
                    "start": 87,
                    "end": 107,
                    "text": "(Liang et al., 2020)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This study was approved by local institutional review boards. A robust US simulation framework is expected to be able to synthesize photo-realistic images with various characteristics, such as different structural shapes, positions, and echo patterns. Hence, three representative datasets of B-mode US images were collected and used in our study: lung US for diagnosis of , infant hip joint US (hip joint), and ovary and follicle US (ovary). Each of the three datasets has its own special characteristics. For the COVID-19 dataset, observing the specific echo patterns is important for diagnosis. For the hip joint dataset, attention is focused on the relative position of different anatomical structures. For the ovary dataset, doctors analyze the size of the ovary and the size and number of follicles.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "All US images had corresponding segmentation maps annotated by experienced doctors. Example images and the corresponding segmentation maps are shown in Fig. 1 . The details of each dataset are described as follows:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 152,
                    "end": 158,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "COVID-19. This dataset contained 6054 images totally, in which 4849 images were used as the training set and the remaining 1205 images as the test set. These images had different resolutions, with the height ranging from 179 to 799 pixels and width ranging from 109 to 1104 pixels. All images were resized to 256\u00d7256 pixels and 512\u00d7512 pixels for training lowresolution and high-resolution synthesis models. The annotated artifacts in lung US images included pleura line, A-line, B-line, and consolidation. The COVID-19 dataset was collected from multiple centers in Wuhan, including Cancer Center of Union Hospital, West of Union Hospital, Jianghan Cabin Hospital, Jingkai Cabin Hospital, and Leishenshan Hospital. Various ultrasound machines were used, including Mindray M7, M8, M9 and GE Logiq E9, Logiq E Portable Ultrasound Machine.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Hip Joint. This dataset contained 1231 images totally, in which 992 images were used as the training set and the remaining 239 images as the test set. To remove the characters in original US images, we cropped them to 512\u00d7512 pixels and then resized them to 256\u00d7256 pixels. Both cropped and resized images are used for training the backbone structure of GAN. Four structures were annotated in the segmentation maps, including ilium, lower limb, labrum, and co-junction. The hip joint dataset was collected from Guangdong Women and Children Hospital with two different machines (Hitachi HI-Vision Preirus and Philips iU22). The frequencies of Hitachi's transducer are between 5-13 MHz, while the frequencies of the other one are between 3-9 MHz.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Ovary. This dataset contained 3261 ovarian images totally, in which 2848 images were used as the training set and the remaining 413 as the test set. The image size was non-uniform with the height ranging from 380 to 530 pixels and width ranging from 610 to 860 pixels. All images were resized to 256\u00d7256 pixels and 512\u00d7512 pixels for training. Ovary and follicles were annotated in the segmentation maps. The ovary dataset was collected from The Third Affiliated Hospital of Guangzhou Medical University with two different machines (Mindray Resona 7S and GE Voluson 6S). The frequencies of Mindray's transvaginal transducer are between 3-9 MHz, while the frequencies of the other one are between 4-10 MHz.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "To synthesize high-fidelity and high-resolution US images from simple segmentation maps, we proposed a GAN-based image synthesis framework, as illustrated in Fig. 2 . Specifically, we first added fine-grained edge sketch to original label maps, which resulted in the composite label maps that can help the generator create images with realistic texture (Section 2.3). Next, the designed backbone GAN structure was used for a warm-up training of low-resolution images of size 256\u00d7256 (Section 2.4). Then, a progressive growing scheme is introduced for synthesizing high-resolution images (Section 2.5). To enable a smooth transition between layers, FIBs were incorporated into the backbone structure (Section 2.6). Finally, a feature loss was employed to further improve the texture fidelity of synthesized images (Section 2.7). The above key components of our image synthesis framework are described in detail in the following sections.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 158,
                    "end": 164,
                    "text": "Fig. 2",
                    "ref_id": null
                }
            ],
            "section": "Overview of our proposed method"
        },
        {
            "text": "There are two common image-to-image translation tasks in the field of medical image analysis: translation between different imaging modalities (such as MRI T1-to-T2 (Liu, 2019; Dar et al., 2019) , CT-to-MRI (Jiang et al., 2018) , MRI-to-CT (Nie et al., 2017; Zhao et al., 2018) , CT-to-PET (Ben-Cohen et al., 2017), PET-to-MRI (Choi and Lee, 2018) ) and transformation from segmentation maps to medical images. Image synthesis from segmentation maps can generate different images by simply modifying the content in the maps, which is a desirable feature for data augmentation. Since segmentation maps only contain the shape of target structures and lack background details, the transformation from segmentation maps to medical images is generally more difficult than translation between different imaging modalities. As for US images, synthesizing from segmentation maps is even harder due to a large amount of noise in US images. Motivated by previous work (Shin et al., 2018; Zhang et al., 2019) , we used the edge information of the background texture as auxiliary sketch guidance to achieve high-fidelity synthesis and customized editing of US images.",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 176,
                    "text": "(Liu, 2019;",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 177,
                    "end": 194,
                    "text": "Dar et al., 2019)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 207,
                    "end": 227,
                    "text": "(Jiang et al., 2018)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 240,
                    "end": 258,
                    "text": "(Nie et al., 2017;",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 259,
                    "end": 277,
                    "text": "Zhao et al., 2018)",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 327,
                    "end": 347,
                    "text": "(Choi and Lee, 2018)",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 958,
                    "end": 977,
                    "text": "(Shin et al., 2018;",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 978,
                    "end": 997,
                    "text": "Zhang et al., 2019)",
                    "ref_id": "BIBREF42"
                }
            ],
            "ref_spans": [],
            "section": "Auxiliary guidance"
        },
        {
            "text": "Specifically, the Canny algorithm (Canny, 1986 ) was applied to real images to extract binary edge sketch because it is robust against noise. We then updated the original segmentation map of the target object O by superposing the edge sketch S onto it, resulting in the composite label\u00d5, which is defined as:",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 46,
                    "text": "(Canny, 1986",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Auxiliary guidance"
        },
        {
            "text": "where M (M \u2208 {0, 1}) denotes the binary map indicating the area for annotated structures. \u2297 refers to the operation of element-wise multiplication. Through the above operation, the auxiliary sketch S is superposed onto the original mask O without affecting the area of the target objects, as shown in Fig. 3 . With the additional auxiliary sketch of background provided for GAN to learn, our method can generate images with realistic background texture.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 301,
                    "end": 307,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Auxiliary guidance"
        },
        {
            "text": "The backbone structure of GAN synthesizing images of size 256\u00d7256, as shown in the middle of Fig. 2 , is a basic structure for subsequent high-resolution image synthesis. The composite label generated in the previous section was used as the input of both the generator and discriminator of the GAN structure.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 93,
                    "end": 99,
                    "text": "Fig. 2",
                    "ref_id": null
                }
            ],
            "section": "Backbone structure of GAN"
        },
        {
            "text": "For the generator, the conditional input was the composite labels and the output was the synthesized US images. We utilized the encoder-decoder architecture with n residual blocks in between. The architecture of the encoder and decoder included three down-sampling blocks and three up-sampling blocks, respectively. Each up-sampling or down-sampling block was comprised of one convolution or deconvolution layer with a stride of 2, one instance normalization (IN) (Ulyanov et al., 2016) layer, and one ReLU activation function.",
            "cite_spans": [
                {
                    "start": 464,
                    "end": 486,
                    "text": "(Ulyanov et al., 2016)",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "Backbone structure of GAN"
        },
        {
            "text": "The number of residual blocks controls the ability of feature extraction in the model, and therefore can be different for specific datasets. Each residual block contained two convolution layers with a stride of 1. Except for the first and the last convolution layers with a kernel size of 1\u00d71 for channel size adaptation, all other convolution and deconvolution layers used a kernel size of 3\u00d73.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Backbone structure of GAN"
        },
        {
            "text": "For the discriminator, we used the PatchGAN (Isola et al., 2017) , whose input was the concatenation of the composite labels and generated/real images. The PatchGAN can be designed to different output sizes. Each unit of the output reflects the possibility of an image patch being real, which is used to calculate adversarial loss when training. Because PatchGAN places more restrictions on local regions and has more highfrequency information to feed back to the generator, it often performs better than the image-based discriminator. Therefore, we used the PatchGAN structure as the discriminator, which consists of five convolution layers.",
            "cite_spans": [
                {
                    "start": 44,
                    "end": 64,
                    "text": "(Isola et al., 2017)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Backbone structure of GAN"
        },
        {
            "text": "The objective function of generator L G was composed of a conditional adversarial loss L GAN G and a L1-loss L L1 for lowfrequency restriction, which are formulated as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Backbone structure of GAN"
        },
        {
            "text": "(2)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Backbone structure of GAN"
        },
        {
            "text": "Overview of our proposed US image synthesis framework. The composite labels contain original annotated structures as well as auxiliary sketches. For the generator, the input is the composite labels and output is the generated US images. For the discriminator, the input is the US images and the corresponding composite labels. In the progressive training scheme, the left backbone structure is adopted as a pre-trained model for low-resolution image (256\u00d7256) synthesis, and then fade-in blocks (FIB-D, FIB-U) are added for synthesizing realistic high-resolution images (512\u00d7512). Moreover, the additional feature extraction network is employed for calculating the mean and covariance of the generated and real images, and then they are used for constructing the feature loss aiming at improving synthesis quality further. where G, D represent the generator and discriminator respectively, x denotes the conditional composite labels, y denotes the ground truth US images, G(x) denotes the synthesized images from input x. The hyperparameter \u03bb 1 was set to 1. The objective function of discriminator L D is calculated as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Backbone structure of GAN"
        },
        {
            "text": "The objective function of the discriminator was maximized while the objective function of the generator was minimized during the adversarial training process. The discriminator training alternated with generator training in each epoch. This alternating process was repeated until the generated images were sufficiently realistic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Backbone structure of GAN"
        },
        {
            "text": "Compared to low-resolution images (256\u00d7256), highresolution images (512\u00d7512) have more fine structures. The backbone architecture alone as described in Section 2.4 is not capable of extracting enough information for generating high-resolution, realistic images. In order to synthesize highresolution US images with high fidelity, we adopted the progressive growing scheme (Karras et al., 2017) to decompose the task as incremental learning ones. This scheme enables us to use only one generator and one discriminator with fast and smooth learning for high-resolution, realistic synthesis. Specifically, we started from an easier task that synthesizes lowresolution images in several warm-up epochs with the backbone structure, and then, the weights of the backbone structure were shared with the generator and discriminator for high-resolution US image synthesis.",
            "cite_spans": [
                {
                    "start": 372,
                    "end": 393,
                    "text": "(Karras et al., 2017)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Progressive growing scheme"
        },
        {
            "text": "The entire training process can be divided into four phases. In phase 1, the backbone architecture of generator and discriminator (Section 2.4) was applied for low-resolution (256\u00d7256) US images synthesis. After several warm-up epochs until convergence, this pre-trained architecture enabled good quality synthesis of low-resolution US images. In phases 2 and 3, we trained the discriminator and generator for high-resolution image synthesis, sequentially and respectively. In this process, new layers were added to the networks, and we faded them in smoothly with the FIBs to avoids sudden shocks to the already well-trained layers (see Fig. 2 ). The discriminator was trained earlier than the generator to replenish the gradient information and force the generator to learn to synthesize higher resolution images. Finally, in phase 4, we trained the discriminator and generator together (the rightmost part of Fig. 2) for several more epochs to further enhance performance.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 638,
                    "end": 644,
                    "text": "Fig. 2",
                    "ref_id": null
                },
                {
                    "start": 912,
                    "end": 919,
                    "text": "Fig. 2)",
                    "ref_id": null
                }
            ],
            "section": "Progressive growing scheme"
        },
        {
            "text": "To avoid the sudden shock during training when new layers are added, FIBs were adopted for a smooth transition in both generator and discriminator. Fig. 4 illustrates the structure of FIB. Following the design of residual neural network, FIB also uses a skip connection (Fig. 4) . The side branch skips over the convolutional layers in the main branch and then merges into . These two blocks are used in the progressive growing scheme, with \u03b1 increasing during transition phases. The from image block represents a layer projecting image channels to feature vectors using 1\u00d71 convolution, and the to image block functions the opposite way. The block 512\u00d7512 contains two 3\u00d73 convolution layers, and the block 256\u00d7256 represents the original structures of backbone adjacent to newly added structures. the main branch through a weighted sum. The weight \u03b1 (\u03b1 \u2208 [0, 1]) controls the balance between the main and side branches. \u03b1 = 0 indicates that only the side branch determines the output of FIB, while \u03b1 = 1 means that the output only depends on the main branch. The use of FIB not only smooths the transition between different resolutions but also makes weights sharing possible and training more efficient.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 148,
                    "end": 154,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 270,
                    "end": 278,
                    "text": "(Fig. 4)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Fade-in Blocks"
        },
        {
            "text": "Two kinds of FIB were designed for the purpose of downsampling and up-sampling and denoted as FIB-Down (FIB-D) and FIB-Up (FIB-U), respectively. The difference between FIB-D and FIB-U is the direction of resizing. For FIB-D, it halves the resolution using average pooling, and for FIB-U, it applies bilinear interpolation for doubling the resolution. FIB-D was used in both generator and discriminator while FIB-U was only used in the generator (Fig. 2) . Fig. 4 shows the two-branch structure in FIB. The lightweight side branch helps the pre-trained network adapt easily. The more complex architecture of the main branch has stronger feature extraction capabilities. Combining these advantages, the \u03b1 is introduced to guide the side branch to gradually switch to the main one. Specifically, it is increased from 0 to \u03b1 max with a fixed step. It is noted that updating the \u03b1 in both discriminator and generator simultaneously may decrease the stability and thus we increased \u03b1 alternately when training the two modules. Besides, we also noticed that \u03b1 max had an impact on the performance of the generator. A lower \u03b1 max may limit feature extraction capabilities, while a larger one tends to cause a sudden increasing loss. In comparison, the loss in the discriminator varies smoothly even with a larger \u03b1 max . For these reasons, we set the \u03b1 max to 0.5 and 1.0 in generator and discriminator, respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 445,
                    "end": 453,
                    "text": "(Fig. 2)",
                    "ref_id": null
                },
                {
                    "start": 456,
                    "end": 462,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Fade-in Blocks"
        },
        {
            "text": "By employing the sketch guidance in segmentation maps, progressive training scheme and FIBs, we achieved editable, high-resolution synthesis with undistorted texture. However, there were some small flaws in generated high-resolution images, which are also commonly seen in other GAN-based synthesis methods. The generated images appeared to be a little blurry compared with ground truth images and had some salt and pepper noise especially in the regions without annotated labels and sketches. The flaws might be caused by the pixel-wise L1-loss when training the generator, since the loss only restricts pixel-wise difference and neglects the spatial similarity between pixels (Blau and Michaeli, 2018) . However, removing the L1loss term from the objective function led to poor performance due to the lack of low-frequency restriction.",
            "cite_spans": [
                {
                    "start": 678,
                    "end": 703,
                    "text": "(Blau and Michaeli, 2018)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Feature loss"
        },
        {
            "text": "Therefore, we proposed a feature loss to add extra restrictions for better texture synthesis. Inspired by the perceptual loss (Johnson et al., 2016) originally used in style transfer and super-resolution tasks, we calculated the high-level features of the real and generated images and tried to minimize the mean and covariance between them. To extract high-level features, images were fed to a ResNet-50 (He et al., 2016) model which was trained with 1500K prenatal US images for standard plane detection, and the output of the layer conv4 of the model was used as the high-level features. We referred to the feature extraction layers in the ResNet-50 as feature extraction network (FEN). The feature loss L F is given by:",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 148,
                    "text": "(Johnson et al., 2016)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 405,
                    "end": 422,
                    "text": "(He et al., 2016)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Feature loss"
        },
        {
            "text": "The updated objective function of the generator is given by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature loss"
        },
        {
            "text": "The hyperparameter \u03bb 2 was set to different values for different datasets, which will be discussed in Section 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature loss"
        },
        {
            "text": "Our experiments were implemented using Pytorch with a single NVIDIA GeForce RTX 2080 Ti GPU. The proposed sp-GAN can be split into four phases, as described in Section 2.5. In phase 1, we first trained the backbone structure of GAN for low-resolution image synthesis. We used a batch size of 4 with Adam optimizer. The learning rates of generator and discriminator were set to 0.001 and 0.0001, respectively. The weight of the feature loss term in the objective function of the generator \u03bb 2 , the number of residual blocks in the generator n, and the output size of PatchGAN s were 10, 15, and 30\u00d730 for the COVID-19 dataset, 10, 15, and 120\u00d7120 for the hip joint dataset, and 5, 10, and 30\u00d730 for the ovary dataset. Note the above settings were determined by grid search and used in all experiments unless specified otherwise. For the COVID-19 and ovary datasets, the models in phase 2 and 3 were trained for 50 epochs, with \u03b1 increasing by a step of 1/50. For the hip joint dataset, the models in phase 2 and 3 were trained for 100 epochs, with \u03b1 increasing by a step of 1/100. In phase 4, the full model was trained for 200 epochs for the COVID-19 and ovary datasets, and for 400 epochs for the hip joint dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation details"
        },
        {
            "text": "We evaluated the performance of our proposed method on three US datasets, including COVID-19, hip joint, and ovary images. Both qualitative and quantitative results were presented in the experiments. For qualitative results, we compared the real US images and synthesized images and showed the heatmap of pixel-level differences between them. For quantitative evaluation, four numerical metrics were adopted, including freshet inception distance (FID) (Heusel et al., 2017) , kernel inception distance (KID) (Bi\u0144kowski et al., 2018) , multi-scale structural similarity (MS-SSIM) (Wang et al., 2003) , and learned perceptual image patch similarity (LPIPS) . Both FID and KID measure the distance between images at the feature level. The difference is that KID estimates are unbiased. The lower value of the FID and KID indicates better performance. MS-SSIM measures the similarity of the paired images, ranging from 0 to 1, where larger value means better performance. For LPIPS, we used a pre-trained ResNet-50 (He et al., 2016) network to calculate the perceptual differences in multiple layers, with smaller differences meaning better performance.",
            "cite_spans": [
                {
                    "start": 452,
                    "end": 473,
                    "text": "(Heusel et al., 2017)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 508,
                    "end": 532,
                    "text": "(Bi\u0144kowski et al., 2018)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 579,
                    "end": 598,
                    "text": "(Wang et al., 2003)",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 1011,
                    "end": 1028,
                    "text": "(He et al., 2016)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Implementation details"
        },
        {
            "text": "In this section, we presented the results of low-resolution (256\u00d7256) image synthesis for different US datasets. To demonstrate the effectiveness of the sketch guidance and feature loss used in our synthesis framework, we compared the results of the following three methods: baseline, baseline+S, and baseline+S+FL. The baseline only used the backbone structure of GAN (Section 2.4), baseline+S incorporated the auxiliary sketch guidance (Section 2.3), and baseline+S+FL additionally incorporated the feature loss (Section 2.7). Due to different number of training samples and different texture complexity in the three datasets, the three methods were trained for different number of epochs: 150, 350, and 300 for the COVID-19, hip joint, and ovary datasets, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Low-resolution image synthesis"
        },
        {
            "text": "The qualitative results are shown in Fig. 5 , and the difference heatmap between the generated and ground truth (GT) images is shown on the bottom right corner of the generated images. The color in the heatmap from blue to red corresponds to the difference from small to large. As shown in Fig. 5 , the addition of sketch guidance to baseline remarkably improved the quality of synthesized images. The checkerboard pattern in the generated COVID-19 images by the baseline method was successfully removed when the baseline+S method was used. Moreover, we saw further visual improvement when the feature loss was applied. Some scattered dark dots were observed in the generated COVID-19 images with the baseline+S method. However, they were removed successfully when the baseline+S+FL was used.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 37,
                    "end": 43,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 290,
                    "end": 296,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Low-resolution image synthesis"
        },
        {
            "text": "More clearly, the quantitative results in Table 1 show a large improvement regarding all performance metrics for all datasets when the sketch guidance was added to the baseline. Addition of the feature loss to the baseline+S method further improved the performance for all datasets in terms of all metrics except the MS-SSIM and LPIPS for the hip joint dataset. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 42,
                    "end": 49,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Low-resolution image synthesis"
        },
        {
            "text": "For synthesizing realistic, high-resolution (512\u00d7512) US images, our proposed spGAN included three key components: the use of the auxiliary sketch guidance in segmentation maps, the pre-trained model for low-resolution image synthesis, and progressive growing of the \u03b1 in the FIB. The proposed spGAN v2 included an additional key component, the feature loss. We conducted extensive ablation studies to evaluate these components. The proposed spGAN was compared with three variants, each with one of the three components removed as shown in Table 2 : spGAN-sketch, spGAN-pretrain, and spGAN-growing. Further, we compared spGAN and spGAN v2 to explore the effect of the feature loss. In addition, we also included as a comparison method the bilinear interpolation result of low-resolution images generated by baseline+S+FL (see Section 3.2), and denoted it as 256+interp.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 540,
                    "end": 547,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "High-resolution image synthesis"
        },
        {
            "text": "The quantitative results are presented in Table 2 . Compared with the interpolation results based on the generated lowresolution images, the proposed spGAN v2 achieved large performance gains regarding almost all evaluation metrics for all three datasets, except the MS-SSIM metric for the hip joint dataset. This means that the proposed high-resolution image synthesis framework can learn feature representations with more realistic details than simple image interpolation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 42,
                    "end": 49,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "High-resolution image synthesis"
        },
        {
            "text": "By removing any one of the three components used in the spGAN, the performance was degraded in most cases, which confirms the effectiveness of these adopted components. Of the three components, the most important one is the auxiliary sketch guidance. Removing the sketch guidance led to a significant decrease in performance for all metrics and for all datasets. This indicates that the sketch guidance is essential for the model to learn convincing texture. Fig. 6, 7, 8 show some examples of the generated images and difference heatmaps for the COVID-19, hip joint, and ovary images, respectively. Consistent with quantitative results, we observed that the proposed spGAN produced images that were more realistic compared with other baselines in most cases.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 459,
                    "end": 471,
                    "text": "Fig. 6, 7, 8",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "High-resolution image synthesis"
        },
        {
            "text": "To conduct user studies 2 3 4 of each dataset, we randomly chose 200 images of four different types: GT and the generated images by three methods (i.e., spGAN-sketch, spGAN, and sp-GAN v2 as explained in Section 3.3), each type with 50 images. Five doctors were asked to view these images and tell whether they were real or fake. For each of the four types, we reported the accuracy as the fraction of images being correctly classified. The results are provided in Table 3 . A lower accuracy of an image synthesis method means that the images generated by this method are more realistic, while the accuracy of GT should be close to 1. Based on the results in Table 3 , radar charts were drawn for each dataset for a more intuitive comparison, as shown in Fig. 9 . Each vertex in the radar chart indicates the accuracy of the corresponding type of image source.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 465,
                    "end": 472,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 659,
                    "end": 666,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 755,
                    "end": 761,
                    "text": "Fig. 9",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "User studies"
        },
        {
            "text": "Most doctors have good ability in recognizing GT images and therefore the average accuracy (last row in Table 3 ) of GT images was high. However, in the ovarian data set, the evaluations of Doctor 2 and other doctors are quite different. Doctor 2 tended to classify almost all images as fake whereas the other doctors tended to classify most images as real.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 104,
                    "end": 111,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "User studies"
        },
        {
            "text": "Among different methods, the accuracy of the spGAN-sketch (i.e., the spGAN without sketch guidance) was higher especially in the COVID-19 dataset. This indicates sketch guidance plays an important role in synthesizing realistic images. Without the sketch guidance, the generated COVID-19 images present obvious checkerboard patterns (Fig. 6) , which makes it very easy for the doctors to distinguish them from real images. We also observed that the accuracy of spGAN-sketch method in the hip joint and ovary datasets was much lower than that in the COVID-19 dataset. This indicates that the sketch guidance is particularly necessary for lung US image synthesis. Among the three image synthesis methods, spGAN v2 achieved lower average accuracy than the other two methods in all datasets except that it achieved higher average accuracy than spGAN in the ovary dataset. This means that generally the use of the sketch guidance and feature loss can improve the quality of generated images.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 333,
                    "end": 341,
                    "text": "(Fig. 6)",
                    "ref_id": null
                }
            ],
            "section": "User studies"
        },
        {
            "text": "Furthermore, we developed a platform for editable image synthesis. Via customized editing of label maps, this platform can be used as a convenient tool to simulate various, meaningful US images for training sonographers and deep neural networks. Fig. 10 shows some synthesized examples after editing the label maps, using the proposed spGAN v2 method. For the COVID-19, the artifacts, such as the pleura line, A-line, B-line, and lung consolidation, are evaluated for grading disease severity. We can simulate a more severe COVID-19 case by adding the B-line to the label map. In hip joint US images, the diagnosis of infant hip dysplasia depends on the relative position of different structures. By changing the relative position of the structures in the label map of a normal case, we can create a case of developmental dislocation of the hip with high fidelity. At last, we can easily synthesize new ovary US images by changing the size of ovary and the number of follicles in the label map. A video demo of the editable synthesis using our developed platform is provided in Supplementary file Editabledemo.mp4.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 246,
                    "end": 253,
                    "text": "Fig. 10",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Editable synthesis"
        },
        {
            "text": "To explore whether using our method for data augmentation can improve the segmentation performance, we compared the results of three settings: no augmentation (baseline), traditional augmentation (trad), and traditional plus GAN-based augmentation (trad+GAN). Furthermore, in addition to using the full training data, we also experimented with 20% of the training data to see how these data augmentation methods performed on small datasets. We used U-Net (Ronneberger et al., 2015) as the segmentation model and DICE as the performance metric.",
            "cite_spans": [
                {
                    "start": 455,
                    "end": 481,
                    "text": "(Ronneberger et al., 2015)",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Segmentation experiments"
        },
        {
            "text": "We employed online augmentation and set the possibility to 0.3 for both traditional and GAN-based augmentation. For traditional augmentation, the operations included rotation, translation, scaling, blurring, gamma transformation, and adding Gaussian noise. For GAN-based augmentation, we randomly edited the label maps by morphological operations and used sp-GAN v2 to synthesize images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Segmentation experiments"
        },
        {
            "text": "The segmentation results are presented in Table 4 . For the hip joint and ovary datasets, Traditional augmentation improved segmentation performance when 20% of the training data were used but failed when the full training data were used. For the COVID-19 dataset, traditional augmentation did not improve performance no matter a portion of or the full training data were used. The reason may be that the lesion areas in COVID-19 images have rich styles and thus it is hard to gain additional information through simple operations in traditional augmentation. In contrast, our GAN-based augmentation method can provide greater variability with editable operations and therefore has great potential to improve performance. As shown in Table 4 , our method in general not only achieved superior segmentation results when a small training set was available but also further improved performance even when the full training set was used.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 42,
                    "end": 49,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 734,
                    "end": 741,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Segmentation experiments"
        },
        {
            "text": "In our proposed spGAN v2 method, there are three important parameters, i.e., the weight of feature loss term \u03bb 2 in the objective function of generator, the number of residual blocks n in generator, and the output size s of discriminator. To investigate the effect of the three parameters on the performance of our proposed spGAN v2 method, we tested different values for parameter \u03bb 2 from {0, 5, 10, 15}, parameter n from {0, 5, 10, 15, 20}, and parameter s from {1\u00d71, 30\u00d730, 60\u00d760, 120\u00d7120, 256\u00d7256}. Fig. 11, 12, and 13 show the results for each of the three parameters, respectively. According to the results in Fig. 11 , we chose \u03bb 2 = 10 for the COVID-19 and hip joint datasets and \u03bb 2 = 5 for the ovary dataset. A larger \u03bb 2 means that more emphasis is put on the similarity of high-level features (feature loss) instead of the Table 3 The classification accuracies of five doctors for each of the four types of images: the GT and the synthesized images by three methods (b: spGAN-sketch, e: spGAN, f: spGAN v2).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 504,
                    "end": 523,
                    "text": "Fig. 11, 12, and 13",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 617,
                    "end": 624,
                    "text": "Fig. 11",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 836,
                    "end": 843,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Effect of parameters"
        },
        {
            "text": "Hip Joint Ovary pixel-wise or structural similarity (L1-loss). Compared with the COVID-19 and the hip joint datasets, the structures (i.e., ovary and follicles) in the ovary US images have a clear boundary and regular shape. Therefore, a smaller \u03bb 2 is more suitable for the ovary dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19"
        },
        {
            "text": "Based on the results in Fig. 12 and visual evaluation of the synthesized images, we set the number of residual blocks n to 15, 15, and 10 for the COVID-19, hip joint, and ovary datasets, respectively. Although for the hip joint dataset, n = 20 achieved better performance in terms of the MS-SSIM and LPIPS metrics than n = 15, the visual quality of synthesized images was worse, so n = 15 was used for the hip joint dataset instead. Generally, more residual blocks used in the generator mean a stronger ability to learn features. However, too many residual blocks tended to degrade performance. As shown in Fig. 12 , fewer residual blocks are needed for the ovary dataset compared with the other two datasets. This is probably because the texture present in the ovary images is simpler than that in the other two types of images.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 24,
                    "end": 31,
                    "text": "Fig. 12",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 607,
                    "end": 614,
                    "text": "Fig. 12",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "COVID-19"
        },
        {
            "text": "The PatchGAN was used as the discriminator in our synthesis framework. Each unit of the discriminator output is like a local receptive field. A smaller output size indicates that each unit of the output represents a larger region. For instance, the output size of 1\u00d71 means that the discrimination is made from the whole image, without local information fed back to the generator. Conversely, the output size of 256\u00d7256 indicates that the discrimination is made from only several pixels, neglecting the context information in surrounding region. As shown in Fig. 13 , the results of 1\u00d71 output size are worse in all three datasets, and too small or too large output size often degrades the performance. According to the results in Fig. 13 and visual quality of the generated images, we set the output size to 30\u00d730, 120\u00d7120, and 30\u00d730 for the COVID-19, hip joint, and ovary datasets, respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 558,
                    "end": 565,
                    "text": "Fig. 13",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 731,
                    "end": 738,
                    "text": "Fig. 13",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "COVID-19"
        },
        {
            "text": "In this paper, to address the challenge of lacking enough data for training sonographers and deep neural networks, we propose an image-to-image translation framework aiming at generating high-resolution and high-fidelity US images from segmentation label maps. The proposed spGAN v2 method consists of four key components: auxiliary sketch guidance, progressive growing scheme, fade-in blocks, and feature loss. Specifically, the auxiliary sketch guidance provides auxiliary information for generating realistic background texture. The progressive growing scheme containing the pre-trained model of generating low-resolution images and fade-in blocks are employed for a smooth transition from low resolution to high resolution. To further improve the quality of the generated images, the feature loss is adopted to suppress noise and deblur the images. Extensive qualitative and quantitative experiments on the COVID-19, hip joint, and ovary datasets demonstrate the effectiveness of the proposed spGAN v2 method. Another important feature of our work is that we have developed an editable image synthesis platform that can easily create various and meaningful US images by modifying segmentation label maps. Overall, our study provides a useful and convenient tool for generating high-resolution and high-fidelity US images from label maps.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A pipeline for the generation of realistic 3d synthetic echocardiographic sequences: Methodology and open-access database",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Alessandrini",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "De Craene",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Bernard",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Giffard-Roisin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Allain",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Waechter-Stehle",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weese",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Saloux",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Delingette",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sermesant",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE transactions on medical imaging",
            "volume": "34",
            "issn": "",
            "pages": "1436--1451",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Virtual pet images from ct data using deep convolutional networks: initial results",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ben-Cohen",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Klang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "P"
                    ],
                    "last": "Raskin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Amitai",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Greenspan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Workshop on Simulation and Synthesis in Medical Imaging",
            "volume": "",
            "issn": "",
            "pages": "49--57",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Demystifying mmd gans",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bi\u0144kowski",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Sutherland",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Arbel",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gretton",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1801.01401"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "The perception-distortion tradeoff",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Blau",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Michaeli",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "6228--6237",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Real-time gpu-based ultrasound simulation using deformable mesh models",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Burger",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bettinghausen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Radle",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hesser",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE transactions on medical imaging",
            "volume": "32",
            "issn": "",
            "pages": "609--618",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Canny",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "679--698",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Computational analysis of pathological images enables a better diagnosis of tfe3 xp11. 2 translocation renal cell carcinoma",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mehra",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ni",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature communications",
            "volume": "11",
            "issn": "",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Computer-aided diagnosis with deep learning architecture: applications to breast lesions in us images and pulmonary nodules in ct scans",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "Z"
                    ],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ni",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "H"
                    ],
                    "last": "Chou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "Tiu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "C"
                    ],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "S"
                    ],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Scientific reports",
            "volume": "6",
            "issn": "",
            "pages": "1--13",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Generation of structural mr images from amyloid pet: application to mr-less quantification",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "S"
                    ],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Journal of Nuclear Medicine",
            "volume": "59",
            "issn": "",
            "pages": "1111--1117",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Image synthesis in multi-contrast mri with conditional generative adversarial networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "U"
                    ],
                    "last": "Dar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yurt",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Karacan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Erdem",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Erdem",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ukur",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE transactions on medical imaging",
            "volume": "38",
            "issn": "",
            "pages": "2375--2388",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Computer-aided diagnosis in medical imaging: historical review, current status and future potential",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Doi",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Computerized medical imaging and graphics",
            "volume": "31",
            "issn": "",
            "pages": "198--211",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Frid-Adar",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Diamant",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Klang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Amitai",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Goldberger",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Greenspan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Neurocomputing",
            "volume": "321",
            "issn": "",
            "pages": "321--331",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Breast ultrasound image synthesis using deep convolutional generative adversarial networks. Diagnostics 9",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Fujioka",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mori",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kubota",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kikuchi",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Katsuta",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Adachi",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Oda",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Nakagawa",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kitazume",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Tateishi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Convolutional neural networks for computer-aided detection or diagnosis in medical image analysis: An overview",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pouget-Abadie",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Mathematical Biosciences and Engineering",
            "volume": "16",
            "issn": "",
            "pages": "2672--2680",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Heusel",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ramsauer",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "6626--6637",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Freehand ultrasound image simulation with spatially-conditioned generative adversarial networks, in: Molecular imaging, reconstruction and analysis of moving body organs, and stroke imaging and treatment",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Gibson",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "105--115",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Image-to-image translation with conditional adversarial networks",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Isola",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "Y"
                    ],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1125--1134",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Field: A program for simulating ultrasound systems",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Jensen",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Medical & Biological Engineering & Computing",
            "volume": "34",
            "issn": "",
            "pages": "351--353",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Simulation of advanced ultrasound systems using field ii",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Jensen",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro",
            "volume": "",
            "issn": "",
            "pages": "636--639",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Tumor-aware, adversarial domain adaptation from ct to mri for lung cancer segmentation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "C"
                    ],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Tyagi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rimner",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "S"
                    ],
                    "last": "Mageras",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "O"
                    ],
                    "last": "Deasy",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Veeraraghavan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "volume": "",
            "issn": "",
            "pages": "777--785",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Perceptual losses for real-time style transfer and super-resolution",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Alahi",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Fei-Fei",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "694--711",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Karras",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Aila",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1710.10196"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Visualization and gpu-accelerated simulation of medical ultrasound from ct images. Computer methods and programs in biomedicine 94",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Kutter",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Shams",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Navab",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "250--266",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Synthesis and edition of ultrasound images via sketch guided progressive growing gans",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "T"
                    ],
                    "last": "Van",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Dou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Mai",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE",
            "volume": "",
            "issn": "",
            "pages": "1793--1797",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Susan: segment unannotated image structure using adversarial network",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Magnetic resonance in medicine",
            "volume": "81",
            "issn": "",
            "pages": "3330--3345",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Deep learning in medical ultrasound analysis: A review. Engineering",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Comparison of texture synthesis methods for content generation in ultrasound simulation for training",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Mattausch",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bajka",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Vanhoey",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Goksel",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Medical Imaging 2017: Image-Guided Procedures, Robotic Interventions, and Modeling, International Society for Optics and Photonics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Conditional generative adversarial nets",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mirza",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Osindero",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1411.1784"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Medical image synthesis with context-aware generative adversarial networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Trullo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lian",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Petitjean",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ruan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "volume": "",
            "issn": "",
            "pages": "417--425",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Generation of synthetic but visually realistic time series of cardiac images combining a biophysical model and clinical images",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Prakosa",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sermesant",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Delingette",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE transactions on medical imaging",
            "volume": "32",
            "issn": "",
            "pages": "99--109",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Metz",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chintala",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1511.06434"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Simulation model of intravascular ultrasound images",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "D R"
                    ],
                    "last": "Ram\u00edrez",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "R"
                    ],
                    "last": "Ivanova",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "200--207",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ronneberger",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "234--241",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Generative adversarial networks (gans): An overview of theoretical model, evaluation metrics, and recent developments",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Salehi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chalechale",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Taghizadeh",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.13178"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Abnormal colon polyp image synthesis using conditional adversarial networks for improved detection performance",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "A"
                    ],
                    "last": "Qadir",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Access",
            "volume": "6",
            "issn": "",
            "pages": "56007--56017",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Simulating patho-realistic ultrasound images using deep generative networks with adversarial learning",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Tom",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1174--1177",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "k-wave: Matlab toolbox for the simulation and reconstruction of photoacoustic wave fields",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "E"
                    ],
                    "last": "Treeby",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "T"
                    ],
                    "last": "Cox",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Journal of biomedical optics",
            "volume": "15",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Instance normalization: The missing ingredient for fast stylization",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ulyanov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lempitsky",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1607.08022"
                ]
            }
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Multiscale structural similarity for image quality assessment",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "P"
                    ],
                    "last": "Simoncelli",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "The Thrity-Seventh Asilomar Conference on Signals",
            "volume": "",
            "issn": "",
            "pages": "1398--1402",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Generative adversarial network in medical imaging: A review",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yi",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Walia",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Babyn",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Medical image analysis",
            "volume": "58",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Isola",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "A"
                    ],
                    "last": "Efros",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Shechtman",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "586--595",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Skrgan: Sketching-rendering unconditional generative adversarial networks for medical image synthesis",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1908.04346"
                ]
            }
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Craniomaxillofacial bony structures segmentation from mri with deep-supervision adversarial learning",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cong",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ahmad",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ho",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Fung",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "H"
                    ],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "volume": "",
            "issn": "",
            "pages": "720--727",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "A framework for the generation of realistic synthetic cardiac ultrasound and magnetic resonance imaging sequences from the same virtual patients",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Giffard-Roisin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE transactions on medical imaging",
            "volume": "37",
            "issn": "",
            "pages": "741--754",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Generation of composite labels by adding the edge sketches onto the original label maps.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Diagram of the fade-in block down (top) and fade-in block up (bottom)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Examples of low-resolution (256\u00d7256) image synthesis for three datasets. The first three rows are results for three different methods, and the last row presents the GT images. In the bottom right corner of each synthesized image is the difference heatmap. S, sketch; FL, feature loss.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Examples of high-resolution (512\u00d7512) image synthesis for the ovary dataset using different methods. The middle two rows show the enlarged patches and the corresponding difference heatmap. Interp, interpolation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Radar charts showing the results of user studies. Four different types of images are used: the GT and the synthesized images by three methods (b spGAN-sketch, e spGAN, f spGAN v2).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Examples of editable synthesis. The yellow arrows indicate the changes before and after editing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Effect of the weight of the feature loss in the objective function of generator. The horizontal axis denotes the weight, and the color-coded vertical axes represent four performance metrics.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Effect of the number of residual blocks in generator. The horizontal axis denotes the number of residual blocks, and the color-coded vertical axes represent four performance metrics.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Effect of the output size of discriminator. The horizontal axis denotes the output size, and the color-coded vertical axes represent four performance metrics.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Quantitative results for low-resolution (256\u00d7256) image synthesis. S, sketch; FL, feature loss.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Quantitative results for high-resolution (512\u00d7512) image synthesis. Interp, interpolation.Fig. 6. Examples of high-resolution (512\u00d7512) image synthesis for the COVID-19 dataset using different methods. The middle two rows show the enlarged patches and the corresponding difference heatmap. Interp, interpolation.Fig. 7. Examples of high-resolution (512\u00d7512) image synthesis for the hip joint dataset using different methods. The middle two rows show the enlarged patches and the corresponding difference heatmap. Interp, interpolation.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Comparison of the segmentation performance (DICE) with and without data augmentation. 20%: training with 20% data.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
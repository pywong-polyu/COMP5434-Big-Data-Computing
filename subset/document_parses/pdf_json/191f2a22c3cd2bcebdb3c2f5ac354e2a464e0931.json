{
    "paper_id": "191f2a22c3cd2bcebdb3c2f5ac354e2a464e0931",
    "metadata": {
        "title": "The RobotSlang Benchmark: Dialog-guided Robot Localization and Navigation",
        "authors": [
            {
                "first": "Shurjo",
                "middle": [],
                "last": "Banerjee",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The University of Michigan",
                    "location": {}
                },
                "email": "shurjo@umich.edu"
            },
            {
                "first": "Jesse",
                "middle": [],
                "last": "Thomason",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Washington",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Jason",
                "middle": [
                    "J"
                ],
                "last": "Corso",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The University of Michigan",
                    "location": {}
                },
                "email": "jjcorso@umich.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Autonomous robot systems for applications from search and rescue to assistive guidance should be able to engage in natural language dialog with people. To study such cooperative communication, we introduce Robot Simultaneous Localization and Mapping with Natural Language (RobotSlang), a benchmark of 169 natural language dialogs between a human DRIVER controlling a robot and a human COMMANDER providing guidance towards navigation goals. In each trial, the pair first cooperates to localize the robot on a global map visible to the COM-MANDER, then the DRIVER follows COMMANDER instructions to move the robot to a sequence of target objects. We introduce a Localization from Dialog History (LDH) and a Navigation from Dialog History (NDH) task where a learned agent is given dialog and visual observations from the robot platform as input and must localize in the global map or navigate towards the next target object, respectively. RobotSlang is comprised of nearly 5k utterances and over 1k minutes of robot camera and control streams. We present an initial model for the NDH task, and show that an agent trained in simulation can follow the RobotSlang dialog-based navigation instructions for controlling a physical robot platform. Code and data are available at https://umrobotslang.github.io/.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Language is a natural medium for people to communicate with and direct robots. Research on language-guided robots lets users specify high-level goals like Push the full barrel [1, 2] and lowerlevel instructions like After the blue bale fly to the right towards the small white bush [3, 4] . Meanwhile, commercial dialog-enabled smart assistants are imbued with expanded language capabilities, but have limited interaction with the real world. Dialog-enabled robots combine these strengths, facilitating task completion [5] , task learning [6, 7] , and language learning [8] .",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 179,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 180,
                    "end": 182,
                    "text": "2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 282,
                    "end": 285,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 286,
                    "end": 288,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 519,
                    "end": 522,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 539,
                    "end": 542,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 543,
                    "end": 545,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 570,
                    "end": 573,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Two related skills are needed across potential robot applications, from search and rescue missions to assistive guidance in an office building: localization and navigation. For a non-expert user, dialog facilitates both. The user can ascertain where the robot is by making requests like Describe your surroundings and give instructions for where to go next, such as go toward the edge of the maze. There is an array of benchmarks for vision-and-language navigation (VLN), where an agent learns to follow such language directions in simulated environments [9, 10, 11, 12] .",
            "cite_spans": [
                {
                    "start": 555,
                    "end": 558,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 559,
                    "end": 562,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 563,
                    "end": 566,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 567,
                    "end": 570,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Models optimizing performance on simulation-only benchmarks do not consider physical robot navigation limitations, employing unrealistic strategies like beam search [13] , repeatedly creating panoramas from egocentric cameras [14] , or assuming pre-exploration of the environment [15] . Where simulation allows the collection of such large-scale benchmarks, efforts in learning-based robotics are limited by expensive data collection and require more robust, sample-efficient models.",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 169,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 226,
                    "end": 230,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 280,
                    "end": 284,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To address this gap, we introduce Robot Simultaneous Localization and Mapping with Natural Language (RobotSlang), a benchmark of human-human cooperative trials for controlling a physical robot to visit object goals while communicating in natural language ( Figure 1 ). RobotSlang data is collected on a physical robot platform and is directly applicable for training a language-guided robot to follow human language instructions, which can otherwise require careful transfer techniques [16] . Figure 1 : The COMMANDER and DRIVER communicate through a web application. The COMMAN-DER sees a static map and an image of the next target object (left). The DRIVER sees the first-person camera feed from the robot (a small rover controlled by a raspberry pi with an attached HD webcam) and can send the robot movement commands (right). Participants cooperate through a chat interface to guide the robot to a sequence of three target objects (bottom right). We synchronize, capture, and annotate the raw dialog text, sensor observations, and map data to create the RobotSlang benchmark.",
            "cite_spans": [
                {
                    "start": 486,
                    "end": 490,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 257,
                    "end": 265,
                    "text": "Figure 1",
                    "ref_id": null
                },
                {
                    "start": 493,
                    "end": 501,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "A learning agent reacting to human language instructions should be embodied in the real world and be able to carry on dialog with human users [17] . To this end, RobotSlang combines aspects of two research efforts: language-guided virtual agents in simulation and situated, human-robot dialog for guiding robot behavior. Table 1 summarizes the key differences between RobotSlang and comparable benchmarks across the language, vision, and robotics communities. RobotSlang provides a resource for studying how humans use language to cooperatively control a physical robot. As with initiatives like the Duckietown [18] and MuSHR [19] , our physical robot setup can be recreated in other labs to make use of RobotSlang data for training and evaluation.",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 146,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 611,
                    "end": 615,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 626,
                    "end": 630,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [
                {
                    "start": 321,
                    "end": 328,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Related Work"
        },
        {
            "text": "Language-Guided Virtual Agents. Given a natural language instruction, an agent attempts to ground [20] this language to its visual surroundings. Early environments used simple texture renderings [21, 9] , but advances in simulation and scene capture have lead to photorealistic indoor [10] and outdoor [12] spaces for VLN and general video understanding [22] . Some benchmarks include object manipulation and state changes, creating a more task-oriented setting [23] .",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 102,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 195,
                    "end": 199,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 200,
                    "end": 202,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 285,
                    "end": 289,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 302,
                    "end": 306,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 354,
                    "end": 358,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 462,
                    "end": 466,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Moving beyond static instructions, some benchmarks are created via human-human dialog, where a DRIVER moves the agent and asks questions that a COMMANDER answers. By enabling an agent to ask questions and gather more information during navigation [24, 25] , ambiguous and underspecified commands can be clarified by a human interlocutor [26, 11, 27] . Some of these benchmarks require global localization [26] , where the COMMANDER does not initially know enough about the location of the DRIVER to provide instructions. These benchmarks are all limited to simulations, while deployed robots operate in noisier, real-world environments. Our RobotSlang benchmark, by contrast, was gathered by pairing a human DRIVER controlling a physical robot and asking questions of a human COMMANDER, where the pair need to perform cooperative global localization while carrying out a navigation task to multiple object targets.",
            "cite_spans": [
                {
                    "start": 247,
                    "end": 251,
                    "text": "[24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 252,
                    "end": 255,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 337,
                    "end": 341,
                    "text": "[26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 342,
                    "end": 345,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 346,
                    "end": 349,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 405,
                    "end": 409,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Language-Guided Physical Agents. Physical robots engaged in task-oriented behaviors with human collaborators benefit from a natural language interface [29] . For example, human language commands can be combined with visual sensory input [28] to learn a controller for a quadcopter platform in an end-to-end fashion [30, 3] . Using natural language and mixed reality can allow users to control quadcopters with little training data [31] .",
            "cite_spans": [
                {
                    "start": 151,
                    "end": 155,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 237,
                    "end": 241,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 315,
                    "end": 319,
                    "text": "[30,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 320,
                    "end": 322,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 431,
                    "end": 435,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Collaborative dialog can enable a robot to acquire new skills [6] and refine perceptual understanding [8] through language interaction. However, generating grounded language requests is often achieved through carefully controlled dialog managers [32] and generation semantics [5] . Learning to generate language from grounded, human-human dialogs [26, 33] such as those in the RobotSlang benchmark is an ambitious alternative for future work.",
            "cite_spans": [
                {
                    "start": 62,
                    "end": 65,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 102,
                    "end": 105,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 246,
                    "end": 250,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 276,
                    "end": 279,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 347,
                    "end": 351,
                    "text": "[26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 352,
                    "end": 355,
                    "text": "33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Localization Navigation Questions  Human  Sensor  Efforts  Type  Task  Task  Allowed  Answers Observations MARCO [21, 9] Virtual -Render VLN [10, 12] Virtual -Photoreal DRIF [28, 3] Physical -Camera RGB DUIL [4] Physical -Full Map VLNA [24, 25] Virtual Photoreal MRDwH [11] Virtual Render CVDN [27] Virtual Photoreal Talk the Walk [26] Virtual Photoreal",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 117,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 118,
                    "end": 120,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 141,
                    "end": 145,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 146,
                    "end": 149,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 174,
                    "end": 178,
                    "text": "[28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 179,
                    "end": 181,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 208,
                    "end": 211,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 236,
                    "end": 240,
                    "text": "[24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 241,
                    "end": 244,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 269,
                    "end": 273,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 294,
                    "end": 298,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 331,
                    "end": 335,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [
                {
                    "start": 24,
                    "end": 106,
                    "text": "Questions  Human  Sensor  Efforts  Type  Task  Task  Allowed  Answers Observations",
                    "ref_id": null
                }
            ],
            "section": "Agent"
        },
        {
            "text": "Physical Camera RGB Table 1 : Compared to existing efforts involving vision and language input for controlling navigation agents, RobotSlang is the first to be built from human-human dialogs between a COMMANDER and DRIVER piloting a physical robot. RobotSlang also involves both a global localization and navigation task, where many previous efforts involve only navigation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 20,
                    "end": 27,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "RobotSlang"
        },
        {
            "text": "In Robot Simultaneous Localization and Mapping with Natural Language (RobotSlang), humans take on the roles of COMMANDER and DRIVER to guide a robot through a 2.5 by 2 meter tabletop maze to visit a series of three target landmark objects ( Figure 1 ). In total, RobotSlang is comprised of 169 trials with associated dialogs and robot sensor streams carried out in 116 unique mazes.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 241,
                    "end": 249,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Robot Simultaneous Localization and Mapping with Natural Language"
        },
        {
            "text": "Information and control asymmetry drives natural language communication to finish tasks, and a leaderboard was used to rank human-human teams and encourage efficient task completion (Section 3.1). The COMMANDER viewed a top-down, static map of the environment that included the landmarks to be visited, but could not see the robot. The DRIVER had access to the robot's sensor streams, including its front-facing camera, but did not have access to a global map. The participants communicated only over a text chat interface. First, the DRIVER needed to provide information about their surroundings to the COMMANDER to perform Localization from Dialog History (LDH). Then, the COMMANDER provided directions to the DRIVER. The DRIVER could ask questions throughout. Previous dialog-based navigation work forced strict turn taking and involved only one target per dialog [26, 27] . In RobotSlang, communication is at-will, and each trial involves visiting a sequence of three target objects. These differences lead to complex dialog phenomena (Section 3.2).",
            "cite_spans": [
                {
                    "start": 867,
                    "end": 871,
                    "text": "[26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 872,
                    "end": 875,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Robot Simultaneous Localization and Mapping with Natural Language"
        },
        {
            "text": "Each dialog in RobotSlang facilitated control of a small robotic car through a tabletop maze to visit a sequence of landmarks. 1 Throughout each trial, the robot collected front-facing camera stream data. The COMMANDER and DRIVER, although physically separated in different rooms, were connected via a web interface, shown in Figure 1 . The DRIVER viewed a real-time stream from the robot camera, while the COMMANDER viewed a static, top-down RGB image of the maze and a photo of the next landmark object to visit. Team members communicated via a text chat interface. A central server hosted the web application and saved synchronized sensor and text dialog data streams. 2 Sixteen participants were recruited to create RobotSlang. Participants were not familiar with the robot platform and were students from the university at which this research was conducted. The participants formed 41 teams, with each participant involved in 5 teams and 21 trials on average. To motivate participant teams to do well on the task, each trial was scored, and teams competed on a leaderboard. Teams were scored by overall time taken to visit all landmarks, with a small penalty for each motion command the DRIVER sent to the robot. The same maze was never presented to a team twice, and each team performed an average of 4 trials, resulting in 169 total trials. Dialogs lasted an average of six and a half minutes. Table 2 : Dialog phenomena we annotated, along with the estimated frequency and examples from a 34 dialogs randomly sampled for annotation (20% of the total trials). We use the symbol | to separate short, sequential messages sent from the same speaker.",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 128,
                    "text": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 672,
                    "end": 673,
                    "text": "2",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [
                {
                    "start": 326,
                    "end": 334,
                    "text": "Figure 1",
                    "ref_id": null
                },
                {
                    "start": 1401,
                    "end": 1408,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Data Collection"
        },
        {
            "text": "The dialogs in RobotSlang are long. An average of 28 messages was sent per dialog, and dialogs averaged 200 words. By contrast, dialogs in the closely related, simulation-only CVDN task [27] contain an average of only 81.6 words per dialog, less than half the verbosity in our trial data. Table 2 gives examples of dialog snippets from RobotSlang that exhibit complex phenomena. Because each trial involves visiting a sequence of objects, over 50% of dialogs involve COMMANDERs referring to past locations in their instructions. Successful modeling may require explicit memory structures. Humans make mistakes, both when giving instructions and when following them, but communication also facilitates repairing those mistakes. Over 25% of the dialogs in Robot-Slang exhibit mistakes and repair. Some of these mistakes require a new localization step, with the COMMANDER requesting location information again during navigation in 15% of dialogs. However, Humans also trust one another to follow \"commonsense\" routes and directions, with COMMANDERs surrendering control to DRIVERs in around 10% of dialogs. Finally, because the COMMANDER and DRIVER see different views of the environment, participants also need to mediate their perceptual differences explicitly in some dialogs, as studied in human-robot communication [34] .",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 190,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1318,
                    "end": 1322,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [
                {
                    "start": 289,
                    "end": 296,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Dialog Data Analysis"
        },
        {
            "text": "In lieu of training on the physical tabletop environment, we design a custom simulator ( Figure 2 ). There are two aspects of the real world we simplify when creating the replay simulation environment: image observations and robot position. We annotate trial maps as occupancy grids where grid values represent corresponding, discretized pixel colors from maze walls. Grid points are 0.07 meters apart. The simulated agent travels between adjacent nodes using forward actions. The agent can change its heading with left and right actions of 45 degrees each. We precompute all shortest paths between navigable points for use during model training (Section 5.2). For training, we treat the shortest path between the robot's starting position and each target visitation object in turn as the human navigation path. We use the Floyd-Warshall algorithm [35] for shortest path planning.",
            "cite_spans": [
                {
                    "start": 848,
                    "end": 852,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [
                {
                    "start": 89,
                    "end": 97,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Replay-Simulation Environment"
        },
        {
            "text": "We represent front-facing camera observations as a set of rays returning color and depth information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Replay-Simulation Environment"
        },
        {
            "text": "In particular, the agent's observation at every timestep consists of, for each ray, a discrete wall color, a distance to the wall, and whether the wall is too close to move towards ( Figure 2 ). Thirteen equally Train  79  120  120  360  69  Val  19  26  26  78  15  Test  20  28  28  84  16   Table 3 : Fold summaries in the RobotSlang benchmark, including the number of mazes, trials, and task instances. We split by mazes, such that no two folds contain trials with the same maze.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 183,
                    "end": 191,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 212,
                    "end": 301,
                    "text": "Train  79  120  120  360  69  Val  19  26  26  78  15  Test  20  28  28  84  16   Table 3",
                    "ref_id": "TABREF7"
                }
            ],
            "section": "Replay-Simulation Environment"
        },
        {
            "text": "spaced rays are cast from the front of the agent, encompassing a field-of-view of 78 degrees-the same as the physical robot. We use Brensenham's line algorithm [36] to perform ray tracing.",
            "cite_spans": [
                {
                    "start": 160,
                    "end": 164,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Fold # Mazes # Trials # LDH # NDH % Dataset by Trial"
        },
        {
            "text": "The replay simulation allows training and testing models for tasks derived from RobotSlang. These models may be transferable to the physical platform for either fine-tuning with real-world camera input or preprocessed image observations using the simulator-familiar ray tracing method. 3 ",
            "cite_spans": [
                {
                    "start": 286,
                    "end": 287,
                    "text": "3",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Fold # Mazes # Trials # LDH # NDH % Dataset by Trial"
        },
        {
            "text": "RobotSlang provides a resource for approaching numerous problems in human-robot communication, from query generation-learning how to ask good questions-to language-driven dynamics prediction-learning to predict the DRIVER's actions in response to language. While some work attempts to create agents that mimic both the COMMANDER and DRIVER, these methods are applied exclusively on data gathered in simulation and without regard to the need for sample efficiency with physical limitations on deployment [26, 33] . We instead focus on two core problems for any physical robot collaborating with a human giving language directions: Localization from Dialog History (LDH) and Navigation from Dialog History(NDH). We split the data into training, validation, and test folds (Table 3 ). Figure 3 give example LDH and NDH instances.",
            "cite_spans": [
                {
                    "start": 503,
                    "end": 507,
                    "text": "[26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 508,
                    "end": 511,
                    "text": "33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [
                {
                    "start": 770,
                    "end": 778,
                    "text": "(Table 3",
                    "ref_id": null
                },
                {
                    "start": 782,
                    "end": 790,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "Tasks"
        },
        {
            "text": "Localization from Dialog History (LDH) We create a benchmark for learning models that perform Localization from Dialog History, emulating the human COMMANDER. We create one LDH instance per trial. For each trial, we annotated when initial localization was complete, before navigation began. A model receives as input the dialog history between the COMMANDER and DRIVER from the trial start to that hand-annotated end of localization. Given this information, the model must predict the location of the robot on the global map, just as the human COMMANDER does before giving navigation instructions. Figure 3 : Instances of the LDH task (left) and NDH task (right). These tasks represent key skills of the COMMANDER and DRIVER: localization and navigation given natural language context. In both tasks, models have access to the dialog history so far as input. In LDH, models also see the global static map, and must predict where the robot is based on the dialog. In NDH, models also see the navigation history so far and the dialog snippet that guided the human-human pair to the next object, and must predict the navigation actions to reach that object.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 598,
                    "end": 606,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "Tasks"
        },
        {
            "text": "As exhibited in Table 2 , over 10% of trials require explicitly re-localizing when participants realize they have mismatched assumptions. In general, localization continues to happen in \"soft\" ways through the dialog, with the COMMANDER and DRIVER offering frequent sanity checks like You should then have blue on your left and white in front of you and i have red walls ahead. Thus, models may need to explicitly continue performing localization after this initial LDH step, which we leave for future work. After initial localization, navigation can begin.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 16,
                    "end": 23,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Tasks"
        },
        {
            "text": "Navigation from Dialog History (NDH) We create a benchmark for learning models that perform Navigation from Dialog History, emulating the human DRIVER. For each trial, we create an NDH instance for each of the three objects to be visited. For each NDH instance, the robot begins at the trial initial position or in front of the last object visited, and must navigate to the next object in the visitation sequence. A model receives as input the robot navigation history, the dialog history between the COMMANDER and the DRIVER so far, and the dialog that guided the DRIVER from the starting location to the goal location. Given this information, the model must predict the controller actions taken by the DRIVER to move the robot according to the given language instructions. Actions are discretized to: forward (0.07 meters), left (45 degrees), right (45 degrees), and stop. We evaluate NDH performance based on the final position (x,\u0177) of the robot.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tasks"
        },
        {
            "text": "Metrics. Both the LDH and NDH tasks involve predicting a location (x,\u0177) on the map, either directly for localizing the robot or indirectly as the result of a sequence of navigation actions. We evaluate performance with a Topological Distance (TD) metric that measures the shortest navigable path between the true position (x * , y * ) and the predicted position (x,\u0177). Concretely, T D = |shortestpath((x,\u0177), (x * , y * ))|. To analyze how much error comes form predicting when to stop, we also measure agent performance under Oracle Stopping [37] , how well the agent would have done if it had stopped at the optimal time along its predicted path P . Oracle stopping distance OTD is calculated as OT D = min (x,y)\u2208P |shortestpath((x, y), (x * , y * ))| for P the predicted path. Figure 4 : Our initial Sequence-to-Sequence model for the NDH task. An LSTM encoder takes as input sequence of GloVe-embedded language tokens representing the dialog history. The encoder LSTM initializes the hidden state of an LSTM decoder, which takes in an encoded visual observation from the robot's (or replay simulation agent's) front-facing camera and predicts an action.",
            "cite_spans": [
                {
                    "start": 542,
                    "end": 546,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [
                {
                    "start": 779,
                    "end": 787,
                    "text": "Figure 4",
                    "ref_id": null
                }
            ],
            "section": "Tasks"
        },
        {
            "text": "We evaluate human performance on the LDH task and create an initial, learned model for the NDH task which we evaluate in the replay simulation environment. We find that humans perform well on the LDH from reading dialog histories. Our initial model for the NDH outperforms simple baselines and ablations, indicating that the RobotSlang can be used to learn language navigation policies.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "The LDH task involves making a discrete prediction for the location of the robot given the COM-MANDER's static map view and the dialog history. However, human COMMANDERs do not necessarily need the precise location of the robot, and may give instructions that are valid for a distribution around their belief of where the robot probably is.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Localization from Dialog History."
        },
        {
            "text": "To assess the difference between humans' ability to make these discrete predictions and the physical locations of the robots, we conducted a human study with 8 participants who were not involved in the collection of RobotSlang. Human predictions for the robot location were, on average over all LDH instances, only 0.446 meters away from the true robot center by TD. This performance establishes an upper bound for future modeling attempts. We note that while humans are not perfect at pinpointing the robot's location, their guesses are often within two robot body lengths of the true center, since the robot is about 0.254 meters long.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Localization from Dialog History."
        },
        {
            "text": "Following closely related work in simulation-only, human-human dialogs for navigation [27] , we focus our initial modeling efforts primarily on the NDH task. Given a dialog between a human COMMANDER and DRIVER, the robot agent must infer the next navigation actions to take, using the DRIVER's next actions as supervision during training. Because the RobotSlang data was gathered on a physical robot platform, the RobotSlang benchmark can be trained and evaluated in both our replay simulation (Section 3.3) and in the real world.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 90,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Navigation from Dialog History."
        },
        {
            "text": "Initial Sequence-to-Sequence Model. We develop an initial, Sequence-to-Sequence (S2S) model, summarized in Figure 4 . This model mirrors initial S2S models used to evaluate prior work in visionand-language navigation [10, 27] . The dialog history is tokenized and words are embedded using GloVe-300 [38] . Special tokens, <COM> and <DRI>, with random initial embeddings, are added to the sequence to indicate the speaker. An LSTM [39] model is used to encode this token sequence, and its final hidden state initializes an LSTM decoder model. The LSTM decoder observes an image I from the robot's front-facing camera, and predicts an action\u00e2. Under a teacher forcing curriculum, at training time the ground truth action a * from the human trial is taken. Under a student forcing curriculum, at training time an action\u00e2 is taken after sampling from the predicted logits [10] . All parameters are learned end-to-end by minimizing a cross entropy loss between predicted actions and the true action. At inference time, predicted action\u00e2 is taken in the simulation environment. 4 We train and evaluate NDH models using the replay simulation environment, approximating camera image observations with simulated ray tracing (Section 3.3). Table 4 : Sequence-to-Sequence model results on the NDH task under studentversus teacherforcing, as well as under unimodal ablations [40] . The Topological Distance (TD) is the primary measure of performance-the length of the shortest path between the predicted final location for the robot and the true location. We also show performance under Oracle Stopping-the shortest distance achieved along the inferred navigation path. All metrics are in meters, and for all metrics lower is better. * V,L indicate when a model with access to both vision and language input statistically significantly outperforms the corresponding Vision-only or Language-only ablation.",
            "cite_spans": [
                {
                    "start": 217,
                    "end": 221,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 222,
                    "end": 225,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 299,
                    "end": 303,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 430,
                    "end": 434,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 868,
                    "end": 872,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1072,
                    "end": 1073,
                    "text": "4",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1363,
                    "end": 1367,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [
                {
                    "start": 107,
                    "end": 115,
                    "text": "Figure 4",
                    "ref_id": null
                },
                {
                    "start": 1230,
                    "end": 1237,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Navigation from Dialog History."
        },
        {
            "text": "Results. Table 4 summarizes our results on the NDH task. Our initial, sequence to sequence model outperforms baselines as well as unimodal ablations. Consistent with prior work [10] , training with a student forcing regime makes the full model significantly 5 more robust to errors at inference time.",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 181,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [
                {
                    "start": 9,
                    "end": 16,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Inputs"
        },
        {
            "text": "The random baseline selects an action (except stop) at random up to a maximum number of steps, while the immediate stop baseline selects stop as the first action. These baselines provide context for the average distance agents need to cover to reach target objects in each NDH instance. In a unimodal ablation, the model is trained and tested with one modality (language or vision) empty. Our initial model does not exhibit substantial unimodal bias pathology [40] . The test fold performance of the vision-only model is not statistically significantly different from the full model.",
            "cite_spans": [
                {
                    "start": 460,
                    "end": 464,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Inputs"
        },
        {
            "text": "We introduce Robot Simultaneous Localization and Mapping with Natural Language (RobotSlang), a dataset of human-human dialogs for controlling a physical robot, and create associated localization and navigation task benchmarks. We present an initial model for Navigation from Dialog History that improves over simple baselines, demonstrating that language-driven instruction following can be achieved from real-robot, human-human dialog data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        },
        {
            "text": "Limitations and Future Work While RobotSlang data was collected on a physical robot platform, our Navigation from Dialog History evaluations were limited to our replay simulation environment. In the future, we will evaluate agents trained in the replay simulation on the physical robot platform. We are also interested in the possibilities of training on large scale vision-and-language navigation benchmarks and fine-tuning on RobotSlang as in-domain data for physical robot control. While the underlying visual and action data distributions differ, similarities in natural language can be leveraged to create more robust models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        },
        {
            "text": "Currently, we focus only on localization and navigation tasks, but dialog can involve training two agents to entirely fill the roles of COMMANDER and DRIVER as tried in simulation-only VLN datasets [26, 33] . Our initial model for NDH does not utilize explicit memory structures or historical visual features, but our analysis of dialog phenomena revealed that both are needed in over 50% of dialogs to resolve commands like Go back to your initial position. Further modeling attempts on the RobotSlang navigation task can make use of historical sensory context beyond language. We are also interested in exploring semi-supervised reinforcement learning for model training, which has been been successfully applied to multi-modal navigation tasks [41, 42] . ",
            "cite_spans": [
                {
                    "start": 198,
                    "end": 202,
                    "text": "[26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 203,
                    "end": 206,
                    "text": "33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 747,
                    "end": 751,
                    "text": "[41,",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 752,
                    "end": 755,
                    "text": "42]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        },
        {
            "text": "We further detail the hyper-parameters used in our replay simulation. The average number of actions to reach the goal is 75.47 \u00b1 22.01. We use this average to inform a maximum episode length of 120, about two standard deviation above the mean. After inferring 120 actions without inferring stop, models are stopped automatically.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Replay-Simulation Environment Details"
        },
        {
            "text": "We represent the jar, mug and paddle objects with distinct color blobs of light blue, pink, and olive respectively ( Figure 2 ). When trying to find the first object, we initialize the agent's position at the DRIVER's initial location. For subsequent objects, we initialize the agent facing the previous object. Many sub-dialogs for finding objects begin with phrases that encourage the DRIVER to change direction, like turn around in Figure 5 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 117,
                    "end": 125,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 435,
                    "end": 443,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Replay-Simulation Environment Details"
        },
        {
            "text": "To validate the effectiveness of our simulation, we annotate and separate the DRIVER feed in to its constituent colors and use this measurement to localize the DRIVER's position in the maze. More details can be found in Figure 7 . The successful localization of the driver shows that transfer is possible between our simulated setup and the real world RobotSlang setup.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 220,
                    "end": 228,
                    "text": "Figure 7",
                    "ref_id": null
                }
            ],
            "section": "Replay-Simulation Environment Details"
        },
        {
            "text": "Our code is built on the backbone of prior VLN works [10, 27] . Following their conventions, we train on the training fold when testing on the validation fold, but expand training data to include both training and validation when evaluating on the held out test set. For test set evaluation, we report model performance when the model has trained for the number of epochs at which it achieved the best performance against the validation fold. That is, we treat the epoch to train to as a hyperparameter set by the validation data per model.",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 57,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 58,
                    "end": 61,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Further Modeling Details"
        },
        {
            "text": "We use the following hyperparameters in all our sequence-to-sequence models: a batch size of 100, a token embedding size of 128, an action embedding size of 8, the LSTM's hidden state size of 128, and the max dialog history token length of 100 (the most recent tokens are used as input).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Further Modeling Details"
        },
        {
            "text": "Follow prior work [10, 27] , we explicitly zero out the possibility of choosing actions that would lead to collisions, both at training and testing time. This choice assumes a a robot can either detect that it made a collision and recover its previous position, or that it can detect a collision is imminent and override a model's choice to continue on a collision course.",
            "cite_spans": [
                {
                    "start": 18,
                    "end": 22,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 23,
                    "end": 26,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Further Modeling Details"
        },
        {
            "text": "For our teacher forcing models we use a learning rate of 0.0001. For our student-forcing models we use 0.001. We empirically find that that the student-forcing models train better and faster using this higher learning rate. We also zero out the agent's stop logit until it is at the destination. This choice prevents preemptive stop selection during student-forcing, which can slow down training immensely given that our episodes are longer by almost a factor of 6 than previous works (i.e., an average of 75 actions versus 6 actions in VLN [10] ). Figure 7 : On the left is the annotated map used in a simulation. Blue dots represent particles from the particle filter. In the center is a top down view of the maze where the robot can be seen. In the upper right we see the DRIVER feed. In the lower right we see DRIVER feed separated in to its constituent colors. Particle filtering uses the constituent separated measurement to localize the DRIVER position successfully as shown by the clustering of the particles. The particle filter can be seen in action in the attached video. Student .00 \u00b1 .00 .00 \u00b1 .00 .00 \u00b1 .00 .00 \u00b1 .00 Student .00 \u00b1 .00 .00 \u00b1 .00 .00 \u00b1 .00 .00 \u00b1 .00 Student .00 \u00b1 .00 .00 \u00b1 .00 .00 \u00b1 .00 .00 \u00b1 .00 ",
            "cite_spans": [
                {
                    "start": 541,
                    "end": 545,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [
                {
                    "start": 549,
                    "end": 557,
                    "text": "Figure 7",
                    "ref_id": null
                }
            ],
            "section": "Further Modeling Details"
        },
        {
            "text": "We ran each model with three random seeds and took a micro-average of performance per-trajectory, then compared model performance using paired t-tests. We ran paired t-tests between the full models under teaching and student supervision, as well as between full models and their unimodal ablations. Data pairs are NDH instances, and we compare the topological distance remaining to the goal between pairs of such instances under different models. We apply a Benjamini-Yekutieli procedure to control the false discovery rate from running multiple t-tests. Because the tests are not all independent, but some are, we estimate c(m) under an arbitrary dependence assumption as c(m) = m i+1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Statistical Analysis of Results"
        },
        {
            "text": "1 i , where m is the number of tests run. We choose a significance threshold of \u03b1 < 0.05. In addition to the unimodal ablation test results in Table 4 , we find that student supervision statistically significantly outperforms teacher supervision for full models. Table 5 presents trained agent results under the Success Rate and Success-weighted Path Length metrics defined in prior work [43] . As recommended, we define the success of an episode by whether the final robot position was within two times the length of the robot body. The most striking difference between success rate and topological distance (Table 4) is that under student-forcing, the agent never stops within the bounds of the target object. Our intuition is that while the agents trained with student-forcing get closer on average to the goal, they may do so by being conservative in their approach towards it, while those trained with teacher-forcing commit to a path and get as close as possible to where they believe the object to be, at the expense of often missing it.",
            "cite_spans": [
                {
                    "start": 388,
                    "end": 392,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                }
            ],
            "ref_spans": [
                {
                    "start": 143,
                    "end": 150,
                    "text": "Table 4",
                    "ref_id": null
                },
                {
                    "start": 263,
                    "end": 270,
                    "text": "Table 5",
                    "ref_id": "TABREF7"
                },
                {
                    "start": 609,
                    "end": 618,
                    "text": "(Table 4)",
                    "ref_id": null
                }
            ],
            "section": "Statistical Analysis of Results"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Language-guided semantic mapping and mobile manipulation in partially observable environments",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Patki",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fahnestock",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "M"
                    ],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Walter",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CoRL",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Multimodal estimation and communication of latent semantic knowledge for robust execution of robot instructions",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Arkin",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Walter",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "M"
                    ],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Paul",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "International Journal of Robotics Research",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Learning to map natural language instructions to physical quadcopter control using simulated flight",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Blukis",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Terme",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Niklasson",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Knepper",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Artzi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Driving under the influence (of language)",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Barrett",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Bronikowski",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Siskind",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Transactions on Neural Networks and Learning Systems",
            "volume": "29",
            "issn": "7",
            "pages": "2668--2683",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Asking for help using inverse semantics",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tellex",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Knepper",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rus",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "RSS",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Language to action: Towards interactive task learning with physical agents",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "Y"
                    ],
                    "last": "Chai",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "She",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Saba-Sadiya",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "In IJCAI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Human-driven feature selection for a robotic agent learning classification tasks from demonstration",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Bullard",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chernova",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Thomaz",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "In ICRA",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Jointly improving parsing and perception for natural language commands through human-robot dialog",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thomason",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Padmakumar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sinapov",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Walker",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yedidsion",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hart",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Stone",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Mooney",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "The Journal of Artificial Intelligence Research",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Learning to interpret natural language navigation instructions from observations",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "L"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Mooney",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "AAAI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Anderson",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Teney",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bruce",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "S\u00fcnderhauf",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Reid",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gould",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Van Den",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hengel",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "A research platform for multi-robot dialogue with humans",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Marge",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Nogar",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hayes",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lukin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bloecker",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Holder",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Voss",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NAACL",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Touchdown: Natural language navigation and spatial reasoning in visual street environments",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Suhr",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Misra",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Snavely",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Artzi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Tactical rewind: Self-correction via backtracking in vision-and-language navigation",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ke",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bisk",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Holtzman",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Gan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Srinivasa",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Speaker-follower models for vision-and-language navigation",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Fried",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Cirik",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rohrbach",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Andreas",
                    "suffix": ""
                },
                {
                    "first": "L.-P",
                    "middle": [],
                    "last": "Morency",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Berg-Kirkpatrick",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Saenko",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "NeurIPS",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Learning to navigate unseen environments: Back translation with environmental dropout",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bansal",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NAACL",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Simto-real transfer for vision-and-language navigation",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Anderson",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shrivastava",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Truong",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Majumdar",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Parikh",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Batra",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Conference on Robot Learning",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Experience grounds language",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bisk",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Holtzman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thomason",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Andreas",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chai",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lapata",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lazaridou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "May",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nisnevich",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Pinto",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Turian",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "EMNLP",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Duckietown: an open, inexpensive and flexible platform for autonomy education and research",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Paull",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tani",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ahn",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Alonso-Mora",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Carlone",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cap",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "F"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dusek",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICRA",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "MuSHR: A low-cost, open-source robotic racecar for education and research. arxiv",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Srinivasa",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lancaster",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Michalove",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schmittle",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Summers",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rockett",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Smith",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chouhury",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Mavrogiannis",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sadeghi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "The symbol grounding problem",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Harnad",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Physica D",
            "volume": "42",
            "issn": "",
            "pages": "335--346",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Walk the talk: Connecting language, knowledge, and action in route instructions",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Macmahon",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Stankiewicz",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kuipers",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "AAAI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Grounded video description",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kalantidis",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "J"
                    ],
                    "last": "Corso",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rohrbach",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shridhar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thomason",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bisk",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mottaghi",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Fox",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Vision-based navigation with language-based assistance via imitation learning with indirect intervention",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dey",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Brockett",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Dolan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Daum\u00e9",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "EMNLP",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Talk the walk: Navigating new york city through grounded dialogue. arXiv",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Vries",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shuster",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Batra",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Parikh",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kiela",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Vision-and-dialog navigation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thomason",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Murray",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cakmak",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CoRL",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Mapping instructions to actions in 3D environments with visual goal prediction",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Misra",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bennett",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Blukis",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Niklasson",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shatkhin",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Artzi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "In EMNLP",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Robots that use language. The Annual Review of Control",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tellex",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Gopalan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kress-Gazit",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Matuszek",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Robotics, and Autonomous Systems",
            "volume": "15",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Mapping navigation instructions to continuous control actions with position visitation prediction",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Blukis",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Misra",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Knepper",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Artzi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Flight, Camera, Action! Using Natural Language and Mixed Reality to Control a Drone",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Bayazit",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ullman",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Gopalan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tellex",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ICRA",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Formal dialogue model for language grounding error recovery",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Danas",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Finkelstein",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Nelson",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Krishnamurthi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tellex",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Combining Learning and Reasoning -Towards Human-Level Robot Intelligence. RSS",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "RMM: A recursive mental model for dialog navigation",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "R"
                    ],
                    "last": "Roman",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bisk",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thomason",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Celikyilmaz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Findings of EMNLP",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Learning to mediate perceptual differences in situated human-robot dialogue",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "Y"
                    ],
                    "last": "Chai",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "AAAI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Algorithm 97: shortest path",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "W"
                    ],
                    "last": "Floyd",
                    "suffix": ""
                }
            ],
            "year": 1962,
            "venue": "Communications of the ACM",
            "volume": "5",
            "issn": "6",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Algorithm for computer control of a digital plotter",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Bresenham",
                    "suffix": ""
                }
            ],
            "year": 1965,
            "venue": "IBM Systems journal",
            "volume": "4",
            "issn": "1",
            "pages": "25--30",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Mapping instructions and visual observations to actions with reinforcement learning",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Misra",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Langford",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Artzi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Glove: Global vectors for word representation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "EMNLP",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural computation",
            "volume": "9",
            "issn": "8",
            "pages": "1735--1780",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Shifting the Baseline: Single Modality Performance on Visual Navigation & QA",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thomason",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bisk",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NAACL",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Using natural language for reward shaping in reinforcement learning",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Niekum",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Mooney",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IJCAI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "A critical investigation of deep reinforcement learning for navigation",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Dhiman",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Griffin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Siskind",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "J"
                    ],
                    "last": "Corso",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "On evaluation of embodied navigation agents. arXiv",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Anderson",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "S"
                    ],
                    "last": "Chaplot",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dosovitskiy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Koltun",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kosecka",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mottaghi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Savva",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "R"
                    ],
                    "last": "Zamir",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Left: The real-world, top down view of a RobotSlang maze. Center: The replaysimulation view of the same maze, with discretized navigable points and vision based on ray tracing. The three objects are are given unique colors to differentiate them from the rest of the maze, and rays are shown here in white. Right: Sample visual input in the replay simulator, showing the colors, depths, and collision possibilities for 13 rays out of the front-facing camera.",
            "latex": null,
            "type": "figure"
        },
        "TABREF4": {
            "text": "Figure 5: An example RobotSlang trial. The COMMANDER and DRIVER first work together to localize the robot, then the COMMANDER guides the DRIVER towards the target objects in sequence: a jar of blocks, a mug, and a paddle. The DRIVER sees the front-facing camera feed from the car",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Figure 6: This example RobotSlang trial exhibits several phenomena analyzed in Section 3.2. The COMMANDER uses commands like go back to the location where you turn right first that need dialog history context to interpret. Additionally, communication repair and re-localization repair happen when the DRIVER suspects they have gone off-course. This arises twice, starting with wait | i now have green walls to my right and blue to my left and i have red walls ahead | is that right?",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "\u00b1 .10 .06 \u00b1 .25 .05 \u00b1 .20 Teacher .00 \u00b1 .00 .00 \u00b1 .00 .00 \u00b1 .00 .00 \u00b1 .00 Teacher .03 \u00b1 .16 .02 \u00b1 .14 .04 \u00b1 .19 .03 \u00b1 .16",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "S2S model Success Rate (SR) and Success Weighted by Path Length (SPL) results.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors are supported in part by ARO grant (W911NF-16-1-0121) and by the US National Science Foundation National Robotics Initiative under Grants 1522904. Additionally, we would like to thank Dr. Jeffrey M. Siskind and Dr. Vikas Dhiman for their substantive discussions that were fundamental to RobotSlang's evolution. Finally, we would like to thank Matthew Dorrow for his help with administrating annotators and data collection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        }
    ]
}
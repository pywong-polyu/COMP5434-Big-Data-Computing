{
    "paper_id": "8221c24818191f0c2c2d74aa1ce3fb75694ef5f5",
    "metadata": {
        "title": "AI based Presentation Creator With Customized Audio Content Delivery",
        "authors": [
            {
                "first": "Muvazima",
                "middle": [],
                "last": "Mansoor",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "ECE PES University",
                    "location": {
                        "settlement": "Bengaluru",
                        "country": "India"
                    }
                },
                "email": "muvazima99@gmail.com"
            }
        ]
    },
    "abstract": [
        {
            "text": "In this paper we propose an architecture to a solve a novel problem statement that has stemmed more so in the recent times with an increase in demand for virtual content delivery due to the COVID-19 pandemic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "A system/product aiming to work on the exact same application is non-existent at the moment. There have been subsets to the application we propose that has garnered significant attention over the last few years. The ideas behind techniques and the algorithms used in summarization models is not entirely new. However, the use of such algorithms in the context of summarizing a research paper and subsequently generating a slides based presentation is unheard of. The concept of voice cloning is one that is gaining traction, but none that we know use the idea in the context of automating the process of virtual content delivery. Multiple speech synthesis startups like Lyrebird and Sonantic have obtained sizable grants and investments. Lyrebird aims to offer their voice cloning API so that third parties can make use of the audio mimicry technology for their own needs. Whereas, sonantic aims to use their voice cloning feature in video games. The objective of our project is to implement the voice cloning feature to read out a presentation created by summarizing a research paper using a custom voice.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The project involves 4 sub-problems:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. STRUCTURE"
        },
        {
            "text": "\u2022 Identification of sup-topics from the paper and converting these topics to hierarchical bullet points which can go on each slide of a presentation. \u2022 Content Generation from these points in each slide. \u2022 Voice recognition-mimic the style and tone of a chosen voice.. \u2022 Present the above content by using a customized textto -content delivery mechanism.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. STRUCTURE"
        },
        {
            "text": "A. Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis Ye Jia, et al. describe a text-to-speech synthesis using multiple neural networks that can generate speech audio in multiple voices, including those that are not seen during training. [1] The system consists of three independently trained components:",
            "cite_spans": [
                {
                    "start": 272,
                    "end": 275,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "III. PAST WORK"
        },
        {
            "text": "Encoder: It is used to generate a fixed-dimensional embedding vector from a few seconds of reference speech. It is trained on a speaker verification task using the LibreSpeech dataset that consists of an untranscribed noisy speech from hundreds of speakers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. PAST WORK"
        },
        {
            "text": "Synthesizer: based on the speaker embedding derived from the encoder, the synthesizer generates a Mel spectrogram from the text. The synthesizer model is based on Tacotron 2. [2] Vocoder: it converts the Mel spectrogram generated by the synthesizer into waveform samples in the time domain. The SV2TTS model uses a WaveNet-based vocoder that is autoregressive. They demonstrate that the model is capable of transferring the speaker variability knowledge that is learned by the encoder to the multispeaker text to speech task and it is able to generate natural speech from speakers that are not seen during training.",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 178,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "III. PAST WORK"
        },
        {
            "text": "The authors mention that a large and diverse speaker dataset that is used for training the encoder is very important in order to obtain the best performance. Finally, they show that speaker embeddings that were randomly sampled can be used to generate the voice of new speakers not seen during training which indicates that the trained model has learned speaker representation of good quality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. PAST WORK"
        },
        {
            "text": "The SV2TTS model is capable of generating realistic speech from speakers unseen in the training set, implying that the model has learned to utilize a realistic depiction of the space of speaker variation. [6] The SV2TTS model does not attain human-level naturalness, despite the use of a WaveNet vocoder. This is due to the added difficulty of generating speech for different types of speakers given very fewer data per speaker, and the use of low-quality datasets. [7] B. Tacotron: Towards End-to-End Speech Synthesis A text-to-speech model usually contains stages like an audio synthesis module, an acoustic model, and a text analysis model. Constructing these models usually requires extensive domain knowledge and may contain fragile design choices. Yuxuan Wang, et al. present Tacotron, an end-to-end genera-tive TTS model that generates speech directly from alphabets. The model can be trained from scratch using text and audio pairs with random initialization.",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 208,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 466,
                    "end": 469,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 761,
                    "end": 781,
                    "text": "Wang, et al. present",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "III. PAST WORK"
        },
        {
            "text": "Advantages of the Tacotron model:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. PAST WORK"
        },
        {
            "text": "The tacotron model reduces the need for arduous feature engineering, which may involve fragile design choices and heuristics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. PAST WORK"
        },
        {
            "text": "The model allows for rich conditioning on various features like speaker, language, or sentiment. This is due to the fact that conditioning can take place at the initial stages of the model rather than only on specific components. Likewise, adjustment to the new data could also be easier.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. PAST WORK"
        },
        {
            "text": "A single model is more likely to be robust compared to a multi-stage model where every stage's errors can multiply.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. PAST WORK"
        },
        {
            "text": "The advantages listed above suggest that an end-to-end model could facilitate training on large amounts of noisy data that is found everywhere. Fig.2 shows the model architecture. Text to speech is a large-scale inverse issue: a highly compressed text is decompressed into audio. The same text can equate to various intonations or speaking styles, which is an arduous learning task for the model. For a given input, the model must endure large disparities at the signal level. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 144,
                    "end": 149,
                    "text": "Fig.2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "III. PAST WORK"
        },
        {
            "text": "In recent years, Generative adversarial networks have seen rapid development and have led to extraordinary developments in the generative modeling of images. Their implementation in the audio domain has received little attention. Autoregressive models like WaveNet are the most widely used in the generative modeling of audio signals. To discuss this paucity, Jeff Donahue et al. introduce Text-to-Speech using Generative Adversarial Network or GAN-TTS. [3] Generative Adversarial Networks form a subgroup of generative models that involves the adversarial training of two networks:",
            "cite_spans": [
                {
                    "start": 454,
                    "end": 457,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "C. High Fidelity Speech Synthesis With Adversarial Networks"
        },
        {
            "text": "\u2022 Generator: it tries to generate samples that resemble the reference distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. High Fidelity Speech Synthesis With Adversarial Networks"
        },
        {
            "text": "\u2022 Discriminator: it imparts a useful gradient signal to the generator by differentiating between generated samples and the real samples. Residual blocks are used in the model. The Convolutional layers have equal output and input channels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. High Fidelity Speech Synthesis With Adversarial Networks"
        },
        {
            "text": "GANs are capable of producing high-precision speech that sounds as natural as the ones produced by the state-of-the-art models. GANs are extremely parallelizable due to an efficient feed-forward generator. [5] Whereas, autoregressive models like WaveNet are not parallelizable. Fig 3. shows the Model Architecture. [8] ",
            "cite_spans": [
                {
                    "start": 206,
                    "end": 209,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 315,
                    "end": 318,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 278,
                    "end": 284,
                    "text": "Fig 3.",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "C. High Fidelity Speech Synthesis With Adversarial Networks"
        },
        {
            "text": "This paper describes a python-based RESTful service that uses Bidirectional Encoder Representations from Transformers model for text embeddings. For summary selection, The project also utilizes K Means clustering to determine the sentences that are nearest to the centroid. Apart from summarization, the project provides features for the management of lectures and summaries and supports collaboration by storing content on the cloud. [4] There are two different types of automatic text summarization:",
            "cite_spans": [
                {
                    "start": 435,
                    "end": 438,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "D. Leveraging BERT for Extractive Text Summarization on Lectures"
        },
        {
            "text": "\u2022 Abstractive: Abstractive summarization resembles human summarization closely by using a vocabulary beyond the text provided. It abstracts the important points present in the text and it is usually smaller in size. Though this approach is very useful, it is arduous to produce it automatically. It requires multiple GPUs and takes many days to train.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Leveraging BERT for Extractive Text Summarization on Lectures"
        },
        {
            "text": "\u2022 Extractive: It uses only the content from the given text like the raw phrases and sentences to provide a summarization of the text.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Leveraging BERT for Extractive Text Summarization on Lectures"
        },
        {
            "text": "Derek Miller uses BERT (Bidirectional Encoder Representations from Transformers) for the process of summarization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Leveraging BERT for Extractive Text Summarization on Lectures"
        },
        {
            "text": "The unsupervised model, BERT is built on top of the Transformer architecture. It performs better than all the existing NLP models for a broad range of functions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Leveraging BERT for Extractive Text Summarization on Lectures"
        },
        {
            "text": "In other summarization models, it was not possible to obtain dynamic summary sizes. The BERT model produces sentence embeddings. These sentence embeddings can be clustered with a size of K which permits dynamic summary sizes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Leveraging BERT for Extractive Text Summarization on Lectures"
        },
        {
            "text": "BERT combines context with the most important sentences and therefore performs much better than methods like Tex-tRank in terms of quality. [15] IV. METHODOLOGY AND SYSTEM DESIGN-TEXT TO SPEECH WITH VOICE CLONING Fig. 4 . Architecture.",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 144,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 213,
                    "end": 219,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "D. Leveraging BERT for Extractive Text Summarization on Lectures"
        },
        {
            "text": "The speaker encoder block is used to derive the embedding of the user's voice from a short audio clip. Our project uses the speaker Encoder model from SV2TTS [1] . In SV2TTS, the speaker encoder network is trained on a speaker verification task using the LibreSpeech dataset that consists of an untranscribed noisy speech from hundreds of speakers. This allows the model to produce an embedding vector of fixed dimensions from only a few seconds of reference speech. Other multi-speaker speech synthesis is done by incorporating hundreds of hours of the target speaker's voice in the training dataset. This method requires many hours of transcribed data and does not allow the fitting of new voices without retraining the computationally heavy model.",
            "cite_spans": [
                {
                    "start": 158,
                    "end": 161,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "A. Speaker Encoder"
        },
        {
            "text": "Based on the speaker embedding derived from the encoder, the synthesizer generates a Mel spectrogram from text. Our project uses the synthesizer model from Tacotron [2] . Tacotron is an end-to-end generative TTS model that generates speech directly from alphabets. The model can be trained from scratch using text and audio pairs with random initialization. This approach does not use complex linguistic and acoustic features as input compared to other models like Deep Voice [3] and VoiceLoop [4] .",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 168,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 476,
                    "end": 479,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 494,
                    "end": 497,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "B. Synthesizer"
        },
        {
            "text": "A vocoder converts the Mel spectrogram generated by the synthesizer into waveform samples in the time domain. Our project uses GAN(Generative Adversarial Networks) [5] as the vocoder.",
            "cite_spans": [
                {
                    "start": 164,
                    "end": 167,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "C. Neural Vocoder"
        },
        {
            "text": "GANs are capable of producing high-precision speech that sounds as natural as the ones produced by the state-of-the-art models. GANs are extremely parallelizable due to an efficient feed-forward generator. Whereas, autoregressive models like WaveNet are not parallelizable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Neural Vocoder"
        },
        {
            "text": "Our project uses BERT (Bidirectional Encoder Representations from Transformers) [6] summarization for the summary creation process. The unsupervised model, BERT is built on top of the Transformer architecture. It performs better than all the existing NLP models for a broad range of functions, including summarization. In other summarization models, it is not possible to obtain dynamic summary sizes. The BERT model produces sentence embeddings. These sentence embeddings can be clustered with a size of K which permits dynamic summary sizes. BERT combines context with the most important sentences and therefore performs much better than methods like TextRank in terms of quality.",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 83,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "V. SUMMARIZATION OF A RESEARCH PAPER AND CREATING A PRESENTATION"
        },
        {
            "text": "Our approach is to apply BERT summarization to every section of the research paper which would go on every new page of the presentation. Each page of the presentation will also contain a hyperlink that would direct the user to the section of the research paper that has been summarized.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. SUMMARIZATION OF A RESEARCH PAPER AND CREATING A PRESENTATION"
        },
        {
            "text": "To validate our results, we compared the rouge-l scores of the summary generated by the BERT summarizer by using author generated highlights as the reference summary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. SUMMARIZATION OF A RESEARCH PAPER AND CREATING A PRESENTATION"
        },
        {
            "text": "This section provides an overview about how the various machine learning models (Summarizer and the Voice Cloning -all 3 segments-have been tied together to make a web based application). Given that this is not very research centric, this section will remain brief. The purpose of including this section is to give an idea to the reader on the implementation aspect of this research, and also how the timing constraints that one may have assumed to be a problem in the case of complex neural networks such as cloning (GAN based), is actually handled. [11] and [12] The architecture used is that of a python implementation which combines Streamlit and Ngrok, using Google Colab as a codebase to utilize its computational power to run the ML models. Streamlit is a platform/library that enables data scripts such as the python modules used in this paper to be converted to web aplications with ease. Ngrok enables such a web application by providing a secure URL to the localhost server through any NAT or firewall. This also solves the issue in terms of security as it provides a secure authenticator key to connect to the local host (web application). The architecture as shown in Fig. 5 is connected to Google Drive as the database. The pretrained ML models-synthesizer,Encoder, and Vocoder, parsed papers, generated PPTs and audio clips are stored in Google Drive. These models are called by Google Colab as per the implementation, a web application is hosted on the local host using streamlit and ngrok is used to securely connect Colab to the local host. ",
            "cite_spans": [
                {
                    "start": 551,
                    "end": 555,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 560,
                    "end": 564,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [
                {
                    "start": 1181,
                    "end": 1187,
                    "text": "Fig. 5",
                    "ref_id": null
                }
            ],
            "section": "VI. IMPLEMENTATION"
        },
        {
            "text": "The architecture proposed in this paper is one that combines different elements from methods to come up with a voice cloning approach. The encoder is from the SV2TTS model, the sythesizer has been inspired from Tacotron, and the Vocoder is the GAN based model. The architecture when combined produced voice cloning results that match the reference audio to a good degree. [13] and [14] To determine the accuracy of the summary generated by the BERT summarizer, we computed rouge scores of the summary generated by using the highlights provided by the author as reference summary. We also computed rouge scores of different summarization models and compared their performance with BERT summarizer. The different summarization models used are:",
            "cite_spans": [
                {
                    "start": 372,
                    "end": 376,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 381,
                    "end": 385,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "VII. RESULTS"
        },
        {
            "text": "\u2022 Abstractive summarization using TensorFlow and Keras based Neural Network. It performed poorly and gave an f-score of 0.115. To compute the accuracy of the PPT generated by our application, we compared it with a PPT created by humans and treated it as a gold standard. It gave a rouge-1 score of 0.49 and rouge-l score of 0.35 The voice cloning results can be found here. As is evident from the voices, the cloning module has successfully copied the fundamental properties of the voices like pitch, depth, timbre, frequency, etc. It however does not account for style specific features like accent, voice style, specific pronunciations etc. The 3 voices considered for the sake of results are the author's, and Amitabh Bachchan (a famour celebrity in Bollywood). The cloned voices can be contrasted with the original voices which may be available to notice the similarity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VII. RESULTS"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
            "authors": [
                {
                    "first": "Ye",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Tacotron: Towards end-to-end speech synthesis",
            "authors": [
                {
                    "first": "Yuxuan",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1703.10135"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "High fidelity speech synthesis with adversarial networks",
            "authors": [
                {
                    "first": "Miko\u0142aj",
                    "middle": [],
                    "last": "Bi\u0144kowski",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1909.11646"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Leveraging BERT for extractive text summarization on lectures",
            "authors": [
                {
                    "first": "Derek",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.04165"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Multi-band MelGAN: Faster Waveform Generation for High-Quality Text-to-Speech",
            "authors": [
                {
                    "first": "Geng",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.05106"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
            "authors": [
                {
                    "first": "Jaehyeon",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.11129"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Semi-supervised Learning for Multi-speaker Text-tospeech Synthesis Using Discrete Speech Representation",
            "authors": [
                {
                    "first": "Tao",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.08024"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Melgan: Generative adversarial networks for conditional waveform synthesis",
            "authors": [
                {
                    "first": "Kundan",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Training Multi-Speaker Neural Text-to-Speech Systems Using Speaker-Imbalanced Speech Corpora",
            "authors": [
                {
                    "first": "Hieu-Thi",
                    "middle": [],
                    "last": "Luong",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1904.00771"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Modeling multi-speaker latent space to improve neural TTS: Quick enrolling new speaker and enhancing premium voice",
            "authors": [
                {
                    "first": "Yan",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "Lei",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Frank",
                    "middle": [],
                    "last": "Soong",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1812.05253"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Sample efficient adaptive text-to-speech",
            "authors": [
                {
                    "first": "Yutian",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1809.10460"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Fitting new speakers based on a short untranscribed sample",
            "authors": [
                {
                    "first": "Eliya",
                    "middle": [],
                    "last": "Nachmani",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1802.06984"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Voiceloop: Voice fitting and synthesis via a phonological loop",
            "authors": [
                {
                    "first": "Yaniv",
                    "middle": [],
                    "last": "Taigman",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1707.06588"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Deep voice: Real-time neural text-to-speech",
            "authors": [
                {
                    "first": "Sercan",
                    "middle": [
                        "O"
                    ],
                    "last": "Arik",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1702.07825"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A supervised approach to extractive summarisation of scientific papers",
            "authors": [
                {
                    "first": "Ed",
                    "middle": [],
                    "last": "Collins",
                    "suffix": ""
                },
                {
                    "first": "Isabelle",
                    "middle": [],
                    "last": "Augenstein",
                    "suffix": ""
                },
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Riedel",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1706.03946"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Structure Flowchart",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Tacotron Model Architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "GAN vocoder Architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 5. Web Application Architecture.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Rouge scores of Abstractive summarization\u2022 Extractive summarization using TextRank algorithm which is an extractive and unsupervised text summarization technique. It gave an f-score of 0.1538.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Rouge scores of Extractive summarization using TextRank\u2022 Extractive summarization using SVM -we scored each sentence based on the words that overlapped between the main body of the paper and the author generated summaries. The top \"n\" sentences with the highest scores were picked as summary sentences. To do this, we used a Support Vector Machine Regressor to predict sentence scores based on document vectors generated by Gensim's doc2vec function. It gave an f-score of 0.16.TABLE III: Rouge scores of Extractive summarization using SVM \u2022 Extractive summarization using BERT which is an unsupervised model built on top of transformer architecture. It performed much better than the other summarization models and gave an f-score of 0.45.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Rouge scores of Extractive summarization using BERT",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Rouge scores of PPT generated by BERT with human generated PPT",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "ACKNOWLEDGMENT This work was done as part of the Undergraduate Final Year Capstone Project in PES University, Computer Science Department. We acknowledge all the support from PES University, the Computer Science Department, and the Electronics and Communication Department towards this work. For this, we would like to thank Dr. Shylaja S S of PES University and the rest of the mentors from the Computer Science department for providing us this opportunity and for their continuous support. We extend our thanks to PES University, who provided us a platform that helped us to team up and pursue this project. We also thank Dr. Anuradha M, for her support.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}
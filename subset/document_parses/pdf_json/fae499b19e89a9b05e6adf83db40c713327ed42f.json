{
    "paper_id": "fae499b19e89a9b05e6adf83db40c713327ed42f",
    "metadata": {
        "title": "Navigating the Kaleidoscope of COVID-19 Misinformation Using Deep Learning",
        "authors": [
            {
                "first": "Yuanzhi",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Nebraska-Lincoln",
                    "location": {
                        "postCode": "68588",
                        "region": "NE",
                        "country": "USA"
                    }
                },
                "email": "yuanzhi@huskers.unl.edu"
            },
            {
                "first": "Mohammad",
                "middle": [
                    "Rashedul"
                ],
                "last": "Hasan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Nebraska-Lincoln",
                    "location": {
                        "postCode": "68588",
                        "region": "NE",
                        "country": "USA"
                    }
                },
                "email": "hasan@unl.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Irrespective of the success of the deep learningbased mixed-domain transfer learning approach for solving various Natural Language Processing tasks, it does not lend a generalizable solution for detecting misinformation from COVID-19 social media data. Due to the inherent complexity of this type of data, caused by its dynamic (context evolves rapidly), nuanced (misinformation types are often ambiguous), and diverse (skewed, finegrained, and overlapping categories) nature, it is imperative for an effective model to capture both the local and global context of the target domain. By conducting a systematic investigation, we show that: (i) the deep Transformerbased pre-trained models, utilized via the mixed-domain transfer learning, are only good at capturing the local context, thus exhibits poor generalization, and (ii) a combination of shallow network-based domain-specific models and convolutional neural networks can efficiently extract local as well as global context directly from the target data in a hierarchical fashion, enabling it to offer a more generalizable solution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Since the start of the Coronavirus or COVID-19 pandemic, online social media (e.g., Twitter) has become a conduit for rapid propagation of misinformation (Johnson et al., 2020) . Although misinformation is considered to be created without the intention of causing harm (Lazer et al., 2018) , it can wreak havoc on society (Ciampaglia, 2018; Neuman, 2020; Hamilton, 2020) and disrupt democratic institutions (Ciampaglia et al., 2018) . Misinformation in general, and COVID-19 misinformation in particular, has become a grave concern for the policymakers due to its fast propagation via online social media. A recent study shows that the majority of the COVID-19 social media data is rife with misinformation (Brennen et al., 2020) . The first step towards preventing misinformation is to detect misinformation in a timely fashion.",
            "cite_spans": [
                {
                    "start": 154,
                    "end": 176,
                    "text": "(Johnson et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 269,
                    "end": 289,
                    "text": "(Lazer et al., 2018)",
                    "ref_id": null
                },
                {
                    "start": 322,
                    "end": 340,
                    "text": "(Ciampaglia, 2018;",
                    "ref_id": null
                },
                {
                    "start": 341,
                    "end": 354,
                    "text": "Neuman, 2020;",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 355,
                    "end": 370,
                    "text": "Hamilton, 2020)",
                    "ref_id": null
                },
                {
                    "start": 407,
                    "end": 432,
                    "text": "(Ciampaglia et al., 2018)",
                    "ref_id": null
                },
                {
                    "start": 707,
                    "end": 729,
                    "text": "(Brennen et al., 2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Building automated systems for misinformation detection from social media data is a Natural Language Processing (NLP) task. Various deep learning models have been successfully employed for this type of NLP task of text classification (Kim, 2014; Conneau et al., 2017; Wang et al., 2017; Tai et al., 2015; Zhou et al., 2016) . These models learn language representations from a domain, which are then used as numeric features in supervised classification. Due to the prohibitive cost of acquiring labeled data on COVID-19 misinformation, training deep learning models directly using the target data is not a suitable approach.",
            "cite_spans": [
                {
                    "start": 234,
                    "end": 245,
                    "text": "(Kim, 2014;",
                    "ref_id": null
                },
                {
                    "start": 246,
                    "end": 267,
                    "text": "Conneau et al., 2017;",
                    "ref_id": null
                },
                {
                    "start": 268,
                    "end": 286,
                    "text": "Wang et al., 2017;",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 287,
                    "end": 304,
                    "text": "Tai et al., 2015;",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 305,
                    "end": 323,
                    "text": "Zhou et al., 2016)",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Background. An alternative approach for detecting COVID-19 misinformation from small labeled data is transfer learning (Hossain et al., 2020) . The dominant paradigm of transfer learning employs a mixed-domain strategy in which representations learned from a general domain (source data) by using domain-agnostic models are transferred into a specific domain (target data) (Pan and Yang, 2009 ). Specifically, it involves creating a pre-trained model (PTM) that learns embedded representations from general-purpose unlabeled data, then adapting the model for a downstream task using the labeled target data (Minaee et al., 2021; Qiu et al., 2020) .",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 141,
                    "text": "(Hossain et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 382,
                    "end": 392,
                    "text": "Yang, 2009",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 607,
                    "end": 628,
                    "text": "(Minaee et al., 2021;",
                    "ref_id": null
                },
                {
                    "start": 629,
                    "end": 646,
                    "text": "Qiu et al., 2020)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Two types of neural networks can be used to create PTMs, i.e., shallow and deep. The shallow models such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) learn word embeddings that capture semantic, syntactic, and some global relationships (Levy and Goldberg, 2014; Srinivasan and Ribeiro, 2019) of the words from the source text using their co-occurrence information. However, these PTMs do not capture the context of the text (Qiu et al., 2020) . On the other hand, deep PTMs can learn contextual embed-dings, i.e., language models (Goldberg and Hirst, 2017) .",
            "cite_spans": [
                {
                    "start": 117,
                    "end": 139,
                    "text": "(Mikolov et al., 2013)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 150,
                    "end": 175,
                    "text": "(Pennington et al., 2014)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 262,
                    "end": 287,
                    "text": "(Levy and Goldberg, 2014;",
                    "ref_id": null
                },
                {
                    "start": 288,
                    "end": 317,
                    "text": "Srinivasan and Ribeiro, 2019)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 450,
                    "end": 468,
                    "text": "(Qiu et al., 2020)",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 556,
                    "end": 582,
                    "text": "(Goldberg and Hirst, 2017)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Two main approaches for creating deep PTMs are based on sequential and non-sequential models. The sequential Recurrent Neural Network (Liu et al., 2016) based model such as ELMo (Embeddings from Language Models) (Peters et al., 2018) is equipped with long short-term memory to capture the local context of a word in sequential order. The non-sequential Transformer (Vaswani et al., 2017) based models such as OpenAI GPT (Generative Pre-training) (Radford and Sutskever, 2018) , BERT (Bidirectional Encoder Representation from Transformer) (Devlin et al., 2019) , and XLNet utilize the attention mechanism (Bahdanau et al., 2016) for learning universal language representation from general-purpose very large text corpora such as Wikipedia and Book-Corpus (Devlin et al., 2019) as well as from web crawls (Liu et al., 2019) . While GPT is an autoregressive model that learns embeddings by predicting words based on previous predictions, BERT utilizes the autoencoding technique based on bidirectional context modeling (Minaee et al., 2021) . XLNet leverages the strengths of autoregressive and autoencoding PLMs .",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 152,
                    "text": "(Liu et al., 2016)",
                    "ref_id": null
                },
                {
                    "start": 212,
                    "end": 233,
                    "text": "(Peters et al., 2018)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 365,
                    "end": 387,
                    "text": "(Vaswani et al., 2017)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 446,
                    "end": 475,
                    "text": "(Radford and Sutskever, 2018)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 539,
                    "end": 560,
                    "text": "(Devlin et al., 2019)",
                    "ref_id": null
                },
                {
                    "start": 605,
                    "end": 628,
                    "text": "(Bahdanau et al., 2016)",
                    "ref_id": null
                },
                {
                    "start": 804,
                    "end": 822,
                    "text": "(Liu et al., 2019)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1017,
                    "end": 1038,
                    "text": "(Minaee et al., 2021)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Unlike the Transformer-based deep PTMs, the shallow Word2Vec and GloVe as well as the deep ELMo PTMs are used only as feature extractors. These features are fed into another model for the downstream task of classification, which needs to be trained from scratch using the target data. The deep PTM based mixed-domain transfer learning has achieved state-of-the-art (SOTA) performance in many NLP tasks including text classification (Minaee et al., 2021 ).",
            "cite_spans": [
                {
                    "start": 432,
                    "end": 452,
                    "text": "(Minaee et al., 2021",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Irrespective of the success of the mixed-domain SOTA transfer learning approach for text classification, there has been no study to understand how effective this approach is for navigating through the kaleidoscope of COVID-19 misinformation. Unlike the curated static datasets on which this approach is tested (Minaee et al., 2021) , the dynamic landscape of the COVID-19 social media data has not been fully explored. Some key properties of the COVID-19 data hitherto identified are: (i) The COVID-19 misinformation spreads faster on social media than any other form of health misinformation (Johnson et al., 2020) . As a consequence, the misinformation narrative evolves rapidly (Cui and Lee, 2020). (ii) The COVID-19 misinforma-tion categories are heavily-skewed (Cui and Lee, 2020; Memon and Carley, 2020) and fine-grained (Memon and Carley, 2020) . (iii) The COVID-19 social media misinformation types are often ambiguous (e.g., fabricated, reconfigured, satire, parody) (Brennen et al., 2020) and categories may not be mutually exclusive (Memon and Carley, 2020) . These properties pose a unique challenge for the mixed-domain SOTA transfer learning approach for creating an effective solution to the COVID-19 misinformation detection problem.",
            "cite_spans": [
                {
                    "start": 310,
                    "end": 331,
                    "text": "(Minaee et al., 2021)",
                    "ref_id": null
                },
                {
                    "start": 593,
                    "end": 615,
                    "text": "(Johnson et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 827,
                    "end": 851,
                    "text": "(Memon and Carley, 2020)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 976,
                    "end": 998,
                    "text": "(Brennen et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 1044,
                    "end": 1068,
                    "text": "(Memon and Carley, 2020)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Previously, it has been shown that the transfer learning approach generalizes poorly when the domain of the source dataset is significantly different from that of the target dataset (Peters et al., 2019) . On the other hand, domain-specific models (DSM), which learn representations from domains that are similar to the target domain, provide a generalizable solution for the downstream NLP task (Beltagy et al., 2019; Lee et al., 2019; Gu et al., 2021) . These models are better at capturing the context of the target domain. However, the efficacy of the DSM-based approach for addressing the COVID-19 misinformation detection problem has not also been investigated.",
            "cite_spans": [
                {
                    "start": 182,
                    "end": 203,
                    "text": "(Peters et al., 2019)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 396,
                    "end": 418,
                    "text": "(Beltagy et al., 2019;",
                    "ref_id": null
                },
                {
                    "start": 419,
                    "end": 436,
                    "text": "Lee et al., 2019;",
                    "ref_id": null
                },
                {
                    "start": 437,
                    "end": 453,
                    "text": "Gu et al., 2021)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we conduct a systematic extensive study to understand the scope and limitations of the mixed-domain transfer learning approach as well as the DSM-based approach to detect COVID-19 misinformation on social media. We use both shallow and deep PTMs for the mixed-domain transfer learning experimentations. The deep PTMs include BERT, XLNet, and two variants of BERT, i.e., RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2020) . While these attention mechanismbased Transformer models are good at learning contextual representations, their ability to learn global relationships among the words in the source text is limited (Lu et al., 2020) .",
            "cite_spans": [
                {
                    "start": 393,
                    "end": 411,
                    "text": "(Liu et al., 2019)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 416,
                    "end": 441,
                    "text": "ALBERT (Lan et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 639,
                    "end": 656,
                    "text": "(Lu et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The DSMs used in our study are based on shallow architectures. We argue that shallow architectures can be trained efficiently using the limited available domain data. Specifically, we pre-train the DSMs using the small social media data on COVID-19. The shallow DSM-based approach is examined in two dimensions: graph-based DSM and non-graph DSM. The graph-based Text GCN (Yao et al., 2019) model can explicitly capture the global relationships (from term co-occurrence) by leveraging the graph structure of the text. It creates a heterogeneous word document graph with words and documents as nodes for the whole corpus, and turns document classification problem into a node classification problem. We include another graphbased model in our study, i.e., the VGCN-BERT (Lu et al., 2020) . It combines the strength of Text GCN (to capture global relationships) and BERT (to capture local relationships).",
            "cite_spans": [
                {
                    "start": 372,
                    "end": 390,
                    "text": "(Yao et al., 2019)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 769,
                    "end": 786,
                    "text": "(Lu et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The non-graph DSM models such as Word2Vec and GloVe can mainly capture local relationships among the words of the source text, represented in the latent space of their word embeddings. For extracting global relationships from these embeddings, we utilize a Convolutional Neural Network (CNN). Specifically, we use the word embeddings as input features to a CNN with a one-dimensional kernel (Kim, 2014) , which then learns global relationships as high-level features. We hypothesize that the local and global relationships should improve the generalization capability of the non-graph DSM+CNN approach.",
            "cite_spans": [
                {
                    "start": 391,
                    "end": 402,
                    "text": "(Kim, 2014)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We evaluate the generalizability of the abovementioned diverse array of NLP techniques via a set of studies that explore various dimensions of the COVID-19 data. We focus on the Twitter social media platform because of its highest number of news-focused users (Hughes and Wojcik, 2019). In addition to analyzing the tweet messages, we use online news articles referred to in the tweets. Our study spans along multiple dimensions of the COVID-19 data that include temporal dimension (the context in the dataset evolves), length dimension (short text such as tweets vs. lengthy text such as news articles), size dimension (small dataset vs. large dataset), and classification-level dimension (binary vs. multi-class data).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Contributions. We design a novel study for examining the generalizability of a diverse set of deep learning NLP techniques on the multi-dimensional space of COVID-19 online misinformation landscape. Our main contributions are as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We identify the unique challenges for the deep learning based NLP techniques to detect misinformation from COVID-19 social media data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We argue that an effective model for this type of data must capture both the local and the global context of the domain in its latent space of embeddings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We show that the mixed-domain deep learning SOTA transfer learning approach is not always effective.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We find that the shallow CNN classifier initialized with word embeddings learned via the non-graph DSMs is more effective across most of the dimensions of the COVID-19 data space, especially when the labeled target data is small.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We explain why the Transformer-based mixed-domain transfer learning approach is not effective on COVID-19 data as well as why the non-graph DSM+CNN may offer a more generalizable solution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The rest of the paper is organized as follows. In section 2, we present the diverse NLP techniques, analyze the multi-dimensional datasets, and describe the study design. Results obtained from the experiments are provided in section 3 followed by a detailed analysis. Section 4 presents the conclusion. Appendix provides related work, additional analysis of the datasets, and experiment setting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "First, we describe how we obtained various PTMs and created DSM embeddings for different models as well as how we fine-tuned/trained the classifiers for the studies. Then, we discuss the datasets and the study design.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "We use the following PTMs: BERT, RoBERTa, ALBERT, XLNet, ELMo, Word2Vec, and GloVe.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mixed-Domain Transfer Learning"
        },
        {
            "text": "Deep PTMs: We get the BERT base model (uncased) for sequence classification from the Hugging Face library (Wolf et al., 2020) . The embedding vectors are 768-dimensional. This BERT PTM adds a single linear layer on top of the BERT base model. The pretrained weights of all hidden layers of the PTM and the randomly initialized weights of the top classification layer are adapted during fine-tuning using a target dataset. The XL-Net is obtained from the Hugging Face library (Wolf et al., 2020) and fine-tuned similar to BERT. Its embedding vectors are 768-dimensional. The RoBERTa (obtained from (Wolf et al., 2020) ) and ALBERT (obtained from (Maiya, 2020) ) are used by first extracting embeddings from their final layer and then adding linear layers. While the RoBERTa embeddings are 768-dimensional, the ALBERT embeddings are 128-dimensional.",
            "cite_spans": [
                {
                    "start": 106,
                    "end": 125,
                    "text": "(Wolf et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 475,
                    "end": 494,
                    "text": "(Wolf et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 597,
                    "end": 616,
                    "text": "(Wolf et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 645,
                    "end": 658,
                    "text": "(Maiya, 2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Mixed-Domain Transfer Learning"
        },
        {
            "text": "Shallow PTMs: We get the ELMo embeddings from TensorFlow-Hub (Abadi et al., 2015) . Each embedding vector has a length of 1024. The Word2Vec embeddings are obtained from Google Code (Google Code, 2013). The embedding vectors are 300-dimensional. We get the GloVe pretrained 300-dimensional embeddings from (Pennington et al., 2014) .",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 81,
                    "text": "(Abadi et al., 2015)",
                    "ref_id": null
                },
                {
                    "start": 306,
                    "end": 331,
                    "text": "(Pennington et al., 2014)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Mixed-Domain Transfer Learning"
        },
        {
            "text": "CNN: The ELMo, Word2Vec, and GloVe embeddings are used to train a CNN classifier with a single hidden layer (Kim, 2014) . The first layer is the embedding layer. Its dimension varies based on the dimension of pretrained embeddings. The second layer is the one-dimensional convolution layer that consists of 100 filters of dimension 5 x 5 with \"same\" padding and ReLU activation. The third layer is a one-dimensional global maxpooling layer, and the fourth layer is a dense layer with 100 units along with ReLU activation. The last layer is the classification layer with softmax activation. We use this setting for the CNN architecture as it was found empirically optimal in our experiments. We use cross-entropy as loss function, Adam as the optimizer, and a batch size of 128. The embedding vectors are kept fixed during the training (Kim, 2014).",
            "cite_spans": [
                {
                    "start": 108,
                    "end": 119,
                    "text": "(Kim, 2014)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Mixed-Domain Transfer Learning"
        },
        {
            "text": "We create the DSMs using two approaches: graphbased and non-graph. For the graph-based approach, we use the following models: Text GCN and VGCN-BERT. For training the Text GCN model, we pre-process the data as follows. First, we clean the text by removing stop words and rare words whose frequencies are less than 5. Then, we build training, validation, and test graphs using the cleaned text. Finally, we train the GCN model using training and validation graphs and test the model using a test graph. During the training, early stopping is used. For training the VGCN-BERT model, first, we clean the data that includes removing spaces, the special symbols as well as URLs. Then, the BERT tokenizer is used to create BERT vocabulary from the cleaned text. The next step is to create training, validation, and the test graphs. The last step is training the VGCN-BERT model. During the training, the model constructs embeddings from word and vocabulary GCN graph.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Domain-Specific Model (DSM) based Learning"
        },
        {
            "text": "For the non-graph approach, we create em-beddings from the target dataset by using the Word2Vec and GloVe models. First, we preprocess the raw text data by converting the text (i.e., a list of sentences) into a list of lists containing tokenized words. During tokenization, we convert words to lowercase, remove words that are only one character, and lemmatize the words. We add bigrams that appear 10 times or more to our tokenized text. The bigrams allow us to create phrases that could be helpful for the model to learn and produce more meaningful representations. Then, we feed our final version of the tokenized text to the Word2Vec and the GloVe model for creating embeddings. After we obtain the embeddings, we use them to train the CNN classifier described in the previous sub-section, except that the domainspecific word embeddings are adapted during the training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Domain-Specific Model (DSM) based Learning"
        },
        {
            "text": "We use two COVID-19 datasets for the study, i.e., CoAID (Cui and Lee, 2020) and CMU-MisCov19 (Memon and Carley, 2020). The CoAID dataset contains two types of data: true information and misinformation. We use this dataset to investigate the generalizability of the models along three dimensions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "\u2022 Temporal dimension: Train a model using data from an earlier time, then test its generalizability at different times in the future.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "\u2022 Size dimension: Train models by varying the size of the training dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "\u2022 Length dimension: Train models by varying the length of the samples, e.g., tweet (shortlength data) and news articles (lengthy data).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "The CMU-MisCov19 dataset is used to analyze a model's performance in fine-grained classification.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "The CoAID dataset (Cui and Lee, 2020) is used for binary classification since it has only two labels: 0 for misinformation and 1 for true information. This dataset contains two types of data: online news articles on COVID-19 and tweets related to those articles. Datasets of these two categories were collected at four different months in 2020: May, July, September, and November. Thus, the total number of CoAID datasets is 8. The class distribution is heavily skewed with significantly more true information samples than misinformation samples. Sample distribution per class (both for the tweets and news articles) is given in the appendix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CoAID: Binary Classification"
        },
        {
            "text": "The CMU-MisCov19 dataset contains 4,573 annotated tweets (Memon and Carley, 2020). The tweets were collected on three days in 2020: March 29, June 15, and June 24. The categories are finegrained comprising of 17 classes with skewed distribution. This dataset does not have any true information category. Its sample distribution per class is given in the appendix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CMU-MisCov19: Fine-Grained Classification"
        },
        {
            "text": "We use the CoAID dataset to understand whether the context of the COVID-19 text evolves. To detect a change in the context over time, we investigate how the distribution of the high-frequency terms evolve for the two categories of the data: tweets and news articles. For each category, we select the top 10 high-frequency words from the 4 non-overlapping datasets belonging to 4 subsequent months, i.e., May, July, September, and November in 2020. Our goal is to determine whether there exists a temporal change in the distribution of high-frequency words. Figure 1 shows context evolution in the tweets category. We see that during May, the two highfrequency words were covid and coronavirus. The frequent words represent broader concepts such as health, disease, spread, etc. However, over time the context shifted towards more loaded terms. For example, in July two new high-frequency words, such as mask and support, emerged. Then, in September words like contact, school, child, and travel became prominent. Finally, during November, we observe a sharp change in the nature of the frequent words. Terms with strong political connotations (e.g., trump, fauci, campaign, and vaccine) started emerging. The evolution in the high-frequency words indicates a temporal shift in the context in the tweets dataset. We observe similar context evolution in the news articles dataset, reported in the Appendix with additional analysis.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 557,
                    "end": 565,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Context Evolution in the COVID-19 Social Media Data"
        },
        {
            "text": "We describe the design of the studies for comparing the NLP approaches for misinformation detection. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Study Design"
        },
        {
            "text": "Study 1 is designed to explore a model's generalizability in the temporal dimension of the data. We fine-tune/train a model using CoAID data collected from May 2020 and test it using data obtained from 3 different months in \"future\": July, September, and November. The following models are tested in this study: BERT (Mixed-domain Transfer Learning), ELMo (Mixed-domain Transfer Learning), Word2Vec (Mixed-domain Transfer Learning and DSM-based), GloVe (Mixed-domain Transfer Learning and DSM-based), Text-GCN (DSM-based), and VGCN-BERT (DSM-based).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Study 1"
        },
        {
            "text": "Study 2 is designed to test the performance of a model along the length dimension of the data. We use both short-length data (tweets) and lengthy data (news articles). Specifically, we train a model using the CoAID Twitter dataset to understand a model's performance on the short-length data. Then, we train a model using the CoAID news articles dataset to study a model's performance on the lengthy data. The models used in this study are the same as in Study 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Study 2"
        },
        {
            "text": "In study 3, we evaluate a model along the size dimension of the data. We replicate studies 1 and 2 using a large target dataset, which is created by merging the datasets from May, July, and September. The November dataset is used as the test set. We experiment with two models for this study: BERT (Mixed-Domain Transfer Learning) and Word2Vec (DSM-based).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Study 3"
        },
        {
            "text": "To further study the effectiveness of the Transformer-based mixed-domain transfer learning approach, we experiment with two variants of the BERT PTM, i.e., RoBERTa and ALBERT. In addition to this, we study the performance of an autoregressive model XLNet that induces the strength of BERT. For this study, we only use the news articles dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Study 4"
        },
        {
            "text": "Study 5 is designed to test a model's performance on the fined-grained CMU-MisCov19 dataset. The models tested are the same as in Study 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Study 5"
        },
        {
            "text": "We evaluate the performance of the models based on the accuracy, precision, recall, and f1 score, with an emphasis on the misinformation class. For each experiment, we average the results for 10 runs. The experiments are done using Scikit-learn (Pedregosa et al., 2011 ), TensorFlow 2.0 (Abadi et al., 2015 , and PyTorch (Paszke et al., 2019) libraries. For creating the Word2Vec embeddings, we used the skip-gram model from the Gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010) . Finally, the GloVe embeddings are created using the model from (Glove-Python, 2016).",
            "cite_spans": [
                {
                    "start": 245,
                    "end": 268,
                    "text": "(Pedregosa et al., 2011",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 269,
                    "end": 306,
                    "text": "), TensorFlow 2.0 (Abadi et al., 2015",
                    "ref_id": null
                },
                {
                    "start": 321,
                    "end": 342,
                    "text": "(Paszke et al., 2019)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 444,
                    "end": 469,
                    "text": "(\u0158eh\u016f\u0159ek and Sojka, 2010)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Results. Table 1 and Table 2 show the results from studies 1 and 2.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 9,
                    "end": 28,
                    "text": "Table 1 and Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "From the results on the CoAID tweets, given in Table 1 , we see that for the July tweet test dataset (Table 1) , VGCN-BERT has the highest misinformation precision. However, misinformation recall and f1 scores for all models are poor. For September, the Text-GCN has outstanding performance for detecting misinformation, but its performance on true information is extremely poor. Other models perform badly on misinformation. For November, the GloVe-based transfer learning approach achieves excellent performance on both true information and misinformation, where precision, recall, and f1 scores are 1. Text-GCN also has decent scores on misinformation but fails to detect true information. The performance of BERT on both true information and misinformation is also good. However, we notice that no model performs well across three different test datasets. Thus, we see that mixed-domain transfer learning is not robust when the context of the short-length data (tweets) changes. This is also true for the DSMbased approach. Table 2 shows the results of CoAID news articles (lengthy text). For the July test dataset, both Text-GCN and Word2Vec (DSM-based) achieve decent precision, recall, and f1 scores on misinformation. However, Text-GCN has extremely poor performance on true information. On the September data, ELMo exhibits the best misinformation precision, and f1 score, while Word2Vec (DSMbased) gives the best misinformation recall score. Both ELMo and Word2Vec perform well on the true information class as well. As for the November data, both transfer learning and DSM-based Word2Vec obtain optimal misinformation precision score and Word2Vec (DSM-based) obtains the highest f1 score. Besides, VGCN-BERT achieves the highest misinformation recall score. We notice that the DSM-based Word2Vec exhibits comparatively better performance across all test datasets. Thus, the non-graph DSM+CNN can capture both global and local relationships from lengthy text relatively well. The performance of the graph- based DSM approach on lengthy text is not as good as on short text. Also, BERT shows unreliable performance as it fails on the misinformation class. Table 3 shows the results of study 3, i.e., largedataset-based experiments. The performance of DSM-based Word2Vec is consistent with its performance on the CoAID news articles data (Table 2) . Its F1 score on tweets misinformation increases significantly compared to the small-data case (Table  1) . Thus, the non-graph DSM+CNN can capture both global and local relationships if we in-crease the size of short-length training data (i.e., tweets). the three datasets. These results corroborate our previous observation on the mixed-domain transfer learning approach, i.e., it is not robust when the context of the data changes. Figure 2 shows the results obtained from study 5. We see that the mixed-domain transfer learning approach performs poorly on the fine-grained dataset. The only model that achieves decent performance is the non-graph DSM Word2Vec with CNN.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 47,
                    "end": 54,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 101,
                    "end": 110,
                    "text": "(Table 1)",
                    "ref_id": null
                },
                {
                    "start": 1028,
                    "end": 1035,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 2165,
                    "end": 2172,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 2346,
                    "end": 2355,
                    "text": "(Table 2)",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 2452,
                    "end": 2462,
                    "text": "(Table  1)",
                    "ref_id": null
                },
                {
                    "start": 2792,
                    "end": 2800,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "Analysis. Based on the results obtained from the studies, we summarize our observations below. First, we discuss a model's generalizability for binary classification scenarios. Given the length of the text and the size of the dataset, we identify 4 cases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Case 1: Length=Short & Size=Small For case 1, we do not find a single best-performing model. For the tweet dataset, the following models perform slightly better: VGCN-BERT, GloVe (transfer learning and DSM-based), and Text GCN. There are two possible explanations for the poor performance of all models on the short-length tweet data. First, the number of test misinformation samples is significantly smaller. For example, in the 2020 July, September, and November tweet test datasets, the true information samples are larger than the misinformation samples by 46, 17, and 96 times, respectively. Second, the short length of the text and the small size of the training set might have influenced the scope of the context learning by the models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Case 2: Length=Long & Size=Small For the news articles data, the best model is DSM Word2Vec+CNN for the July and November datasets. It achieved the highest precision and recall on the misinformation class. For the September dataset, the ELMo outperforms DSM Word2Vec+CNN.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Case 3: Length=Short & Size=Large Both DSM Word2Vec+CNN and BERT-based transfer learning performed well. However, BERT's performance is not consistent. On the tweet dataset (short-length text), the precision of BERT is poor. It indicates that even with larger training data, BERTbased transfer learning does not provide an effective solution for short-length samples. One possible reason is that although BERT is good at capturing the local relationships (e.g., word order), it does not do equally well on capturing the global relationships from short-length data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Case 4: Length=Long & Size=Large Both DSM Word2Vec+CNN and BERT-based transfer learning perform well in this case. BERT's performance is slightly better. This indicates that Transformer-based models are suitable when target data is large and texts are lengthy. The results from fine-grained classification show that DSM Word2Vec+CNN outperforms other approaches by a large margin. Apart from the case of binary short length and small size dataset, DSM Word2Vec+CNN is shown to achieve the most effective solution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "One possible reason for the better generalization capability of the non-graph DSM+CNN-based approach is that its hierarchical feature-extraction mechanism is conducive for learning both the local context (the non-graph DSM, e.g., Word2Vec captures the local relationships of words in the target text) and the global context (the CNN learns global relationships from the word embeddings), which validates our hypothesis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Based on the insights garnered from the above analysis, we draw the following conclusions, summarized in the framework in Figure 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 122,
                    "end": 130,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "\u2022 The Transformer-based mixed-domain transfer learning approach is effective in limited cases. Also, its performance is not consistent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "\u2022 The graph-based DSM approach does not yield an effective solution in any of the cases. The VGCN-BERT that combines the benefits of Text GCN with BERT is not effective either.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "\u2022 The non-graph DSM + CNN approach generalizes well across the last three cases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Our study suffers from some limitations. The lack of labeled data narrowed the scope of our investigation. The data scarcity affected our study in two ways. First, due to the small size of the test data, we obtained noisy estimates for the short length and small size data. Second, we could not conduct a multi-dimensional study on the finegrained classification problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "When an unanticipated pandemic like COVID-19 breaks out, various types of misinformation emerge and propagate at warp speed over online social media. For detecting such misinformation, NLP techniques require to capture the context of the discourse from its evolving narrative. We argue that irrespective of the success of the deep learning based mixed-domain transfer learning approach for solving various NLP tasks, it does not yield a generalizable solution. We emphasize the importance of learning the context (both local and global) directly from the target domain via the DSM-based approach. A feasible way to implement a DSM is to utilize shallow neural networks that capture the local relationships in the target data. Representations learned from this type of model can then be used by shallow CNNs to learn global relationships as high-level features. Thus, a combination of non-graph DSM and CNN may lend a more generalizable solution. We perform an extensive study using Twitter-based COVID-19 social media data that includes tweets and news articles referred to in the tweets. Our investigation is performed along the following dimensions of the data: temporal dimension (evolving context), length dimension (varying text length), size dimension (varying size of datasets), and classification-level dimension (binary vs. multi-class data). We show that the mixed-domain transfer learning approach does not always work well. We found the combination of the non-graph DSM (for capturing local relationships) and CNN (for extracting global relationships) to be a promising approach towards creating a generalizable solution for detecting COVID-19 online misinformation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "In the future, we plan to investigate the generalizability of the DSM models created using deep learning architectures such as BERT. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Solving Natural Language Processing (NLP) tasks using Deep Learning (DL) based models is a challenging venture. Unlike computer vision problems in which deep learning supervised model can learn expressive representations directly from raw pixels of the input data while performing a discrimination task, the deep learning based supervised NLP systems cannot use raw text input while solving NLP tasks. The text input data needs to be encoded with latent representations or embeddings. These embeddings are learned by neural models from general-purpose unlabeled data using the selfsupervised learning approach (Tendle and Hasan, 2021) . The embeddings must capture the multidimensional relationships of the text components, which are non-contextual and contextual relationships. The non-contextual relationship includes syntactic relationships and semantic relationships. On the other hand, the contextual relationship includes dynamic representations of words, which requires embeddings to capture the local and global relationships of the words. Sequence DL models such as Recurrent Neural Network (RNN) have been used to learn the local context of a word in sequential order (Sutskever et al., 2014) . RNNs process text as a sequence of words for capturing word dependencies and text structures. However, they suffer from two limitations. First, they are unable to create good representations due to the uni-directional processing (Peters et al., 2018) . Second, these models struggle with capturing long-term dependency (Hochreiter and Schmidhuber, 1997). These two issues were partially resolved by introducing the bi-directional LSTM model (Schuster and Paliwal, 1997) . This model was combined with two-dimensional maxpooling in (Zhou et al., 2016) for capturing text features. In addition to this type of chain-structured LSTM, tree-structured LSTM such as the Tree-LSTM model was developed for learning rich semantic representations (Tai et al., 2015) . Irrespective of the progress harnessed by RNN-based models, they do not perform well in capturing global relationships (i.e., long-term dependencies) among the words of the source text. Also, training this type of model on large data is inefficient.",
            "cite_spans": [
                {
                    "start": 610,
                    "end": 634,
                    "text": "(Tendle and Hasan, 2021)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1178,
                    "end": 1202,
                    "text": "(Sutskever et al., 2014)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1434,
                    "end": 1455,
                    "text": "(Peters et al., 2018)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1646,
                    "end": 1674,
                    "text": "(Schuster and Paliwal, 1997)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1736,
                    "end": 1755,
                    "text": "(Zhou et al., 2016)",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1942,
                    "end": 1960,
                    "text": "(Tai et al., 2015)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "An efficient approach for some NLP tasks such as text classification is a shallow Convolutional Neural Network (CNN) with a one-dimensional convolutional kernel (Kim, 2014) . This model is good at capturing local patterns such as key phrases in the text. However, it does not work effectively if the weights of the input layer are initialized randomly (Kim, 2014) . It was shown to be effective only in transfer learning in which, first, word embeddings are created using a self-supervised pretrained model (PTM) such as Word2Vec (Mikolov et al., 2013) , then the CNN uses its single layer of convolution on top of the word embeddings to learn high-level representations.",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 172,
                    "text": "(Kim, 2014)",
                    "ref_id": null
                },
                {
                    "start": 352,
                    "end": 363,
                    "text": "(Kim, 2014)",
                    "ref_id": null
                },
                {
                    "start": 530,
                    "end": 552,
                    "text": "(Mikolov et al., 2013)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The use of PTMs for mixed-domain transfer learning ushered in a new era in NLP (Qiu et al., 2020) . The PTMs are created from general-purpose unlabeled data by using the self-supervised learning technique. In general, the SSL technique learns representations by predicting a hidden property of the input from the observable properties (LeCun and Misra, 2021). Two types of PTMs are used in NLP: (i) PTMs that are feature extractors, i.e., learn word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) , which are used as input to another model for solving a downstream NLP task (Kim, 2014) , and (ii) PTMs that learn language models and the same PTM is adapted (fine-tuned) for solving downstream NLP tasks (Devlin et al., 2019) . The feature extractor PTMs such as Word2Vec (Mikolov et al., 2013) , GloVe (Pennington et al., 2014) , and ELMo (Peters et al., 2018) are based on both shallow and deep neural network architectures. While shallow Word2Vec and GloVe models learn non-contextual word embeddings from unlabeled source data, the deep ELMo model is good for creating contextual embeddings. Features learned from these PTMs are used as input to another neural network for solving a downstream NLP task using labeled target data.",
            "cite_spans": [
                {
                    "start": 79,
                    "end": 97,
                    "text": "(Qiu et al., 2020)",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 461,
                    "end": 483,
                    "text": "(Mikolov et al., 2013;",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 484,
                    "end": 508,
                    "text": "Pennington et al., 2014;",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 509,
                    "end": 529,
                    "text": "Peters et al., 2018)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 607,
                    "end": 618,
                    "text": "(Kim, 2014)",
                    "ref_id": null
                },
                {
                    "start": 804,
                    "end": 826,
                    "text": "(Mikolov et al., 2013)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 835,
                    "end": 860,
                    "text": "(Pennington et al., 2014)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 872,
                    "end": 893,
                    "text": "(Peters et al., 2018)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [
                {
                    "start": 736,
                    "end": 757,
                    "text": "(Devlin et al., 2019)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Related Work"
        },
        {
            "text": "Both Word2Vec and GloVe learn word embeddings from their co-occurrence information. While Word2Vec leverages co-occurrence within the local context, GloVe utilizes global word-toword co-occurrence counts from the entire corpus. Word2Vec is a shallow feed-forward neural network-based predictive model that learns embeddings of the words while improving their predictions within the local context. On the other hand, GloVe is a count-based model that applies dimensionality reduction on the co-occurrence count matrix for learning word embeddings. These two models are good at capturing syntactic as well as semantic relationships. Although they can capture some global relationships between words in a text (Levy and Goldberg, 2014; Srinivasan and Ribeiro, 2019) , their embeddings are context-independent. Thus, these two models are not good at language modeling. A language model can predict the next word in the sequence given the words that precede it (Goldberg and Hirst, 2017) , which requires it to capture the context of the text. The deep architecture feature extractor PTM ELMo (Embeddings from Language Models) (Peters et al., 2018) learns contextualized word embeddings, i.e., it maps a word to different embedding vectors depending on their context. It uses two LSTMs in the forward and backward directions to encode the context of the words. The main limitation of this deep PTM is that it is computationally complex due to its sequential processing of text. Thus, it is prohibitively expensive to train using a very large text corpus. Another limitation of this model, which also applies to feature extractor PTMs in general, is that for solving downstream NLP tasks we need to train the entire model, except for the input embedding layer, from scratch.",
            "cite_spans": [
                {
                    "start": 707,
                    "end": 732,
                    "text": "(Levy and Goldberg, 2014;",
                    "ref_id": null
                },
                {
                    "start": 733,
                    "end": 762,
                    "text": "Srinivasan and Ribeiro, 2019)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 956,
                    "end": 982,
                    "text": "(Goldberg and Hirst, 2017)",
                    "ref_id": null
                },
                {
                    "start": 1122,
                    "end": 1143,
                    "text": "(Peters et al., 2018)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The above two limitations of the feature extractor PTMs are addressed by a very deep architecturebased Transformer model (Vaswani et al., 2017) . Unlike the feature extractor sequential PTMs, Transformer is a non-sequential model that uses self-attention (Bahdanau et al., 2016) to compute an attention score for capturing the influence of every word on other words in a sentence or document. This process is parallelized, which enables training deep Transformer models efficiently using very large text corpus such as Wikipedia and Book-Corpus (Devlin et al., 2019) as well as web crawls (Liu et al., 2019) .",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 143,
                    "text": "(Vaswani et al., 2017)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 255,
                    "end": 278,
                    "text": "(Bahdanau et al., 2016)",
                    "ref_id": null
                },
                {
                    "start": 589,
                    "end": 607,
                    "text": "(Liu et al., 2019)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "There are two main types of Transformer-based deep PTMs: autoregressive and autoencoding. The OpenAI GPT (Generative Pre-training) (Radford and Sutskever, 2018 ) is an autoregressive model that learns embeddings by predicting words based on previous predictions. Specifically, it is a unidirectional model that predicts words sequentially in a text. On the other hand, BERT (Devlin et al., 2019) utilizes the autoencoding technique based on bi-directional context modeling. Specifically, for training, it uses a masked language modeling (MLM) task. The MLM randomly masks some tokens in a text sequence, then it predicts the masked tokens by learning the encoding vectors.",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 159,
                    "text": "(Radford and Sutskever, 2018",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 369,
                    "end": 395,
                    "text": "BERT (Devlin et al., 2019)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Variants of BERT such as RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2020) were proposed to improve its effectiveness as well as efficiency. RoBERTa (Robustly optimized BERT) improves the effectiveness of BERT by using several strategies that include the following. It trains the model longer using more data, lengthy input, and larger batches. It uses a dynamic masking strategy and removes BERT's Next Sentence Prediction (NSP) task. ALBERT (A Lite BERT) improves the efficiency of BERT by employing fewer parameters, which increases its training speed.",
            "cite_spans": [
                {
                    "start": 33,
                    "end": 51,
                    "text": "(Liu et al., 2019)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 56,
                    "end": 81,
                    "text": "ALBERT (Lan et al., 2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "There have been attempts such as in XLNet to integrate the strengths of the autoregressive and autoencoding Transformer techniques. XLNet is an autoregressive model that uses a permutation language modeling objective. This allows XLNet to retain the advantages of autoregressive models while leveraging the benefit of the autoencoding models, i.e., to capture the bi-directional context.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Irrespective of the state-of-the-art (SOTA) performance of the deep PTM based mixed-domain transfer learning approach on many NLP tasks (Minaee et al., 2021) , this approach is not suitable for detecting misinformation from COVID-19 social media data. It generalizes poorly when the domain of the source dataset used to create the PTMs is significantly different from that of the target dataset (Peters et al., 2019) . One solution to this generalizability problem is to create a PTM using data that shares context similar to the target domain, i.e., pretrain a domain-specific model (DSM). This type of model encodes the context of the target domain more effectively to provide a generalizable solution for the downstream task (Beltagy et al., 2019; Lee et al., 2019; Gu et al., 2021) . However, pre-training a deep architecture-based DSM (e.g., BERT) for the COVID-19 misinformation detection task in a timely fashion could be infeasible as it requires collecting a large amount of COVID-19 social media data, which must cover the diverse landscape of COVID-19 misinformation. While there was an effort to create such a deep DSM using COVID-19 tweets in (M\u00fcller et al., 2020) , capturing the dynamic context of the pandemic requires the collection of various types of social media data at a large scale.",
            "cite_spans": [
                {
                    "start": 136,
                    "end": 157,
                    "text": "(Minaee et al., 2021)",
                    "ref_id": null
                },
                {
                    "start": 395,
                    "end": 416,
                    "text": "(Peters et al., 2019)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 728,
                    "end": 750,
                    "text": "(Beltagy et al., 2019;",
                    "ref_id": null
                },
                {
                    "start": 751,
                    "end": 768,
                    "text": "Lee et al., 2019;",
                    "ref_id": null
                },
                {
                    "start": 769,
                    "end": 785,
                    "text": "Gu et al., 2021)",
                    "ref_id": null
                },
                {
                    "start": 1156,
                    "end": 1177,
                    "text": "(M\u00fcller et al., 2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Thus, to create DSMs for the COVID-19 domain using quickly collectible small data, shallow architecture based PTMs such as Word2Vec and GloVe are suitable. However, as mentioned earlier, these PTMs are context-independent and are not good at capturing the global relationships well. To compensate for these shortcomings, we used the extracted features from the Word2Vec and GloVe DSMs for training a one-dimensional convolutional kernel-based CNN similar to the shallow architecture given in (Kim, 2014) . The CNN learns global relationships by extracting local patterns in a hierarchical fashion by convolving over the word embeddings.",
            "cite_spans": [
                {
                    "start": 492,
                    "end": 503,
                    "text": "(Kim, 2014)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Another type of DSM we used is graph-based that leverages the linguistic-aware graph structure of the text for learning contextual representations, then uses those representations to solve a downstream NLP task. The main intuition driving the graph-based technique is that by modeling the vocabulary graph, it will be possible to encode global relationships in the embeddings. Text GCN (Text Graph Convolutional Network) (Yao et al., 2019) is a graph-based model that explicitly captures the global term-co-occurrence information by leveraging the graph structure of the text. It models the global word co-occurrence by incorporating edges between words as well as edges between a document and a word. Word-word edges are created by using word co-occurrence information and word-document edges are created by using word frequency and word-document frequency. Its input is a one-hot vector representation of every word in the document, which is used to create a heterogeneous text graph that has word nodes and document nodes. These are fed into a two-layer GCN (Graph Convolutional Network) (Kipf and Welling, 2017) that turns document classification into a node classification problem.",
            "cite_spans": [
                {
                    "start": 421,
                    "end": 439,
                    "text": "(Yao et al., 2019)",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Although Text GCN is good at convolving the global information in the graph, it does not take into account local information such as word orders. To address this issue, Text GCN was combined with BERT, which is good at capturing local information. The resulting model is VGCN-BERT (Lu et al., 2020) . BERT captures the local context by focusing on local word sequences, but it is not good at capturing global information of a text. It learns representations from a sentence or a document. However, it does not take into account the knowledge of the vocabulary. Thus its language model may be incomplete. On the other hand, Text GCN captures the global vocabulary information. The VGCN-BERT aims to capture both local and global relationships by integrating GCN with BERT. Both the graph embeddings and word embeddings are fed into a self-attention encoder in BERT. When the classifier is trained, these two types of embeddings interact with each other through the self-attention mechanism. As a consequence, the classifier creates representations by fusing global information with local information in a guided fashion.",
            "cite_spans": [
                {
                    "start": 281,
                    "end": 298,
                    "text": "(Lu et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "We describe the sample distribution of both the CoAID (binary) and CMU datasets. Then, we analyze context evolution in the CoAID new articles dataset. CoAID Sample Distribution. Figures 4 and 5 show the distributions of tweets and news articles per category, respectively. We see that the datasets contain significantly more true information than misinformation. Thus, the CoAID data is heavily skewed. For the tweets dataset, the sizes of May and July data are larger than that of September and November. Also, the number of misinformation tweets during September and November are negli- gibly smaller, making it challenging to use these as test datasets.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 178,
                    "end": 193,
                    "text": "Figures 4 and 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "CMU Sample Distribution. The CMU-MisCov19 or CMU short-length text dataset misinformation categories are fine-grained comprising of 17 classes. It consists of 4,573 annotated tweets from 3,629 users with an average of 1.24 tweets per user. Figure 6 shows the heavily skewed distribution of 4,292 tweets for all categories that were extracted after some pre-processing. Class 7 (calling out or correction) has the most tweets, while class 2 (true treatment) has 0 tweets. 6.0.1 CoAID News Articles: Study of Context Evolution Figure 7 shows context evolution in the news articles category via the evolution of the distribution of the top ten high-frequency terms. We see, similar to the tweet dataset, context changes over time in the news articles datasets. For example, The May and July datasets have only 4 common high-frequency words: covid, coronavirus, health, data. In the July dataset, we observe the emergence of three new high-frequency words attack, security, and protect, which indicates a change in context. The context in the September dataset seems to be similar to that of the July dataset. These two datasets have eight high-frequency words in common: covid, service, online, attack, security, health, information, coronavirus. The November dataset shares seven common words with the September dataset: covid, service, online, attack, people, health, coronavirus. However, we notice an increase in the frequency in some words such as protect and attack. Also, a new word pandemic is seen to emerge. We gather similar observations about the context evolution from the word clouds in Figure 8 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 240,
                    "end": 248,
                    "text": "Figure 6",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 525,
                    "end": 533,
                    "text": "Figure 7",
                    "ref_id": null
                },
                {
                    "start": 1598,
                    "end": 1606,
                    "text": "Figure 8",
                    "ref_id": null
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "We provide the experimental setting for conducting our studies as well as the training statistics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setting & Training Statistics"
        },
        {
            "text": "Experimental Setting. Table 5 shows the experimental setting for the studies. We used the default learning rate and batch size for all experiments.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 22,
                    "end": 29,
                    "text": "Table 5",
                    "ref_id": "TABREF7"
                }
            ],
            "section": "Experimental Setting & Training Statistics"
        },
        {
            "text": "Training Statistics. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setting & Training Statistics"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "authors": [
                {
                    "first": "Yinhan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Myle",
                    "middle": [],
                    "last": "Ott",
                    "suffix": ""
                },
                {
                    "first": "Naman",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "Jingfei",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "Mandar",
                    "middle": [],
                    "last": "Joshi",
                    "suffix": ""
                },
                {
                    "first": "Danqi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Omer",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "Mike",
                    "middle": [],
                    "last": "Lewis",
                    "suffix": ""
                },
                {
                    "first": "Luke",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                },
                {
                    "first": "Veselin",
                    "middle": [],
                    "last": "Stoyanov",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Vgcn-bert: Augmenting bert with graph embedding for text classification",
            "authors": [
                {
                    "first": "Zhibin",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Pan",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "Jian-Yun",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Advances in Information Retrieval -42nd European Conference on IR Research",
            "volume": "2020",
            "issn": "",
            "pages": "369--382",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "2020. ktrain: A low-code library for augmented machine learning",
            "authors": [
                {
                    "first": "Arun",
                    "middle": [
                        "S"
                    ],
                    "last": "Maiya",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2004.10703"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Characterizing COVID-19 misinformation communities using a novel twitter dataset",
            "authors": [
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Shahan",
                    "suffix": ""
                },
                {
                    "first": "Kathleen",
                    "middle": [
                        "M"
                    ],
                    "last": "Memon",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Carley",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Efficient estimation of word representations in vector space",
            "authors": [
                {
                    "first": "Tomas",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Greg",
                    "middle": [],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "CoRR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. 2021. Deep learning based text classification: A comprehensive review",
            "authors": [
                {
                    "first": "Shervin",
                    "middle": [],
                    "last": "Minaee",
                    "suffix": ""
                },
                {
                    "first": "Nal",
                    "middle": [],
                    "last": "Kalchbrenner",
                    "suffix": ""
                },
                {
                    "first": "Erik",
                    "middle": [],
                    "last": "Cambria",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "and Per Egil Kummervold. 2020. Covid-twitter-bert: A natural language processing model to analyse COVID-19 content on twitter",
            "authors": [
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "Marcel",
                    "middle": [],
                    "last": "Salath\u00e9",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Man dies, woman hospitalized after taking form of chloroquine to prevent covid-19",
            "authors": [
                {
                    "first": "Scott",
                    "middle": [],
                    "last": "Neuman",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A survey on transfer learning",
            "authors": [
                {
                    "first": "Qiang",
                    "middle": [],
                    "last": "Sinno Jialin Pan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE Transactions on knowledge and data engineering",
            "volume": "22",
            "issn": "10",
            "pages": "1345--1359",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "authors": [
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Paszke",
                    "suffix": ""
                },
                {
                    "first": "Sam",
                    "middle": [],
                    "last": "Gross",
                    "suffix": ""
                },
                {
                    "first": "Francisco",
                    "middle": [],
                    "last": "Massa",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Lerer",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Bradbury",
                    "suffix": ""
                },
                {
                    "first": "Gregory",
                    "middle": [],
                    "last": "Chanan",
                    "suffix": ""
                },
                {
                    "first": "Trevor",
                    "middle": [],
                    "last": "Killeen",
                    "suffix": ""
                },
                {
                    "first": "Zeming",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Natalia",
                    "middle": [],
                    "last": "Gimelshein",
                    "suffix": ""
                },
                {
                    "first": "Luca",
                    "middle": [],
                    "last": "Antiga",
                    "suffix": ""
                },
                {
                    "first": "Alban",
                    "middle": [],
                    "last": "Desmaison",
                    "suffix": ""
                },
                {
                    "first": "Andreas",
                    "middle": [],
                    "last": "Kopf",
                    "suffix": ""
                },
                {
                    "first": "Edward",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Zachary",
                    "middle": [],
                    "last": "Devito",
                    "suffix": ""
                },
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Raison",
                    "suffix": ""
                },
                {
                    "first": "Alykhan",
                    "middle": [],
                    "last": "Tejani",
                    "suffix": ""
                },
                {
                    "first": "Sasank",
                    "middle": [],
                    "last": "Chilamkurthy",
                    "suffix": ""
                },
                {
                    "first": "Benoit",
                    "middle": [],
                    "last": "Steiner",
                    "suffix": ""
                },
                {
                    "first": "Lu",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Junjie",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "Soumith",
                    "middle": [],
                    "last": "Chintala",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "32",
            "issn": "",
            "pages": "8024--8035",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Scikit-learn: Machine learning in Python",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pedregosa",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Varoquaux",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gramfort",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Michel",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Thirion",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Grisel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Blondel",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Prettenhofer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Weiss",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Dubourg",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Vanderplas",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Passos",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cournapeau",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Brucher",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Perrot",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Duchesnay",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Journal of Machine Learning Research",
            "volume": "12",
            "issn": "",
            "pages": "2825--2830",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "GloVe: Global vectors for word representation",
            "authors": [
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {
                "DOI": [
                    "10.3115/v1/D14-1162"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Deep contextualized word representations",
            "authors": [
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Peters",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [],
                    "last": "Neumann",
                    "suffix": ""
                },
                {
                    "first": "Mohit",
                    "middle": [],
                    "last": "Iyyer",
                    "suffix": ""
                },
                {
                    "first": "Matt",
                    "middle": [],
                    "last": "Gardner",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Clark",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Luke",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "1",
            "issn": "",
            "pages": "2227--2237",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/N18-1202"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "To tune or not to tune? adapting pretrained representations to diverse tasks",
            "authors": [
                {
                    "first": "Matthew",
                    "middle": [
                        "E"
                    ],
                    "last": "Peters",
                    "suffix": ""
                },
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Ruder",
                    "suffix": ""
                },
                {
                    "first": "Noah",
                    "middle": [
                        "A"
                    ],
                    "last": "Smith",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
            "volume": "",
            "issn": "",
            "pages": "7--14",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/W19-4302"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Pre-trained models for natural language processing: A survey",
            "authors": [
                {
                    "first": "Xipeng",
                    "middle": [],
                    "last": "Qiu",
                    "suffix": ""
                },
                {
                    "first": "Tianxiang",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Yige",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Yunfan",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "Ning",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Xuanjing",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Improving language understanding by generative pre-training",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Software Framework for Topic Modelling with Large Corpora",
            "authors": [
                {
                    "first": "Petr",
                    "middle": [],
                    "last": "Radim\u0159eh\u016f\u0159ek",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sojka",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks",
            "volume": "",
            "issn": "",
            "pages": "45--50",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Bidirectional recurrent neural networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schuster",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "K"
                    ],
                    "last": "Paliwal",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Trans. Sig. Proc",
            "volume": "45",
            "issn": "11",
            "pages": "2673--2681",
            "other_ids": {
                "DOI": [
                    "10.1109/78.650093"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "On the equivalence between node embeddings and structural graph representations",
            "authors": [
                {
                    "first": "Balasubramaniam",
                    "middle": [],
                    "last": "Srinivasan",
                    "suffix": ""
                },
                {
                    "first": "Bruno",
                    "middle": [],
                    "last": "Ribeiro",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Sequence to sequence learning with neural networks",
            "authors": [
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Oriol",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "Quoc V",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "27",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Improved semantic representations from tree-structured long short-term memory networks",
            "authors": [
                {
                    "first": "Kai Sheng",
                    "middle": [],
                    "last": "Tai",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Cite arxiv:1503.00075Comment: Accepted for publication at ACL",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "A study of the generalizability of self-supervised representations",
            "authors": [
                {
                    "first": "Atharva",
                    "middle": [],
                    "last": "Tendle",
                    "suffix": ""
                },
                {
                    "first": "Mohammad Rashedul",
                    "middle": [],
                    "last": "Hasan",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Machine Learning with Applications",
            "volume": "6",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1016/j.mlwa.2021.100124"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "Ashish",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "Noam",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "Niki",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "Jakob",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "Llion",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "Aidan",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "Lukasz",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "Illia",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Combining knowledge with deep convolutional neural networks for short text classification",
            "authors": [
                {
                    "first": "Jin",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Zhongyuan",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Dawei",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI'17",
            "volume": "",
            "issn": "",
            "pages": "2915--2921",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Transformers: State-of-the-art natural language processing",
            "authors": [
                {
                    "first": "Thomas",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                },
                {
                    "first": "Lysandre",
                    "middle": [],
                    "last": "Debut",
                    "suffix": ""
                },
                {
                    "first": "Victor",
                    "middle": [],
                    "last": "Sanh",
                    "suffix": ""
                },
                {
                    "first": "Julien",
                    "middle": [],
                    "last": "Chaumond",
                    "suffix": ""
                },
                {
                    "first": "Clement",
                    "middle": [],
                    "last": "Delangue",
                    "suffix": ""
                },
                {
                    "first": "Anthony",
                    "middle": [],
                    "last": "Moi",
                    "suffix": ""
                },
                {
                    "first": "Pierric",
                    "middle": [],
                    "last": "Cistac",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Rault",
                    "suffix": ""
                },
                {
                    "first": "R\u00e9mi",
                    "middle": [],
                    "last": "Louf",
                    "suffix": ""
                },
                {
                    "first": "Morgan",
                    "middle": [],
                    "last": "Funtowicz",
                    "suffix": ""
                },
                {
                    "first": "Joe",
                    "middle": [],
                    "last": "Davison",
                    "suffix": ""
                },
                {
                    "first": "Sam",
                    "middle": [],
                    "last": "Shleifer",
                    "suffix": ""
                },
                {
                    "first": "Clara",
                    "middle": [],
                    "last": "Patrick Von Platen",
                    "suffix": ""
                },
                {
                    "first": "Yacine",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Julien",
                    "middle": [],
                    "last": "Jernite",
                    "suffix": ""
                },
                {
                    "first": "Canwen",
                    "middle": [],
                    "last": "Plu",
                    "suffix": ""
                },
                {
                    "first": "Teven",
                    "middle": [
                        "Le"
                    ],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Sylvain",
                    "middle": [],
                    "last": "Scao",
                    "suffix": ""
                },
                {
                    "first": "Mariama",
                    "middle": [],
                    "last": "Gugger",
                    "suffix": ""
                },
                {
                    "first": "Quentin",
                    "middle": [],
                    "last": "Drame",
                    "suffix": ""
                },
                {
                    "first": "Alexander",
                    "middle": [
                        "M"
                    ],
                    "last": "Lhoest",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Rush",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
            "volume": "",
            "issn": "",
            "pages": "38--45",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "authors": [
                {
                    "first": "Zhilin",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Zihang",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Yiming",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Jaime",
                    "middle": [],
                    "last": "Carbonell",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Russ",
                    "suffix": ""
                },
                {
                    "first": "Quoc V",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "32",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Graph convolutional networks for text classification",
            "authors": [
                {
                    "first": "Liang",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "Chengsheng",
                    "middle": [],
                    "last": "Mao",
                    "suffix": ""
                },
                {
                    "first": "Yuan",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "33",
            "issn": "",
            "pages": "7370--7377",
            "other_ids": {
                "DOI": [
                    "10.1609/aaai.v33i01.33017370"
                ]
            }
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Text classification improved by integrating bidirectional lstm with twodimensional max pooling",
            "authors": [
                {
                    "first": "Peng",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Zhenyu",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "Suncong",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Jiaming",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Hongyun",
                    "middle": [],
                    "last": "Bao",
                    "suffix": ""
                },
                {
                    "first": "Bo",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "COLING",
            "volume": "",
            "issn": "",
            "pages": "3485--3495",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF1": {
            "text": "CoAID (Tweets): Frequency of top 10 words in four datasets from four subsequent months.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Study 5: CMU-MisCov19 (Fine-grained classification).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "A framework for COVID-19 online misinformation detection.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "CoAID Tweets: Sample distribution (0: misinformation, 1: true information).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "CoAID News Articles: Sample distribution (0: misinformation, 1: true information).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "CMU-MisCov19 Dataset: Sample distribution.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "CoAID News Articles: Frequency of top 10 words. CoAID News Articles: Word Clouds for the four datasets.",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "Study 1 & 2: CoAID -News Articles (Temporal & Text Length Dimension). Best results, as well as the optimal models, are highlighted in red.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Study 3: CoAID Large Dataset (Dataset Size Dimension). Best results are highlighted in red.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "shows the results obtained from study 4. For the July test dataset misinformation detection, RoBERTa achieves the best performance, while XLNet shows the worst performance. However, for September misinformation, we observe the exact opposite scenario. As for November misinformation, ALBERT achieves the best performance, while XLNet's performance is the worst. No single Transformer-based model performs well on",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Study 4: CoAID Large Dataset (Various-based Transformer Models). Best results are highlighted in red. No single model performs well across three datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "and claims of covid-19 misinformation. Reuters Institute for the Study of Journalism.",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Experimental setting.",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "shows the training statistics that include the number of parameters for each model, dataset, and the average training time. The inference time is not significant, thus not reported. All experiments were done on a Tesla V100 GPU, except the Text GCN and VGCN BERT based experiments, which were conducted using a CPU. For DSM and CNN based experiments, the CNN was trained for 5 epochs on the CoAID tweet data, 10 epochs on the CoAID news articles data, and 10 epochs on the CMU fine-grained data. The number of epochs was chosen based on the convergence behavior of the models.GloVe DSM + CNNGloVe: 4.6M CNN: 160,301 CoAID News Articles (small data) 1.33 mins",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "Training Statistics -DSM: Domain-Specific Model, PTM: Pre-Trained Model",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "In this section, first, we discuss the related work. Then, we present an analysis of the dataset. Finally, we report the experimental setting and training statistics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        }
    ]
}
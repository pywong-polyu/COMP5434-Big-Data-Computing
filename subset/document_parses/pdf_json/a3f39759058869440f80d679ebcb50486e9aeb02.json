{
    "paper_id": "a3f39759058869440f80d679ebcb50486e9aeb02",
    "metadata": {
        "title": "Self-supervised Disentanglement of Modality-Specific and Shared Factors Improves Multimodal Generative Models",
        "authors": [
            {
                "first": "Imant",
                "middle": [],
                "last": "Daunhawer",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "ETH Zurich",
                    "location": {
                        "settlement": "Z\u00fcrich",
                        "country": "Switzerland"
                    }
                },
                "email": ""
            },
            {
                "first": "Thomas",
                "middle": [
                    "M"
                ],
                "last": "Sutter",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "ETH Zurich",
                    "location": {
                        "settlement": "Z\u00fcrich",
                        "country": "Switzerland"
                    }
                },
                "email": ""
            },
            {
                "first": "Ri\u010dards",
                "middle": [],
                "last": "Marcinkevi\u010ds",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "ETH Zurich",
                    "location": {
                        "settlement": "Z\u00fcrich",
                        "country": "Switzerland"
                    }
                },
                "email": ""
            },
            {
                "first": "Julia",
                "middle": [
                    "E"
                ],
                "last": "Vogt",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "ETH Zurich",
                    "location": {
                        "settlement": "Z\u00fcrich",
                        "country": "Switzerland"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Multimodal generative models learn a joint distribution over multiple modalities and thus have the potential to learn richer representations than unimodal models. However, current approaches are either inefficient in dealing with more than two modalities or fail to capture both modality-specific and shared variations. We introduce a new multimodal generative model that integrates both modality-specific and shared factors and aggregates shared information across any subset of modalities efficiently. Our method partitions the latent space into disjoint subspaces for modality-specific and shared factors and learns to disentangle these in a purely self-supervised manner. Empirically, we show improvements in representation learning and generative performance compared to previous methods and showcase the disentanglement capabilities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The online version of this chapter (https://",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The promise of multimodal generative models lies in their ability to learn rich representations across diverse domains and to generate missing modalities. As an analogy, humans are able to integrate information across senses to make more informed decisions [33] , and exhibit cross-modal transfer of perceptual knowledge [41] ; for instance, people can visualize objects given only haptic cues [42] . For machine learning, multimodal learning is of interest in any setting where information is integrated across two or more modalities.",
            "cite_spans": [
                {
                    "start": 257,
                    "end": 261,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 321,
                    "end": 325,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 394,
                    "end": 398,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Alternatives to multimodal generative models include unimodal models with late fusion or with coordinated representations, as well as conditional models that translate between pairs of modalities [3] . Yet, both alternatives have disadvantages compared to multimodal approaches. While unimodal models cannot handle missing modalities, conditional models only learn a mapping between sources, and neither integrate representations from different modalities into a joint representation. In contrast, multimodal generative models approximate the joint distribution and thus implicitly provide the marginal and conditional distributions. However, learning a joint distribution remains the more challenging task and there still exists a gap in the generative performance compared to unimodal and conditional models.",
            "cite_spans": [
                {
                    "start": 196,
                    "end": 199,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We bridge this gap by proposing a new self-supervised multimodal generative model that disentangles modality-specific and shared factors. We argue that this disentanglement is crucial for multimodal learning, because it simplifies the aggregation of representations across modalities. For conditional generation, this decomposition allows sampling from modality-specific priors without affecting the shared representation computed across multiple modalities. Further, decomposed representations have been found to be more interpretable [6, 17] and more amenable for certain downstream tasks [26] .",
            "cite_spans": [
                {
                    "start": 536,
                    "end": 539,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 540,
                    "end": 543,
                    "text": "17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 591,
                    "end": 595,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The main contribution of this work is the development of a new multimodal generative model that learns to disentangle modality-specific and shared factors in a self-supervised manner. We term this new method disentangling multimodal variational autoencoder (DMVAE). It extends the class of multimodal variational autoencoders by modeling modality-specific in addition to shared factors and by disentangling these groups of factors using a self-supervised contrastive objective. In two representative toy experiments, we demonstrate the following advantages compared to previous multimodal generative models:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-Effective disentanglement of modality-specific and shared factors. This allows sampling from modality-specific priors without changing the joint representation computed from multiple modalities. -Improvements in representation learning over state-of-the-art multimodal generative models. For any subset of modalities, our model aggregates shared information effectively and efficiently. -Improvements in generative performance over previous work. In a fair comparison, we demonstrate that modeling modality-specific in addition to shared factors significantly improves the conditional generation of missing modalities. For unconditional generation, we demonstrate the effectiveness of using ex-post density estimation [8] to further improve joint generation across all methods, including trained models from previous work.",
            "cite_spans": [
                {
                    "start": 719,
                    "end": 722,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Broadly, our work can be categorized as an extension of the class of multimodal generative models that handle more than two modalities (including missing ones) efficiently. Among this class, we present the first method that partitions the latent space into modality-specific and shared subspaces and disentangles these in a self-supervised fashion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Multimodal Generative Models. Current approaches are mainly based on encoder-decoder architectures which learn the mapping between modalities based on reconstructions or adversarial objectives (for a comprehensive review, see [3] ). Among this class, methods can be distinguished by the type of mapping they use to translate between inputs and outputs and by how they handle missing modalities. Early approaches [16, 35] try to learn all possible mappings, which in the case of missing modalities results in 2 M encoders for M modalities. A more efficient alternative is proposed by [40] who introduce the multimodal variational autoencoder (MVAE) which uses a joint posterior that is proportional to a product of experts (PoE) [14] . Their method handles missing modalities efficiently, because it has a closed form solution for the aggregation of marginal Gaussian posteriors. However, their derivation of the joint posterior is based on the assumption that all modalities share a common set of factors-an assumption that is often violated in practice, because modalities exhibit a high degree of modality-specific variation. Our model also uses a joint latent space with a product of experts aggregation layer, and thus shares the same theoretical advantages, but it considers modality-specific factors in addition to shared factors. The limitations of the MVAE were shown empirically in [31] , where it is stated that the MVAE lacks the abilities of latent factorization and joint generation. With latent factorization the authors refer to the decomposition into modality-specific and shared factors, and by joint generation they mean the semantic coherence of unconditionally generated samples across modalities. They attribute these problems to the joint posterior used by the MVAE and demonstrate empirically that using a mixture of experts, instead of a product, improves generative performance. In contrast, we argue that the product of experts is not a problem per se, but that it is an ill-defined aggregation operation in the presence of modality-specific factors. We resolve this model misspecification by modeling modality-specific factors in addition to shared factors. Compared to the mixture of experts multimodal variational autoencoder (MMVAE) [31] , our model has the advantage that it can sample from a modality-specific prior without affecting the shared representation which can still be aggregated efficiently across modalities through the PoE. Especially with more than two modalities, the aggregation of representations, as it is done in our model, shows its benefits compared to the MMVAE (see Sect. 4.2).",
            "cite_spans": [
                {
                    "start": 226,
                    "end": 229,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 412,
                    "end": 416,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 417,
                    "end": 420,
                    "text": "35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 583,
                    "end": 587,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 728,
                    "end": 732,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1391,
                    "end": 1395,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 2263,
                    "end": 2267,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Domain Adaption/Translation. The research areas of domain adaption and domain translation are in many regards closely related to multimodal generative models. Approaches that have explored many-to-many mappings between different domains have been based on adversarial methods [7, 24] , shared autoencoders [36] and cycle-consistency losses [2] . Translation methods have shown remarkable progress on image-to-image style transfer and the conceptual manipulation of images, however, their focus lies on learning conditional mappings, while our method models the joint distribution directly. Further, through the PoE our method aggregates shared representations across any subset of modalities and therefore handles missing modalities efficiently.",
            "cite_spans": [
                {
                    "start": 276,
                    "end": 279,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 280,
                    "end": 283,
                    "text": "24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 306,
                    "end": 310,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 340,
                    "end": 343,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Disentanglement. Our goal is not the unsupervised disentanglement of all generative factors, which was shown to be theoretically impossible with a factorizing prior and claimed to be impossible in general [25] . Instead, we are concerned with the disentanglement of modality-specific and shared sets of factors. In the multi-view and multimodal case, there is theoretical evidence for the identifiability of shared factors [9, 19, 27, 37] . Further, the self-supervised disentanglement of shared factors has been previously explored based on grouping information [4] , temporal dependencies [23] , partly labeled data [18, 38, 39] , and spatial information [5] . We take a first step towards disentanglement given multimodal data with modality-specific factors and an implicit, unknown grouping.",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 209,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 423,
                    "end": 426,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 427,
                    "end": 430,
                    "text": "19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 431,
                    "end": 434,
                    "text": "27,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 435,
                    "end": 438,
                    "text": "37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 563,
                    "end": 566,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 591,
                    "end": 595,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 618,
                    "end": 622,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 623,
                    "end": 626,
                    "text": "38,",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 627,
                    "end": 630,
                    "text": "39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 657,
                    "end": 660,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this section, we introduce multimodal generative models and derive the variational approximations and information-theoretic objectives that our method optimizes. All proofs are provided in the appendix. We consider a generative process with a partition into modality-specific and modality-invariant (i.e., shared) latent factors ( Fig. 1) . A multimodal sample x = (x 1 , . . . , x M ) with data from M modalities is assumed to be generated from a set of shared factors c and a set of modality-specific factors s m . Consequently, samples from different modalities are assumed to be conditionally independent given c. In the following, we denote the set of all modality-specific factors of a multimodal sample as s = (",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 334,
                    "end": 341,
                    "text": "Fig. 1)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Method"
        },
        {
            "text": "of multimodal samples, our goal is to learn a generative model p \u03b8 (x|c, s) with a neural network parameterized by \u03b8. From the above assumptions on the data generating process, it follows the joint distribution",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "which allows to consider only the observed modalities for the computation of the marginal likelihood. The computation of the exact likelihood is intractable, therefore, we resort to amortized variational inference and instead maximize the evidence lower bound",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "which is composed of M log-likelihood terms and KL-divergences between approximate posteriors q \u03c6 (s m |x) and priors p(s m ). Above objective describes M modality-specific VAEs, each of which takes as input an additional context vector c that encodes shared information (described in Sect. 3.2). We use neural networks for each encoder q \u03c6m (s m |x m ) as well as for each decoder p \u03b8m (x m |c, s m ) and denote the network parameters by the respective subscripts for decoder parameters \u03b8 and encoder parameters \u03c6. Further, we follow the convention of using an isotropic Gaussian prior and Gaussian variational posteriors parameterized by the estimated means and variances that are the outputs of the encoder.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "For each modality-specific VAE, it is possible to control the degree of disentanglement of arbitrary factors with a weight on the respective KL-divergence term, like in the \u03b2-VAE [13] . However, there exist theoretical limitations on the feasibility of unsupervised disentanglement of arbitrary factors [25] . In contrast, we focus on the disentanglement of modality-specific and shared factors, for which we use two additional objectives that are introduced in Subsect. 3.2.",
            "cite_spans": [
                {
                    "start": 179,
                    "end": 183,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 303,
                    "end": 307,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "A key aspect in the design of multimodal models should be the capability to handle missing modalities efficiently [3] . In our case, only the shared representation depends on all modalities and should ideally be able to cope with any combination of missing inputs, which would require 2 M inference networks in a naive implementation. A more efficient alternative is offered in [40] , where a product of experts (PoE) [14] is used to handle missing modalities. Under the assumption of shared factors, previous work [40] has shown that the posterior p(c | x) is proportional to a product of unimodal posteriors",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 117,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 378,
                    "end": 382,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 418,
                    "end": 422,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 515,
                    "end": 519,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Multimodal Inference Network"
        },
        {
            "text": "which-for the special case of Gaussian posteriors-has an efficient closed-form solution (see Appendix A.3). We also assume Gaussian unimodal posteriors q \u03c8m (c|x m ) where \u03c8 denotes the encoder parameters, part of which can be shared with the encoder parameters \u03c6 m of a unimodal VAE. The choice of Gaussian posteriors allows us to employ the PoE as an aggregation layer for shared factors. This allows the model to use M unimodal inference networks to handle all 2 M combinations of missing modalities for the inference of shared factors. While the PoE is a well defined aggregation operation for shared factors, it is not suitable for modality-specific factors, because it averages over representations from different modalities. 1 Therefore, we partition the latent space into M + 1 independent subspaces, one that is specific for each modality (denoted by s m ) and one that has shared content between all modalities (denoted by c), as illustrated in Fig. 1 . The PoE is only used for the shared representation, so modality-specific information is not forced through the aggregation layer.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 955,
                    "end": 961,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Multimodal Inference Network"
        },
        {
            "text": "In theory, a partitioned latent space provides the possibility to encode both modality-specific and shared information in separate subspaces; in practice, however, objective L VAE does not specify what information (modality-specific or shared) should be encoded in which subspace. For example, the first loglikelihood term log p \u03b8 (x 1 | c, s 1 ) can be maximized if all information from input x 1 flows through the modality-specific encoder q \u03c6 (s 1 | x 1 ) and none through the shared encoder. Thus, we posit that the model requires an additional objective for disentangling modality-specific and modality-invariant information. Next, we formalize our notion of disentanglement and introduce suitable contrastive objectives.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multimodal Inference Network"
        },
        {
            "text": "We take an information-theoretic perspective on disentanglement and representation learning. Consider multimodal data to be a random variable X and let h 1 (X) and h 2 (X) be two functions, each of which maps the data to a lowerdimensional encoding. Consider the objective max h1,h2\u2208H",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Disentanglement of c and s"
        },
        {
            "text": "where I denotes the mutual information between two random variables and H is the set of functions that we optimize over, for instance, the parameters of a neural network. Objective (3) is maximized by an encoding that is maximally informative about the data while being maximally independent between h 1 (X) and h 2 (X). In our case, these two functions should encode modality-specific and shared factors respectively. The proposed model learns such a representation by using suitable estimators for the individual information terms. The objective optimized by a VAE can be viewed as a lower bound on the mutual information between data and encoding (e.g., see [1, 15] ). However, on itself a VAE does not suffice to learn a disentangled encoding, because of theoretical limitations on disentanglement in an unsupervised setting [25] . So in addition, we equip the VAE with two contrastive objectives: one that learns an encoding of information shared between modalities, maximizing a lower bound on I(x; c), and one that infers modality-specific factors by regularizing out shared information from the latent space of a modality-specific VAE. The overall objective that is being maximized is defined as",
            "cite_spans": [
                {
                    "start": 661,
                    "end": 664,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 665,
                    "end": 668,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 829,
                    "end": 833,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Disentanglement of c and s"
        },
        {
            "text": "where L VAE is the ELBO optimized by the VAEs, L shared learns an encoding of shared factors, L disent disentangles shared and modality-specific information, and the hyperparameters \u03b3 and \u03b4 can be used to control these terms respectively. The proposed objective estimates shared factors directly, while modalityspecific factors are inferred indirectly by regularizing out shared information from the encoding of a modality-specific VAE. Further, as in the \u03b2-VAE [13] , the reconstruction loss and KL-divergence contained in L VAE can be traded off to control the quality of reconstructions against the quality of generated samples. Figure 1 shows a schematic of the network including all loss terms that are being optimized. In the following, we define the contrastive objectives used for the approximation of the respective mutual information terms.",
            "cite_spans": [
                {
                    "start": 462,
                    "end": 466,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [
                {
                    "start": 632,
                    "end": 640,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Disentanglement of c and s"
        },
        {
            "text": "To learn shared factors, we use a contrastive objective [10, 32] that maximizes a lower bound on the mutual information I(x; c) (see Appendix A for the derivation). We estimate the mutual information with the sample-based InfoNCE estimator [29] adapted to a multimodal setting. The objective is defined as",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 60,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 61,
                    "end": 64,
                    "text": "32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 240,
                    "end": 244,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Disentanglement of c and s"
        },
        {
            "text": "where the expectation goes over K independent samples",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Disentanglement of c and s"
        },
        {
            "text": "wherex is a subset of modalitiesx \u2282 x and f is a critic that maps to a real-valued score. In particular, we use an inner product critic f \u03c6 (x,x) = c,c where c andc are the representations computed from a full multimodal sample and a subset of modalities respectively. Intuitively, the objective contrasts between a positive pair coming from the same multimodal sample and K \u2212 1 negative pairs from randomly paired samples [e.g., 11]. By using a large number of negative samples, the bound becomes tighter [29] , therefore we use a relatively large batch size of K = 1024 such that for every positive, we have 1023 negative samples by permuting the batch. In Appendix A we prove that the contrastive objective is a lower bound on I(x; c) and we further discuss the approximation as well as our choice of critic.",
            "cite_spans": [
                {
                    "start": 506,
                    "end": 510,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Disentanglement of c and s"
        },
        {
            "text": "To regularize out shared information from the encoding of a modality-specific VAE, we use a discriminator that minimizes the total correlation T C(c, s m ), a measure of statistical dependence between a group of variables. In the case of two variables, the total correlation is equivalent to the mutual information. We approximate the total correlation using the density-ratio trick [28, 34] and refer to the approximation by L disent (see Appendix A). This procedure is very similar to the one used by [20] with the important difference that we do not estimate the total correlation between all elements in a single latent representation, but between partitions c, s m of the latent space, of which c is shared between modalities. In theory, one can use a single discriminator to minimize T C(c, s) jointly, however, we found that in practice one has more control over the disentanglement by using individual terms L disent = \u03b4 m m L disent (c, s m ) weighted by separate disentanglement coefficients \u03b4 m , instead of a global \u03b4.",
            "cite_spans": [
                {
                    "start": 383,
                    "end": 387,
                    "text": "[28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 388,
                    "end": 391,
                    "text": "34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 503,
                    "end": 507,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Disentanglement of c and s"
        },
        {
            "text": "In this section, we compare our method to previous multimodal generative models both qualitatively and quantitatively. In the first experiment, we use a bimodal dataset that has been used in previous studies and compare our method to the MVAE [40] and MMVAE [31] , the current state-of-the-art multimodal generative models. In the second experiment, we go beyond two modalities and construct a dataset with 5 simplified modalities that allows us to analyze the aggregation of representations across multiple modalities, which, to the best of our knowledge, has not been done previously.",
            "cite_spans": [
                {
                    "start": 243,
                    "end": 247,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 258,
                    "end": 262,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "For the quantitative evaluation, we employ metrics that were used in previous studies. Mainly, we focus on generative coherence [31] , which takes a classifier (pretrained on the original data) to classify generated samples and computes the accuracy of predicted labels compared to a ground truth. For unconditional samples, coherence measures how often the generated samples match across all modalities. To measure the quality of generated images, we compute Fr\u00e9chet Inception Distances (FIDs) [12] . It is important to note that a generative model can have perfect coherence yet very bad sample quality (e.g., blurry images of the correct class, but without any diversity). Analogously, a model can achieve very good FID without producing coherent samples. Therefore, we also propose to compute class-specific conditional FIDs for which the set of input images is restricted to a specific class and the set of conditionally generated images is compared to images of that class only. Hence, class-specific conditional FID provides a measure of both coherence and sample quality. Finally, we evaluate the quality of the learned representations by training a linear classifier on the outputs of the encoders.",
            "cite_spans": [
                {
                    "start": 128,
                    "end": 132,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 495,
                    "end": 499,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "A popular dataset for the evaluation of multimodal generative models is the MNIST-SVHN dataset [31, 38] , which consists of digit images from two different domains, hand-written digits from MNIST [22] and street-view house numbers from SVHN [30] . The images are paired by corresponding digit labels, and similar to [31] we use 20 random pairings for each sample in either dataset. The pairing is done for the training and test sets separately and results in a training set of 1,121,360 and test set of 200,000 image pairs. The dataset is convenient for the evaluation of multimodal generative models, because it offers a clear separation between shared semantics (digit labels) and perceptual variations across modalities. This distinctive separation is required for the quantitative evaluation via generative coherence and class-specific conditional FID.",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 99,
                    "text": "[31,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 100,
                    "end": 103,
                    "text": "38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 196,
                    "end": 200,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 241,
                    "end": 245,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 316,
                    "end": 320,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "MNIST-SVHN"
        },
        {
            "text": "For a fair comparison to previous work, we employ the same architectures, likelihood distributions, and training regimes across all models. The setup is adopted from the official implementation of the MMVAE. 2 For our model we use a 20 dimensional latent space of which 10 dimensions are shared between modalities and 10 dimensions are modality-specific. 3 This does not increase the total number of parameters compared to the MMVAE or MVAE where a 20 dimensional latent space is used respectively. All implementation details are listed in Appendix C.",
            "cite_spans": [
                {
                    "start": 355,
                    "end": 356,
                    "text": "3",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "MNIST-SVHN"
        },
        {
            "text": "Qualitative Results. Figure 2 illustrates the conditional generation of SVHN given MNIST. Only our method is capable of keeping consistent styles across rows, because our model allows to draw samples from the modality-specific prior without changing the shared representation computed from the input. For both MVAE and MMVAE, we sample from the posterior to generate diverse images along one column. 4 One can already observe that our model and the MMVAE are both capable of generating images with coherent digit labels, while the MVAE struggles to produce matching digits, as already observed in [31] . The results are similar for the conditional generation of MNIST given SVHN (see Appendix B), demonstrating that our method is effective in disentangling modality-specific and shared factors in a self-supervised manner.",
            "cite_spans": [
                {
                    "start": 400,
                    "end": 401,
                    "text": "4",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 597,
                    "end": 601,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [
                {
                    "start": 21,
                    "end": 29,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "MNIST-SVHN"
        },
        {
            "text": "Quantitative Results. Since the setup of this experiment is equivalent to the one used by [31] to evaluate the MMVAE, we report the quantitative results from their paper. However, we decided to implement the MVAE ourselves, because we found that the results reported in [31] were too pessimistic. ",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 94,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 270,
                    "end": 274,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "MNIST-SVHN"
        },
        {
            "text": "Unconditional Table 1 presents linear latent classification accuracies as well as conditional and unconditional coherence results. Across all metrics, our model achieves significant improvements over previous methods. Most strikingly, joint coherence improves from 42.1% to 85.9% as a result of ex-post density estimation. As previously noted, it can be misleading to look only at latent classification and coherence, because these metrics do not capture the diversity of generated samples. Therefore, in Table 2 we also report FIDs for all models. In terms of FIDs, our model shows the best overall performance, with an exception in the conditional generation of SVHN given MNIST, for which the MVAE has slightly lower FIDs. However, looking at the results as a whole, DMVAE demonstrates a notable improvement compared to state-of-the-art multimodal generative models. Ablations across individual loss terms are provided in Appendix B.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 14,
                    "end": 21,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 505,
                    "end": 512,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Method"
        },
        {
            "text": "Ex-post Density Estimation. [8] , which we employ for sampling from the shared space of the DMVAE, proves to be very effective for improving certain metrics (Table 3 ). In particular, it can be used as an additional step after training, to improve the joint coherence and, partially, unconditional FIDs of already trained models. Note that ex-post density estimation does not influence any other metrics reported in Tables 1 and 2 (i.e., latent classification, conditional coherence, and conditional FID). ",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 31,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 157,
                    "end": 165,
                    "text": "(Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Method"
        },
        {
            "text": "To investigate how well the aggregation of shared representations works for more than two modalities, we create a modified version of the MNIST dataset, which consists of M -tuples of images that depict the same digit. We view each image in the tuple (x 1 , . . . , x M ) as coming from a different modality x m -X m , even though each instance is drawn from MNIST. Further we perturb each image with a high degree of Gaussian noise, which makes it difficult to infer digit labels from a single image (for an example, see Appendix B), and train the models as denoising variational autoencoders. We use comparable architectures, likelihoods, and training regimes across all methods. All implementation details are provided in Appendix C. The dataset is generated by repeatedly pairing M images with the same label. We vary M = 2, ...5 to investigate how the methods perform with an increasing number of modalities. This pairing is done separately for training and test data and results in 60,000 and 10,000 image M -tuples for the training and test sets respectively. The resulting dataset offers a simple benchmark that requires no modality-specific weights for the likelihood terms, has a clear characterization of shared and modality-specific factors, and allows visual inspection of the results. 5 The goal of this experiment is to test whether models are able to integrate shared information across multiple modalities and if the aggregated representation improves with more modalities. To the best of our knowledge, experiments evaluating the aggregation with more than two modalities have not been performed before. Unlike the previous experiment, paired MNIST allows measuring how well models generate a missing modality given two or more inputs. To quantify this, we measure the average coherence over leave-one-out mappings {x i } i =j \u2192 x j . Further, we compute the average class-specific conditional FID over leave-one-out mappings, which combines both coherence and generative quality in a single metric. Figure 3 presents the results for an increasing number of input modalities. The left subplot shows that for the MVAE and DMVAE leave-one-out coherence consistently improves with additional modalities, supporting our hypothesis that the PoE is effective in aggregating shared information. Notably, the MMVAE fails to take advantage of more than two modalities, as it does not have a shared representation that aggregates information. The right subplot shows that the DMVAE outperforms the other methods in class-specific conditional FIDs, demonstrating that it can achieve both high sample quality and strong coherence. We provide further metrics and ablations for this experiment in Appendix B. Fig. 3 . Results on paired MNIST with varying number of \"modalities\". Markers denote median values, error-bars standard deviations, computed across 5 runs. Left: Leaveone-out conditional coherence (higher is better). Right: Class-specific conditional FIDs (lower is better).",
            "cite_spans": [
                {
                    "start": 1299,
                    "end": 1300,
                    "text": "5",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 2018,
                    "end": 2026,
                    "text": "Figure 3",
                    "ref_id": null
                },
                {
                    "start": 2713,
                    "end": 2719,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Paired MNIST"
        },
        {
            "text": "We have introduced DMVAE, a novel multimodal generative model that learns a joint distribution over multiple modalities and disentangles modality-specific and shared factors completely self-supervised. The disentanglement allows sampling from modality-specific priors and thus facilitates the aggregation of shared information across modalities. We have demonstrated significant improvements in representation learning and generative performance compared to previous methods. Further, we have found that ex-post density estimation, that was used to sample from the shared latent space of the DMVAE, improves certain metrics dramatically when applied to trained models from existing work. This suggests that the latent space learned by multimodal generative models is more expressive than previously expected, which offers exciting opportunities for future work. Moreover, the DMVAE is currently limited to disentangling modality-specific and shared factors and one could extend it to more complex settings, such as graphs of latent factors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Deep variational information bottleneck",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "A"
                    ],
                    "last": "Alemi",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "V"
                    ],
                    "last": "Dillon",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Murphy",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Augmented CycleGAN: learning many-to-many mappings from unpaired data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Almahairi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Rajeswar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sordoni",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bachman",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Courville",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Multimodal machine learning: a survey and taxonomy",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Baltru\u0161aitis",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ahuja",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "P"
                    ],
                    "last": "Morency",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "41",
            "issn": "2",
            "pages": "423--443",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Multi-level variational autoencoder: learning disentangled representations from grouped observations",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Bouchacourt",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tomioka",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Nowozin",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Disentangled representation learning in cardiac image analysis",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chartsias",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Med. Image Anal",
            "volume": "58",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Info-GAN: interpretable representation learning by information maximizing generative adversarial nets",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Houthooft",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schulman",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Abbeel",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "StarGAN: unified generative adversarial networks for multi-domain image-to-image translation",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "W"
                    ],
                    "last": "Ha",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Choo",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "From variational to deterministic autoencoders",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ghosh",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "S M"
                    ],
                    "last": "Sajjadi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vergari",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Black",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Scholkopf",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "The incomplete Rosetta Stone problem: identifiability results for multi-view nonlinear ICA",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Gresele",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "K"
                    ],
                    "last": "Rubenstein",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mehrjou",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Locatello",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Conference on Uncertainty in Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Noise-contrastive estimation: a new estimation principle for unnormalized statistical models",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gutmann",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hyv\u00e4rinen",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Momentum contrast for unsupervised visual representation learning",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "B"
                    ],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "GANs trained by a two time-scale update rule converge to a local Nash equilibrium",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Heusel",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ramsauer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Unterthiner",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Nessler",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "beta-VAE: learning basic visual concepts with a constrained variational framework",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Higgins",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Training products of experts by minimizing contrastive divergence",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Neural Comput",
            "volume": "14",
            "issn": "8",
            "pages": "1771--1800",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Learning deep representations by mutual information estimation and maximization",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "D"
                    ],
                    "last": "Hjelm",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Disentangling by partitioning: a representation learning framework for multimodal sensory data",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "N"
                    ],
                    "last": "Hsu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Glass",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1805.11264"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Unsupervised learning of disentangled and interpretable representations from sequential data",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "N"
                    ],
                    "last": "Hsu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Glass",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "DIVA: domain invariant variational autoencoders",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ilse",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Tomczak",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Louizos",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.10427"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Variational autoencoders and nonlinear ICA: a unifying framework",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Khemakhem",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "P"
                    ],
                    "last": "Monti",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hyv\u00e4rinen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Disentangling by factorising",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mnih",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Multi-source neural variational inference",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kurle",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Guennemann",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Van Der Smagt",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Gradient-based learning applied to document recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bottou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Haffner",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proc. IEEE",
            "volume": "86",
            "issn": "11",
            "pages": "2278--2324",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Disentangled sequential autoencoder",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mandt",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "A unified feature disentangler for multi-domain image translation and manipulation",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "H"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "C"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "Y"
                    ],
                    "last": "Yeh",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "C F"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Challenging common assumptions in the unsupervised learning of disentangled representations",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Locatello",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "On the fairness of disentangled representations",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Locatello",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Abbati",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Rainforth",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bauer",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Bachem",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Weakly-supervised disentanglement without compromises",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Locatello",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Poole",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "R\u00e4tsch",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Bachem",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tschannen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Wainwright",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "IEEE Trans. Inf. Theory",
            "volume": "56",
            "issn": "11",
            "pages": "5847--5861",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Representation learning with contrastive predictive coding",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oord",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1807.03748"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Convolutional neural networks applied to house numbers digit classification",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sermanet",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chintala",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "International Conference on Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3288--3291",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Variational mixture-of-experts autoencoders for multi-modal deep generative models",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Siddharth",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Paige",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Torr",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Contrastive estimation: training log-linear models on unlabeled data",
            "authors": [
                {
                    "first": "N",
                    "middle": [
                        "A"
                    ],
                    "last": "Smith",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Eisner",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "354--362",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "The neural basis of multisensory integration in the midbrain: its organization and maturation",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "E"
                    ],
                    "last": "Stein",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "R"
                    ],
                    "last": "Stanford",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "A"
                    ],
                    "last": "Rowland",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Hear. Res",
            "volume": "258",
            "issn": "1-2",
            "pages": "4--15",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Density-ratio matching under the Bregman divergence: a unified framework of density-ratio estimation",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sugiyama",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Suzuki",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kanamori",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Ann. Inst. Stat. Math",
            "volume": "64",
            "issn": "5",
            "pages": "1009--1044",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Joint multimodal learning with deep generative models",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Suzuki",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nakayama",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Matsuo",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1611.01891"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Latent translation: crossing modalities by bridging generative models",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Engel",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1902.08261"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Is independence all you need? On the generalization of representations learned from correlated data",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Tr\u00e4uble",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.07886"
                ]
            }
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Learning factorized multimodal representations",
            "authors": [
                {
                    "first": "Y",
                    "middle": [
                        "H H"
                    ],
                    "last": "Tsai",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "P"
                    ],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zadeh",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "P"
                    ],
                    "last": "Morency",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Inverse learning of symmetry transformations",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wieser",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Parbhoo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wieczorek",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Multimodal generative models for scalable weaklysupervised learning",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Goodman",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "From perception to conception: learning multisensory representations",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Yildirim",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Transfer of object category knowledge across visual and haptic modalities: experimental and computational studies",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Yildirim",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Jacobs",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Cognition",
            "volume": "126",
            "issn": "2",
            "pages": "135--148",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Graphical model and network architecture for the special case of two modalities. Left: A sample xm from modality m is assumed to be generated by modality-specific factors sm and modality-invariant factors c. Center: Inference network that aggregates shared factors through a product of experts (PoE) layer. Dashed lines represent simulated missing modalities as used during training. Right: Decoder network (black) for modality m and loss terms (green). Dotted lines denote paths that are not being backpropagated through. Shared factors are learned by a contrastive objective which takes as input representations c andc computed from different subsets of modalities. Modality-specific factors are inferred by regularizing out shared information from the latent space of the VAE. All loss terms are defined in Subsect. 3.2. (Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Comparison of conditionally generated SVHN samples given the respective MNIST digit in the first row. Across a column, we sample from the modality-specific prior (our model) or from the posterior (other models). Only our model keeps consistent styles across rows, as it disentangles modality-specific and shared factors (without supervision).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "DMVAE 95.0 (\u00b1 0.6) 79.9 (\u00b1 1.4) 92.9 (\u00b1 1.8) 85.9 (\u00b1 1.0) 91.6 (\u00b1 0.8) 76.4 (\u00b1 0.4)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "MVAE 21.2 (\u00b1 1.1) 68.2 (\u00b1 1.9) 65.0 (\u00b1 2.2) 19.3(\u00b1 0.4)   83.8 (\u00b1 1.8) 53.6 (\u00b1 1.9) MMVAE 36.6 (\u00b1 3.1) 98.9 (\u00b1 1.5) 97.0 (\u00b1 0.6) 28.6 (\u00b1 1.1) 125.3 (\u00b1 0.8) 52.6 (\u00b1 4.8) DMVAE 15.7 (\u00b1 0.7) 57.3 (\u00b1 3.6) 67.6 (\u00b1 4.0) 18.7(\u00b1 0.9)   91.9 (\u00b1 4.4) 23.3 (\u00b1 1.0)",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Comparison of generative quality on MNIST/SVHN, where x1 corresponds to MNIST and x2 to SVHN. Numbers represent median FIDs (lower is better) computed across 5 runs with standard deviations in parentheses. For the MMVAE, we computed FIDs based on the publicly available code.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Comparison of sampling from the prior vs. using ex-post density estimation with a Gaussian mixture model (GMM) with 100 components and full covariance matrix. After training, the GMM is fitted on the embeddings computed from the training data. For FIDs, the first number refers to MNIST, the second to SVHN, respectively. Overall, ex-post density estimation improves most metrics for both MVAE and MMVAE.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements. Thanks to Mario Wieser for discussions on learning invariant subspaces, to Yuge Shi for providing code, and to Francesco Locatello for sharing his views on disentanglement in a multimodal setting. ID is supported by the SNSF grant #200021 188466.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}
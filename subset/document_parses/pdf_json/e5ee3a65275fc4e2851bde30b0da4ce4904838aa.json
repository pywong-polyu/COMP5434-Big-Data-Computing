{"paper_id": "e5ee3a65275fc4e2851bde30b0da4ce4904838aa", "metadata": {"title": "Hashing Based Prediction for Large-Scale Kernel Machine", "authors": [{"first": "Lijing", "middle": [], "last": "Lu", "suffix": "", "affiliation": {"laboratory": "", "institution": "Chinese Academy of Sciences", "location": {"settlement": "Beijing", "country": "China"}}, "email": "lulijing@iie.ac.cn"}, {"first": "Rong", "middle": [], "last": "Yin", "suffix": "", "affiliation": {"laboratory": "", "institution": "Chinese Academy of Sciences", "location": {"settlement": "Beijing", "country": "China"}}, "email": "yinrong@iie.ac.cn"}, {"first": "Yong", "middle": [], "last": "Liu", "suffix": "", "affiliation": {"laboratory": "", "institution": "Chinese Academy of Sciences", "location": {"settlement": "Beijing", "country": "China"}}, "email": "liuyong@iie.ac.cn"}, {"first": "Weiping", "middle": [], "last": "Wang", "suffix": "", "affiliation": {"laboratory": "", "institution": "Chinese Academy of Sciences", "location": {"settlement": "Beijing", "country": "China"}}, "email": "wangweiping@iie.ac.cn"}]}, "abstract": [{"text": "Kernel Machines, such as Kernel Ridge Regression, provide an effective way to construct non-linear, nonparametric models by projecting data into high-dimensional space and play an important role in machine learning. However, when dealing with large-scale problems, high computational cost in the prediction stage limits their use in realworld applications. In this paper, we propose hashing based prediction, a fast kernel prediction algorithm leveraging hash technique. The algorithm samples a small subset from the input dataset through the locality-sensitive hashing method and computes prediction value approximately using the subset. Hashing based prediction has the minimum time complexity compared to the state-of-art kernel machine prediction approaches. We further present a theoretical analysis of the proposed algorithm showing that it can keep comparable accuracy. Experiment results on most commonly used large-scale datasets, even with millionlevel data points, show that the proposed algorithm outperforms the state-of-art kernel prediction methods in time cost while maintaining satisfactory accuracy.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Kernel methods have been widely implemented in practice, allowing one to discover non-linear structure by mapping input data points into a feature space, where all pairwise inner products can be computed via a nonlinear kernel function. Kernel machines, such as Kernel Ridge Regression (KRR), have attracted a lot of attention as they can effectively approximate any function or decision boundary with enough training data [17] [18] [19] .", "cite_spans": [{"start": 423, "end": 427, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 428, "end": 432, "text": "[18]", "ref_id": "BIBREF17"}, {"start": 433, "end": 437, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Introduction"}, {"text": "Despite excellent theoretical properties, they have been limited applications in large scale learning because computing the decision function for new test samples is extremely expensive. As the scale of data increases, not only the training time will become longer, but the prediction time will also increase. However, more and more improvement methods devoted to reducing training complexity have been proposed, while fewer algorithms have been designed to improve the performance of the prediction stage [12] . As is known to us all, the Nystr\u00f6m and random feature methods are devoted to reaching faster training and prediction speed by constructing low-rank approximation of kernel matrix. Nystr\u00f6m method [4, 14, 16 ] construct a small scale subset of landmark data points by sampling to approximate the raw kernel matrix. Random feature [17, 20] maps data into a relative low-dimensional randomized feature space to improve both training and prediction speed. Random sketch [15, 23] is another family of techniques that projects the kernel matrix into a small matrix to reduce the computational requirement. Although these methods perform well and are applied into practice widely, they still need huge computational requirements in the prediction stage when faced with large-scale datasets. Based on the innovation of further reducing the computational costs when dealing with large-scale datasets, we attempt to develop a fast prediction algorithm for kernel methods.", "cite_spans": [{"start": 506, "end": 510, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 708, "end": 711, "text": "[4,", "ref_id": "BIBREF3"}, {"start": 712, "end": 715, "text": "14,", "ref_id": "BIBREF13"}, {"start": 716, "end": 718, "text": "16", "ref_id": "BIBREF15"}, {"start": 841, "end": 845, "text": "[17,", "ref_id": "BIBREF16"}, {"start": 846, "end": 849, "text": "20]", "ref_id": "BIBREF19"}, {"start": 978, "end": 982, "text": "[15,", "ref_id": "BIBREF14"}, {"start": 983, "end": 986, "text": "23]", "ref_id": "BIBREF22"}], "ref_spans": [], "section": "Introduction"}, {"text": "Hashing is an efficient algorithm to solve the approximate large-scale nearest Neighbor search problem. The main idea of the hashing method is to construct a family of hash functions to map the data points into a binary feature vector such that the produced hash code preserves the structure of the original space [9] . In recent years, a large number of effective hashing methods have emerged, but they are rarely used in the prediction of kernel machines. Charikar and Siminelakis presented a hashing-based framework for kernel density estimate problem in [5] . Inspired by this paper, we consider applying the hashing algorithm to the prediction stage of kernel machine and proposed the hashing based prediction (HBP) algorithm. The algorithm leverages locality-sensitive hashing (LSH) [6, 8] method to search the nearest neighbors of the test data point to approximately compute the decision value in the kernel prediction problem.", "cite_spans": [{"start": 314, "end": 317, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 558, "end": 561, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 789, "end": 792, "text": "[6,", "ref_id": "BIBREF5"}, {"start": 793, "end": 795, "text": "8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Introduction"}, {"text": "Specifically, our HBP algorithm consists of two stages: the pre-processing stage and the query stage. LSH was used to find the neighbors of the test point as the sampled subset in the pre-processing stage. And in the query stage, the samples are used to approximately compute the decision value. We provide a theoretical analysis that we can compute the decision function in O( log(n/ \u03c4 ) \u03c4 0.5 2.5 ) time cost with accuracy guarantees \u0177 \u2212 y * p \u2264 \u00b7 (3\u03c4 n 1/p + y * p ), where , \u03c4 \u2208 (0, 1). It is novel to use the hash method directly in the KRR problem which is an attempt to break through the traditional method. As will be demonstrated later, experiment results on most commonly used large-scale datasets, even with million-level data points, show that the proposed HBP algorithm outperforms the state-of-art kernel prediction methods in time cost while maintaining satisfactory accuracy.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "The remainder of the paper is organized as below: We start with presenting related work in Sect. 2, and the background material is given in Sect. 3. In Sect. 4, we introduce our hashing-based prediction algorithm for the prediction of kernel machine, while in Sect. 5, we discuss the theoretical analysis. Section 6 presents the experimental results on real-world datasets. The following section is conclusions and the last section presents proof of theoretical results.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "In this section, we will introduce several important works on reducing time complexity in the prediction stage of KRR, and most of the algorithms can be used to other kernel machines [11] . Practical methods are proposed to overcome the computational and memory bottleneck of kernel machines. One popular family of techniques is based on low-rank approximation of the kernel matrix to improve both training and prediction speed. Nystr\u00f6m [13, 14, 22] method is one of the most well-known representation methods. Nystr\u00f6m samples a subset C of m columns to approximate the kernel matrix G \u2208 R n\u00d7n . Typically, the subset of columns is randomly selected by uniform sampling without replacement [13, 22] . Recently, more and more outstanding extension methods of Nystr\u00f6m are proposed to solve the KRR problem. For example, an accurate and scalable Nystr\u00f6m scheme has been proposed in [14] which first samples a large column subset from the input matrix, but then only performs an approximate SVD on the inner submatrix by using the recent randomized low-rank matrix approximation algorithms. [16] present a new Nystr\u00f6m algorithm based on recursive leverage score sampling. Based on Nystr\u00f6m method, a fast prediction method called DC-Pred++ was proposed in [11] . However, the scale of the subset in Nystr\u00f6m method can't be too small to keep the accuracy, and as a result, the lower bound of computational complexity is limited. Random features [17, 20] project the data into a relative low-dimensional feature space where the inner product between a pair of input points approximates their kernel evaluation so as to reduce the computation requirement of training and prediction stage. Another family of techniques is random sketches which improving the computational speed by projecting the kernel matrix into a small matrix [15, 23] . As shown in [23] , a simple hash method was applied to generate the randomized sketch matrix.", "cite_spans": [{"start": 183, "end": 187, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 437, "end": 441, "text": "[13,", "ref_id": "BIBREF12"}, {"start": 442, "end": 445, "text": "14,", "ref_id": "BIBREF13"}, {"start": 446, "end": 449, "text": "22]", "ref_id": "BIBREF21"}, {"start": 690, "end": 694, "text": "[13,", "ref_id": "BIBREF12"}, {"start": 695, "end": 698, "text": "22]", "ref_id": "BIBREF21"}, {"start": 879, "end": 883, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 1087, "end": 1091, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 1251, "end": 1255, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 1439, "end": 1443, "text": "[17,", "ref_id": "BIBREF16"}, {"start": 1444, "end": 1447, "text": "20]", "ref_id": "BIBREF19"}, {"start": 1821, "end": 1825, "text": "[15,", "ref_id": "BIBREF14"}, {"start": 1826, "end": 1829, "text": "23]", "ref_id": "BIBREF22"}, {"start": 1844, "end": 1848, "text": "[23]", "ref_id": "BIBREF22"}], "ref_spans": [], "section": "Related Work"}, {"text": "Hashing algorithms [6, 8, 9] have been a continuously \"hot topic\" since it birth because of its superior performance in the Approximate Nearest Neighbor Search. However, hashing algorithms have not widely been used in kernel prediction problems. Inspired by the idea of \"sample\" of the Nystr\u00f6m method, we consider employing Locality-sensitive hashing (LSH) to sample the subset and approximately compute the result using the subset. Compared with the currently optimal and most popular Nystr\u00f6m methods, our HBP method can further greatly reduce the number of samples while ensuring accuracy. HBP reduces computational efficiency from O(n) to O( log(n/ \u03c4 ) \u03c4 0.5 2.5 ) with less accuracy loss \u0177 \u2212 y * p \u2264 \u00b7 (3\u03c4 n 1/p + y * p ).", "cite_spans": [{"start": 19, "end": 22, "text": "[6,", "ref_id": "BIBREF5"}, {"start": 23, "end": 25, "text": "8,", "ref_id": "BIBREF7"}, {"start": 26, "end": 28, "text": "9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Related Work"}, {"text": "This paper focuses on the typical kernel machines: KRR. [7, 10, 21] . Given dataset", "cite_spans": [{"start": 56, "end": 59, "text": "[7,", "ref_id": "BIBREF6"}, {"start": 60, "end": 63, "text": "10,", "ref_id": "BIBREF9"}, {"start": 64, "end": 67, "text": "21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Preliminaries"}, {"text": "denotes the kernel function value of the two points x i , x j ,\u03b1 \u2208 R n in the formula is generated in the training process by solving the Fig. 1 . Schematic diagram of algorithm structure. Given the training data points and a test point q, the HBP algorithm samples some hash functions and construct a separate hash table for each hash function leveraging LSH in the pre-processing stage. In the query stage, for each hash table, we sample a point randomly from the hash bucket that the test point maps to. below optimization problem:", "cite_spans": [], "ref_spans": [{"start": 138, "end": 144, "text": "Fig. 1", "ref_id": null}], "section": "Preliminaries"}, {"text": "is the response vector, \u03bb > 0 is the regularization parameter. Our goal in the prediction process is to compute the decision value of a testing datax, following the equation:", "cite_spans": [], "ref_spans": [], "section": "Preliminaries"}, {"text": "whose time cost is O(n). The problem to solve Eq. (2) is challenging as n increases. To improve the speed of kernel prediction, we proposed the following HBP algorithm.", "cite_spans": [], "ref_spans": [], "section": "Preliminaries"}, {"text": "The proposed HBP method is an importance sampling algorithm. The algorithm solves the problem of designing a model that given a set of data points", "cite_spans": [], "ref_spans": [], "section": "Hashing Based Prediction"}, {"text": "\u2282 R d and a kernel function k, returns the approximation of the decision value of a test pointx:", "cite_spans": [], "ref_spans": [], "section": "Hashing Based Prediction"}, {"text": ". Given a hashing based data structure, HBP performs a two-stage computation to approximate the prediction result. The architecture of HBP is illustrated in Fig. 1 . In the pre-processing stage, we select data points which close to the test data point as the sampling points. In the query stage, we use the sampling points to estimate the response value. As shown in Fig. 1 , the hashing technique plays an important role in the proposed method. The hashing technique we leverage is called Localizationsensitive-hashing (LSH). Before explaining the algorithm in detail, we will first introduce the LSH method.", "cite_spans": [], "ref_spans": [{"start": 157, "end": 163, "text": "Fig. 1", "ref_id": null}, {"start": 367, "end": 373, "text": "Fig. 1", "ref_id": null}], "section": "Hashing Based Prediction"}, {"text": "Localization-sensitive hashing (LSH) is an approximate neighbor search technology. The key idea of LSH is that two adjacent data points in the original data space are mapped by the same projection, the probability that these two data points are still adjacent in new data space is high. For example, the LSH method employed by this paper map a d dimension vector x into a \u03c1 dimension vector consisting entirely of 0 and 1, where \u03c1 < d. The vector of \u03c1 dimension is the hash value. If two data points get the same hash value through the hashing technique, it is said that the two data points fall into the same hash bucket. The construction goal of the hash scheme is to make the adjacent points in the original data space fall into the same hash bucket through the hashing function. So the hash functions need to meet two conditions shown in definition 2. We will first introduce the definition of collision probability which indicates the probability that two data points are mapped to the same hash bucket through the given hash function.", "cite_spans": [], "ref_spans": [], "section": "Localization-Sensitive Hashing"}, {"text": "The collision probability of two data points x andx is closely related to the distance between the two points. The larger the distance, the smaller the collision probability.", "cite_spans": [], "ref_spans": [], "section": "Definition 1. Given a hashing scheme H, the collision probability between two elements"}, {"text": "Let D(\u00b7, \u00b7) be a distance function of elements from a dataset S, and for any x \u2208 S, let B(x, r) denote the set of elements from S within the distance r from x.", "cite_spans": [], "ref_spans": [], "section": "Definition 1. Given a hashing scheme H, the collision probability between two elements"}, {"text": "In order to guarantee the hash functions in a locality-sensitive family to be useful, it has to satisfy that p 1 > p 2 and r 1 < r 2 . The hash methods employed by this paper are presented in Algorithm 1.", "cite_spans": [], "ref_spans": [], "section": "Definition 2. A family of functions"}, {"text": "For every x,x \u2208 R n , the LSH family H constructed by Algorithm 1 satisfies:", "cite_spans": [], "ref_spans": [], "section": "Proposition 1."}, {"text": "The proof is shown in Sect. 8 ", "cite_spans": [{"start": 28, "end": 29, "text": "8", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Proposition 1."}, {"text": "As mentioned before, HBP uses LSH to create a two-stage structure to compute the decision value approximately. Now, we will explain the main algorithm in detail.", "cite_spans": [], "ref_spans": [], "section": "Hashing Based Prediction"}, {"text": "Pre-processing. In the pre-processing stage, training data points are hashed to different buckets according to the hash function.", "cite_spans": [], "ref_spans": [], "section": "Hashing Based Prediction"}, {"text": "1) Randomly construct L hash functions h 1 ...h L from hash scheme H. 2) For each hash function h j , sample a subset from dataset X j \u2208 X, every point in X j is selected with probability \u03b4 = L n . In order to reduce computational complexity, we only run the hash process on a small subset rather than the whole training set. The result of our theorem proves that the method can still guarantee accuracy.", "cite_spans": [], "ref_spans": [], "section": "Hashing Based Prediction"}, {"text": "3) For each hash function, each data point in X j is mapped to a hash value through the hash function. All data points in X j of the same hash value constitute a hash bucket. And all hash buckets form a hash table corresponding to the hash function. 4) L hash functions can produce L hash tables.", "cite_spans": [], "ref_spans": [], "section": "Hashing Based Prediction"}, {"text": "Query. In the query stage, the sampling points were used to estimate the response value. ", "cite_spans": [], "ref_spans": [], "section": "Hashing Based Prediction"}, {"text": "where b j (x) = {x \u2208 X j : h j (x) = h j (x)} represents the set of the elements in the hash bucket where the pointx mapped, and |b j (x)| represents the number of elements in b j (x). 4) Compute the accurate prediction value by averaging all of the samples produced by hash tables:\u0177", "cite_spans": [], "ref_spans": [], "section": "Hashing Based Prediction"}, {"text": "5) The approximate prediction value can achieve the accuracy of \u0177 \u2212 y * p \u2264 \u00b7 (3\u03c4 n 1/p + y * p ) with the time and space cost of O( log(n/ \u03c4 ) \u03c4 0.5 2.5 ). ", "cite_spans": [], "ref_spans": [], "section": "Hashing Based Prediction"}, {"text": "The problem of our proposed algorithm aims to solve is to obtain an approximation\u0177 to y * = n i=1\u03b1 i k (x i ,x). In this section, we introduce the theoretical bound of the proposed algorithm.", "cite_spans": [], "ref_spans": [], "section": "Theoretical Assessments"}, {"text": "then we can compute an approximate vector\u0177 for kernel prediction in time O( log(n/ \u03c4 ) \u03c4 0.5 2.5 ) such that with probability at least 1 \u2212 n \u22121 for all i \u2208 [n] it holds |\u0177 i \u2212 y * i | \u2264 3 \u03c4 + |y * i | and \u0177 \u2212 y * p \u2264 \u00b7 (3\u03c4 n 1/p + y * p ). The proof of Theorem 1 is in Sect. 8.2. \u03c4 \u2208 (0, 1) in Theorem 1 denotes a lower bound of 1 n i k (x i ,x). As shown in Eq. (3) in Sect. 4.2, the hashing scheme constructed by HBP algorithm satisfies the condition in Theorem 1. The above result shows the upper bound of error and the time complexity of the HBP method. As discussed before, the computational complexity of kernel prediction is O(n), while we need only O( log(n/ \u03c4 ) \u03c4 0.5 2.5 ) with accuracy guarantees \u0177 \u2212 y * p \u2264 \u00b7 (3\u03c4 n 1/p + y * p ).", "cite_spans": [], "ref_spans": [], "section": "Theorem 1. Given a kernel k, if there exists a distribution H of hash functions and M \u2265 1 such that for every"}, {"text": "We evaluate the efficiency and effectiveness of the proposed algorithm by experiments on 3 large-scale datasets. The experiments with these datasets use the Laplacian kernel", "cite_spans": [], "ref_spans": [], "section": "Experiments"}, {"text": "All of the experiments are conducted on a server with 2.40 GHZ Inter(R) Xeon(R) E5-2630 v3 CPU and 32 GB of RAM in Matlab.", "cite_spans": [], "ref_spans": [], "section": "Experiments"}, {"text": "The performance of our proposed algorithm is presented on three large-scale real-world datasets: covtype 1 , SUSY 2 and Census 3 , which are generally used in the field of kernel machines [18, 19] . The details of the datasets are shown in Table 1 .", "cite_spans": [{"start": 188, "end": 192, "text": "[18,", "ref_id": "BIBREF17"}, {"start": 193, "end": 196, "text": "19]", "ref_id": "BIBREF18"}], "ref_spans": [{"start": 240, "end": 247, "text": "Table 1", "ref_id": "TABREF1"}], "section": "Datasets Preparation"}, {"text": "We randomly selected 2.5 \u00d7 10 5 data points on each dataset. The features of datasets have been normalized. 70% of instances are used for training experiment and the rest for prediction. We measure the error by calculating root-meansquare error (RMSE) for regression problems and calculating the classification error for classification problems.", "cite_spans": [], "ref_spans": [], "section": "Datasets Preparation"}, {"text": "Our experiment contains two parts. Every experiment is repeated 10 times to avoid contingency. The final result is the average value of 10 experiments. To focus on the effectiveness of our algorithm in the prediction stage, we consider the model\u03b1 for kernel machine is given in the first part of our experiment. We randomly generate\u03b1 and compare the error between the results of our approximation method and the exact value without using any approximation algorithm as well as the time complexity of the two methods. Figure 2 shows the error and prediction time concerning the number of hash table L. The horizontal coordinate represents the number of hash table L. The vertical coordinate in the left represents the average errors of the prediction value between our algorithm and the exact algorithm, and the vertical coordinate in the right represent the prediction time of that two algorithms. With the increase of L, the error decreases at the beginning, and the rate of decrease become slow when L increases to a value. The prediction time of HBP increases with the increase of L. Our algorithm has a significant advantage over the accurate algorithm in prediction time.", "cite_spans": [], "ref_spans": [{"start": 517, "end": 525, "text": "Figure 2", "ref_id": "FIGREF1"}], "section": "Performance of Hashing Based Prediction"}, {"text": "In the second part of our experiment, we also compare our algorithm with 2 representative methods. Our algorithm employs KRR to finish the training process. For ensuring fairness, we use the same way to tune parameters \u03c3 in 2 [\u22122:0.5 :5] and \u03bb in 2 [\u221240:3:8] on each dataset and algorithm. The selected parameters are sufficient to achieve satisfactory results, although they may not be optimal.", "cite_spans": [{"start": 234, "end": 237, "text": ":5]", "ref_id": null}], "ref_spans": [], "section": "Performance of Hashing Based Prediction"}, {"text": "The general introduction of the methods used in the experiment is as follows:", "cite_spans": [], "ref_spans": [], "section": "Performance of Hashing Based Prediction"}, {"text": "1) HBP: Our proposed algorithm, which uses Locality-Sensitive Hashing (LSH) for importance sampling to generate a fast prediction method for kernel machines. 2) Nystr\u00f6m [14] : An accurate and scalable Nystr\u00f6m scheme that made largescale Nystr\u00f6m approximation possible. The algorithm first samples a large column subset from the input matrix, then only performs an approximate SVD on the inner submatrix.", "cite_spans": [{"start": 169, "end": 173, "text": "[14]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Performance of Hashing Based Prediction"}, {"text": "3) Recursive RLS-Nystr\u00f6m [16] : A recently Nyst\u00f6m scheme using leverage score sampling. Table 2 shows the experiment result of our algorithm and other methods mentioned before. Our algorithm is at a faster prediction speed than other methods. The larger the scale of data, the more obvious the time advantage of the proposed algorithm is. Simultaneously, HBP keeps the optimal value or just a little gap with the optimal, which validates the effectiveness of our algorithm.", "cite_spans": [{"start": 25, "end": 29, "text": "[16]", "ref_id": "BIBREF15"}], "ref_spans": [{"start": 88, "end": 95, "text": "Table 2", "ref_id": "TABREF3"}], "section": "Performance of Hashing Based Prediction"}, {"text": "We propose an algorithm HBP to effectively reduce the prediction cost of kernel machines on large-scale datasets. The algorithm opens the door of solving the kernel prediction problem by the hashing method. By using Locality-sensitive hashing (LSH) for importance sampling to achieve approximate calculation, the HBP algorithm reduces computational complexity and storage cost with less prediction accuracy loss. The experimental analysis on large-scale datasets showed that our HBP algorithm outperforms previous state-of-art solutions.", "cite_spans": [], "ref_spans": [], "section": "Conclusions"}, {"text": "Since LSH is the data-independent hash scheme, the result of the approximate search is not extremely accurate. In the future, we intend to replace the LSH with the existing data-dependent hash algorithm to further reduce the prediction errors. For example, the data-dependent hashing schemes designed by [1] and [2] can be attempted to apply to our hashing based prediction method.", "cite_spans": [{"start": 304, "end": 307, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 312, "end": 315, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Conclusions"}, {"text": "In this section, we will provide proofs of some theorems in this paper. Section 8.1 presents the proof of Proposition 1 which references [3] . In Sect. 8.2, we present the proof of Theorem 1 which is our main theoretical result.", "cite_spans": [{"start": 137, "end": 140, "text": "[3]", "ref_id": "BIBREF2"}], "ref_spans": [], "section": "Proof"}, {"text": "Proof. First assume that the data point x,x is one-dimensional. For the sake of clarity, we will describe LSH families H such that Pr", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "e \u2212 x\u2212x 1 /\u03c3 . The Proposition 1 then follows simply by doubling the bandwidth \u03c3. As shown in the Algorithm 1, we randomly sample \u03be \u2208 [0, 1] and set the hash", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "Then we consider the case that x,x \u2208 [0, 1] d , applying this to a random dimension \u03b6 \u2208 {1.....d}, we get:", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "We repeat \u03c1 times independently, then,", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "At last, we sample \u03c1 \u223c P ossion(d/\u03c3), ", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "We may assume that all elements of\u03b1 are positive otherwise we apply our algorithm to\u03b1 + and\u03b1 \u2212 separately. The vector\u03b1 can be geometrically divided into S 1 , ..., S T such that all elements in each group differ by at most a factor of two, where T = log 2 (n/ \u03c4 ). Therefore, our problem can be expressed as T subproblem.", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "On the t th subproblem, our estimated response value is:", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "where I is a random index from H(x) \u2286 S t , A t = i\u2208St\u03b1 i . Now we bound the variance:", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "The last term can be split into two expressions:", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "and 1 \u03b4 2", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "Since j = i in Eq. (16), we have:", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "Therefore, Eq. (16) is upper bounded by i\u03b1 Using the inequalities and the definition in the theorem ", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "and therefore Eq. (17) is upper bounded by", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "Since 1 \u2264 2 \u2212 \u03b2 and \u03c9 i \u2264 1, it is easy to get:", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "And we have known that \u03b4 = 1 n\u03c4 1\u2212\u03b2 \u2265 1 n\u03bc 1\u2212\u03b2 , so we get the upper bound of Eq. (17) is 4M\u03bc 2\u2212\u03b2 .", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "Equation (16) ", "cite_spans": [], "ref_spans": [], "section": "The Proof of Proposition 1"}, {"text": "It is obvious that the variance of our estimator is at most 4 times larger than the variance bound of kernel density estimate using Hashing-based-estimate method which is (M 3 + M ) \u00b7 \u03bc 2\u2212\u03b2 . To derive Theorem 1, set \u03b2 = 1/2. It is sufficient to obtain a (1+ )-approximation for the t th subproblem over O( 1 \u03c4 0.5 2.5 ) independent samples. For all t \u2208 T , the overhead of the whole process is at most a multiplicative factor T = log(n/ \u03c4 ) compared to the case that we were creating a single data-structure for the same problem. Combining with the conclusion of [5] , we obtain a straightforward analysis of the estimation error of the algorithm that for all i \u2208 [n] with probability at least 1 \u2212 n \u22121 , it holds |\u0177 i \u2212 y * i | \u2264 3 \u03c4 + |y * i |. Summing over all indices and using triangle inequality gives \u0177 \u2212 y * p \u2264 \u00b7 (3\u03c4 n 1/p + y * p ).", "cite_spans": [{"start": 564, "end": 567, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "The Proof of Proposition 1"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Optimal hashingbased time-space trade-offs for approximate near neighbors", "authors": [{"first": "A", "middle": [], "last": "Andoni", "suffix": ""}, {"first": "T", "middle": [], "last": "Laarhoven", "suffix": ""}, {"first": "I", "middle": [], "last": "Razenshteyn", "suffix": ""}, {"first": "E", "middle": [], "last": "Waingarten", "suffix": ""}], "year": 2017, "venue": "Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms", "volume": "", "issn": "", "pages": "47--66", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Optimal data-dependent hashing for approximate near neighbors", "authors": [{"first": "A", "middle": [], "last": "Andoni", "suffix": ""}, {"first": "I", "middle": [], "last": "Razenshteyn", "suffix": ""}], "year": 2015, "venue": "Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing", "volume": "", "issn": "", "pages": "793--801", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Space and time efficient kernel density estimation in high dimensions", "authors": [{"first": "A", "middle": [], "last": "Backurs", "suffix": ""}, {"first": "P", "middle": [], "last": "Indyk", "suffix": ""}, {"first": "T", "middle": [], "last": "Wagner", "suffix": ""}], "year": 2019, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "15773--15782", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "NYTRO: when subsampling meets early stopping", "authors": [{"first": "R", "middle": [], "last": "Camoriano", "suffix": ""}, {"first": "T", "middle": [], "last": "Angles", "suffix": ""}, {"first": "A", "middle": [], "last": "Rudi", "suffix": ""}, {"first": "L", "middle": [], "last": "Rosasco", "suffix": ""}], "year": 2016, "venue": "Artificial Intelligence and Statistics", "volume": "", "issn": "", "pages": "1403--1411", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Hashing-based-estimators for kernel density in high dimensions", "authors": [{"first": "M", "middle": [], "last": "Charikar", "suffix": ""}, {"first": "P", "middle": [], "last": "Siminelakis", "suffix": ""}], "year": 2017, "venue": "2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)", "volume": "", "issn": "", "pages": "1032--1043", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Locality-sensitive hashing scheme based on p-stable distributions", "authors": [{"first": "M", "middle": [], "last": "Datar", "suffix": ""}, {"first": "N", "middle": [], "last": "Immorlica", "suffix": ""}, {"first": "P", "middle": [], "last": "Indyk", "suffix": ""}, {"first": "V", "middle": ["S"], "last": "Mirrokni", "suffix": ""}], "year": 2004, "venue": "Proceedings of the Twentieth Annual symposium on Computational Geometry", "volume": "", "issn": "", "pages": "253--262", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Applications of Empirical Process Theory", "authors": [{"first": "S", "middle": ["A"], "last": "Van De Geer", "suffix": ""}], "year": 2000, "venue": "", "volume": "91", "issn": "", "pages": "", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Similarity search in high dimensions via hashing", "authors": [{"first": "A", "middle": [], "last": "Gionis", "suffix": ""}, {"first": "P", "middle": [], "last": "Indyk", "suffix": ""}, {"first": "R", "middle": [], "last": "Motwani", "suffix": ""}], "year": 1999, "venue": "", "volume": "99", "issn": "", "pages": "518--529", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Fast supervised discrete hashing", "authors": [{"first": "J", "middle": [], "last": "Gui", "suffix": ""}, {"first": "T", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Z", "middle": [], "last": "Sun", "suffix": ""}, {"first": "D", "middle": [], "last": "Tao", "suffix": ""}, {"first": "T", "middle": [], "last": "Tan", "suffix": ""}], "year": 2017, "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "volume": "40", "issn": "2", "pages": "490--496", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "authors": [{"first": "T", "middle": [], "last": "Hastie", "suffix": ""}, {"first": "R", "middle": [], "last": "Tibshirani", "suffix": ""}, {"first": "J", "middle": [], "last": "Friedman", "suffix": ""}], "year": 2009, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1007/978-0-387-84858-7"]}}, "BIBREF10": {"ref_id": "b10", "title": "Fast prediction for large-scale kernel machines", "authors": [{"first": "C", "middle": ["J"], "last": "Hsieh", "suffix": ""}, {"first": "S", "middle": [], "last": "Si", "suffix": ""}, {"first": "I", "middle": ["S"], "last": "Dhillon", "suffix": ""}], "year": 2014, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "3689--3697", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "A hash based method for large scale nonparallel support vector machines prediction", "authors": [{"first": "X", "middle": [], "last": "Ju", "suffix": ""}, {"first": "T", "middle": [], "last": "Wang", "suffix": ""}], "year": 2017, "venue": "Proc. Comput. Sci", "volume": "108", "issn": "", "pages": "1281--1291", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Sampling techniques for the Nystrom method", "authors": [{"first": "S", "middle": [], "last": "Kumar", "suffix": ""}, {"first": "M", "middle": [], "last": "Mohri", "suffix": ""}, {"first": "A", "middle": [], "last": "Talwalkar", "suffix": ""}], "year": 2009, "venue": "Artificial Intelligence and Statistics", "volume": "", "issn": "", "pages": "304--311", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Making large-scale Nystr\u00f6m approximation possible", "authors": [{"first": "M", "middle": [], "last": "Li", "suffix": ""}, {"first": "J", "middle": ["T"], "last": "Kwok", "suffix": ""}, {"first": "B", "middle": ["L"], "last": "Lu", "suffix": ""}], "year": 2010, "venue": "ICML", "volume": "", "issn": "", "pages": "631--638", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Sharp theoretical analysis for nonparametric testing under random projection", "authors": [{"first": "M", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Z", "middle": [], "last": "Shang", "suffix": ""}, {"first": "G", "middle": [], "last": "Cheng", "suffix": ""}], "year": 2019, "venue": "Conference on Learning Theory", "volume": "", "issn": "", "pages": "2175--2209", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Recursive sampling for the Nystrom method", "authors": [{"first": "C", "middle": [], "last": "Musco", "suffix": ""}, {"first": "C", "middle": [], "last": "Musco", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "3833--3845", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Random features for large-scale kernel machines", "authors": [{"first": "A", "middle": [], "last": "Rahimi", "suffix": ""}, {"first": "B", "middle": [], "last": "Recht", "suffix": ""}], "year": 2008, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "1177--1184", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "Less is more: Nystr\u00f6m computational regularization", "authors": [{"first": "A", "middle": [], "last": "Rudi", "suffix": ""}, {"first": "R", "middle": [], "last": "Camoriano", "suffix": ""}, {"first": "L", "middle": [], "last": "Rosasco", "suffix": ""}], "year": 2015, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "1657--1665", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "FALKON: an optimal large scale kernel method", "authors": [{"first": "A", "middle": [], "last": "Rudi", "suffix": ""}, {"first": "L", "middle": [], "last": "Carratino", "suffix": ""}, {"first": "L", "middle": [], "last": "Rosasco", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "3888--3898", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Generalization properties of learning with random features", "authors": [{"first": "A", "middle": [], "last": "Rudi", "suffix": ""}, {"first": "L", "middle": [], "last": "Rosasco", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "3215--3225", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Kernel Methods for Pattern Analysis", "authors": [{"first": "J", "middle": [], "last": "Shawe-Taylor", "suffix": ""}, {"first": "N", "middle": [], "last": "Cristianini", "suffix": ""}], "year": 2004, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "Using the Nystr\u00f6m method to speed up kernel machines", "authors": [{"first": "C", "middle": ["K"], "last": "Williams", "suffix": ""}, {"first": "M", "middle": [], "last": "Seeger", "suffix": ""}], "year": 2001, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "682--688", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "Incremental randomized sketching for online kernel learning", "authors": [{"first": "X", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "S", "middle": [], "last": "Liao", "suffix": ""}], "year": 2019, "venue": "International Conference on Machine Learning", "volume": "", "issn": "", "pages": "7394--7403", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Dataset {xi, yi} n i=1 ; test data pointx \u2208 R d ; kernel k(\u00b7, \u00b7); LSH family H; interger 1 \u2264 L \u2264 n; the result of training stage\u03b1; Ensure: Estimated response value\u0177. 1: Pre-processing: 2: For j = 1.....L: 3: Sample a random hash function hj from H. 4: Get sampling dataset Xj \u2282 X, each point of Xj was selected with independent probability \u03b4 = L n . 5: For every x \u2208 Xj, compute the hash value hj(x). 6: Query: 7: For j = 1.....L: 8: Compute the hash value of test data hj(x). 9: Sample a random point x (j) from bj(x) = {x \u2208 X j : hj(x) = hj(x)}. 10: Let Zj \u2190\u2212\u03b1 j n i=1\u03b1 j \u00b7 k(x (j) ,x)\u00b7|b j (x)| \u03b4\u00b7p(x (j) ,x)", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Validate error between the HBP algorithm and the exact prediction method as well as the average prediction time for each test point of the two methods with respect to the number of hash table L on 2 large scale datasets: Census and covtype.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "p j . As 0 \u2264 2 \u2212 2\u03b2 \u2264 1 and 0 \u2264 \u03b2 \u2264 1,", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Z ] \u2264 4(M 3 + M )\u03bc 2\u2212\u03b2 .", "latex": null, "type": "figure"}, "TABREF0": {"text": ".1. After all input data points have been hashed into hash buckets, one can determine neighbors of query point by hashing the query point and searching elements in the bucket containing that query point. Algorithm 1. Localization-sensitive hashing. The hash value is the concatenation of b1.....b\u03c1.", "latex": null, "type": "table"}, "TABREF1": {"text": "Compute the hash value of the test pointx through each hash function, and map the test pointx to a hash bucket for each hash table. 2) For each hash table, randomly select a data point x (j) , j \u2208 (1, L) from the hash bucket to which the test point is mapped if that hash bucket is not empty except for the test pointx. The L hash tables allow us to produce at most L independent samples because if the hash bucket where the test point is mapped is empty except for the test point, we can't get a sample for this hash table. 3) Compute the estimated values using every sample point:", "latex": null, "type": "table"}, "TABREF2": {"text": "Datasets used in this paper.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Dataset covtype </td><td>Instance </td><td>Feature </td><td>Type </td><td>Bandwidth\n</td></tr><tr><td>581,012 </td><td>54 </td><td>Multi-classification </td><td>0.1\n</td></tr><tr><td>SUSY </td><td>1,000,000 </td><td>18 </td><td>Bi-classification </td><td>0.1\n</td></tr><tr><td>Census </td><td>2,458,285 </td><td>68 </td><td>Regression </td><td>0.05\n</td></tr></table></body></html>"}, "TABREF3": {"text": "Comparison of prediction time and test error in solving KRR problem between HBP, Nystr\u00f6m and RLS-Nystr\u00f6m on covtype, SUSY and Census datasets. We bold the numbers of the best algorithm.", "latex": null, "type": "table"}, "TABREF4": {"text": "Note 1. Given a dataset X = {x 1 , ....., x n } \u2282 R d and a test data pointx \u2208 R d . \u03c9 i = k (x i ,x) denoted the kernel value of data point x i and test pointx. Let H be a family of hash function. For every x i , p i = Pr h\u223cH [h(x i ) = h(x)] denoted the collision probability withx. b h (x) = {i : h(x i ) = h(x)} is the set of points with the same hash value asx. Proof. In our algorithm, we hash each point only with probability \u03b4 = 1/(n\u03c4 1\u2212\u03b2 ), where \u03c4 \u2264 1 n i \u03c9 i . Set r 1 , ......, r n be Bernoulli random variables with Pr [r", "latex": null, "type": "table"}, "TABREF5": {"text": "i \u03c9 i = \u03bc, Eq. (16) is upper bounded by 4M 3 \u03bc 2\u2212\u03b2 . We observe that E", "latex": null, "type": "table"}, "TABREF6": {"text": "Table 2. Comparison of prediction time and test error in solving KRR problem between HBP, Nystro\u0308m and RLS-Nystro\u0308m on covtype, SUSY and Census datasets. We bold the numbers of the best algorithm.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Dataset covtype </td><td>Metric </td><td>HBP </td><td>Nystro\u0308m </td><td>RLS-Nystro\u0308m\n</td></tr><tr><td>Time (s) error </td><td>4.409 </td><td>73.154 </td><td>73.311\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>0.2874\u00b10.00003 </td><td>1.4919\u00b1 0.00263 </td><td>0.7713\u00b1 0.00890\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>L = 50 </td><td>m = 2500 </td><td>m = 2500\n</td></tr><tr><td>SUSY </td><td>Time (s) error </td><td>2.974 </td><td>24.130 </td><td>24.211\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>0.6471 \u00b1 0.0000 </td><td>0.6388\u00b1 0.00169 </td><td>0.6552 \u00b1 0.00036\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>L = 50 </td><td>m = 2500 </td><td>m = 2500\n</td></tr><tr><td>Census </td><td>Time (s) error </td><td>5.354 </td><td>87.970 </td><td>87.683\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>0.2867\u00b10.00079 </td><td>0.2900 \u00b1 0.00163 </td><td>1.2297 \u00b1 0.00712\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>L = 50 </td><td>m = 2500 </td><td>m = 2500\n</td></tr></table></body></html>"}}, "back_matter": []}
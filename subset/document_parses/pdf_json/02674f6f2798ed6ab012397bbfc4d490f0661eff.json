{
    "paper_id": "02674f6f2798ed6ab012397bbfc4d490f0661eff",
    "metadata": {
        "title": "Multi-Objective Model Selection for Time Series Forecasting",
        "authors": [
            {
                "first": "Oliver",
                "middle": [],
                "last": "Borchert",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AWS AI Labs",
                    "institution": "Technical University of Munich",
                    "location": {}
                },
                "email": "borchero@in.tum.de"
            },
            {
                "first": "David",
                "middle": [],
                "last": "Salinas",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AWS AI Labs",
                    "institution": "Technical University of Munich",
                    "location": {}
                },
                "email": "dsalina@amazon.com"
            },
            {
                "first": "Valentin",
                "middle": [],
                "last": "Flunkert",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AWS AI Labs",
                    "institution": "Technical University of Munich",
                    "location": {}
                },
                "email": "flunkert@amazon.com"
            },
            {
                "first": "Tim",
                "middle": [],
                "last": "Januschowski",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AWS AI Labs",
                    "institution": "Technical University of Munich",
                    "location": {}
                },
                "email": "tim.januschowski@zalando.de"
            },
            {
                "first": "Stephan",
                "middle": [],
                "last": "G\u00fcnnemann",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AWS AI Labs",
                    "institution": "Technical University of Munich",
                    "location": {}
                },
                "email": "guennemann@in.tum.de"
            }
        ]
    },
    "abstract": [
        {
            "text": "Research on time series forecasting has predominantly focused on developing methods that improve accuracy. However, other criteria such as training time or latency are critical in many real-world applications. We therefore address the question of how to choose an appropriate forecasting model for a given dataset among the plethora of available forecasting methods when accuracy is only one of many criteria.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "For this, our contributions are two-fold. First, we present a comprehensive benchmark, evaluating 7 classical and 6 deep learning forecasting methods on 44 heterogeneous, publicly available datasets. The benchmark code is open-sourced along with evaluations and forecasts for all methods. These evaluations enable us to answer open questions such as the amount of data required for deep learning models to outperform classical ones.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Second, we leverage the benchmark evaluations to learn good defaults that consider multiple objectives such as accuracy and latency. By learning a mapping from forecasting models to performance metrics, we show that our method PARETOSELECT is able to accurately select models from the Pareto front -alleviating the need to train or evaluate many forecasting models for model selection. To the best of our knowledge, PARETOSELECT constitutes the first method to learn default models in a multi-objective setting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "For decades, businesses have been using time series forecasting to drive strategic decision-making (Simchi-Levi et al., 2013; Hyndman & Athanasopoulos, 2018; Petropoulos et al., 2020) . Analysts leverage forecasts to gauge resource requirements, retailers forecast future product demand to optimize their supply chains, cloud providers predict future web traffic to scale server fleets, and in the energy sector, forecasting plays a crucial role e.g. to predict load and energy prices. In domains like these and many other, more precise predictions directly translate into an increase in profit. Thus, it is no surprise that research on forecasting methods has historically focused on improving accuracy.",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 125,
                    "text": "(Simchi-Levi et al., 2013;",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 126,
                    "end": 157,
                    "text": "Hyndman & Athanasopoulos, 2018;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 158,
                    "end": 183,
                    "text": "Petropoulos et al., 2020)",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In addition to more classical local forecasting methods which fit a model per time series, global forecasting models such as deep learning and tree-based models have demonstrated state-of-the-art forecasting accuracy (Wen et al., 2017; Oreshkin et al., 2019; Salinas et al., 2020a; Smyl, 2020) when sufficient training data is available (Makridakis et al., 2018; . This research has led to a large variety of different forecasting models and hyperparameter choices , where different models exhibit vastly different characteristics, including accuracy, training time, model size, and inference latency.",
            "cite_spans": [
                {
                    "start": 217,
                    "end": 235,
                    "text": "(Wen et al., 2017;",
                    "ref_id": null
                },
                {
                    "start": 236,
                    "end": 258,
                    "text": "Oreshkin et al., 2019;",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 259,
                    "end": 281,
                    "text": "Salinas et al., 2020a;",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 282,
                    "end": 293,
                    "text": "Smyl, 2020)",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 337,
                    "end": 362,
                    "text": "(Makridakis et al., 2018;",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "While this variety of forecasting models is a great resource, it introduces challenging questions. On the one hand, researchers would like to understand patterns in the performance of different models and to benchmark new models against existing methods. On the other hand, practitioners are interested in understanding which model performs best for a particular dataset or application.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this work, we address both of these problems: we release one of the most comprehensive publicly available evaluations of forecasting models across 44 datasets. Using this benchmark dataset, we develop a novel method for learning good defaults for forecasting models on previously unseen datasets. Importantly, we adopt a multiobjective perspective that allows us to select forecasting models that are simultaneously accurate and satisfy constraints such as inference latency, training time, or model size.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The main contributions of this work can be summarized as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We release the evaluations of 13 forecasting methods on 44 public datasets with respect to multiple performance criteria (different forecast accuracy metrics, inference latency, training time, and model size). This constitutes, by far, the most comprehensive publicly available evaluation of forecasting methods. Those evaluations can be leveraged, for example, to assess the relative performance of future forecasting methods with little effort.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 As an example application of this benchmark, we use the data to perform a statistical analysis which shows that only a few thousands observations are required for deep learning methods to outperform classical methods. In addition, we investigate the benefit of ensembling forecasting models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We propose a novel method that can leverage offline evaluations to learn good default models for unseen datasets. Default models are selected to optimize for multiple objectives.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We introduce a technique for ensembling models in a multi-objective setting where the resulting ensemble is not only highly accurate but also optimized for other objectives, such as a low inference latency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Time Series Forecasting. Time series forecasting has seen a recent surge in attention by the academic community that has started to reflect its relevance in business applications. Traditionally, univariate, so-called local models that consider time series individually have dominated (Hyndman & Athanasopoulos, 2018) . However, in modern applications, methods that learn globally across a set of time series can be more accurate (Oreshkin et al., 2019; Salinas et al., 2020a; Lim et al., 2021; ) -in particular, methods that rely on deep learning. With a considerable choice of models available, it is unclear which methods should perform best on which forecasting dataset, unlike in other machine learning domains where dominant approaches exist.",
            "cite_spans": [
                {
                    "start": 284,
                    "end": 316,
                    "text": "(Hyndman & Athanasopoulos, 2018)",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 429,
                    "end": 452,
                    "text": "(Oreshkin et al., 2019;",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 453,
                    "end": 475,
                    "text": "Salinas et al., 2020a;",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 476,
                    "end": 493,
                    "text": "Lim et al., 2021;",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In domains such as computer vision or neural architecture search (NAS), offline computations have been harvested and leveraged successfully to perform extensive model comparisons or compare different NAS strategies (Ying et al., 2019; Dong & Yang, 2020; Pfisterer et al., 2021) . However, to the best of our knowledge, no comprehensive set of model evaluations has been released in the realm of forecasting. Consequently, some questions remain open: how much data is required for global deep learning methods to outperform classical local methods such as ARIMA or ETS (Hyndman & Athanasopoulos, 2018) ? What is the impact on accuracy when ensembling different forecasting models? While some recent methods incorporate ensembling (Oreshkin et al., 2019; Jeon & Seong, 2021) , comparisons are often made against individual models which may cloud the benefit of the method proposed versus the sole benefit of ensembling.",
            "cite_spans": [
                {
                    "start": 215,
                    "end": 234,
                    "text": "(Ying et al., 2019;",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 235,
                    "end": 253,
                    "text": "Dong & Yang, 2020;",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 254,
                    "end": 277,
                    "text": "Pfisterer et al., 2021)",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 568,
                    "end": 600,
                    "text": "(Hyndman & Athanasopoulos, 2018)",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 729,
                    "end": 752,
                    "text": "(Oreshkin et al., 2019;",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 753,
                    "end": 772,
                    "text": "Jeon & Seong, 2021)",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Learning Default Models. Finding the best model or set of hyperparameters is often performed via Bayesian optimization given its theoretical regret guarantees (Srinivas et al., 2012) . However, even with early-stopping techniques (Golovin et al., 2017; Li et al., 2017) , practitioners often restrict the search to a single model due to the large cost of training many models. One technique to drastically speed up the model/hyperparameter search is to reuse offline evaluations of related datasets via transfer learning. For instance, Wistuba et al. (2015) ; Winkelmolen et al. (2020) ; Pfisterer et al. (2021) leverage offline evaluations to alleviate the need for training many models by learning a small list of n good defaults that provide a small joint error when evaluated on all datasets. This approach bears a lot of similarity with ours. Key differences are that we consider multiple objectives and model ensembles. In the time series domain, Shah et al. (2021) also considers the task of automatically choosing from a large pool of forecasting models the one performing best on a particular dataset but also only optimizes for a single objective and relies on potentially expensive model training for model selection.",
            "cite_spans": [
                {
                    "start": 159,
                    "end": 182,
                    "text": "(Srinivas et al., 2012)",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 230,
                    "end": 252,
                    "text": "(Golovin et al., 2017;",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 253,
                    "end": 269,
                    "text": "Li et al., 2017)",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 536,
                    "end": 557,
                    "text": "Wistuba et al. (2015)",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 560,
                    "end": 585,
                    "text": "Winkelmolen et al. (2020)",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 588,
                    "end": 611,
                    "text": "Pfisterer et al. (2021)",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this section, we formally introduce the problem of time series forecasting and subsequently provide an overview of the benchmark evaluations that we release. By evaluating 13 forecasting methods along with different hyperparameter choices on all 44 benchmark datasets and for two random seeds, the benchmark comprises 4,708 training runs and amasses over 1 TiB of forecast data. At the end of this section, we demonstrate how this benchmark data can be used to perform an extensive comparison of contemporary forecasting methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Benchmarking Forecasting Methods"
        },
        {
            "text": "The goal of time series forecasting is to predict the future values of one or more time series based on historical observations. Formally, we consider a set Z = {z (i) 1:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Series Forecasting"
        },
        {
            "text": ") denotes the vector of observations for the i -th time series in the time interval a \u2264 t \u2264 b. In probabilistic time series forecasting, a model then estimates the probability distribution across the forecast horizon \u03c4 , i.e. the distribution over the \u03c4 future values, from the historical observations:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Series Forecasting"
        },
        {
            "text": "Models typically approximate the joint distribution of Eq. (1) via Monte Carlo sampling (Salinas et al., 2020a) or learn to directly predict a set of distribution quantiles for multiple time steps using quantile regression (Wen et al., 2017; Lim et al., 2021) .",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 111,
                    "text": "(Salinas et al., 2020a)",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 223,
                    "end": 241,
                    "text": "(Wen et al., 2017;",
                    "ref_id": null
                },
                {
                    "start": 242,
                    "end": 259,
                    "text": "Lim et al., 2021)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Time Series Forecasting"
        },
        {
            "text": "We consider 13 models in total that can be distinguished into 7 local methods (estimating parameters individually from each time series) and 6 global methods (learning from all available time series jointly). Details about models can be found in Appendix A.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Local Methods. We use Seasonal Na\u00efve Athanasopoulos, 2018) and NPTS (Rangapuram et al., 2021) as simple, non-parametric baselines. Further, we consider ARIMA and ETS (Hyndman & Athanasopoulos, 2018) as well-known statistical methods and STL-AR (Talagala, 2021) and Theta (Assimakopoulos & Nikolopoulos, 2000) as inexpensive alternatives. Lastly, we include Prophet (Taylor & Letham, 2018) , an interpretable model that has received plenty of attention.",
            "cite_spans": [
                {
                    "start": 37,
                    "end": 62,
                    "text": "Athanasopoulos, 2018) and",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 63,
                    "end": 93,
                    "text": "NPTS (Rangapuram et al., 2021)",
                    "ref_id": null
                },
                {
                    "start": 166,
                    "end": 198,
                    "text": "(Hyndman & Athanasopoulos, 2018)",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 244,
                    "end": 260,
                    "text": "(Talagala, 2021)",
                    "ref_id": null
                },
                {
                    "start": 271,
                    "end": 308,
                    "text": "(Assimakopoulos & Nikolopoulos, 2000)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 365,
                    "end": 388,
                    "text": "(Taylor & Letham, 2018)",
                    "ref_id": "BIBREF67"
                }
            ],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Global Methods. All global methods that we use are deep learning models, namely: Simple Feedforward (Alexandrov et al., 2020) , MQ-CNN and MQ-RNN (Wen et al., 2017) , DeepAR (Salinas et al., 2020a) , N-BEATS (Oreshkin et al., 2019) , and TFT (Lim et al., 2021) . For each model (except MQ-RNN), we consider three hyperparameter settings: this includes the default set provided by their implementation as well as hyperparameter sets that roughly halve and double the default model capacity (see Table 2 in the appendix). Additionally, we consider three different context lengths that govern the length of the time series that predictions are conditioned on.",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 125,
                    "text": "(Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 146,
                    "end": 164,
                    "text": "(Wen et al., 2017)",
                    "ref_id": null
                },
                {
                    "start": 174,
                    "end": 197,
                    "text": "(Salinas et al., 2020a)",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 200,
                    "end": 231,
                    "text": "N-BEATS (Oreshkin et al., 2019)",
                    "ref_id": null
                },
                {
                    "start": 242,
                    "end": 260,
                    "text": "(Lim et al., 2021)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [
                {
                    "start": 494,
                    "end": 501,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Methods"
        },
        {
            "text": "Model Training. Model training is only required for deep learning models since parametric local methods estimate parameters at prediction time. Deep learning models are trained for a fixed duration depending on the size of the dataset. Further details can be found in Appendix C.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Our benchmark provides 44 heterogeneous public datasets in total. The datasets greatly differ in the number of time series (from 5 to \u2248170,000), their mean length (from 19 to \u2248500,000), their frequency (minutely, hourly, daily, weekly, monthly, quarterly, yearly), and the forecast horizon (from 4 to 60). Thus, we expect them to cover a wide range of datasets encountered in practice.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Datasets are taken from various forecasting competitions, the UCI (Dua & Graff, 2017) , and the Monash time series forecasting repository (Godahewa et al., 2021) . Dataset sources, descriptions, basic statistics, and an explanation of the data preparation procedure can be found in Appendix B.",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 85,
                    "text": "(Dua & Graff, 2017)",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 138,
                    "end": 161,
                    "text": "(Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "To measure the accuracy of forecasting models, we employ the normalized continuous ranked probability score (nCRPS) (Matheson & Winkler, 1976; Gneiting & Raftery, 2007) whose definition is given in Appendix A. The benchmark dataset that we release contains four additional forecast accuracy metrics (MAPE, sMAPE, NRMSE, ND). Besides forecasting accuracy metrics, we store inference latency, model size (i.e. number of parameters) and training time for deep learning models. Latency is measured by dividing the time taken to generate predictions for all test time series in the dataset by their number.",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 142,
                    "text": "(Matheson & Winkler, 1976;",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 143,
                    "end": 168,
                    "text": "Gneiting & Raftery, 2007)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "Having outlined the benchmark, we now want to demonstrate the usefulness of the evaluations for research. For this, we want to analyze how local (classical) and global (deep learning) forecasting methods compare against each other. In fact, this comparison has been the object of heated discussions in the forecasting community (Makridakis et al., 2018; .",
            "cite_spans": [
                {
                    "start": 328,
                    "end": 353,
                    "text": "(Makridakis et al., 2018;",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Benchmark Analysis"
        },
        {
            "text": "Method Comparison. In order to compare methods, we consider their latency and accuracy (in terms of nCRPS) across all datasets. Table 1 shows each method's relative latency compared to Seasonal Na\u00efve as well as the the methods' nCRPS rank 1 . As far as latency is concerned, global methods, once trained, allow to generate forecasts considerably faster than local methods (except for Seasonal Na\u00efve). The reason is simple: once trained, they simply need to run a forward pass. In contrast, the implementations for the local methods chosen here do not differentiate between training and inference, but rather estimate their parameters at prediction time. Across datasets, the relative latency of all considered deep learning models improves between 15% and 95% upon the latency of the fastest local method (excluding Seasonal Na\u00efve). As far as accuracy is concerned, complex deep learning models -namely DeepAR and TFT -generally perform best across the benchmark datasets. Except for MQ-RNN, global methods compare favorably against local methods.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 128,
                    "end": 135,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Benchmark Analysis"
        },
        {
            "text": "As a measure of relative model stability, Table 1 additionally shows the standard deviation of the nCRPS rank across all benchmark datasets. Not only is TFT the method which performs best on average, its rank is also comparatively consistent compared to other methods. Especially, it is more stable than DeepAR which seems to generate relatively inaccurate forecasts on some datasets. Rel. Improvement (Jeon & Seong, 2021) . While generally bearing a significant cost in latency, the benefit of ensembling is significant. For instance, the Simple Feedforward hyper-ensemble yields a competitive model that outperforms DeepAR in terms of both accuracy and latency. Lastly, DeepAR and TFT hyper-ensembles result in the most competitive ensembles.",
            "cite_spans": [
                {
                    "start": 402,
                    "end": 422,
                    "text": "(Jeon & Seong, 2021)",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [
                {
                    "start": 42,
                    "end": 49,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Benchmark Analysis"
        },
        {
            "text": "Comparison of Classical and Deep Learning Methods. We further conduct a statistical analysis to compare the classical time series models with the deep learning models listed in Section 3.2. For this, we construct the null hypothesis that the accuracy of classical models is equal to or better than the accuracy of deep learning methods. This is a claim put forward, for example, by Makridakis et al. (2018) . More formally, we write H 0 : Q class \u2264 Q deep where Q class and Q deep describe the distribution of nCRPS values of classical and deep learning models, respectively. Samples of the distributions are derived from the respective single-model evaluations in our benchmark. Q class and Q deep are continuous and belong to unknown families of distributions with unequal variances. Hence, we choose the nonparametric two-sample Kolmogorov-Smirnov test (Gibbons & Chakraborti, 2003) to compute the p-value, i.e. the probability of H 0 holding true, on each individual dataset.",
            "cite_spans": [
                {
                    "start": 382,
                    "end": 406,
                    "text": "Makridakis et al. (2018)",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 856,
                    "end": 885,
                    "text": "(Gibbons & Chakraborti, 2003)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Benchmark Analysis"
        },
        {
            "text": "We evaluate the null hypothesis H 0 on all 44 benchmark datasets and plot the p-value for the different dataset sizes in Figure 1 . At a significance level of \u03b1 = 5%, H 0 can be rejected for 30 out of 44 datasets. In fact, deep learning models exhibit competitive performance for almost all datasets and regularly outperform classical methods. Further, they tend to perform comparatively better with increasing dataset size -even if for some large datasets, classical models are indistinguishable -and, nonetheless, show competitive performance for small datasets. The latter is of particular interest since it contradicts tribal knowledge that large datasets are needed to outperform local/classical methods and disputes the claim presented by Makridakis et al. (2018) since only a few thousands observations seem sufficient to outperform the classical models considered.",
            "cite_spans": [
                {
                    "start": 745,
                    "end": 769,
                    "text": "Makridakis et al. (2018)",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [
                {
                    "start": 121,
                    "end": 129,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Benchmark Analysis"
        },
        {
            "text": "These statements are further supported by Figure 2 where we compare the best deep learning model x deep against the best classical model x class on each dataset. We compute the relative improvement given by deep learning models as",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 42,
                    "end": 50,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Benchmark Analysis"
        },
        {
            "text": "nCRPS(x class ) \u22121. Except for few outliers, the best deep learning model outperforms all classical models on 40 out of 44 datasets (see Appendix F for a further analysis). Fitting a linear regression model on the relative improvements shows that the improvement by using deep learning models is only slightly amplified as the datasets grow in size.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Benchmark Analysis"
        },
        {
            "text": "We note, that we conflate deep learning and global models in our above discussion. show the general superiority of global models over local models both theoretically and empirically and our results confirm their findings further.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Benchmark Analysis"
        },
        {
            "text": "The benchmark introduced in the previous section outlined how to acquire offline evaluations of forecasting methods on various datasets. However, in the presence of multiple conflicting objectives such as accuracy and latency, it is not clear how one could choose the best models. In this section, we first formalize the problem of learning good defaults from these evaluations for a single objective. Afterwards, we formally introduce multiobjective optimization and show how learning defaults can be extended to account for multiple objectives. In the end, we report experimental results obtained by using our method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Learning Multi-Objective Defaults"
        },
        {
            "text": "In our setting, we consider an objective function f : X \u2192 R m for a given task (in our case, a task is a dataset). The objective function maps any time series model x \u2208 X to m objectives (such as nCRPS, latency, etc.) that ought to be minimized. In addition, we assume that model evaluations on T different but related tasks with objective functions f (1) , . . . , f (T ) are available. The set of offline model evaluations is then given as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "where N j evaluations are available for task j and y (j) i denotes the evaluation of x (j) i on task j.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "Learning Defaults. To learn a set of n good default models for unseen datasets in the single-objective setting, Pfisterer et al. (2021) propose to pick the models that minimize the joint error obtained when evaluating them on all datasets. This amounts to picking the set of models {x 1 , . . . , x n } \u2282 X which minimizes the following objective:",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 135,
                    "text": "Pfisterer et al. (2021)",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "where k denotes a single fixed objective (for instance, the classification error). Since the problem is NPcomplete, a greedy heuristic with a provable approximation guarantee is used: it iteratively adds the model minimizing Eq. 3 to the current selection. However, this approach only works for a single objective of choice f k rather than taking all objectives f (such as accuracy and latency) into account. In addition, it does not easily support the selection of ensembles since objectives can interact in differently when models are ensembled: while accuracy will likely increase by a small amount, the latencies of ensemble members need to be added up.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "Taking into account all possible ensembles of a given size n would also blow up the combinatoric search when optimizing Eq. 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "Multi-Objective Optimization. In multi-objective optimization, there generally exists no singular solution. Thus, when considering the objective function f , there exists no single optimal model, but rather a set of optimal models (Emmerich & Deutz, 2018) .",
            "cite_spans": [
                {
                    "start": 231,
                    "end": 255,
                    "text": "(Emmerich & Deutz, 2018)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "The set of all non-dominated solutions (i.e. models) is denoted as the Pareto front P f (X ) whose members are commonly referred to as Pareto-optimal solutions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "To quantify the quality of a set S = {x 1 , . . . , x n } \u2282 X of selected models, we use the hypervolume error (Zitzler & Thiele, 1998; Li & Yao, 2019) . Given a set of points Y \u2282 R m and a reference point r \u2208 R m , we first define the hypervolume as the Lebesgue measure \u039b of the dominated space:",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 135,
                    "text": "(Zitzler & Thiele, 1998;",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 136,
                    "end": 151,
                    "text": "Li & Yao, 2019)",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "In turn, the hypervolume error \u03b5 of the set S is defined as the difference in hypervolume compared to the true Pareto front P f (X ):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "Thus, the hypervolume error \u03b5(S) reaches its minimum when S \u2287 P f (X ). To account for different scales in the optimization objectives, we further standardize all objectives by quantile normalization (Bolstad et al., 2003) such that they follow a uniform distribution U(0, 1) -this is feasible as X is finite. While the normalization makes our comparisons robust to monotonic transformations of the objectives (Binois et al., 2020) , it also allows us to choose the reference point for Eq. 5 as r = 1 1 1 m .",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 222,
                    "text": "(Bolstad et al., 2003)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 410,
                    "end": 431,
                    "text": "(Binois et al., 2020)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "Learning Multi-Objective Defaults. Using the hypervolume error, we can extend the minimization problem of Eq. 3 to learn defaults in the multi-objective setting. For this, we propose the following minimization problem:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "In words, we seek to find a set of complementary model configurations {x 1 , . . . , x n } \u2282 X that provide a good approximation of the Pareto front P f (j) (X ) across tasks j \u2208 {1, . . . , T }.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prerequisites"
        },
        {
            "text": "We now introduce PARETOSELECT which tackles the minimization of Eq. 7. On a high-level, PARETOSELECT works as follows: first, it fits a parametric surrogate modelf \u03b8 that predicts the performances of model configurations by leveraging the evaluations of D. Then, it uses the surrogatef \u03b8 to estimate the objectives for all models in X and applies the non-dominated sorting algorithm on the objectives to select a list of default models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": "The surrogate modelf \u03b8 is trained by attempting to correctly rank model configurations within each task in the available offline evaluations D. For each model x \u2208 X ,f \u03b8 outputs a vector\u1ef9 \u2208 R m for m optimization objectives. Models can then be ranked for each objective independently. In order to fitf \u03b8 on the data D, we use listwise ranking (Xia et al., 2008) and apply linear discounting to focus on correctly identifying the top configurations similar to Z\u00fcgner et al. (2020) . The corresponding loss function can be found in Appendix D.",
            "cite_spans": [
                {
                    "start": 343,
                    "end": 361,
                    "text": "(Xia et al., 2008)",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 459,
                    "end": 479,
                    "text": "Z\u00fcgner et al. (2020)",
                    "ref_id": "BIBREF75"
                }
            ],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": "For the parametric surrogate modelf \u03b8 , we choose a simple MLP similar to Salinas et al. (2020b) . We use two hidden layers of size 32 with LeakyReLU activations after each hidden layer and apply a weight decay of 0.01 as regularization. Importantly, we do not tune these hyperparameters. Predictive performances for every model can eventually be computed as\u1ef8",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 96,
                    "text": "Salinas et al. (2020b)",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": "We note that for minimizing Eq. 7, either regression or ranking objectives can be used: since we apply quantile normalization to compute the hypervolume, the magnitude of the values predicted byf \u03b8 is irrelevant. However, we find the ranking setting to be superior to regression for finding good default models (see Appendix D for more details).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": "Multi-Objective Sorting. The \"best\" configurations are then determined by applying the non-dominated sorting algorithm (NDSA) -an extension of sorting to the multi-dimensional setting (Srinivas & Deb, 1994; Emmerich & Deutz, 2018) -to the predictive performances\u1ef8. Figure 3 provides an illustration of the sorting procedure. NDSA can be described as a two-level procedure. First, it partitions the configurations to be sorted in multiple layers by iteratively computing the Pareto fronts, then it applies a sorting function sort to rank the configurations in each layer:",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 206,
                    "text": "(Srinivas & Deb, 1994;",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 207,
                    "end": 230,
                    "text": "Emmerich & Deutz, 2018)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 265,
                    "end": 273,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "PARETOSELECT"
        },
        {
            "text": "We choose sort to compute an -net (Clarkson, 2006) for which there exists a simple greedy algorithm. When sorting a set X, the -net sorting operation yields an order sort(X) = [x 1 , . . . , x |X| ]. The first element x 1 is chosen randomly from X and x i+1 is defined iteratively as the item which is farthest from the current selection in terms of Euclidean distance, formally:",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 50,
                    "text": "(Clarkson, 2006)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": "The final ordering is eventually obtained by concatenating the sorted layers. While previous work followed different approaches for the sort operation in the NDSA algorithm (Srinivas & Deb, 1994; Deb et al., 2002) , we leverage the -net since highlighted its theoretical guarantees and Schmucker et al. (2021) provided empirical evidence for its good performance.",
            "cite_spans": [
                {
                    "start": 173,
                    "end": 195,
                    "text": "(Srinivas & Deb, 1994;",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 196,
                    "end": 213,
                    "text": "Deb et al., 2002)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 286,
                    "end": 309,
                    "text": "Schmucker et al. (2021)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": "The pseudo-code of PARETOSELECT is given in Algorithm 1 in the appendix. In the case where predictions of the surrogate are error-free, the recommendations of the algorithm can be guaranteed to be perfect with zero hypervolume error.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": "Proposition 1. Assume that n \u2265 |P f (X ) | and for all x \u2208 X ,f \u03b8 (x) = f (x). Then, \u03b5({x 1 , . . . , x n }) = 0 where x 1 , . . . , x n are the first n models selected by our method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": "Proof. By Eq. 9, the first k recommendations of Pareto Select are such that {x 1 , . . . ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": ". It follows that for any k \u2264 n, P f (X ) = {x 1 , . . . , x k } \u2282 {x 1 , . . . , x n } and consequently \u03b5({x 1 , . . . , x n }) = 0 since the hypervolume error of any set of points containing the Pareto front is zero.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "PARETOSELECT"
        },
        {
            "text": "In order to evaluate our approach, we leverage the data collected with the benchmark presented in Section 3. For evaluation, we perform leave-one-out-cross-validation where we use each dataset one after the other as the test dataset and estimate the parameters \u03b8 of our parametric surrogate modelf \u03b8 on the remaining 43 datasets. The multi-objective setting in the following experiments aims to minimize latency as well as nCRPS.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "In Figure 4 , we report the average hypervolume obtained for PARETOSELECT and several baselines, averaged over all 44 test datasets. PARETOSELECT picks the top elements of the non-dominated sort on the surrogate's predictive performances. As baselines, we consider a variant of PARETOSELECT which considers the nCRPS as the single objective (Single-Objective), the approach of Pfisterer et al. (2021) which greedily picks models to minimize the joint nCRPS on the training data (Greedy Model-Free), and random search over the entire model space of size |X | = 247 (Random). The hypervolume obtained by PARETOSELECT with 10 default models is small (\u2248 0.06) compared to the best baseline that reaches \u2248 0.20. Further, random search requires a total of 27 models to be on par with the hypervolume of only 10 models selected via our method. Figure 5 depicts the top n = 10 recommendations produced by PARETOSELECT when predicting default models on a single dataset. The default models cover the true Pareto front well: our method is able to pick models with low latency, low nCRPS, and a good trade-off between these two objectives.",
            "cite_spans": [
                {
                    "start": 377,
                    "end": 400,
                    "text": "Pfisterer et al. (2021)",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 4",
                    "ref_id": null
                },
                {
                    "start": 837,
                    "end": 845,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Results"
        },
        {
            "text": "As shown in Section 3.5, ensembling models yields significant gains in accuracy but comes at the cost of considerable latency. Since a massively large ensemble model is of no practical use, it is critical to be able to select an ensemble with a latency that is acceptable for an application. Thus, we consider ensembles under different latency constraints.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-Objective Ensembling"
        },
        {
            "text": "For a latency constraint c, we build an ensemble with n \u2264 10 members x 1 , . . . , x n by iteratively picking the model with best nCRPS such that the ensemble latency constrain c is still met. To generate ensemble predictions, we combine the forecasts of the base models x 1 , . . . , x n by averaging all 10-quantiles {0. average. This aligns with current literature showing that simple averaging is often most effective (Petropoulos et al., 2020) . Figure 6 shows the performance of different latency-constrained ensembles, relative to the performance of an unconstrained ensemble that picks the n = 10 models with lowest predicted nCRPS. As expected, higher latency constraints c allows the ensemble to perform better. For c \u2265 100 ms, the constrained ensemble recovers the Pareto front and outperforms all individual models x \u2208 X as well as ensembles. The small gap to the Pareto front for constraints c \u2264 50 ms can be attributed to multiple effects. First, the nCRPS values predicted by the surrogatef \u03b8 are not optimal. Second, models may not satisfy c on every dataset although they have a lower latency on average (e.g. the default TFT configuration violates c = 2.5 ms on 17/44 datasets). Table 1 additionally reports performance metrics of latency-constrained ensembles: for instance, the ensemble for c = 10 ms outperforms all individual base models in terms of average nCRPS rank while the ensemble for c = 100 ms outperforms all hyper-ensembles as well.",
            "cite_spans": [
                {
                    "start": 422,
                    "end": 448,
                    "text": "(Petropoulos et al., 2020)",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [
                {
                    "start": 451,
                    "end": 459,
                    "text": "Figure 6",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1197,
                    "end": 1204,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Multi-Objective Ensembling"
        },
        {
            "text": "In this work, we have presented (i) a new benchmark for evaluating forecasting methods in terms of multiple metrics and (ii) PARETOSELECT, a new algorithm that can learn default model configurations in a multi-objective setting. In future work, we will consider applying those techniques to other domains such as computer vision where offline evaluations of multiple datasets were recently made available by Dong & Yang (2020) . Additionally, future research may extend the presented predictive surrogate model to the case of ensembles: this would enable PARETOSELECT to be used directly to select members for ensembles of any latency.",
            "cite_spans": [
                {
                    "start": 408,
                    "end": 426,
                    "text": "Dong & Yang (2020)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Public datasets and method implementations were used to ensure the reproducibility of our experiments. In Appendix A, we provide all hyperparameters used for training the benchmark forecasting models via GluonTS. The source and processing of each dataset as well as the procedure for generating data splits is detailed in Appendix B. We also provide details on the surrogate model used and a comparison with other choices in Appendix D. The pseudocode of PARETOSELECT is given in Appendix E. As a source of truth, we share the entire code used to run the benchmark and its evaluation with the submission. This code as well as the generated evaluations will be released with the paper. Table 2 provides an overview over all benchmark models along with hyperparameters considered. In addition to three hyperparameter settings per deep learning model (except for MQ-RNN), three different context lengths l are considered. These represent multiples of the forecast horizon \u03c4 and are thus dataset-dependent. Eventually, this results in 9 model configurations per deep learning model (and 3 for MQ-RNN).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 685,
                    "end": 692,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Reproducibility"
        },
        {
            "text": "The implementation of all models are taken from GluonTS (Alexandrov et al., 2020) . All deep learning models are implemented in MXNet (Chen et al., 2015) . ARIMA, ETS, STL-AR, and Theta are available in GluonTS as well but forward computations to the R forecast package .",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 81,
                    "text": "(Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 134,
                    "end": 153,
                    "text": "(Chen et al., 2015)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "A Models"
        },
        {
            "text": "Model Evaluation. To measure the accuracy of forecasting models, we employ the continuous ranked probability score (CRPS) (Matheson & Winkler, 1976; Gneiting & Raftery, 2007) . Given the quantile function (F (i) t ) \u22121 of the forecast probability distribution in Eq. (1) for a time series z (i) at time step t, the CRPS is defined as",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 148,
                    "text": "(Matheson & Winkler, 1976;",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 149,
                    "end": 174,
                    "text": "Gneiting & Raftery, 2007)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "A Models"
        },
        {
            "text": "where \u039b \u03b1 is the quantile loss (or pinball loss) defined over a quantile level 0 < \u03b1 < 1:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Models"
        },
        {
            "text": "Here, I is the indicator function. To compute an aggregated score for a model forecasting multiple time series and time steps, we compute a normalized CRPS (nCRPS):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Models"
        },
        {
            "text": "We approximate the nCRPS using the 10-quantiles \u03b1 \u2208 {0.1, 0.2, 0.3, . . . , 0.9}. While we focus on nCRPS for the evaluations in this paper, the benchmark dataset that we release contains four additional forecast accuracy metrics (MAPE, sMAPE, NRMSE, ND).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Models"
        },
        {
            "text": "Our benchmark provides 44 datasets which were obtained from multiple sources. 16 datasets were obtained though GluonTS (Alexandrov et al., 2020) , 24 datasets are taken from the Monash Time Series Forecasting Repository (Godahewa et al., 2021) , and the remaining 4 datasets were originally published as part of Kaggle 2 forecasting competitions. Table 3 provides basic statistics about all datasets.",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 144,
                    "text": "(Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 220,
                    "end": 243,
                    "text": "(Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [
                {
                    "start": 347,
                    "end": 354,
                    "text": "Table 3",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "B Datasets"
        },
        {
            "text": "In the following, we provide brief descriptions of all datasets and provide their sources. We link the original source (if possible) along with any work which pre-processed the data (if applicable). The datasets \"Corporaci\u00f3n Favorita\", \"Restaurant\", \"Rossmann\", and \"Walmart\" -which were obtained from Kaggle -are processed by ourselves. Bitcoin (Godahewa et al., 2021) provides over a dozen of potential influencers of the price of Bitcoin. Among others, these influencers are daily hash rate, block size, or mining difficulty. We exclude one time series from the original dataset as its absolute values are \u2265 10 18 . Data is provided from January 2009 to July 2021.",
            "cite_spans": [
                {
                    "start": 346,
                    "end": 369,
                    "text": "(Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "B.1 Dataset Descriptions"
        },
        {
            "text": "CIF 2016 (\u0160t\u011bpni\u010dka & Burda, 2017; Godahewa et al., 2021) is the dataset from the \"Computational Intelligence in Forecasting\" competition in 2016. One third of the time series originate from the banking sector while the remaining two thirds are generated artificially. In the competition, the time series have two different prediction horizons, however, GluonTS forces us to adopt only a single horizon (for which we choose the most common one).",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 34,
                    "text": "(\u0160t\u011bpni\u010dka & Burda, 2017;",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 35,
                    "end": 57,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "COVID Deaths Godahewa et al., 2021) provides the daily COVID-19 death counts of various countries between January 22 and August 21 in the year 2020.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 35,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Car Parts (Hyndman, 2015; Godahewa et al., 2021) provides intermittent time series of car parts sold monthly by a US car company between January 1998 and April 2002. Periods in which no parts are sold have a value of zero.",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 25,
                    "text": "(Hyndman, 2015;",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 26,
                    "end": 48,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Corporaci\u00f3n Favorita (Favorita, 2017) contains the daily unit sales of over 4,000 items at 54 different stores of the supermarket company Corporaci\u00f3n Favorita in Ecuador. Unit sales were manually set to 0 on days where there was no data available. Data ranges from January 2013 to August 2017. All covariates that are provided in the Kaggle competition are discarded.",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 37,
                    "text": "(Favorita, 2017)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Dominick (James M. Kilts Center, 2020; Godahewa et al., 2021) contains time series with the weekly profits of numerous stock keeping units (SKUs). The data is obtained from the grocery store company Dominick's over a period of seven years.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 38,
                    "text": "(James M. Kilts Center, 2020;",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 39,
                    "end": 61,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Electricity (Dua & Graff, 2017; Alexandrov et al., 2020) is comprised of the hourly electricity consumption (in kWh) of hundreds of households between January 2012 and June 2014.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 31,
                    "text": "(Dua & Graff, 2017;",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 32,
                    "end": 56,
                    "text": "Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Exchange Rate (Lai et al., 2018; Alexandrov et al., 2020) provides the daily exchange rates (on weekdays) between the US dollar and the currencies of eight countries (Australia, Great Britain, Canada, Switzerland, China, Japan, New Zealand, Singapore) in the period from 1990 to 2013. Hospital (Hyndman, 2015; Godahewa et al., 2021) provides the monthly patient counts for various products that are related to medical problems between 2000 and 2007.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 32,
                    "text": "(Lai et al., 2018;",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 33,
                    "end": 57,
                    "text": "Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 294,
                    "end": 309,
                    "text": "(Hyndman, 2015;",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 310,
                    "end": 332,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "KDD 2018 (Bekkerman et al., 2018; Godahewa et al., 2021) M1 (Makridakis et al., 1982; Godahewa et al., 2021) datasets are taken from the M1 competition in 1982.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 33,
                    "text": "(Bekkerman et al., 2018;",
                    "ref_id": null
                },
                {
                    "start": 34,
                    "end": 56,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 60,
                    "end": 85,
                    "text": "(Makridakis et al., 1982;",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 86,
                    "end": 108,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Time series have varying start-and end dates and cover a wide semantic spectrum of domains (demographics, micro, macro, industry).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "M3 (Makridakis & Hibon, 2000; Alexandrov et al., 2020) datasets are obtained from the M3 competition. Across frequencies, time series were collected from six different domains: demographics, micro, macro, industry, finance, and other. For the time series with no specified frequency, we assume a quarterly frequency (as this is the default in GluonTS).",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 29,
                    "text": "(Makridakis & Hibon, 2000;",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 30,
                    "end": 54,
                    "text": "Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "M4 (Makridakis et al., 2020b; Alexandrov et al., 2020) datasets are taken from the prestigious M4 competition. The time series are collected from the same domains as in the M3 competition.",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 29,
                    "text": "(Makridakis et al., 2020b;",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 30,
                    "end": 54,
                    "text": "Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "M5 (Makridakis et al., 2020a; Alexandrov et al., 2020) is the dataset from the M5 competition and contains daily unit sales of 3,049 products across 10 Walmart stores located in California, Texas, and Wisconsin. While covariates are available (e.g. product prices or special events), none of the forecasting methods we consider makes use of them. Sales data is provided from January 2011 to April 2016. Restaurant (Holdings, 2017) provides the number of daily visitors of hundreds of restaurants in Japan between January 2016 and April 2017, as tracked by the AirREGI system. As dates for which restaurants are closed do not provide visitor numbers, we impute zeros for these missing values. The Kaggle competition where this dataset is obtained from also provides data about reservations but this data remains unused for our purposes.",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 29,
                    "text": "(Makridakis et al., 2020a;",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 30,
                    "end": 54,
                    "text": "Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Rideshare (Godahewa et al., 2021) is comprised of hourly time series representing various attributes related to services offered by Uber in Lyft in New York. Time series include e.g. minimum, maximum, and average price, or the maximum distance traveled -grouped by starting location and provider. Data is available for the period of a month, starting in late November 2018.",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 33,
                    "text": "(Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Rossmann (Rossmann, 2015) provides daily sales counts at hundreds of different stores of the Rossmann drug store chain. Just like for the M5 dataset, covariates are available (e.g. distance to competition, special events, or discounts) but not used by any method. Data is available from January 2013 until August 2015.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 25,
                    "text": "(Rossmann, 2015)",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "San Francisco Traffic (Caltrans, 2020; Godahewa et al., 2021) contains hourly occupancy rates of freeways in the San Francisco Bay area. Data is available for two full years, starting from January 1, 2015.",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 38,
                    "text": "(Caltrans, 2020;",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 39,
                    "end": 61,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Solar (Zhang, 2006; Alexandrov et al., 2020) is comprised of the hourly power consumption of dozens of photovoltaic power stations in the state of Alabama in the year 2006.",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 19,
                    "text": "(Zhang, 2006;",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 20,
                    "end": 44,
                    "text": "Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Taxi (NYC Taxi and Limousine Commission, 2015; Alexandrov et al., 2020) contains the number of taxi rides in hundreds of locations around New York in 30 minute windows. Training data contains data from January 2015 while test data contains data from January in the subsequent year.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 46,
                    "text": "(NYC Taxi and Limousine Commission, 2015;",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 47,
                    "end": 71,
                    "text": "Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Temperature Rain (Godahewa et al., 2021) provides the daily temperature observations and rain forecasts gather from 422 weather stations across Australia. Data ranges from May 2015 through April 2017.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 40,
                    "text": "(Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Tourism (Athanasopoulos et al., 2011; Godahewa et al., 2021) Weather (Sparks, 2021; Godahewa et al., 2021) is comprised of daily data collected from hundreds of weather stations in Australia. Collected data is the amount of rain, the minimum and maximum temperature, and the amount of solar radiation.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 37,
                    "text": "(Athanasopoulos et al., 2011;",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 38,
                    "end": 60,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 69,
                    "end": 83,
                    "text": "(Sparks, 2021;",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 84,
                    "end": 106,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Wiki Alexandrov et al., 2020) similarly provides daily pages views for several thousand Wikipedia pages in the period from January 2012 to July 2014.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 29,
                    "text": "Alexandrov et al., 2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "Wind Farms (AEMO, 2020; Godahewa et al., 2021) contains time series with the minutely power production of hundreds of wind farms in Australia. Data is available for a period of one year, starting in August 2019.",
            "cite_spans": [
                {
                    "start": 24,
                    "end": 46,
                    "text": "Godahewa et al., 2021)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Australian Electricity Demand"
        },
        {
            "text": "For all of the datasets in Table 3 except for \"Taxi\", we perform similar preprocessing. Let Z = {z (i) 1:T i } K i=1 be the set of K univariate time series of lengths T i . Then, we run the following preprocessing steps:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 27,
                    "end": 34,
                    "text": "Table 3",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "B.2 Data Preparation"
        },
        {
            "text": "1. Remove all time series which are constant to always be able to compute the MASE metric.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Data Preparation"
        },
        {
            "text": "2. Remove all time series which are too short. We exclude all time series of length T \u2264 (p + 1)\u03c4 where \u03c4 is the prediction horizon and p is a dataset-specific integer which governs over how many prediction lengths we want to perform testing 3 . We are using p = 1 for all datasets but \"Electricity\" (p = 7), \"Exchange Rate\" (p = 3), \"Solar\" (p = 4), \"Traffic\" (p = 7) and \"Wiki\" (p = 3).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Data Preparation"
        },
        {
            "text": "3. Split time series into training, validation and test data. For each time series, we split off the prediction horizon p \u2212 1 times to obtain time series for the test data. Then, we split off the prediction horizon another time to obtain a validation time series per training time series. Eventually, we construct the following sets:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Data Preparation"
        },
        {
            "text": "For the \"Taxi\" dataset, we need to slightly augment the preprocessing to allow for discontinuities in the available time series. Essentially, each time series z \u2208 Z consists of an old part z old and a new part z new (data throughout January of two successive years). We then obtain the training data from z old and construct the validation data by cutting the prediction horizon from z old . The test dataset is constructed from z new as in bullet point 3 with p = 56.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Data Preparation"
        },
        {
            "text": "This section describes in detail how the deep learning models from Table 2 are trained on the datasets outlined in the previous section. As outlined in Section 3.2, parametric local methods directly estimate their parameters at training time. While deep learning models are trained on Z train such that models at different checkpoints can be compared using Z val , parameters of parametric local methods are estimated using Z val .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 67,
                    "end": 74,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "C Training Details"
        },
        {
            "text": "For all deep learning models, we run training for a predefined duration depending on the dataset size. For n total observations in the training data, we run training for d hours where d = 2 \u03b7 with \u03b7 = min max log 10 n 10000",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1 Training Time"
        },
        {
            "text": ", 0 , 3 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1 Training Time"
        },
        {
            "text": "Training therefore always runs between one and eight hours (refer to Table 3 for training times on individual datasets). Even on small datasets, we train for an hour to ensure that the trained model receives sufficiently many gradient updates.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 69,
                    "end": 76,
                    "text": "Table 3",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "C.1 Training Time"
        },
        {
            "text": "Forecasts of all methods are required to be probabilistic to store the 10-quantiles q 0.1 , q 0.2 , . . . , q 0.9 . While some methods forecast the quantiles directly (e.g. MQ-CNN, TFT), other methods provide samples of the output distribution (e.g. DeepAR) or point forecasts (e.g. N-BEATS). For sampling-based models, we simply compute the empirical 10-quantiles. For the point forecasts, we interpret forecasts as Dirac distributions centered around the forecasted value y . Thus, each quantile can be set to the value y .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Probabilistic Forecasts"
        },
        {
            "text": "All deep learning models are trained using the Adam optimizer (Kingma & Ba, 2014) with an initial learning rate of \u03c1 = 10 \u22123 . During training, we decrease the learning rate three times by a factor of \u03bb = 1 2 using a linear schedule. When training for a duration d, we decrease the learning rate after durations D = { d 4 , d 2 , 3d 4 }. This results in a final learning rate of \u03bb 3 \u03c1 = 1.25 \u00b7 10 \u22124 . The decay is always applied after the first batch exceeding one of the durations in D.",
            "cite_spans": [
                {
                    "start": 62,
                    "end": 81,
                    "text": "(Kingma & Ba, 2014)",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "C.3 Learning Rate Scheduling"
        },
        {
            "text": "During training, we save the model and compute its loss L as well as the nCRPS Q on the validation data for a total of 11 times. For a model being trained for a duration t, we perform these computations after durations",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.4 Validation and Testing"
        },
        {
            "text": "For each d \u2208 D, we run validation on Z val after the first batch where the total training time exceeds d and compute the losses L d and Q d .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.4 Validation and Testing"
        },
        {
            "text": "Note that we do not add the time taken to run validation to the training time to evaluate whether it exceeds d. We argue that validation can be run swiftly. If necessary, it could also be run on a subset of all available time series for further speedup.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.4 Validation and Testing"
        },
        {
            "text": "For testing, we choose 5 of the 11 models that we saved during training. For each d \u2208 I, we choose the model with the lowest nCRPS that was encountered up to d. Note that this may result in choosing the same model multiple times if continued training did not yield any improvements on the validation data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.4 Validation and Testing"
        },
        {
            "text": "All training jobs were scheduled on AWS Sagemaker 4 using CPU-only ml.c5.2xlarge instances (8 CPUs, 16 GiB memory). In few cases, we ran jobs on other instance types:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.5 Infrastructure and Training Statistics"
        },
        {
            "text": "In the following, we discuss the choice of the surrogate model. First, we outline why a parametric surrogate model is desirable. Then, we describe how model configurations are vectorized to be used by parametric surrogate models. Afterwards, we list the ranking loss with linear discounting that we mention in Section 4.2 and eventually compare the ranking performance of different surrogate models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D Surrogate Model"
        },
        {
            "text": "Intuitively, a nonparametric surrogate model might appear to be optimal for ranking models. However, a parametric surrogate has several advantages. First, it can interpolate between neighboring configurations, which is essential when the configurations differ between different datasets or when the objective is noisy. Second, a parametric model allows for suggesting new configurations that e.g. maximize capacity under a given constraint. Third, a parametric surrogate modelf \u03b8 may also be conditioned on the dataset by taking as input dataset features (such as number of time series or observations in the task at hand) by concatenating features to the input vector off \u03b8 -although we do not consider this possibility here.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.1 Choice of Surrogate Model"
        },
        {
            "text": "When using XGBoost or an MLP as surrogate model for predicting model performance, models must be represented as feature vectors. For this, we proceed as follows to encode model configurations:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2 Input Vectorization"
        },
        {
            "text": "\u2022 The model type (i.e. ARIMA, DeepAR, . . . ) is encoded via a one-hot encoding, yielding a 13dimensional vector.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2 Input Vectorization"
        },
        {
            "text": "\u2022 Every model hyperparameter (across models) defines a new real-valued feature. Hyperparameters that are shared among deep learning models (context length multiple, training time, learning rate 5 ) are reused across models. This results in 15 additional features which are all standardized to have zero mean and unit variance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2 Input Vectorization"
        },
        {
            "text": "Inputs to the MLP are, thus, 28-dimensional. Features that are missing (e.g. all classical models do not provide a context length multiple) are imputed by the features' means (resulting in zeros due to the standardization).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2 Input Vectorization"
        },
        {
            "text": "Section 4.2 outlines that the MLP surrogate modelf \u03b8 is trained via listwise ranking using linear discounting. For training on the offline evaluations D,f \u03b8 minimizes the loss L via SGD. As ranking is performed for each objective k \u2208 {1, . . . , m} and task j \u2208 {1, . . . , T } independently, L can be written as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.3 Ranking Loss with Linear Discounting"
        },
        {
            "text": "wheref \u03b8;k yields the surrogate model's outputs for objective k and L kj (f \u03b8;k ) is defined as follows (where we ignore indices k and j in the formula for notational convenience):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.3 Ranking Loss with Linear Discounting"
        },
        {
            "text": "Here, N provides the number of available evaluations for task j and r (i ) defines the index of the configuration x that is ranked at position i with respect to objective k among all configurations for task j. The ranks of the configurations x 1 , . . . , x N with respect to an objective k can be computed from the corresponding evaluations y 1;k , . . . , y N;k . Note that rank i = 1 provides the index for the \"best\" configuration andf \u03b8 outputs a smaller value for configurations that have a smaller rank (i.e. are \"better\").",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.3 Ranking Loss with Linear Discounting"
        },
        {
            "text": "The following compares the MLP surrogate trained via listwise ranking introduced in Section 4 to other possible choices for the surrogate model. Much like the approach described in Section 4.3, we evaluate surrogate models by performing LOOCV: surrogates are evaluated on each of the 44 benchmark datasets one after the other, being trained on the remaining 43 datasets. Surrogates are evaluated with respect to different ranking metrics that give an overview of the models' ability to rank the nCRPS values of model configurations on different datasets. As metrics for measuring the ranking performance on a single dataset, we consider the following which yield metrics between 0 and 1:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.4 Comparison of Different Models"
        },
        {
            "text": "\u2022 Mean Reciprocal Rank (MRR) (Liu, 2009 ): Computed as one divided by the predicted rank of the model with the lowest true nCRPS.",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 39,
                    "text": "(Liu, 2009",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "D.4 Comparison of Different Models"
        },
        {
            "text": "\u2022 Precision@k (Liu, 2009) : The fraction of the k models with the lowest true nCRPS captured in the top k predictions.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 25,
                    "text": "(Liu, 2009)",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "D.4 Comparison of Different Models"
        },
        {
            "text": "\u2022 Normalized Discounted Cumulative Gain (NDCG) (Liu, 2009 ): A measure for the overall ranking performance. It first computes the discounted cumulative gain (DCG) by summing the discounted relevance scores 1, 0.9, . . . , 0.1 of the 10 models with the lowest true nCRPS scores -more specifically, a relevance score \u03c0(i ) is discounted by log 2 k + 1 where k is the predicted rank of the model with true rank i . Then, it normalizes the DCG by using the ideal DCG (iDCG) to obtain the NDCG. As a comparison to the ranking MLP surrogate with linear discount (MLP Ranking + Discounting), we first consider a random surrogate which predicts nCRPS by sampling from U(0, 1) as a baseline (Random). Then, we use a nonparametric model which predicts the nCRPS as the average among all training datasets (Nonparametric Regression) and one that predicts the average rank of the nCRPS across training datasets (Nonparametric Ranking). Additionally, we consider XGBoost surrogates which are trained via regression (XGBoost Regression) and pairwise ranking 6 (XGBoost Ranking), respectively. Lastly, we consider an MLP with the same architecture but trained via regression instead of listwise ranking (MLP Regression) and one without any discounting (MLP Ranking). ",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 57,
                    "text": "(Liu, 2009",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "D.4 Comparison of Different Models"
        },
        {
            "text": "In this section, we try to answer the following question: are there some simple dataset characteristics that can help to know which model is going to perform best? In particular, we analyze in details the results of Section 3.5 where we showed that, on some datasets, (1) classical methods outperformed deep learning methods and that (2) the performance of classical and deep learning methods could not be distinguished.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E PARETOSELECT pseudo code F Analysis of datasets characteristics"
        },
        {
            "text": "We start by analyzing more in depth the four out of 44 datasets where deep learning models do not outperform classical methods - Figure 2 showed that the best deep learning method outperforms the best classical methods on all but four of our benchmark datasets. On these four datasets (\"M4 Hourly\", \"Temperature Rain\", \"Tourism Monthly\", \"Vehicle Trips\"), the summary statistics differ wildly as can be seen from Table 3 both in terms of the number of observations, the average length, and other characteristics. Further, the classical method that outperforms the best deep learning method is inconsistent across these datasets (\"M4 Hourly\": STL-AR, \"Temperature Rain\": NPTS, \"Tourism Monthly\": ETS, \"Vehicle Trips\": NPTS). Notably, this does not include the local method that performs best on average (i.e. ARIMA, see Table 1 ). Figure 1 showed that for 14 out of the 44 benchmark datasets, the performance of classical methods is indistinguishable from the performance of deep learning methods. To further study these datasets, we plot in Figure 7 the correlation matrix of all k benchmark methods' nCRPS ranks across the datasets (using only the ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 129,
                    "end": 137,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 413,
                    "end": 420,
                    "text": "Table 3",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 819,
                    "end": 826,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 830,
                    "end": 838,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1041,
                    "end": 1049,
                    "text": "Figure 7",
                    "ref_id": "FIGREF11"
                }
            ],
            "section": "E PARETOSELECT pseudo code F Analysis of datasets characteristics"
        }
    ],
    "bib_entries": {
        "BIBREF1": {
            "ref_id": "b1",
            "title": "GluonTS: Probabilistic and Neural Time Series Modeling in Python",
            "authors": [
                {
                    "first": "Alexander",
                    "middle": [],
                    "last": "Alexandrov",
                    "suffix": ""
                },
                {
                    "first": "Konstantinos",
                    "middle": [],
                    "last": "Benidis",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Bohlke-Schneider",
                    "suffix": ""
                },
                {
                    "first": "Valentin",
                    "middle": [],
                    "last": "Flunkert",
                    "suffix": ""
                },
                {
                    "first": "Jan",
                    "middle": [],
                    "last": "Gasthaus",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Januschowski",
                    "suffix": ""
                },
                {
                    "first": "Danielle",
                    "middle": [
                        "C"
                    ],
                    "last": "Maddix",
                    "suffix": ""
                },
                {
                    "first": "Syama",
                    "middle": [],
                    "last": "Rangapuram",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Salinas",
                    "suffix": ""
                },
                {
                    "first": "Jasper",
                    "middle": [],
                    "last": "Schulz",
                    "suffix": ""
                },
                {
                    "first": "Lorenzo",
                    "middle": [],
                    "last": "Stella",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Caner T\u00fcrkmen",
                    "suffix": ""
                },
                {
                    "first": "Yuyang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Machine Learning Research",
            "volume": "21",
            "issn": "116",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "The Theta Model: A Decomposition Approach to Forecasting",
            "authors": [
                {
                    "first": "Vassilis",
                    "middle": [],
                    "last": "Assimakopoulos",
                    "suffix": ""
                },
                {
                    "first": "Konstantinos",
                    "middle": [],
                    "last": "Nikolopoulos",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "International Journal of Forecasting",
            "volume": "16",
            "issn": "4",
            "pages": "521--530",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "The tourism forecasting competition",
            "authors": [
                {
                    "first": "George",
                    "middle": [],
                    "last": "Athanasopoulos",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Rob",
                    "suffix": ""
                },
                {
                    "first": "Haiyan",
                    "middle": [],
                    "last": "Hyndman",
                    "suffix": ""
                },
                {
                    "first": "Doris C",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "International Journal of Forecasting",
            "volume": "27",
            "issn": "3",
            "pages": "822--844",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "The kalai-smorodinsky solution for many-objective bayesian optimization",
            "authors": [
                {
                    "first": "Micka\u00ebl",
                    "middle": [],
                    "last": "Binois",
                    "suffix": ""
                },
                {
                    "first": "Victor",
                    "middle": [],
                    "last": "Picheny",
                    "suffix": ""
                },
                {
                    "first": "Patrick",
                    "middle": [],
                    "last": "Taillandier",
                    "suffix": ""
                },
                {
                    "first": "Abderrahmane",
                    "middle": [],
                    "last": "Habbal",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "J. Mach. Learn. Res",
            "volume": "21",
            "issn": "150",
            "pages": "1--42",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A comparison of normalization methods for high density oligonucleotide array data based on variance and bias",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Benjamin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bolstad",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rafael",
                    "suffix": ""
                },
                {
                    "first": "Magnus",
                    "middle": [],
                    "last": "Irizarry",
                    "suffix": ""
                },
                {
                    "first": "Terence",
                    "middle": [
                        "P"
                    ],
                    "last": "\u00c5strand",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Speed",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Bioinformatics",
            "volume": "19",
            "issn": "2",
            "pages": "185--193",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Caltrans performance measurement system",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Caltrans",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems",
            "authors": [
                {
                    "first": "Tianqi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Mu",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Yutian",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Min",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Naiyan",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Minjie",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Tianjun",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "Bing",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Chiyuan",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Zheng",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1512.01274"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Pedestrian counting system -monthly (counts per hour",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "City Of Melbourne",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Nearest-Neighbor Searching and Metric Space Dimensions. Nearest-Neighbor Methods for Learning and Vision: Theory and Practice",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Kenneth L Clarkson",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "15--59",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II",
            "authors": [
                {
                    "first": "Kalyanmoy",
                    "middle": [],
                    "last": "Deb",
                    "suffix": ""
                },
                {
                    "first": "Amrit",
                    "middle": [],
                    "last": "Pratap",
                    "suffix": ""
                },
                {
                    "first": "Sameer",
                    "middle": [],
                    "last": "Agarwal",
                    "suffix": ""
                },
                {
                    "first": "Tamt",
                    "middle": [],
                    "last": "Meyarivan",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "IEEE Transactions on Evolutionary Computation",
            "volume": "6",
            "issn": "2",
            "pages": "182--197",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "An interactive web-based dashboard to track COVID-19 in real time",
            "authors": [
                {
                    "first": "Ensheng",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "Hongru",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "Lauren",
                    "middle": [],
                    "last": "Gardner",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "The Lancet Infectious Diseases",
            "volume": "20",
            "issn": "",
            "pages": "533--534",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Nas-bench-201: Extending the scope of reproducible neural architecture search",
            "authors": [
                {
                    "first": "Xuanyi",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "Yi",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "UCI Machine Learning Repository",
            "authors": [
                {
                    "first": "Dheeru",
                    "middle": [],
                    "last": "Dua",
                    "suffix": ""
                },
                {
                    "first": "Casey",
                    "middle": [],
                    "last": "Graff",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Corporaci\u00f3n Favorita Grocery Sales Forecasting",
            "authors": [
                {
                    "first": "Corporaci\u00f3n",
                    "middle": [],
                    "last": "Favorita",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Uber TLC FOIL Response",
            "authors": [
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Flowers",
                    "suffix": ""
                },
                {
                    "first": "Reuben",
                    "middle": [],
                    "last": "Fischer-Baum",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Probabilistic Forecasting with Spline Quantile Function RNNs",
            "authors": [
                {
                    "first": "Jan",
                    "middle": [],
                    "last": "Gasthaus",
                    "suffix": ""
                },
                {
                    "first": "Konstantinos",
                    "middle": [],
                    "last": "Benidis",
                    "suffix": ""
                },
                {
                    "first": "Yuyang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Syama",
                    "middle": [],
                    "last": "Sundar Rangapuram",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Salinas",
                    "suffix": ""
                },
                {
                    "first": "Valentin",
                    "middle": [],
                    "last": "Flunkert",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Januschowski",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "volume": "",
            "issn": "",
            "pages": "1901--1910",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Nonparametric Statistical Inference. Marcel Dekker",
            "authors": [
                {
                    "first": "Jean",
                    "middle": [],
                    "last": "Dickinson Gibbons",
                    "suffix": ""
                },
                {
                    "first": "Subhabrata",
                    "middle": [],
                    "last": "Chakraborti",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Strictly Proper Scoring Rules, Prediction, and Estimation",
            "authors": [
                {
                    "first": "Tilmann",
                    "middle": [],
                    "last": "Gneiting",
                    "suffix": ""
                },
                {
                    "first": "Adrian",
                    "middle": [
                        "E"
                    ],
                    "last": "Raftery",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Journal of the American Statistical Association",
            "volume": "102",
            "issn": "477",
            "pages": "359--378",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Monash Time Series Forecasting Archive",
            "authors": [
                {
                    "first": "Rakshitha",
                    "middle": [],
                    "last": "Godahewa",
                    "suffix": ""
                },
                {
                    "first": "Christoph",
                    "middle": [],
                    "last": "Bergmeir",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [
                        "I"
                    ],
                    "last": "Webb",
                    "suffix": ""
                },
                {
                    "first": "Rob",
                    "middle": [
                        "J"
                    ],
                    "last": "Hyndman",
                    "suffix": ""
                },
                {
                    "first": "Pablo",
                    "middle": [],
                    "last": "Montero-Manso",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2105.06643"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Google Vizier: A Service for Black-Box Optimization",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Golovin",
                    "suffix": ""
                },
                {
                    "first": "Benjamin",
                    "middle": [],
                    "last": "Solnik",
                    "suffix": ""
                },
                {
                    "first": "Subhodeep",
                    "middle": [],
                    "last": "Moitra",
                    "suffix": ""
                },
                {
                    "first": "Greg",
                    "middle": [],
                    "last": "Kochanski",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Recruit Restaurant Visitor Forecasting",
            "authors": [],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Automated Machine Learning: Methods, Systems, Challenges",
            "authors": [
                {
                    "first": "Frank",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [],
                    "last": "Kotthoff",
                    "suffix": ""
                },
                {
                    "first": "Joaquin",
                    "middle": [],
                    "last": "Vanschoren",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "forecast: Forecasting functions for time series and linear models",
            "authors": [
                {
                    "first": "Rob",
                    "middle": [],
                    "last": "Hyndman",
                    "suffix": ""
                },
                {
                    "first": "George",
                    "middle": [],
                    "last": "Athanasopoulos",
                    "suffix": ""
                },
                {
                    "first": "Christoph",
                    "middle": [],
                    "last": "Bergmeir",
                    "suffix": ""
                },
                {
                    "first": "Gabriel",
                    "middle": [],
                    "last": "Caceres",
                    "suffix": ""
                },
                {
                    "first": "Leanne",
                    "middle": [],
                    "last": "Chhay",
                    "suffix": ""
                },
                {
                    "first": "O&apos;",
                    "middle": [],
                    "last": "Mitchell",
                    "suffix": ""
                },
                {
                    "first": "Fotios",
                    "middle": [],
                    "last": "Hara-Wild",
                    "suffix": ""
                },
                {
                    "first": "Slava",
                    "middle": [],
                    "last": "Petropoulos",
                    "suffix": ""
                },
                {
                    "first": "Earo",
                    "middle": [],
                    "last": "Razbash",
                    "suffix": ""
                },
                {
                    "first": "Farah",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Yasmeen",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "expsmooth: Data Sets from \"Forecasting with Exponential Smoothing",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Rob",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hyndman",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Forecasting: Principles and Practice",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Rob",
                    "suffix": ""
                },
                {
                    "first": "George",
                    "middle": [],
                    "last": "Hyndman",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Athanasopoulos",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Dominick's Dataset",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "Kilts"
                    ],
                    "last": "James",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Center",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Criteria for classifying forecasting methods",
            "authors": [
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Januschowski",
                    "suffix": ""
                },
                {
                    "first": "Jan",
                    "middle": [],
                    "last": "Gasthaus",
                    "suffix": ""
                },
                {
                    "first": "Yuyang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Salinas",
                    "suffix": ""
                },
                {
                    "first": "Valentin",
                    "middle": [],
                    "last": "Flunkert",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Bohlke-Schneider",
                    "suffix": ""
                },
                {
                    "first": "Laurent",
                    "middle": [],
                    "last": "Callot",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Journal of Forecasting",
            "volume": "36",
            "issn": "1",
            "pages": "167--177",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Smart meters in London",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Jean-Michael",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Robust recurrent network model for intermittent time-series forecasting",
            "authors": [
                {
                    "first": "Yunho",
                    "middle": [],
                    "last": "Jeon",
                    "suffix": ""
                },
                {
                    "first": "Sihyeon",
                    "middle": [],
                    "last": "Seong",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "International Journal of Forecasting",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Adam: A Method for Stochastic Optimization",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Diederik",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.6980"
                ]
            }
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Modeling Long-and Short-Term Temporal Patterns with Deep Neural Networks",
            "authors": [
                {
                    "first": "Guokun",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "Wei-Cheng",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Yiming",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Hanxiao",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International ACM SIGIR Conference on Research & Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "95--104",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
            "authors": [
                {
                    "first": "Lisha",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Jamieson",
                    "suffix": ""
                },
                {
                    "first": "Giulia",
                    "middle": [],
                    "last": "Desalvo",
                    "suffix": ""
                },
                {
                    "first": "Afshin",
                    "middle": [],
                    "last": "Rostamizadeh",
                    "suffix": ""
                },
                {
                    "first": "Ameet",
                    "middle": [],
                    "last": "Talwalkar",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Journal of Machine Learning Research",
            "volume": "18",
            "issn": "1",
            "pages": "6765--6816",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey",
            "authors": [
                {
                    "first": "Miqing",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Xin",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACM Computing Surveys",
            "volume": "52",
            "issn": "2",
            "pages": "1--38",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting",
            "authors": [
                {
                    "first": "Bryan",
                    "middle": [],
                    "last": "Lim",
                    "suffix": ""
                },
                {
                    "first": "\u00d6",
                    "middle": [],
                    "last": "Sercan",
                    "suffix": ""
                },
                {
                    "first": "Nicolas",
                    "middle": [],
                    "last": "Ar\u0131k",
                    "suffix": ""
                },
                {
                    "first": "Tomas",
                    "middle": [],
                    "last": "Loeff",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pfister",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "International Journal of Forecasting",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Learning to Rank for Information Retrieval",
            "authors": [
                {
                    "first": "Tie-Yan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Foundations and Trends in Information Retrieval",
            "volume": "3",
            "issn": "3",
            "pages": "225--331",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "The M5 accuracy competition: Results, findings and conclusions",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Makridakis",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Spiliotis",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Assimakopoulos",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Journal of Forecasting",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "The M3-Competition: results, conclusions and implications",
            "authors": [
                {
                    "first": "Spyros",
                    "middle": [],
                    "last": "Makridakis",
                    "suffix": ""
                },
                {
                    "first": "Michele",
                    "middle": [],
                    "last": "Hibon",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "International Journal of Forecasting",
            "volume": "16",
            "issn": "4",
            "pages": "451--476",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "The Accuracy of Extrapolation (lime Series) Methods: Results of a Forecasting Competit ion",
            "authors": [
                {
                    "first": "Spyros",
                    "middle": [],
                    "last": "Makridakis",
                    "suffix": ""
                },
                {
                    "first": "Allan",
                    "middle": [],
                    "last": "Andersen",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Carbone",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Fildes",
                    "suffix": ""
                },
                {
                    "first": "Michele",
                    "middle": [],
                    "last": "Hibon",
                    "suffix": ""
                },
                {
                    "first": "Rudolf",
                    "middle": [],
                    "last": "Lewandowski",
                    "suffix": ""
                },
                {
                    "first": "Joseph",
                    "middle": [],
                    "last": "Newton",
                    "suffix": ""
                },
                {
                    "first": "Emanuel",
                    "middle": [],
                    "last": "Parzen",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Winkler",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "Journal of Forecasting",
            "volume": "1",
            "issn": "2",
            "pages": "111--153",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Evangelos Spiliotis, and Vassilios Assimakopoulos. Statistical and Machine Learning Forecasting Methods: Concerns and Ways Forward",
            "authors": [
                {
                    "first": "Spyros",
                    "middle": [],
                    "last": "Makridakis",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "13",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "The M4 Competition: 100,000 time series and 61 forecasting methods",
            "authors": [
                {
                    "first": "Spyros",
                    "middle": [],
                    "last": "Makridakis",
                    "suffix": ""
                },
                {
                    "first": "Evangelos",
                    "middle": [],
                    "last": "Spiliotis",
                    "suffix": ""
                },
                {
                    "first": "Vassilios",
                    "middle": [],
                    "last": "Assimakopoulos",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Journal of Forecasting",
            "volume": "36",
            "issn": "1",
            "pages": "54--74",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Scoring rules for continuous probability distributions",
            "authors": [
                {
                    "first": "James",
                    "middle": [
                        "E"
                    ],
                    "last": "Matheson",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [
                        "L"
                    ],
                    "last": "Winkler",
                    "suffix": ""
                }
            ],
            "year": 1976,
            "venue": "Management Science",
            "volume": "22",
            "issn": "10",
            "pages": "1087--1096",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "FRED-MD: A Monthly Database for Macroeconomic Research",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Michael",
                    "suffix": ""
                },
                {
                    "first": "Serena",
                    "middle": [],
                    "last": "Mccracken",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Journal of Business & Economic Statistics",
            "volume": "34",
            "issn": "4",
            "pages": "574--589",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Principles and Algorithms for Forecasting groups of Time Series: Locality and Globality",
            "authors": [
                {
                    "first": "Pablo",
                    "middle": [],
                    "last": "Montero-Manso",
                    "suffix": ""
                },
                {
                    "first": "Rob",
                    "middle": [
                        "J"
                    ],
                    "last": "Hyndman",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "International Journal of Forecasting",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "TLC Trip Record Data",
            "authors": [
                {
                    "first": "Limousine",
                    "middle": [],
                    "last": "Nyc Taxi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Commission",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "tsibbledata: Diverse Datasets for 'tsibble",
            "authors": [
                {
                    "first": "O&apos;",
                    "middle": [],
                    "last": "Mitchell",
                    "suffix": ""
                },
                {
                    "first": "Rob",
                    "middle": [],
                    "last": "Hara-Wild",
                    "suffix": ""
                },
                {
                    "first": "Earo",
                    "middle": [],
                    "last": "Hyndman",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting",
            "authors": [
                {
                    "first": "Dmitri",
                    "middle": [],
                    "last": "Boris N Oreshkin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Carpov",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.10437"
                ]
            }
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Theory and Practice",
            "authors": [
                {
                    "first": "Fotios",
                    "middle": [],
                    "last": "Petropoulos",
                    "suffix": ""
                },
                {
                    "first": "Daniele",
                    "middle": [],
                    "last": "Apiletti",
                    "suffix": ""
                },
                {
                    "first": "Vassilios",
                    "middle": [],
                    "last": "Assimakopoulos",
                    "suffix": ""
                },
                {
                    "first": "Mohamed",
                    "middle": [
                        "Zied"
                    ],
                    "last": "Babai",
                    "suffix": ""
                },
                {
                    "first": "Devon",
                    "middle": [
                        "K"
                    ],
                    "last": "Barrow",
                    "suffix": ""
                },
                {
                    "first": "Christoph",
                    "middle": [],
                    "last": "Bergmeir",
                    "suffix": ""
                },
                {
                    "first": "Ricardo",
                    "middle": [
                        "J"
                    ],
                    "last": "Bessa",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [
                        "E"
                    ],
                    "last": "Boylan",
                    "suffix": ""
                },
                {
                    "first": "Jethro",
                    "middle": [],
                    "last": "Browell",
                    "suffix": ""
                },
                {
                    "first": "Claudio",
                    "middle": [],
                    "last": "Carnevale",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2012.03854"
                ]
            }
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Learning Multiple Defaults for Machine Learning Algorithms",
            "authors": [
                {
                    "first": "Florian",
                    "middle": [],
                    "last": "Pfisterer",
                    "suffix": ""
                },
                {
                    "first": "Jan",
                    "middle": [
                        "N"
                    ],
                    "last": "Van Rijn",
                    "suffix": ""
                },
                {
                    "first": "Philipp",
                    "middle": [],
                    "last": "Probst",
                    "suffix": ""
                },
                {
                    "first": "Andreas",
                    "middle": [
                        "C"
                    ],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "Bernd",
                    "middle": [],
                    "last": "Bischl",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the Genetic and Evolutionary Computation Conference Companion",
            "volume": "",
            "issn": "",
            "pages": "241--242",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Deep Non-Parametric Time Series Forecaster. to be published",
            "authors": [
                {
                    "first": "Jan",
                    "middle": [],
                    "last": "Syama Sundar Rangapuram",
                    "suffix": ""
                },
                {
                    "first": "Lorenzo",
                    "middle": [],
                    "last": "Gasthaus",
                    "suffix": ""
                },
                {
                    "first": "Valentin",
                    "middle": [],
                    "last": "Stella",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Flunkert",
                    "suffix": ""
                },
                {
                    "first": "Bernie",
                    "middle": [],
                    "last": "Salinas",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Januschowski",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Rossmann Store Sales",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Rossmann",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Salinas",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Bohlke-Schneider",
                    "suffix": ""
                },
                {
                    "first": "Laurent",
                    "middle": [],
                    "last": "Callot",
                    "suffix": ""
                },
                {
                    "first": "Roberto",
                    "middle": [],
                    "last": "Medico",
                    "suffix": ""
                },
                {
                    "first": "Jan",
                    "middle": [],
                    "last": "Gasthaus",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1910.03002"
                ]
            }
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Salinas",
                    "suffix": ""
                },
                {
                    "first": "Valentin",
                    "middle": [],
                    "last": "Flunkert",
                    "suffix": ""
                },
                {
                    "first": "Jan",
                    "middle": [],
                    "last": "Gasthaus",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Januschowski",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Journal of Forecasting",
            "volume": "36",
            "issn": "3",
            "pages": "1181--1191",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "A quantile-based approach for hyperparameter transfer learning",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Salinas",
                    "suffix": ""
                },
                {
                    "first": "Huibin",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Valerio",
                    "middle": [],
                    "last": "Perrone",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 37th International Conference on Machine Learning",
            "volume": "119",
            "issn": "",
            "pages": "13--18",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "A Multi-Objective Perspective on Jointly Tuning Hardware and Hyperparameters",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Salinas",
                    "suffix": ""
                },
                {
                    "first": "Valerio",
                    "middle": [],
                    "last": "Perrone",
                    "suffix": ""
                },
                {
                    "first": "Olivier",
                    "middle": [],
                    "last": "Cruchant",
                    "suffix": ""
                },
                {
                    "first": "Cedric",
                    "middle": [],
                    "last": "Archambeau",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "ICLR Workshop on Neural Architecture Search",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "AutoAI-TS: AutoAI for Time Series Forecasting",
            "authors": [
                {
                    "first": "Dhaval",
                    "middle": [],
                    "last": "Syed Yousaf Shah",
                    "suffix": ""
                },
                {
                    "first": "Long",
                    "middle": [],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "Xuan-Hong",
                    "middle": [],
                    "last": "Vu",
                    "suffix": ""
                },
                {
                    "first": "Bei",
                    "middle": [],
                    "last": "Dang",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Horst",
                    "middle": [],
                    "last": "Kirchner",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Samulowitz",
                    "suffix": ""
                },
                {
                    "first": "Gregory",
                    "middle": [],
                    "last": "Wood",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bramble",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wesley",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gifford",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2102.12347"
                ]
            }
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "The Logic of Logistics: Theory, Algorithms, and Applications for Logistics Management",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Simchi-Levi",
                    "suffix": ""
                },
                {
                    "first": "Xin",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Julien",
                    "middle": [],
                    "last": "Bramel",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "A hybrid method of Exponential Smoothing and Recurrent Neural Networks for time series forecasting",
            "authors": [
                {
                    "first": "Slawek",
                    "middle": [],
                    "last": "Smyl",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Journal of Forecasting",
            "volume": "36",
            "issn": "1",
            "pages": "75--85",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "bomrang: Australian Government Bureau of Meteorology (BOM) Data Client",
            "authors": [
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Sparks",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Multiobjective Optimization Using Nondominated Sorting in Genetic Algorithms",
            "authors": [
                {
                    "first": "Nidamarthi",
                    "middle": [],
                    "last": "Srinivas",
                    "suffix": ""
                },
                {
                    "first": "Kalyanmoy",
                    "middle": [],
                    "last": "Deb",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Evolutionary Computation",
            "volume": "2",
            "issn": "3",
            "pages": "221--248",
            "other_ids": {}
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "Information-theoretic regret bounds for gaussian process optimization in the bandit setting",
            "authors": [
                {
                    "first": "Niranjan",
                    "middle": [],
                    "last": "Srinivas",
                    "suffix": ""
                },
                {
                    "first": "Andreas",
                    "middle": [],
                    "last": "Krause",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sham",
                    "suffix": ""
                },
                {
                    "first": "Matthias",
                    "middle": [
                        "W"
                    ],
                    "last": "Kakade",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Seeger",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "58",
            "issn": "5",
            "pages": "3250--3265",
            "other_ids": {
                "DOI": [
                    "10.1109/TIT.2011.2182033"
                ]
            }
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "On the Results and Observations of the Time Series Forecasting Competition CIF 2016",
            "authors": [
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "\u0160t\u011bpni\u010dka",
                    "suffix": ""
                },
                {
                    "first": "Michal",
                    "middle": [],
                    "last": "Burda",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE International Conference on Fuzzy Systems",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition",
            "authors": [
                {
                    "first": "Gianluca",
                    "middle": [],
                    "last": "Souhaib Ben Taieb",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bontempi",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Amir",
                    "suffix": ""
                },
                {
                    "first": "Antti",
                    "middle": [],
                    "last": "Atiya",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sorjamaa",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Expert Systems with Applications",
            "volume": "39",
            "issn": "8",
            "pages": "7067--7083",
            "other_ids": {}
        },
        "BIBREF67": {
            "ref_id": "b67",
            "title": "Forecasting at Scale. The American Statistician",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sean",
                    "suffix": ""
                },
                {
                    "first": "Benjamin",
                    "middle": [],
                    "last": "Taylor",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Letham",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Walmart. Walmart Recruiting -Store Sales Forecasting",
            "volume": "72",
            "issn": "",
            "pages": "37--45",
            "other_ids": {}
        },
        "BIBREF69": {
            "ref_id": "b69",
            "title": "Practical and sample efficient zero-shot HPO",
            "authors": [
                {
                    "first": "Fela",
                    "middle": [],
                    "last": "Winkelmolen",
                    "suffix": ""
                },
                {
                    "first": "Nikita",
                    "middle": [],
                    "last": "Ivkin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "H Furkan",
                    "suffix": ""
                },
                {
                    "first": "Zohar",
                    "middle": [],
                    "last": "Bozkurt",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Karnin",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2007.13382"
                ]
            }
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "Learning hyperparameter optimization initializations",
            "authors": [
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Wistuba",
                    "suffix": ""
                },
                {
                    "first": "Nicolas",
                    "middle": [],
                    "last": "Schilling",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [],
                    "last": "Schmidt-Thieme",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)",
            "volume": "",
            "issn": "",
            "pages": "1--10",
            "other_ids": {
                "DOI": [
                    "10.1109/DSAA.2015.7344817"
                ]
            }
        },
        "BIBREF71": {
            "ref_id": "b71",
            "title": "Listwise Approach to Learning to Rank: Theory and Algorithm",
            "authors": [
                {
                    "first": "Fen",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "Tie-Yan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jue",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Wensheng",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Hang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "International Conference on Machine learning",
            "volume": "",
            "issn": "",
            "pages": "1192--1199",
            "other_ids": {}
        },
        "BIBREF72": {
            "ref_id": "b72",
            "title": "Nas-bench-101: Towards reproducible neural architecture search",
            "authors": [
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "Esteban",
                    "middle": [],
                    "last": "Real",
                    "suffix": ""
                },
                {
                    "first": "Eric",
                    "middle": [],
                    "last": "Christiansen",
                    "suffix": ""
                },
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "Frank",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF73": {
            "ref_id": "b73",
            "title": "Solar Power Data for Integration Studies",
            "authors": [
                {
                    "first": "Yingchen",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF74": {
            "ref_id": "b74",
            "title": "Multiobjective Optimization Using Evolutionary Algorithms -A Comparative Case Study",
            "authors": [
                {
                    "first": "Eckart",
                    "middle": [],
                    "last": "Zitzler",
                    "suffix": ""
                },
                {
                    "first": "Lothar",
                    "middle": [],
                    "last": "Thiele",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "International Conference on Parallel Problem Solving from Nature",
            "volume": "",
            "issn": "",
            "pages": "292--301",
            "other_ids": {}
        },
        "BIBREF75": {
            "ref_id": "b75",
            "title": "Adversarial attacks on graph neural networks: Perturbations and their patterns",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Z\u00fcgner",
                    "suffix": ""
                },
                {
                    "first": "Oliver",
                    "middle": [],
                    "last": "Borchert",
                    "suffix": ""
                },
                {
                    "first": "Amir",
                    "middle": [],
                    "last": "Akbarnejad",
                    "suffix": ""
                },
                {
                    "first": "Stephan",
                    "middle": [],
                    "last": "G\u00fcnnemann",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ACM Transactions on Knowledge Discovery from Data",
            "volume": "14",
            "issn": "5",
            "pages": "1--31",
            "other_ids": {}
        },
        "BIBREF76": {
            "ref_id": "b76",
            "title": "32 GiB memory) to counteract out-of-memory-issues. Used for N-BEATS on \"Corporaci\u00f3n Favorita\" and \"Weather",
            "authors": [],
            "year": null,
            "venue": "DeepAR on \"Corporaci\u00f3n Favorita",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF77": {
            "ref_id": "b77",
            "title": "64 GiB memory) to allow for even more memory-intensive models. Used for N-BEATS on",
            "authors": [],
            "year": null,
            "venue": "\u2022 ml.m5.4xlarge instances (16 CPUs",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF78": {
            "ref_id": "b78",
            "title": "144 GiB memory) to speed up predictions by parallelization. Used for ARIMA on",
            "authors": [],
            "year": null,
            "venue": "\u2022 ml.c5.18xlarge instances (72 CPUs",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF79": {
            "ref_id": "b79",
            "title": "Including validation and testing, this amounted to roughly 684 days of total training time (\u223c3 hours and 30 minutes per training job). Parallelized over 200 instances, actual training time for our benchmark was roughly 4 days and amounts to approximately $7",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The p-values of the null hypothesis that statistical methods perform equal to or better than deep learning methods, evaluated for datasets of different sizes. The red line displays the significance level \u03b1 = 5%.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The relative improvement of the best deep learning method versus the best classical method on all benchmark datasets. The red line is fitted on the visualized datapoints via linear regression in the logspace.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Illustration of non-dominated sorting. The layers show the partitioning of the data in Pareto fronts. The numbers depict the overall rank by computing the -net within each layer. Hypervolume error when iteratively choosing models via different model selection approaches. Errors are averaged over all 44 benchmark datasets and five random seeds.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Visualization of forecast latency and nCRPS for models trained and evaluated on the \"M4 Yearly\" dataset. Each color describes a family of models with dots representing different hyperparameter configurations and training times (checkpoints for deep learning models). Crosses show the default hyperparameter configuration and the maximum training time for the dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Comparison of individual forecasting models, hyper-ensembles and latency-constrained ensembles. The axes show the average relative latency and nCRPS compared to the unconstrained ensemble. Results are averaged across all 44 benchmark datasets. Light crosses show default configurations of deep learning models, bold crosses show their corresponding hyper-ensembles. Classical models are not colored as most can only be found (far) beyond the range of the plot.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "(O'Hara-Wild et al., 2021;Godahewa et al., 2021) provides the 30-minute electricity demand of five Australian states (New South Wales, Queensland, South Australia, Tasmania, Victoria). Time series range from January 2002 to April 2015.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "MD (McCracken & Ng, 2016;Godahewa et al., 2021) contains monthly time series of macro-economic indicators (e.g. interest rates, employment rates, . . . ) from the Federal Reserve Bank since January 1959 (until September 2019).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "is the dataset used by the KDD Cup 2018. It contains hourly time series with air quality levels (represented by various chemical compounds) measured by stations in Beijing and London between January 2017 and April 2018. London Smart Meters (Jean-Michael, 2019; Godahewa et al., 2021) was originally published on Kaggle and contains the half-hourly (electrical) energy consumption (in kWh) of thousands of households in the period from November 2011 to February 2014.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "(Taieb et al., 2012;Godahewa et al., 2021) data was used in the NN5 competition run in 2008. The time series provide daily amounts of cash withdrawals at dozens of ATMs in the UK from March 1996 to May 1998. Pedestrian Count (City of Melbourne, 2017; Godahewa et al., 2021) provides hourly pedestrian counts in Melbourne from May 2009 to May 2020. The counts are captured by sensors scattered in the city.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "datasets come from a Kaggle forecasting competition where time series were supplied by tourism bodies from Australia, Hong Kong, and New Zealand as well as academics. Monthly data ranges from 1979 through 2007, quarterly data from from 1975 through 2007, and yearly data from 1960 through 2008.Vehicle Trips(Flowers & Fischer-Baum, 2015;Godahewa et al., 2021) contains the daily number of trips served by hundreds of for-hire vehicle (FHV) companies (such as Uber ). Data ranges from January to September 2015.Walmart (Walmart, 2014) contains data about Walmart store sales similarly to the M5 competition. Data is provided as the weekly sales (in USD) across 45 stores and 81 departments in different regions and ranges from February 2010 to November 2012. Just like for many of the other Kaggle competitions, covariates are available, yet unused.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Correlation matrix of all benchmark methods' nCRPS ranks across all benchmark datasets. For the computation of the ranks, all classical methods and the default configuration of deep learning models are considered.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The average performance of various forecasting methods across all datasets with respect to relative latency (compared to Seasonal Na\u00efve) and nCRPS. The table shows classical methods (top), deep learning methods (middle, left), hyper-ensembles using all hyperparameter configurations of deep learning models (middle, right), and latency-constrained ensembles (bottom). Best values across within each group are displayed in bold, best values across all models are starred.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Overview of all models considered and their associated hyperparameters. The upper seven models do not require training. The lower six models are deep learning models. For all deep learning models, three context lengths are considered as well as three hyperparameter configurations (except for MQ-RNN) that result in small, medium and large models, respectively. Hyperparameters that are not augmented keep their default values. The hyperparameters of the \"default\" models correspond to the medium size.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Statistics of the benchmark datasets. Frequencies read as follows: MIN -minutely, H -hourly, D -daily, B -business daily, W -weekly, M -monthly, Q -quarterly, Y -yearly. Number of observations are calculated from the training data.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Comparison of different surrogate models with respect to their ability to rank model configurations according to their nCRPS. Ranking metrics are averaged across all benchmark datasets and multiplied by 100. Metrics for non-deterministic surrogates (random and MLP) are further averaged by running the entire evaluation over five random seeds. Best values for each metric are displayed in bold.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "clearly shows that the MLP with listwise ranking and linear discounting markedly outperforms all other choices for the surrogate model. Especially, it is much more capable of identifying the top configurations, stressed by the very high values of MRR and Precision@5 compared to the other surrogate models. This can likely be attributed to the linear discounting that encourages the model to focus on ranking the top configurations correctly. Thus, the loss formulation is more stable in the presence of outliers.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Algorithm 1: Overview of PARETOSELECT Data: Time series models X , offline evaluations D, number of default models n Result: A set {x 1 , . . . , xn} \u2282 X of model defaults// Choose and remove a random element while X = \u2205 do D = {} // Mapping from remaining elements to minimum distances for x \u2208 X do // Compute minimum distance to all chosen elementsdefault configuration for the deep learning models). Notably, for twelve datasets (\"Exchange Rate\" through \"M1 Yearly\" on the y-axis), we observe a very low correlation with a large number of datasets. On these datasets, all models perform similarly due to strong stochasticity (e.g. \"Exchange Rate\", \"Bitcoin\", \"M1 Quarterly\", ...) or seasonality (e.g. \"Tourism\", \"Ride share\", ...) and predicting their ranking is therefore hard. Notably, 11 of these datasets overlap with the 14 datasets where classical methods are indistinguishable from deep learning methods.In line with these observations, we found that adding simple dataset grouping features such as domain type (electricity, retail, finance) as an input for a surrogate model predicting the performance of forecasting methods does not have much effect. Our conclusion is that the ranks of the best performing models seem to be relatively hard to identify only from simple dataset grouping rules. Nonetheless, we hope that by releasing the benchmark data, we help future research to identify dominant methods from dataset characteristics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}
{
    "paper_id": "da08b50e39cc9aad146fd226e037a08416a487e5",
    "metadata": {
        "title": "Abstraction and Analogy-Making in Artificial Intelligence",
        "authors": [
            {
                "first": "Melanie",
                "middle": [],
                "last": "Mitchell",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Santa Fe Institute",
                    "location": {
                        "addrLine": "Santa Fe",
                        "region": "NM",
                        "country": "USA"
                    }
                },
                "email": ".mm@santafe.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Conceptual abstraction and analogy-making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite of a long history of research on constructing AI systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Without concepts there can be no thought, and without analogies there can be no concepts. -D. Hofstadter and E. Sander 50 In their 1955 proposal for the Dartmouth summer AI project, John McCarthy and colleagues wrote, \"An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.\" 77 Now, nearly seven decades later, all of these research topics remain open and actively investigated in the AI community. While AI has made dramatic progress over the last decade in areas such as computer vision, natural language processing, and robotics, current AI systems almost entirely lack the ability to form humanlike concepts and abstractions.",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 121,
                    "text": "Hofstadter and E. Sander 50",
                    "ref_id": null
                },
                {
                    "start": 392,
                    "end": 394,
                    "text": "77",
                    "ref_id": "BIBREF76"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "For example, while today's computer vision systems can recognize perceptual categories of objects, such as labeling a photo of the Golden Gate as \"a bridge,\" these systems lack the rich conceptual knowledge humans have about these objects-knowledge that enables robust recognition of such objects in a huge variety of contexts. Moreover, humans are able to form abstractions and apply them to novel situations in ways that elude even the best of today's machines. Continuing with the \"bridge\" example, humans can easily understand extended and metaphorical notions such as \"water bridges,\" \"ant bridges,\" \"bridging one's fingers,\" \"bridge of one's nose,\" \"the bridge of a song,\" \"bridging the gender gap,\" \"a bridge loan,\" \"burning one's bridges,\" \"water under the bridge,\" and so on. Indeed, for humans, any perceptual category such as bridge is understood via the rich conceptual structure underlying it. This conceptual structure makes it easy for humans to answer commonsense questions like \"what would happen if you drove across a raised drawbridge?\" or \"what is on each side of a bridge across the gender gap?\" Moreover, conceptual structures in the mind make it easy for humans to generate \"bridges\" at different levels of abstraction; for example, imagine yourself forming a bridge from a couch to a coffee table with your leg, or forming a bridge between two notes on your piano with other notes, or bridging differences with your spouse via a conversation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Most, if not all, human concepts can be abstracted in this way, via analogy (for example, as an interesting exercise, consider the possible abstractions of everyday concepts such as mirror, shadow, ceiling, driving, and sinking). Douglas Hofstadter goes so far as to define concepts by this property: \"a concept is a package of analogies.\" 54 Humans' abilities for abstraction and analogy are at the root of many of our most important cognitive capacities, and the lack of these abilities is at least partly responsible for current AI's brittleness and its difficulty in adapting knowledge and representations to new situations. Today's state-of-the-art AI systems often struggle in transferring what they have learned to situations outside their training regimes, 81 they make unexpected and unhumanlike errors, 1 and they are vulnerable to \"adversarial examples\" in a very unhumanlike way. 26, 103 Concepts, abstraction, and analogy are foundational areas of study in cognitive psychology. Psychological theories of concepts have focused on ideas such as core knowledge, 10,101 exemplars and prototypes, 84, 90 the \"theory theory\" of concepts, 44 perceptual simulations, 6 experience-based metaphors, 67 and stochastic functions in a probabilistic \"language of thought.\" 43 Some cognitive scientists have postulated that all concepts, even the most abstract ones, correspond to mental models of the world-grounded in perception-that can be simulated to yield predictions and counterfactuals. 6, 28, 61, 66, 68 Others have argued that concept formation, abstraction, and adaptation are all undergirded by processes of analogy-the act of perceiving essential similarities between entities or situations. 41, 54, 56 Understanding what concepts are-how they are formed, how they can be abstracted and flexibly used in new situations, how they can be composed to produce new concepts-is not only key to a deeper understanding of intelligence, but will be essential for engineering nonbrittle AI systems, ones that can abstract, robustly generalize, resist adversarial inputs, and adapt what they have learned to diverse domains and modalities.",
            "cite_spans": [
                {
                    "start": 340,
                    "end": 342,
                    "text": "54",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 765,
                    "end": 767,
                    "text": "81",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 892,
                    "end": 895,
                    "text": "26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 896,
                    "end": 899,
                    "text": "103",
                    "ref_id": "BIBREF102"
                },
                {
                    "start": 1106,
                    "end": 1109,
                    "text": "84,",
                    "ref_id": "BIBREF83"
                },
                {
                    "start": 1110,
                    "end": 1112,
                    "text": "90",
                    "ref_id": "BIBREF89"
                },
                {
                    "start": 1146,
                    "end": 1148,
                    "text": "44",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 1203,
                    "end": 1205,
                    "text": "67",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 1494,
                    "end": 1496,
                    "text": "6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1497,
                    "end": 1500,
                    "text": "28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1501,
                    "end": 1504,
                    "text": "61,",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 1505,
                    "end": 1508,
                    "text": "66,",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 1509,
                    "end": 1511,
                    "text": "68",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 1704,
                    "end": 1707,
                    "text": "41,",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 1708,
                    "end": 1711,
                    "text": "54,",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 1712,
                    "end": 1714,
                    "text": "56",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The purpose of this paper is to review selected AI research-both old and very recent-on abstraction and analogy-making, to make sense of what this research has yielded for the broader problem of creating machines with these general abilities, and to make some recommendations for the path forward in this field.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The remainder of the paper is organized as follows. The next section gives a brief discussion of the role of abstraction and analogy-making in human intelligence, and the need for such abilities in AI. The following sections review several approaches in the AI literature to capturing abstraction and analogy-making abilities, particularly using idealized domains. The paper concludes with a discussion that appraises the domains and methods discussed here and proposes several steps for making generalizable progress on these issues.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "One makes an conceptual abstraction one extends that concept to novel situations, including ones that are removed from perceptual entities, as in the examples of \"bridge\" I gave earlier. The process of abstraction is driven by analogy, in which one mentally maps the essence of one situation to a different situation (e.g., such as mapping one's concept of a bridge across a river to a bridge across the gender gap). We make an analogy every time our current situation reminds us of a past one, or when we respond to a friend's story with \"the same thing happened to me,\" even though \"the same thing\" can be superficially very different. In fact, the primary way we humans make sense of novel situations is by making analogies to situations we have previously experienced. Analogies underlie our abilities to flexibly recognize new instances of visual concepts such as \"a ski race\" or \"a protest march.\" Analogies drive our conceptualizations of wholly new situations, such as the novel coronavirus pandemic that erupted in early 2020, in terms of things we know something about-the pandemic has been variously described as a fire, a tsunami, a tornado, a volcano, \"another Katrina,\" or a war. Recognizing abstract styles of art or music is also a feat of analogy-making. Many scientific insights are based on analogies, such as Darwin's realization that biological competition is analogous to economic competition 97 and Von Neumann's analogies between the computer and the brain. 106 (These are just a few examples; see Ref. 50 for many more  examples at all levels of cognition.) These examples illustrate two important facts: (1) analogy-making is not a rare and exalted form of reasoning, but rather a constant, ubiquitous, and lifelong mode of thinking; (2) analogy is a key aspect not only of reasoning but also of flexible categorization, concept formation, abstraction, and counterfactual inference. 41, 50, 54, 105 In short, analogy is a central mechanism for unlocking meaning from perception. (See Ref. 7 for a broad discussion of analogy and its relation to other mechanisms of cognition and reasoning.)",
            "cite_spans": [
                {
                    "start": 1415,
                    "end": 1417,
                    "text": "97",
                    "ref_id": "BIBREF96"
                },
                {
                    "start": 1482,
                    "end": 1485,
                    "text": "106",
                    "ref_id": "BIBREF105"
                },
                {
                    "start": 1909,
                    "end": 1912,
                    "text": "41,",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 1913,
                    "end": 1916,
                    "text": "50,",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1917,
                    "end": 1920,
                    "text": "54,",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 1921,
                    "end": 1924,
                    "text": "105",
                    "ref_id": "BIBREF104"
                }
            ],
            "ref_spans": [
                {
                    "start": 1486,
                    "end": 1582,
                    "text": "(These are just a few examples; see Ref. 50 for many more  examples at all levels of cognition.)",
                    "ref_id": null
                }
            ],
            "section": "Abstraction and Analogy-Making in Intelligence"
        },
        {
            "text": "Analogy-making gives rise to human abilities lacking in even the best current-day AI systems. Many researchers have pointed out the need for AI systems that are more robust and general-that is, those that can perform sophisticated transfer learning or \"low-shot\" learning, that robustly can figure out how to make sense of novel situations, and that can form and use abstract concepts. Given this, it seems that analogy-making, and its role in abstraction, is an understudied area in AI that deserves more attention.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstraction and Analogy-Making in Intelligence"
        },
        {
            "text": "In the following sections I review several selected approaches to studying abstraction and analogy making in AI systems, ranging from older symbolic or hybrid approaches such as the structuremapping approach of Gentner et al. and the \"active symbol\" approach of Hofstadter et al., to more recent recent methods that employ deep neural networks and probabilistic program induction. This is not meant to be an exhaustive survey of this topic, but rather a review of some of the more prominent approaches, ones that I will analyze in the Discussion section below. Note that I will focus on concepts and analogies that involve multipart situations, rather than the simpler single-relation \"proportional\" analogies such as \"man is to woman as king is to ?\" 79,92 While the term \"analogy\" often brings to mind proportional analogies like these, the range of human analogymaking is far more interesting, rich, and ubiquitous.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstraction and Analogy-Making in Intelligence"
        },
        {
            "text": "Symbolic approaches to abstraction and analogy typically represent input as structured sets of logic statements, with concepts expressed as natural-language words or phrases. Early examples include Evans' geometric-analogy solver, 25 Winston's frame-based system for analogy-making between stories, 113 and Falkenhainer et al.'s Structure-Mapping Engine (SME). 27 In this section I will describe SME, as well as the Active Symbol Architecture of Hofstadter et al., which combines symbolic and subsymbolic elements.",
            "cite_spans": [
                {
                    "start": 231,
                    "end": 233,
                    "text": "25",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 299,
                    "end": 302,
                    "text": "113",
                    "ref_id": "BIBREF112"
                },
                {
                    "start": 361,
                    "end": 363,
                    "text": "27",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Symbolic Methods"
        },
        {
            "text": "The Structure-Mapping Engine (SME) 27 is based on Gentner's structure-mapping theory of human analogy-making. 40 In Gentner's theory, analogical mapping of one entity to another depends only on \"syntactic properties of the knowledge representation [describing the entities], and not on the specific content of the domains.\" 40 Furthermore, according to the theory, mappings are primarily made between relations rather than object attributes, and analogies are driven primarily by mappings between higher-order relations (the \"systematicity\" principle). SME's input consists of descriptions of two entities or situations, a base and a target, where each description consists of a set of logical propositions. In Figure 1 , adapted from one of Falkenhainer The descriptions in Figure 1 are logical statements represented as trees. The statements include entities (e.g., planet), attributes (e.g., yellow ), first-order relations (e.g., revolves-around ), and higher-order relations (e.g., causes). SME's job is to create a coherent mapping from the base to the target. The program uses a set of \"match rules\" to make candidate pairings between elements of the base and target. Examples of such rules are: \"If two relations have the same name, then pair them\"; \"If two objects play the same role in two already paired relations (i.e., are arguments in the same position), then pair them.\" The program then scores each of the pairings, based on factors such as having the same name, having the same type (e.g., object, attribute, nth-order relationship), and being part of \"systematic structures,\" that is more deeply nested propositions rather than isolated relations.",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 112,
                    "text": "40",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [
                {
                    "start": 711,
                    "end": 719,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 775,
                    "end": 783,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "Once all plausible pairings have been made, the program makes all possible sets of consistent combinations of these pairings, making each set (or \"global match\") as large as possible. \"Consistency\" here means that each element can match only one other element, and a pair is allowed to be in the global match only if all the arguments of each element are also paired up in the global match. After all possible global matches have been formed, each is given a score based on the individual pairings it is made up of, the inferences it suggests, and its degree of systematicity (relations that are part of a coherent interconnected system are preferentially mapped over relatively isolated relations). The output of the system is the set of possible global matches, ranked by score.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "It is important to note that SME is considered to be a model solely of the mapping process of analogy-making; it assumes that the situations to be mapped have already been represented in logical form. SME has often been used as a mapping module in a system that has other modules for representation, retrieval, and inference. 8, 18, 32 One notable example of such a system is the work of Lovett et al. 72 on solving Ravens Progressive Matrices (RPMs). I'll use this work as an illustrative example, since RPMs have more recently been studied extensively in the deep learning community as a domain for studying abstraction and analogy.",
            "cite_spans": [
                {
                    "start": 326,
                    "end": 328,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 329,
                    "end": 332,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 333,
                    "end": 335,
                    "text": "32",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 402,
                    "end": 404,
                    "text": "72",
                    "ref_id": "BIBREF71"
                }
            ],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "RPMs, originally created by John Raven in the 1930s as way to assess \"fluid intelligence,\" have long been given to both children and adults as nonverbal intelligence tests. Figure 2 gives a sample problem. The 3 \u00d7 3 grid on the left shows patterns of change along the rows and columns; the challenge is to decide which figure from the eight choices on the right fits the blank square. Here the answer is 5. This example is one of the easier RPM problems; often they involve more than two shapes per square and changes in multiple attributes (shape, color, position, number, etc.) Lovett et al. 72 used SME as one component of a system to solve RPM problems. Their system did not address lower-level vision at all; the input was a vectorized image, pre-segmented (by humans) into objects placed in the grid squares. The input to SME was a set of predicate-logic descriptions of each square, generated by the CogSketch system. 33 To form these descriptions, CogSketch is programmed with a repertoire of possible object attributes (e.g., size, position, degree of symmetry) and object relations (e.g., inside/outside, intersection, rotation) that it looks for in each box in a given RPM problem. The system can also create qualitative descriptions of the edges making up objects (e.g., relations such as relative orientation, relative length, etc.) as well as descriptions of object groupings based on proximity and similarity.",
            "cite_spans": [
                {
                    "start": 594,
                    "end": 596,
                    "text": "72",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 925,
                    "end": 927,
                    "text": "33",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [
                {
                    "start": 173,
                    "end": 181,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "These predicate-logic representations are input to SME, which returns the highest-scoring mapping between descriptions of pairs of images in a row. This mapping is used by the system to determine a higher-level description of the pattern of change across each of the top two rows, from an existing vocabulary of possible changes between corresponding objects (e.g., identity, deformation, shape change, addition/removal).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "These higher-level descriptions of the top two rows are themselves mapped by SME to produce a description of more general pattern. SME scores each of the eight possible answers to see which one best completes the third row according to this pattern. (This brief description leaves out some additional details, such as special-purpose operations for certain kinds of problems.) Lovett et al. ran this system on the Standard Progressive Matrices test. Their system was able to solve 56 out of the 60 problems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "Lovett et al.'s experiments provide an evaluation of SME as part of a larger system for making analogies in the context of RPMs. I will say more about the RPM domain in the next section. SME captures an important aspect of analogy-making: people tend to prefer systematic analogies involving more abstract concepts rather than less systematic mappings involving more superficial concepts. 40 However, I believe the SME approach is limited in its ability to capture analogy-making more generally, for three main reasons, as I describe below.",
            "cite_spans": [
                {
                    "start": 389,
                    "end": 391,
                    "text": "40",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "Focus on syntax rather than semantics. The goal of SME is to give a domain-independent account of analogy-making; thus, true to its name, SME focuses on mapping the structure or syntax of its input representations rather than domain-specific semantics. However, this reliance on syntactic structure requires that representations of situations be cleanly partitioned into objects, attributes, functions, first-order relations, second-order relations, and so on. The problem is that humans' mental representations of real-world situations are generally not so rigidly categorized. Building on the \"bridge\" example from the introduction, consider a mapping between the Golden Gate bridge and President Joe Biden's statement that he is a \"bridge\" to a future generation of leaders. 85 Should \"bridge\" be represented as an attribute of the object \"Golden Gate\" (bridge(GoldenGate)), an object itself, with its own attributes (e.g., golden(Bridge)), or a relation between the areas on either side: bridge(GoldenGate, San Francisco, MarinCounty)? In the latter case, if the Joe Biden \"bridge\" is represented only as a two-argument relation (bridge(JoeBiden, FutureGeneration)), it would not be paired with the three-argument Golden Gate relationship. Or what if \"future generation\" is not a unitary object but an attribute (e.g., FutureGeneration(Leaders))? Then it could not be matched with, say, the \"object\" MarinCounty. These examples illustrate that real-world situations are not easily reducible to unambiguous propositions that can be mapped via syntax alone.",
            "cite_spans": [
                {
                    "start": 778,
                    "end": 780,
                    "text": "85",
                    "ref_id": "BIBREF84"
                }
            ],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "The distinctions between \"object,\" \"attribute,\" and \"relation,\" not to mention the order of the relation, depend heavily on context, and people (if they assign such categories at all) have to use them very flexibly, allowing initial classifications to slide if necessary at the drop of a hat. However, a major tenet of SME is that the mapping process is performed on existing representations. This naturally leads into the second issue, the separation of representation-building and mapping.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "Separation of representation and mapping processes. The SME approach separates \"representation-building\" and \"mapping\" into two distinct and independent phases in making an analogy. The base and target entities are rendered (by a human or another program) into predicate logic descriptions, which is then given to SME to construct mappings between these descriptions. Some versions of SME enable a limited \"re-representation\" of the base or target in response to low-scoring mappings. 31 Such re-representations might modify certain predicates (e.g., factoring them into \"subpredicates\") but in the examples given in the literature, SME almost entirely relies on another module to build representations before mapping takes place.",
            "cite_spans": [
                {
                    "start": 485,
                    "end": 487,
                    "text": "31",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "In the next section I will discuss a different view, arguing that in human analogy-making, the process of building representations is inextricably intertwined with the mapping process, and that we should adopt this principle in building AI systems that make analogies. This argument was also made in detail in several previous publications 12, 36, 55 and counterarguments were given by the SME authors. 31 Semi-exhaustive search. Finally, SME relies on semi-exhaustive search over matchings. The program considers matches between all \"plausible pairings\" (defined by its match rules) to create multiple global matches. While this is not a problem if representations have been boiled down to a relatively small number of propositions, it is not clear that this semi-exhaustive method will scale well in general.",
            "cite_spans": [
                {
                    "start": 344,
                    "end": 347,
                    "text": "36,",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 348,
                    "end": 350,
                    "text": "55",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 403,
                    "end": 405,
                    "text": "31",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "In short, the structure-mapping theory makes some very useful points about what features appealing analogies tend to have, but in dealing only with the mapping process while leaving aside the problem of how situations become understood and how this process of interpretation interacts with the mapping process, it leaves out some of the most important aspects of how analogies are made.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "In addition to symbolic approaches such as SME, several hybrid symbolic-connectionist approaches have also been explored, such as the ACME and LISA systems, 57,59 neural networks to perform mapping between symbolic representations, 17 as well as systems based on cognitive models of memory. 63 ",
            "cite_spans": [
                {
                    "start": 291,
                    "end": 293,
                    "text": "63",
                    "ref_id": "BIBREF62"
                }
            ],
            "ref_spans": [],
            "section": "The Structure-Mapping Engine"
        },
        {
            "text": "In the 1980s, Hofstadter designed a general architecture for abstract perception and analogy-making that I'll call the \"active symbol architecture,\" based in part on Hofstadter's notion of active symbols in the brain: \"active elements [groups of neurons] which can store information and transmit it and receive it from other active elements\" 51 -and in part on inspiration from information processing in other complex systems such as ant colonies and cellular metabolism.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Active Symbol Architecture"
        },
        {
            "text": "The active symbol architecture was the basis for several AI programs exploring abstract perception and analogy-making, many of which were described in Ref. 53 . Here I'll focus on Hofstadter and Mitchell's Copycat program. 55, 80 The name \"Copycat\" is a humorous reference to the idea that the act of making an analogy is akin to being a \"copycat\"-that is, understanding one situation and \"doing the same thing\" in a different situation. A key idea of the Copycat program is that analogy-making should be modeled as a process of abstract perception. Like sensory perception, analogy-making is a process in which one's prior concepts are activated by a situation, either perceived via the senses or in the mind's eye; those activated concepts adapt to the situation at hand and feed back to affect how that situation is perceived.",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 158,
                    "text": "53",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 223,
                    "end": 226,
                    "text": "55,",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 227,
                    "end": 229,
                    "text": "80",
                    "ref_id": "BIBREF79"
                }
            ],
            "ref_spans": [],
            "section": "Active Symbol Architecture"
        },
        {
            "text": "Hofstadter and Mitchell developed and tested Copycat using the domain of letter-string analogy problems, created by Hofstadter. While these analogy problems are idealized \"toy\" problems, they are, similar to Ravens' matrices, designed to capture something of the essence of real-world abstraction and analogy-making. Each string is an idealized \"situation\" containing objects (e.g., letters or groupings of letters), relations among objects, events (a change from the first to second string), and a requirement for abstraction via what Hofstadter termed conceptual slippages 52 (e.g., the role of \"letter\" in one situation is played by \"group\" in another situation, or the role of \"successsor\" in one situation is played by \"predecessor\" in another situation). In the Copycat system, the process of analogical mapping between two situations (here, letter strings) is interleaved with the process of building representations of those situations, with continual feedback between these processes. This is achieved via four interacting components: a concept network, which contains the system's prior knowledge in symbolic form; a workspace, which serves as a working memory in which representation of and mappings between the input situations takes place; a set of perceptual agents, which-competitively and cooperatively-attempt to adapt the system's prior knowledge to the input situations over a series of time steps; and a temperature variable, which measures the quality and coherence of the system's representations and mappings at a given time, and which feeds back to control the degree of randomness of the perceptual agents. When the system is far from a solution, the temperature is high, and the perceptual agents' actions are more random; as the system zeroes in on a coherent solution, the temperature falls, and the perceptual agents are more deterministic. Figure 3 illustrates the architecture of the Copycat program. Figure 3 (a) illustrates part of the program's concept network, which contains the program's prior (symbolic) knowledge about the letter-string domain, corresponding to long-term memory. The concept network models a symbolic \"semantic space,\" in which concepts are nodes (ellipses) and links (lines) between between concepts represent semantic distance, which can change with context during a run of the program. A concept (e.g., letter-group) is activated when instances of that concept are discovered in the workspace, and in turn, activated concepts (the program's \"active symbols\") trigger perceptual agents that attempt to discover additional instances. Activation can also spread between conceptual neighbors. Activation decays over time if not reinforced. Figure 3 (b) illustrates the program's workspace, a short-term memory, inspired by blackboard systems, 23 in which perceptual agents construct (and sometimes destroy) structures (relations, groupings, correspondences, and rules) that form the program's current representation of the input situations and the analogy between them, at any given time during a run. Dashed lines or arcs represent structures with low confidence; solid lines or arcs represent structures with high confidence; the confidence of a structure can change during the run and structures can be destroyed depending on their confidence. A temperature variable (represented by the thermometer in the bottom right of the workspace) measures the quality of the current structures and feeds back to affect the randomness of the perceptual agents. Figure 4 gives the state of the workspace at selected timesteps during a run of the program, illustrating how the program constructs representations of, and analogies between, its input situations. The workspace serves as a global blackboard on which agents explore, build, and destroy possible structures. The actions of agents are probabilistic, and depend on the current state of the workspace, concept network, and temperature. Perceived correspondences between objects in different situations (here letters and letter-groups) lead to conceptual slippages (e.g., letter slips to letter-group) that give rise to a coherent analogy. Details of Copycat's operations are described in Ref. 80 .",
            "cite_spans": [
                {
                    "start": 4197,
                    "end": 4199,
                    "text": "80",
                    "ref_id": "BIBREF79"
                }
            ],
            "ref_spans": [
                {
                    "start": 1870,
                    "end": 1878,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1932,
                    "end": 1940,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 2695,
                    "end": 2703,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 3508,
                    "end": 3516,
                    "text": "Figure 4",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Active Symbol Architecture"
        },
        {
            "text": "In summary, the Copycat program is an example of Hofstadter's active symbol architecture, in which symbolic concepts become activated via bottom-up perceptions, spread activation to semantically related neighbors, and influence perception in a top-down manner by triggering probabilistic perceptual agents to find instances of the associated concepts in a blackboard-like workspace. In this way, processing in the system consists of a continual interplay between bottom-up and topdown processes. A temperature variable controls the degree of randomness in the system and in turn is dynamically determined by the quality of perceptual structures constructed by the system. Coherent representations of input situations, and analogies between them, result from the perceptual structures constructed by these probabilistic agents. A central idea underlying the active symbol architecture is that, in analogy-making, the mapping process cannot be separated from the representation-building process-these must be interleaved. This is a central point of disagreement with the structure-mapping engine approach described in the previous section (see also Ref. 12).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Active Symbol Architecture"
        },
        {
            "text": "As described in Ref 55, Copycat's emergent dynamics show a gradual transition from a largely bottom-up (perception-driven), random, and parallel mode of processing-in which many possible representations of the input are explored--to one that is largely top-down (concept-driven), deterministic, and serial. Copycat does not fit neatly into the traditional AI dichotomy between symbolic and neural systems; rather it incorporates symbolic, subsymbolic, and probabilistic elements. The architecture resonates with several ideas in psychology, psychophysics, and neuroscience, such as the Global Workspace hypothesis of Baars et al., 3, 98 in which multiple, parallel, specialist processes compete and cooperate for access to a global workspace, and the proposal that visual cortex areas V1 and V2 work as \"'active blackboards' that integrate and sustain the result of computations performed in higher areas.\" 42, 89 Copycat also resonates with the idea of neural \"object files\" 62temporary and modifiable perceptual structures, created on the fly in working memory, which interact with a permanent network of concepts. The system's dynamics are also in accord with Treisman's 104 notion of perception as a shift from parallel, random, \"pre-attentive\" bottom-up processing and more deterministic, focused, serial, \"attentive\" top-down processing.",
            "cite_spans": [
                {
                    "start": 631,
                    "end": 633,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 634,
                    "end": 636,
                    "text": "98",
                    "ref_id": "BIBREF97"
                },
                {
                    "start": 907,
                    "end": 910,
                    "text": "42,",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 911,
                    "end": 913,
                    "text": "89",
                    "ref_id": "BIBREF88"
                }
            ],
            "ref_spans": [],
            "section": "Active Symbol Architecture"
        },
        {
            "text": "Mitchell and Hofstadter showed how Copycat was able to solve a wide selection of letter-string problems; they also described the program's limitations. 55, 80 The Copycat program inspired numerous other active-symbol-architecture approaches to analogy-making, some of which are described in Ref. 53 , as well as approaches to music cognition, 82 image recognition, 87 and more general cognitive architectures. 3 It bears repeating that Copycat was not meant to model analogy-making on letter strings per se. Rather, the program was meant to illustrate-using the letter-string analogy domain-a domainindependent model of high-level perception and analogy. However, the program has several limitations that need to be overcome to make it a more general model of analogy-making. For example, Copycat's concept network was manually constructed, not learned; the program illustrated how to adapt pre-existing concepts flexibly to new situations, rather than how to learn new concepts. Moreover, the program was given a \"source\" and \"target\" situation to compare rather than having to retrieve a relevant situation from memory. Finally, the program's architecture and parameter tuning were complicated and somewhat ad hoc. Additional research on all of these issues is needed to make active symbol architectures more generally applicable.",
            "cite_spans": [
                {
                    "start": 152,
                    "end": 155,
                    "text": "55,",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 156,
                    "end": 158,
                    "text": "80",
                    "ref_id": "BIBREF79"
                },
                {
                    "start": 296,
                    "end": 298,
                    "text": "53",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 365,
                    "end": 367,
                    "text": "87",
                    "ref_id": "BIBREF86"
                },
                {
                    "start": 410,
                    "end": 411,
                    "text": "3",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Active Symbol Architecture"
        },
        {
            "text": "At the other end of the spectrum from symbolic approaches are deep learning approaches, in which knowledge is encoded as high-dimensional vectors and reasoning is the manipulation of these representations via numeric operations in a deep neural network. Moreover, in deep learning, knowledge representation, abstraction, and analogy-making abilities must be learned, typically via large training sets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "Here, I won't survey the substantial literature on pre-deep-learning connectionist modeling of analogy-making (though see, e.g., Refs. 4, 36) . Probably the best-known result of the (post-2010) deep learning era on analogy are the proportional analogies (e.g., \"man is to woman as king is to ?\") that can arise from vector arithmetic on word embeddings. 79 More recently, numerous papers have been published applying deep learning methods to make analogies between words or simple images 73, 86, 92 as well as on simplified Ravens-Progressive-Matrix-like problems. 47, 111 In addition, one group demonstrated a deep neural network that can learn to make mappings on symbolic representations (like the ones in Figure 1 ) that roughly agree with the mappings made by SME. 17 Attempts to get deep learning systems to learn abstract concepts and create analogies are fascinating as ways to explore what these architectures are capable of learning and what kind of reasoning they are able to do. However, these investigations can be complicated by the amount of data needed to train deep learning systems, as well as by their lack of transparency. In this paper, as an illustrative example of the promise and pitfalls of deep learning approaches to abstraction and analogy, I'll describe in detail a series of attempts to use deep neural networks on Ravens Progressive Matrices. This domain has became a popular benchmark in the deep learning community.",
            "cite_spans": [
                {
                    "start": 129,
                    "end": 137,
                    "text": "Refs. 4,",
                    "ref_id": null
                },
                {
                    "start": 138,
                    "end": 141,
                    "text": "36)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 354,
                    "end": 356,
                    "text": "79",
                    "ref_id": "BIBREF78"
                },
                {
                    "start": 488,
                    "end": 491,
                    "text": "73,",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 492,
                    "end": 495,
                    "text": "86,",
                    "ref_id": "BIBREF85"
                },
                {
                    "start": 496,
                    "end": 498,
                    "text": "92",
                    "ref_id": "BIBREF91"
                },
                {
                    "start": 565,
                    "end": 568,
                    "text": "47,",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 569,
                    "end": 572,
                    "text": "111",
                    "ref_id": "BIBREF110"
                },
                {
                    "start": 770,
                    "end": 772,
                    "text": "17",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 709,
                    "end": 717,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "As I described above, the symbolic Structure Mapping Engine was tested on part of a standard 60problem RPM test. However, 60 problems is orders of magnitude too small a dataset to successfully train and test a deep neural net. In order to apply deep learning to this task, researchers need an automated way of generating a very large number of distinct, well-formed, and challenging RPM problems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "In 2015, Wang and Su 110 proposed an RPM-problem-generation method, guided by an earlier analysis of RPM problems. 11 Their method generated a problem by repeatedly sampling from a fixed set of object, attribute and relation types. Building on this approach, Barrett et al. 5 proposed a related method to produce a large dataset of RPM problems, which they called Procedurally Generated Matrices (PGMs). Each PGM is a set S of triples, where each triple consists of a specific relation, an object type, and an attribute. The generating process first samples from a fixed set of relation, object, and attribute types to create between one and four triples. The generating process then samples allowed values for the object types and attributes. All other necessary attributes (e.g., number of objects) are sampled from allowed values that do not induce spurious relationships. The PGM S is then rendered into pixels. See Ref. 5 for more details on this process, though the paper did not give details on how the eight candidate answers were generated. Barrett et al. used this generation method to produce their PGM corpus of Raven-style problems. They split the corpus into 1.2M training problems, 20K validation problems, and 200K test problems. They used these splits to train and test several deep learning methods, including their own novel method, the Wild Relation Network (WReN), which utilized Relation Network modules. 96 For each network, the input was the eight matrix panels and the eight candidate-answer panels (i.e., 16 grayscale images). The output was a probability distribution over the eight candidate answers. Each network was trained to maximize the probability of the correct answer for a given input. Random guessing would yield 12.5% accuracy. Among their many experiments, the authors found that their WReN method achieved the best performance of the various networks, an impressive 63% accuracy on the test problems for the so-called \"neutral split\" (training and test set can contain problems generated with any triples). Other training/test splits that required extrapolation from a restricted set of triples or attribute values to a different set produced notably poorer performance. Subsequent to the work of Barrett et al., several other groups developed new deep-learning-based methods that improved state-of-the-art accuracy on the PGM corpus. 45, 60, 102, 118 In a 2019 paper, Zhang et al. 115 questioned whether the PGM dataset was diverse enough to be a good test of abstract reasoning abilities. They noted that even though the PGM corpus is large (over 1.2 million problems), the PGM generation procedure is limited in the possible kind of problems that can be generated. Zhang et al. noted that \"PGM's gigantic size and limited diversity might disguise model fitting as a misleading reasoning ability, which is unlikely to generalize to other scenarios.\" In other words, the PGM corpus might make it too easy for learning systems to overfit to its particular, limited types of problems.",
            "cite_spans": [
                {
                    "start": 115,
                    "end": 117,
                    "text": "11",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 274,
                    "end": 275,
                    "text": "5",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1050,
                    "end": 1069,
                    "text": "Barrett et al. used",
                    "ref_id": null
                },
                {
                    "start": 1427,
                    "end": 1429,
                    "text": "96",
                    "ref_id": "BIBREF95"
                },
                {
                    "start": 2376,
                    "end": 2379,
                    "text": "45,",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 2380,
                    "end": 2383,
                    "text": "60,",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 2384,
                    "end": 2388,
                    "text": "102,",
                    "ref_id": "BIBREF101"
                },
                {
                    "start": 2389,
                    "end": 2392,
                    "text": "118",
                    "ref_id": "BIBREF117"
                },
                {
                    "start": 2423,
                    "end": 2426,
                    "text": "115",
                    "ref_id": "BIBREF114"
                }
            ],
            "ref_spans": [],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "To remedy this limitation, Zhang et al. devised a new stochastic image-grammar method for generating a more diverse, comprehensive set of Ravens-like problems. They generated a new 70,000problem corpus called RAVEN-smaller in size than PGM, but more diverse. Using a split with 42,000 training examples and 14,000 test examples, Zhang et al. compared some of the same deep learning methods examined by Barrett et al., plus their own novel \"dynamic residual tree\" method, which dynamically builds a tree-structured computation graph. They found that, whereas Barrett et al.'s WReN method had relatively high accuracy on the PGM dataset, its accuracy on the RAVEN dataset was 15%, barely above chance. The highest scoring method, with 60% accuracy, was a Residual Network 46 (ResNet) using features computed by a dynamic residual tree. The authors also tested humans (college students) on the RAVEN corpus and found human accuracy to be about 84%. Other groups were able to improve on the machine-learning accuracy on RAVEN with larger, pretrained networks 119 and contrastive learning, 116 among other novel mechanisms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "However, Hu et al. 58 discovered a major flaw in the RAVEN dataset. Recall that there are eight candidate answers, only one of which is correct. In RAVEN, the seven incorrect answers were generated by randomly modifying a single attribute of the correct answer, as illustrated in Figure 5 . In the figure, answer #1 is the correct answer, and each of the other answers differs from it in exactly one attribute. But this allows a possible shortcut to determining the correct answer: just take the majority vote among the figures' attributes, here, shape, fill-pattern, and size. Indeed, when Hu et al. trained a ResNet only on the eight answer panels in each training example in RAVEN (i.e., leaving out the matrix itself), the ResNet achieved an accuracy of about 90%, competitive with the state of the art reported in the original RAVEN paper. This discovery casts doubt on previous claims that deep neural networks that perform well on the RAVEN dataset are actually solving the task that the authors intended them to solve.",
            "cite_spans": [
                {
                    "start": 19,
                    "end": 21,
                    "text": "58",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [
                {
                    "start": 280,
                    "end": 288,
                    "text": "Figure 5",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "Hu et al. proposed a more complex sampling method to create what they claimed was an \"unbiased answer set\" for each problem; the result is what they call \"Impartial-RAVEN.\" The authors showed that some deep learning methods reported to do well on RAVEN did much more poorly on Impartial-RAVEN. Hu et al. also described a new method that outperformed other methods. Impartial-RAVEN (previously called \"Balanced-RAVEN\") has become the subject of further competition for state-of-the-art accuracy. 114, 117 In addition, 112 demonstrated a mechanism for variable-binding in a neural-network system that performed well on simple abstraction problems including simplified versions of Ravens Progressive Matrices.",
            "cite_spans": [
                {
                    "start": 495,
                    "end": 499,
                    "text": "114,",
                    "ref_id": "BIBREF113"
                },
                {
                    "start": 500,
                    "end": 503,
                    "text": "117",
                    "ref_id": "BIBREF116"
                }
            ],
            "ref_spans": [],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "In this section I've reviewed some recent work on using deep learning for abstraction and analogymaking in Ravens-like problems. While this is only one example of work on deep learning in this area, it serves as an illustrative microcosm, and highlights some of the advantages and limitations of such approaches. On the advantages side, deep learning avoids some of the issues I discussed with SME and Copycat. Deep learning approaches learn end-to-end from pixels, so avoid relying on built-in knowledge. Unlike SME, there is no separation of representation-building and mapping, and no semi-exhaustive search over possible matchings. However, these advantages are at the expense of two major limitations: the need for a huge training set, and the lack of transparency in what has been learned and how it is applied to new problems. The procedural generation of problems leaves open possibility of biases that allow shortcuts-the networks learn ways of performing well on the task that do not require the abstraction and analogy-making mechanisms that we assume humans bring to the task, and that we are trying to capture in machines. This can be seen in a simple way with biases in the RAVENS set, but such biases can be more subtle and hard to perceive, and have been identified time and again in machine-learning systems. 37 All this leads to the conclusion that accuracy alone can be a misleading measure of performance. I will propose additional evaluation metrics that might be more informative in the discussion section below.",
            "cite_spans": [
                {
                    "start": 1326,
                    "end": 1328,
                    "text": "37",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "Even setting aside these problems with biases and lack of transparency, the approach of training such a system with a large set of examples is questionable, given the original intention of RPMs as a way of measuring general human intelligence. Humans don't need to train extensively in advance to perform well on RPMs; in fact, doing so would invalidate the test's ability to measure general intelligence. Rather, the idea is that humans learn how to do successful abstraction and analogymaking in the world, and intelligence tests such as Raven's are meant to be a measure of these general skills. In contrast, the neural networks that do well on Raven's test after extensive training are unable to transfer any of these skills to any other task. It's not clear that such approaches bring us any closer to the goal of giving machines general abilities for abstraction and analogy-making.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "A potentially more promising (though less explored) set of approaches to abstraction uses metalearning 109 -enabling a neural network to adapt to new tasks by training it on a training set of tasks rather than only on examples from a single task. Several meta-learning approaches for few-shot learning have been evaluated on Lake et al.'s Omniglot dataset, 64 among other benchmarks 30,88,95,100 . Meta-learning methods produced substantially improved generalization abilities for agents in a grounded-language task in a simple simulated environment, 49 and improved fewshot learning accuracy in a simplified version of Bongard problems. 83 However, at the time of this writing, meta-learning approaches have not yet been explored for other abstraction and analogy domains. Another proposed approach is that of \"meta-mapping\" 69 which directly maps a representation of one task to a representation of a related task.",
            "cite_spans": [
                {
                    "start": 551,
                    "end": 553,
                    "text": "49",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 638,
                    "end": 640,
                    "text": "83",
                    "ref_id": "BIBREF82"
                }
            ],
            "ref_spans": [],
            "section": "Deep Learning Approaches"
        },
        {
            "text": "So far I have covered methods illustrating symbolic and deep learning approaches. In this section I'll describe a different family of methods for concept learning and analogy-making: probabilistic program induction methods, in which a space of possible programs is defined (often via a program grammar) and a probability distribution is computed over this space with respect to a given task. Solving the task amounts to sampling from the space of possible programs guided by this probability distribution. Here, the task is concept induction, and a concept is identified with a program. I will illustrate probabilistic program induction by describing its application to two task domains: (1) recognizing and generating handwritten characters, and (2) solving Bongard problems. As I will discuss, program-induction methods can combine both symbolic and neural-network representations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Probabilistic Program Induction"
        },
        {
            "text": "Lake et al. 64 applied probabilistic program induction to the task of learning visual concepts of handwritten characters from 50 different writing systems-the \"Omniglot\" task. The goal was to study one-shot learning-learning a concept from a single example. Here, learning a concept means seeing one example of a handwritten character and being able to not only recognize new examples of this character, but also being able to generate new examples. In this work, a concept is represented as a program that generates examples of the concept.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 14,
                    "text": "64",
                    "ref_id": "BIBREF63"
                }
            ],
            "ref_spans": [],
            "section": "Recognizing and Generating Handwritten Characters"
        },
        {
            "text": "In Omniglot, a program representing a concept consists of a hierarchical character-generation procedure. A character consists of a number of parts (complete pen strokes), each of which itself is made of sub-parts (movements separated by pauses of the pen). The generation procedure consists of sampling from a hierarchy of generative models describing the probabilities of the sub-parts, parts, and relationships among them. See Figure 6 for an example. Figure 6 : Illustration of the generative model used by Lake et al. for generating Omniglot characters, given a set of primitive pen strokes. To generate a new character, first sample the number of parts; then sample the number of sub-parts that will make up each part; then sample these sub-parts from the primitives set; then sample possible sequences of the sub-parts to make a part; then sample relations between parts (from a library of possible relations). This sequence of samples is the program for generating the character concept. To generate a token of the character concept (i.e., a rendered character), run the character-concept program with motor noise added to selected points. Adapted from Ref. 64. How is this hierarchy of generative models learned? In Ref. 64 the authors describe a system that obtains prior knowledge from a training set of human-drawn characters spanning 30 alphabets (taken from the Omniglot dataset). The training set included the human-drawn pen strokes for each character, as well as the final character images. The learning system collected a library of primitive pen strokes and learned probability distributions over features of these pen strokes (e.g., starting positions). It also learned transition probabilities between strokes and probabilities over types of relations among the strokes, among other probabilities. All in all, this collection of probabilities is the prior knowledge of the system.",
            "cite_spans": [
                {
                    "start": 1165,
                    "end": 1168,
                    "text": "64.",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 429,
                    "end": 437,
                    "text": "Figure 6",
                    "ref_id": null
                },
                {
                    "start": 454,
                    "end": 462,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "Recognizing and Generating Handwritten Characters"
        },
        {
            "text": "The system used this prior knowledge in a Bayesian framework for the tasks of one-shot classification and generation of characters not contained in the training set. (The authors also explored related tasks but I won't cover those here.) In the one-shot classification task, the system was presented with a single test image I (t) of a new character of class t, along with 20 distinct characters I (c) in the same alphabet produced by human drawers. Only one of the 20 was the same class as I (t) . The task was to choose that character from the 20 choices.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Recognizing and Generating Handwritten Characters"
        },
        {
            "text": "The Omniglot system computed an approximation to the probability P (I (t) |I (c) ) for each of the 20 I (c) , and chose the I (c) that yielded the highest probability. By Bayes rule, P (I (t) |I (c) ) \u221d P (I (c) |I (t) )P (I (t) ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Recognizing and Generating Handwritten Characters"
        },
        {
            "text": "The term P (I (t) ) is approximated by a probabilistic search method to generate a program to represent I (t) of the form shown in Figure 6 ; the prior probabilities learned from the original training set can be used to approximate P (I (t) ). The term P (I (c) |I (t) ) can be approximated by attempts to \"refit\" the program representing I (t) to I (c) . See Ref. 65 for details. In the experiments reported by Lake et al., 64 the Omniglot system's performance on one-shot classification matched or exceeded that of the humans tested on this task.",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 367,
                    "text": "65",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 425,
                    "end": 427,
                    "text": "64",
                    "ref_id": "BIBREF63"
                }
            ],
            "ref_spans": [
                {
                    "start": 131,
                    "end": 139,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "Recognizing and Generating Handwritten Characters"
        },
        {
            "text": "The one-shot generation task was, given an example image I (c) of a hand-drawn character of class c, to generate a new image of a character of class c. The Omniglot system did this by first searching to find a program representing I (c) as above, and then running this program as in Figure 6 to generate a new example. Lake et al. enlisted human judges to compare the characters generated by their system with those generated under the same one-shot conditions by humans-what they called a \"visual Turing Test.\" The judges were typically not able to distinguish between machine-generated and human-generated characters.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 283,
                    "end": 291,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "Recognizing and Generating Handwritten Characters"
        },
        {
            "text": "Feinman and Lake 29 proposed an interesting \"neuro-symbolic\" extension of Lake et al.'s system that integrated neural networks with probabilistic programs to learn generative models of handwritten characters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Recognizing and Generating Handwritten Characters"
        },
        {
            "text": "A second example of recent work on probabilistic program induction is Depeweg et al.'s system for solving Bongard programs. 19 Here the idea is to induce a rule rather than a runnable program, but the general idea is the same.",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 126,
                    "text": "19",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Solving Bongard Problems"
        },
        {
            "text": "Bongard problems are visual concept recognition tasks, first presented in M. Bongard's 1970 book Pattern Recognition. 9 Figure 7 shows four sample problems from Bongard's collection. For each problem the task is to identify the concept that distinguishes the set of six frames on the left from the set of six on the right (e.g., large vs. small or three vs. four ). Often the concept is simple to express, but is represented quite abstractly in the figures (e.g., BP #91).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 120,
                    "end": 128,
                    "text": "Figure 7",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Solving Bongard Problems"
        },
        {
            "text": "Bongard devised 100 such problems as a challenge for artificial intelligence systems. Harry Foundalis created a web site 34 to make available the original 100 problems plus many additional Bongard-like problems created by other people. It is notable that in the decades since Bongard's book was published, no artificial vision system has come close to solving all of Bongard's original problems. In contrast to the attention garnered in the AI community by Ravens Progressive Matrices, Bongard problems have seen less attention. In a 1996 paper, Saito and Nakano 93 showed that an inductive logic programming approach could solve 41 of the original 100 problems, starting not from the raw pixels but from logic formulas created by humans to represent each problem. In his 2006 PhD dissertation, Foundalis 35 described the construction of a \"cognitive architecture inspired by Bongard's problems,\" whose input was raw pixels of the 12 frames and whose output was an English phrase describing one or both sides of the problem. Foundalis's architecture was meant to model human concept induction in general rather than Bongard problems specifically, and was able to reliably solve about 10 problems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solving Bongard Problems"
        },
        {
            "text": "Here I'll describe Depeweg et al.'s 19 probabilistic rule-induction approach, which was inspired by the Omniglot system described above. Depeweg et al.'s system's task was to input the raw pixels of the 12 frames, and to output a rule R-in a logic-like language-that is true of all frames on one side (left or right) and none of the frames on the other side. (Note that this differs from Bongard's original task, which was to output English-language expressions contrasting the left and right sets of boxes.) In Depeweg et al.'s system, the space of possible rules is given by a human-designed grammar, given in Figure 8(a) . A rule can be derived from this grammar by choosing one of the sides (right or left), starting with the start symbol S and probabilistically choosing one of the possible expansions of that symbol (e.g., S \u2192 \u2203(L)), then probabilistically choosing possible expansions of the symbols in that expression (e.g., L \u2192 Contains(L)) and so on until all the variables have been expanded.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 612,
                    "end": 623,
                    "text": "Figure 8(a)",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Solving Bongard Problems"
        },
        {
            "text": "Given a Bongard problem, the goal is to search the possible space of rules to find the most probable rule. The authors define a probability distribution P (R|E, G) over possible rules. Here R is a rule, E = (E LEF T , E RIGHT ) is the set of 12 input frames, and G is the grammar. By Bayes rule, this probability distribution can be factored into prior probability P (R|G) and likelihood P (E|R):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solving Bongard Problems"
        },
        {
            "text": "The authors define the likelihood P (E|R) as equal to 1 if two conditions are met, and equal to zero otherwise. The conditions are (1) R is true of all the frames on one side (left or right) and none of the frames on the other side, and (2) the frames in E are \"informative\" about R, meaning that the frames must contain the objects or relationships that are mentioned in the rule. For example, the rule \"there exist triangles or squares inside circles\" is true of all the frames on the left side of Figure 8(b) , but since no squares actually appear, these frames are not considered to be informative about the rule, and the rule would be given zero likelihood. The prior probability term P (R|G) is defined by the authors in a way that favors shorter rules, along with some other structural properties of rules (see Ref. 19 for details).",
            "cite_spans": [
                {
                    "start": 823,
                    "end": 825,
                    "text": "19",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [
                {
                    "start": 500,
                    "end": 511,
                    "text": "Figure 8(b)",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Solving Bongard Problems"
        },
        {
            "text": "Given the raw pixels of a Bongard problem, Depeweg et al.'s system applies simple image processing operations to extract objects, attributes, and relationships in each frame. The system was given a small repertoire of object, attribute, and relation types that it was able to extract. The authors note, \"Our aim was not to build a general vision system, instead we decided to focus on these visual shapes, properties and relations that appear in many of the Bongard problems and hence seem to be likely candidates for a natural vocabulary from which relevant visual concepts can be built.\" The limitations of the system's repertoire means that their system is able to deal with only a subset of 39 out of the 100 original problems. Once the image processing has been completed on the input Bongard problem, the system searches for a rule by sampling (using a form of the Metropolis-Hastings algorithm) from possible rules under the probability distribution described above. The system is allowed 300,000 samples to find a compatible rule for each Bongard problem. Of the 39 problems the grammar could deal with, the system was able to find compatible rules for 35 of them.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solving Bongard Problems"
        },
        {
            "text": "The Omniglot and Bongard-problem examples illustrate the promise and challenges of probabilistic program induction approaches. Framing concept learning as the task of generating a program enables many of the advantages of programming in general, including flexible abstraction, reusability, modularity, and interpretability. Bayesian inference methods seamlessly combine prior knowledge and preferences with likelihoods, and enable powerful sampling methods. Such advantages have fueled strong interest and progress in probabilistic program induction in recent years, including combining program induction with neural networks, reinforcement learning, and other methods, 13, 21, 22, 38 , as well as incorporating program induction with methods inspired by neuroscience 70 and psychology. 24, 91 The work of L\u00e1zaro-Gredilla et al. 70 is notable in that it combines Many challenges remain to make probabilistic program induction a more general-purpose AI method for concept learning, abstraction, and analogy. These methods currently need substantial builtin knowledge, structured by humans, in the form of the program primitives and grammar (the \"domain-specific language\") for a given problem. Moreover, these methods require humans to define prior probability and likelihood distributions over possible programs, which is not always a straightforward task. Perhaps most important, solving a given task can require an enormous amount of search in the space of possible programs, which currently limits scaling up such program induction methods to more complex problems (though some of the hybrid methods cited above are focused on dealing with this combinatorial explosion of possibilities.) Finally, it remains to be seen whether the concept-as-program notion allows for the flexibility, extensibility, and analogy-making abilities that are the hallmark of human concepts.",
            "cite_spans": [
                {
                    "start": 671,
                    "end": 674,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 675,
                    "end": 678,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 679,
                    "end": 682,
                    "text": "22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 683,
                    "end": 685,
                    "text": "38",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 769,
                    "end": 771,
                    "text": "70",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 788,
                    "end": 791,
                    "text": "24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 792,
                    "end": 794,
                    "text": "91",
                    "ref_id": "BIBREF90"
                },
                {
                    "start": 830,
                    "end": 832,
                    "text": "70",
                    "ref_id": "BIBREF69"
                }
            ],
            "ref_spans": [],
            "section": "Summary of Program Induction Approaches"
        },
        {
            "text": "This section does not describe a new AI method for abstraction and analogy, but rather a new promising benchmark-the Abstraction and Reasoning Corpus (ARC)-for evaluating these abilities. ARC was developed by Fran\u00e7ois Chollet as part of a project on how to measure intelligence in AI systems. 15 ARC is a collection of visual analogy \"tasks.\" Figure 9 gives a sample task from the corpus. The left side presents three \"task demonstrations\"; each of these consists of two grids with colored boxes. In each demonstration, you can think of the first (left) grid as \"transforming\" into the second (right) grid. The right side of Figure 9 presents a \"test\"; the task is to transform this grid analogously to the demonstrations. This task could be thought of as one of \"few-shot\" learning-the demonstrations on the left are the training examples, and the grid on the right is a test example.",
            "cite_spans": [
                {
                    "start": 293,
                    "end": 295,
                    "text": "15",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 343,
                    "end": 351,
                    "text": "Figure 9",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 625,
                    "end": 633,
                    "text": "Figure 9",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Abstraction and Reasoning Corpus"
        },
        {
            "text": "The ARC domain features visual analogies between idealized \"situations\" that can express abstract concepts in unlimited variations. In this way it exhibits some of the combined advantages of the other idealized domains I've discussed in this paper, such as Copycat's letter-string analogies, Bongard problems, and Ravens Progressive Matrices.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstraction and Reasoning Corpus"
        },
        {
            "text": "Chollet manually designed 1,000 ARC tasks, with the motivation of enabling a fair comparison of \"general intelligence\" between AI systems and humans-a comparison that does not involve language or other acquired human knowledge. Instead, ARC tasks are meant to rely only on the innate core knowledge systems proposed by Spelke, 101 which include intuitive knowledge about objects, agents and their goals, numerosity, and basic spatial-temporal concepts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstraction and Reasoning Corpus"
        },
        {
            "text": "An important aspect of ARC is its relatively small size. Chollet made 400 tasks publicly available and reserved 600 tasks as a \"hidden\" evaluation set. In 2020 the Kaggle website hosted a threemonth \"Abstraction and Reasoning Challenge\" in which researchers were encouraged to submit programs to be evaluated on the hidden evaluation set. The best-performing submissions had about 20% accuracy on a top-3 metric (three answers were allowed per task, and if one or more was correct, the task was considered to be solved). However, none of the submissions used an approach that was likely to be generalizable. 16 Thus the ARC challenge remains wide open.",
            "cite_spans": [
                {
                    "start": 608,
                    "end": 610,
                    "text": "16",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Abstraction and Reasoning Corpus"
        },
        {
            "text": "In the sections above I have described several diverse AI approaches to abstraction and analogy, including symbolic methods, deep learning, and probabilistic program induction. These approaches each have their own advantages and limitations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Symbolic systems such as SME can be explicitly programmed with prior knowledge (in symbolic form) as well as with important heuristics of abstraction and analogy making, such Gentner's systematicity principle. Symbolic representation methods such as predicate logic or semantic networks offer unambiguous variables and variable binding, clear type-token distinctions, and explicit measures of conceptual similarity and conceptual abstraction, among other facilities associated with human reasoning. These systems also have the advantage of interpretability, since their \"reasoning\" on a given problem is readable in symbolic form. However, representations that focus on the syntax of logic-like representations can suffer from brittleness; moreover symbolic approaches often require humans to create and structure substantial prior knowledge, and these systems often rely on semi-exhaustive search. There have been several interesting neuro-symbolic approaches that implement symbolic-like behavior in neural networks (e.g., see Refs. 20, 75, 99) , but these remain limited in their generality and can suffer from some of the limitations of symbolic systems discussed above.",
            "cite_spans": [
                {
                    "start": 1029,
                    "end": 1038,
                    "text": "Refs. 20,",
                    "ref_id": null
                },
                {
                    "start": 1039,
                    "end": 1042,
                    "text": "75,",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 1043,
                    "end": 1046,
                    "text": "99)",
                    "ref_id": "BIBREF98"
                }
            ],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Active symbol architectures, such as the Copycat program, were claimed to address some of the limitations of purely symbolic methods 12,53 by enabling the system to actively build its representation of a situation in a workspace, via a continual interaction between bottom-up and top-down information processing, avoiding any kind of exhaustive search. However, Copycat and other examples of active symbol architectures remain dependent on prior knowledge provided and structured by humans, and these systems as yet have no mechanism for learning new permanent concepts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Deep learning systems do not require the pre-programmed structured knowledge of symbolic systems; they are able to learn from essentially raw data. However, they require large training corpora, as well as significant re-training (and often re-tuning of hyperparameters) for each new task. In addition, they are susceptible to learning statistical shortcuts rather than the actual concepts that humans intend, and their lack of intepretability often makes it difficult to ascertain the extent to which they are actually performing abstraction or analogy. Moreover, if the goal is to imbue machines with general humanlike abstraction abilities, it doesn't make sense to have to train them on tens of thousands of examples, since the essence of abstraction and analogy is few-shot learning. While some meta-learning systems have shown interesting performance on certain few-shot-learning tasks, their overall generality and robustness still needs to be demonstrated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Probabilistic program induction, by framing concept learning as the task of generating a program, enables many of the advantages of programming in general, including flexible abstraction, reusability, modularity, and interpretability. Moreover, Bayesian inference methods enable the combination of prior knowledge and preferences with likelihoods, and a probabilistic approach enables powerful sampling methods. However, like symbolic approaches, current program induction approaches require significant human-engineered prior knowledge in the form of a domain-specific language. And the more expressive the language, the more daunting the combinatorial search problem these methods face.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Stepping back from any individual approach, it is difficult to assess how much general progress has been made on AI methods for abstraction and analogy, since each AI system is developed for and evaluated on a particular domain. Moreover, as I described in the sections above, these evaluations have largely relied on the system's accuracy on a particular set of test problems. What's missing are assessments based on generality across diverse domains, as well as the robustness of a given system to factors such as noise, variations on an abstract concept, or scaling of task complexity. In order to make further progress, we need to rethink how we choose or design domains and what evaluation criteria and evaluation processes we use.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "The following are my own recommendations for research and evaluation methods to make quantifiable and generalizable progress in developing AI systems for abstraction and analogy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Focus on idealized domains. AI researchers have used diverse idealized domains for developing and evaluating systems to perform abstraction and analogy, including Ravens Progressive Matrices, letter-string analogy problems, the Omniglot challenge, and Bongard problems, among others (e.g., more recently, new idealized visual domains have been proposed in Refs. 70,83.) While some approaches have used natural image-or language-based domains, 2, 8, 73, 79, 86, 92 there are advantages to using idealized non-linguistic domains. In idealized domains, it is possible to be explicit about what prior knowledge is assumed, as opposed to the open-ended knowledge requirements of human language and imagery. Real-world image-or language-based tasks have much richer meanings to humans than to the machines processing this data; by avoiding such tasks we can avoid anthropomorphizing and overestimating what an AI system has actually achieved. There are risks with idealized domains, since it is not always clear that these domains capture the kind of real-world phenomena we want to model, but as in other sciences, the first step to progress is to isolate the phenomenon we are studying, in as idealized a form as possible. While I believe that important general cognitive abilities can be developed using these idealized challenges, others have argued that such abilities can be enabled only by exposing systems to much richer data or experience. 48 Focus on \"core knowledge.\" Chollet has suggested that challenge domains for assessing AI's \"intelligence\" should rely only on human \"core knowledge\" rather than acquired knowledge such as language. 15 Spelke and colleagues 101 have proposed that human cognition is founded on of a set of four core knowledge systems, which include objects and intuitive physics; agents and goaldirectedness; numbers and elementary arithmetic; and spatial geometry of the environment (which includes relational concepts such as \"in front of\" or \"contains). The idealized domains I have discussed above-especially the ARC domain-mostly rely only on such core knowledge. (Note that in the letter-string analogy domain, letters of the alphabet are used as idealized representatives of objects that can be related to other objects in specific ways; knowledge of language, such as how letters are used in words, or the visual characteristics of letters, are outside of the idealized domain. 55 ) Restricting a domain's required prior knowledge to such concepts makes it possible to fairly compare performance among AI systems as well as between AI systems and humans. Indeed, I would argue that unless we create AI systems that can master such core, non-linguistic knowledge, we have little hope of creating anything like human-level AI.",
            "cite_spans": [
                {
                    "start": 443,
                    "end": 445,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 446,
                    "end": 448,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 449,
                    "end": 452,
                    "text": "73,",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 453,
                    "end": 456,
                    "text": "79,",
                    "ref_id": "BIBREF78"
                },
                {
                    "start": 457,
                    "end": 460,
                    "text": "86,",
                    "ref_id": "BIBREF85"
                },
                {
                    "start": 461,
                    "end": 463,
                    "text": "92",
                    "ref_id": "BIBREF91"
                },
                {
                    "start": 1443,
                    "end": 1445,
                    "text": "48",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1644,
                    "end": 1646,
                    "text": "15",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1669,
                    "end": 1672,
                    "text": "101",
                    "ref_id": "BIBREF100"
                },
                {
                    "start": 2414,
                    "end": 2416,
                    "text": "55",
                    "ref_id": "BIBREF54"
                }
            ],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Evaluate systems across multiple domains. All of the domains I have discussed here capture interesting facets of abstraction and analogy-making, including the recognition of abstract similarity, conceptual extrapolation, and adaptation of knowledge to novel situations. However, the common practice of focusing on a single domain limits progress. AI research focusing exclusively on any one domain has the risk of \"overfitting\" to that domain. I believe that the research community needs to adopt a diverse suite of challenge domains on which systems can be evaluated for generality; such a strategy has a better chance to develop truly general and robust approaches. This is the strategy taken by the natural language processing community, for example, with the GLUE and SuperGLUE benchmarks, 107,108 though these benchmarks focus on tasks associated with large training sets and fixed test sets, which have allowed for successful solutions based on \"shortcuts.\" 39, 71, 78 For that reason I endorse a focus on tasks that do not allow large amounts of specific training data, as I detail below.",
            "cite_spans": [
                {
                    "start": 964,
                    "end": 967,
                    "text": "39,",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 968,
                    "end": 971,
                    "text": "71,",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 972,
                    "end": 974,
                    "text": "78",
                    "ref_id": "BIBREF77"
                }
            ],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Focus on tasks that require little or no training. Since abstraction and analogy are at core concerned with flexibly mapping one's knowledge to new situations, it makes little sense to have to extensively train a system to deal with each new abstraction or analogy problem. Thus I believe the research community should focus on tasks that do not require extensive training on examples from the domain itself. This echoes Chollet's criteria for such tasks: \"It should not be possible to 'buy' performance on the benchmark by sampling unlimited training data. The benchmark should avoid tasks for which new data can be generated at will. It should be, in effect, a game for which it is not possible to practice in advance of the evaluation session.\" 15 When humans make abstractions or analogies, it can be argued that the \"training\" for such abilities is the process of developing core concepts, much of which takes place in early childhood. 10, 74, 101 Similarly, AI systems that can solve problems in idealized domains should require \"training\" only on the core concepts required in each domain. Rather than training and testing on the same task, enabling machines to have general abstraction abilities will require that they learn core concepts and then adapt that knowledge to a multitude of different tasks, without being trained specifically for any one of them.",
            "cite_spans": [
                {
                    "start": 941,
                    "end": 944,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 945,
                    "end": 948,
                    "text": "74,",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 949,
                    "end": 952,
                    "text": "101",
                    "ref_id": "BIBREF100"
                }
            ],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Include generative tasks. Some idealized domains offer discriminative tasks-e.g., Raven's Progressive Matrices, in which the solver chooses from a set of candidate answers. Others, such as the letter-string analogies or ARC, are generative: the solver has to generate their own answer. Generative tasks are likely more resistant to shortcut learning than discriminative tasks, and systems that generate answers are in many cases more interpretable. Most importantly, if the space of problems is diverse enough, having to generate answers forces a deeper understanding of the task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "Evaluate systems on \"hidden,\" human-curated, changing sets of problems. It is important that problems to be used for evaluation are not be made available to the developers of AI systems that will be evaluated on these problems. Furthermore, these evaluation sets should not remain fixed for a long period of time, since systems can indirectly \"overfit\" to a fixed set of evaluation problems, even when they are hidden. In addition, the evaluation problems should be created and curated by humans, rather than relying on automatic generation by algorithms; I described above how procedurally generated problems can unintentionally allow shortcut solutions; they can also allow a system to reverse-engineer the generating algorithm instead of being able to solve the problems in a general way. 15 Of course, human-generated problems can also allow for shortcut solutions; thus evaluation sets need to be carefully curated to avoid shortcut solutions as much as possible, for example via adversarial filtering methods. 94 Evaluate systems on their robustness, not simply their accuracy. Like other research in AI, methods for abstraction and analogy are often evaluated on their accuracy on a set of test problems from a given domain. However, as I discussed above, measuring accuracy on a fixed set of test problems does not reveal possible shortcuts-strategies a system takes to solve problems that don't reflect the actual general abilities that the evaluation is meant to test. In order to make progress on general abstraction and analogy abilities, in addition to evaluating systems across multiple domains, we need to evaluate them along multiple dimensions of robustness. For example, the evaluation benchmarks should feature various kinds of challenges in order to measure a system's robustness to \"noise\" and other irrelevant distractions, and to variations in a given concept (e.g., if one example tests recognition of the abstract concept \"monotonically increasing,\" other examples should test variations of this concept with different degrees of abstraction). Finally, the evaluation problems should also test a system's ability to scale to more complex examples of a given concept (e.g., if a system is able to recognize \"monotonically increasing\" with small number of elements, it should also be tested on the same concept with a larger number of elements, or with more complex elements).",
            "cite_spans": [
                {
                    "start": 792,
                    "end": 794,
                    "text": "15",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1016,
                    "end": 1018,
                    "text": "94",
                    "ref_id": "BIBREF93"
                }
            ],
            "ref_spans": [],
            "section": "Discussion: How to Make Progress in AI on Abstraction and Analogy"
        },
        {
            "text": "In this paper I have argued that humanlike abstraction and analogy-making abilities will be key to constructing more general and trustworthy AI systems, ones that can learn from a small number of examples, robustly generalize, and reliably adapt their knowledge to diverse domains and modalities. I have reviewed several approaches to building systems with these abilities, including symbolic and \"active symbol\" approaches, deep learning, and probabilistic program induction. I have discussed advantages and current limitations of each of these approaches, and argued that it remains difficult to assess progress in this area due to the lack of evaluation methods addressing generality, robustness, and scaling abilities. Finally, I proposed several steps towards making quantifiable and generalizable progress, by designing appropriate challenge suites and evaluation methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The quest for machines that can make abstractions and analogies is as old as the AI field itself, but the problem remains almost completely open. I hope that this paper will help spur renewed interest and attention in the AI community to understanding these core abilities which form the foundations of general intelligence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Alcorn",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mai",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "S"
                    ],
                    "last": "Ku",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "volume": "",
            "issn": "",
            "pages": "4840--4849",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "We have so much in common: Modeling semantic relational set abstractions in videos",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Andonian",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Fosco",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Monfort",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Feris",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Vondrick",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oliva",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2008.05596"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "How conscious experience and working memory interact",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "J"
                    ],
                    "last": "Baars",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Franklin",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Trends in Cognitive Sciences",
            "volume": "7",
            "issn": "4",
            "pages": "166--172",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Advances in Connectionist and Neural Computation Theory",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Barnden",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Holyoak",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "",
            "volume": "2",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Measuring abstract reasoning in neural networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "G T"
                    ],
                    "last": "Barrett",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Santoro",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Morcos",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lillicrap",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the International Conference on Machine Learning, ICML",
            "volume": "",
            "issn": "",
            "pages": "4477--4486",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Simulation, situated conceptualization, and prediction",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "W"
                    ],
                    "last": "Barsalou",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences",
            "volume": "364",
            "issn": "",
            "pages": "1281--1289",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Analogy and analogical reasoning",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bartha",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The Stanford Encyclopedia of Philosophy",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Modeling commonsense reasoning via analogical chaining: A preliminary report",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Blass",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Forbus",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 38th Annual Meeting of the Cognitive Science Society",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Pattern Recognition. Spartan Books",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Bongard",
                    "suffix": ""
                }
            ],
            "year": 1970,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Pr\u00e9cis of The Origin of Concepts",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Carey",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Behavioral and Brain Sciences",
            "volume": "34",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "What one intelligence test measures: A theoretical account of the processing in the Raven progressive matrices test",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "A"
                    ],
                    "last": "Carpenter",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Just",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Shell",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Psychological Review",
            "volume": "97",
            "issn": "3",
            "pages": "404--431",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "High-level perception, representation, and analogy: A critique of artificial intelligence methodology",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Chalmers",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "French",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Hofstadter",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Journal of Experimental and Theoretical Artificial Intelligence",
            "volume": "4",
            "issn": "3",
            "pages": "185--211",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Execution-guided neural program synthesis",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the International Conference on Learning Representations, ICLR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "The Abstraction and Reasoning Corpus (ARC)",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chollet",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "On the measure of intelligence",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chollet",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1911.01547"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Personal Communication",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chollet",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Neural analogical matching",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Crouse",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Nakos",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Abdelaziz",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Forbus",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2004.03573"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "An integrated reasoning approach to moral decision making",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dehghani",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Forbus",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Tomai",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Klenk",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the National Conference on Artificial Intelligence, AAAI",
            "volume": "",
            "issn": "",
            "pages": "1280--1286",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Solving Bongard problems with a visual language and pragmatic reasoning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Depeweg",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "A"
                    ],
                    "last": "Rothkopf",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "J\u00e4kel",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.04452"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Relation learning in a neurocomputational architecture supports cross-domain transfer",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "A A"
                    ],
                    "last": "Doumas",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Puebla",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Martin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Hummel",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 42nd Annual Meeting of the Cognitive Science Society",
            "volume": "",
            "issn": "",
            "pages": "932--937",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Learning to infer graphics programs from hand-drawn images",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ellis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Solar-Lezama",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ritchie",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ellis",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nye",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sabl\u00e9-Meyer",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cary",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Morales",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hewitt",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Solar-Lezama",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.08381"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "The Hearsay-II speechunderstanding system: Integrating knowledge to resolve uncertainty",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "D"
                    ],
                    "last": "Erman",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hayes-Roth",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "R"
                    ],
                    "last": "Lesser",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Reddy",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "ACM Computing Surveys (CSUR)",
            "volume": "12",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Making sense of sensory input",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Evans",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hern\u00e1ndez-Orallo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Welbl",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kohli",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sergot",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Artificial Intelligence",
            "volume": "293",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "A program for the solution of a class of geometric-analogy intelligence-test questions",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "G"
                    ],
                    "last": "Evans",
                    "suffix": ""
                }
            ],
            "year": 1968,
            "venue": "Semantic Information Processing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Robust physical-world attacks on deep learning visual classification",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Eykholt",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Evtimov",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fernandes",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rahmati",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Prakash",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kohno",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "volume": "",
            "issn": "",
            "pages": "1625--1634",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "The structure-mapping engine",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Falkenhainer",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Forbus",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gentner",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "Proceedings of the National Conference on Artificial Intelligence, AAAI",
            "volume": "",
            "issn": "",
            "pages": "272--277",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Mappings in Thought and Language",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Fauconnier",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Learning task-general representations with generative neurosymbolic modeling",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Feinman",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "M"
                    ],
                    "last": "Lake",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Finn",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Abbeel",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Levine",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the International Conference on Machine Learning, ICML",
            "volume": "",
            "issn": "",
            "pages": "1126--1135",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Analogy just looks like high level perception: Why a domain-general approach to analogical mapping is right",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Forbus",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gentner",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Markman",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "W"
                    ],
                    "last": "Ferguson",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Journal of Experimental and Theoretical Artificial Intelligence",
            "volume": "10",
            "issn": "2",
            "pages": "231--257",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Analogical learning of visual/conceptual relationships in sketches",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Forbus",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Usher",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Tomai",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of the National Conference on Artificial Intelligence, AAAI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Proceedings of the National Conference on Artificial Intelligence, AAAI",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Forbus",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lovett",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lockwood",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wetzel",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Matuk",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Jee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Usher",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Cogsketch",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1878--1879",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Index of Bongard Problems",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "E"
                    ],
                    "last": "Foundalis",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "PHAEACO: A Cognitive Architecture Inspired by Bongard's Problems",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "E"
                    ],
                    "last": "Foundalis",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "The computational modeling of analogy-making",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "French",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Trends in Cognitive Sciences",
            "volume": "6",
            "issn": "5",
            "pages": "200--205",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Five points to check when comparing visual perception in humans and machines",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "Funke",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Borowski",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Stosio",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Brendel",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "S A"
                    ],
                    "last": "Wallis",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bethge",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2004.09406"
                ]
            }
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Synthesizing programs for images using reinforced adversarial learning",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ganin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kulkarni",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Babuschkin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "M A"
                    ],
                    "last": "Eslami",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 35th International Conference on Machine Learning, ICML",
            "volume": "4",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Shortcut learning in deep neural networks",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Geirhos",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Jacobsen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Michaelis",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zemel",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Brendel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bethge",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "A"
                    ],
                    "last": "Wichmann",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature Machine Intelligence",
            "volume": "2",
            "issn": "11",
            "pages": "665--673",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Structure-mapping: A theoretical framework for analogy",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gentner",
                    "suffix": ""
                }
            ],
            "year": 1983,
            "venue": "Cognitive Science",
            "volume": "7",
            "issn": "2",
            "pages": "155--170",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Analogy and abstraction",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gentner",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hoyos",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Topics in Cognitive Science",
            "volume": "9",
            "issn": "3",
            "pages": "672--693",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Brain states: Top-down influences in sensory processing",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Gilbert",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sigman",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Neuron",
            "volume": "54",
            "issn": "5",
            "pages": "677--696",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Concepts in a probabilistic language of thought",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Goodman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Gerstenberg",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "The Conceptual Mind: New Directions in the Study of Concepts",
            "volume": "",
            "issn": "",
            "pages": "623--654",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "The theory theory as an alternative to the innateness hypothesis",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gopnik",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Chomsky and His Critics",
            "volume": "",
            "issn": "",
            "pages": "238--254",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Attention on abstract visual reasoning",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hahne",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "L\u00fcddecke",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "W\u00f6rg\u00f6tter",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kappel",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1911.05990"
                ]
            }
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, CVPR",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Learning to make analogies by contrasting abstract relational structure",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Santoro",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "G T"
                    ],
                    "last": "Barrett",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Morcos",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lillicrap",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the International Conference on Learning Representations, ICLR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Environmental drivers of systematicity and generalization in a situated agent",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lampinen",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schneider",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Clark",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Botvinick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "L"
                    ],
                    "last": "Mcclelland",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Santoro",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the International Conference on Learning Representations, ICLR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Grounded language learning fast and slow",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Tieleman",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Von Glehn",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Merzic",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Clark",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Surfaces and Essences. Basic Books",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hofstadter",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Sander",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Bach: an Eternal Golden Braid",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Hofstadter",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "G\u00f6del",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Escher",
                    "suffix": ""
                }
            ],
            "year": 1979,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Analogies and roles in human and machine thinking",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Hofstadter",
                    "suffix": ""
                }
            ],
            "year": 1985,
            "venue": "In Metamagical Themas, chapter",
            "volume": "24",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Fluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Hofstadter",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Analogy as the core of cognition",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Hofstadter",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "The Analogical Mind: Perspectives from Cognitive Science",
            "volume": "",
            "issn": "",
            "pages": "499--538",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "The Copycat project: A model of mental fluidity and analogy-making",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Hofstadter",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mitchell",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Advances in Connectionist and Neural Computation Theory",
            "volume": "2",
            "issn": "",
            "pages": "31--112",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Analogical thinking and human intelligence",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Holyoak",
                    "suffix": ""
                }
            ],
            "year": 1984,
            "venue": "Advances in the Psychology of Human Intelligence",
            "volume": "2",
            "issn": "",
            "pages": "199--230",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Analogical mapping by constraint satisfaction",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Holyoak",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Thagard",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "Cognitive Science",
            "volume": "13",
            "issn": "3",
            "pages": "295--355",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Stratified rule-aware network for ab stract visual reasoning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI 2021",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "LISA: A computational model of analogical inference and schema induction",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Hummel",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Holyoak",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Proceedings of the Eighteenth Annual Meeting of the Cognitive Science Society",
            "volume": "",
            "issn": "",
            "pages": "352--357",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Solving Raven's Progressive Matrices with Multi-Layer Relation Networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jahrens",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Martinetz",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the World Congress on Computational Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Embodied Mind, Meaning, and Reason: How Our Bodies Give Rise to Understanding",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "The reviewing of object files: Object-specific integration of information",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kahneman",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Treisman",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "J"
                    ],
                    "last": "Gibbs",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Cognitive Psychology",
            "volume": "24",
            "issn": "2",
            "pages": "175--219",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Integrating memory and reasoning in analogy-making: The AMBR model",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "N"
                    ],
                    "last": "Kokinov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Petrov",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "The Analogical Mind: Perspectives from Cognitive Science",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "Human-level concept learning through probabilistic program induction",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "M"
                    ],
                    "last": "Lake",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Science",
            "volume": "350",
            "issn": "6266",
            "pages": "1332--1338",
            "other_ids": {}
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "Supplementary Material for Human-level concept learning through probabilistic program induction",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "M"
                    ],
                    "last": "Lake",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Science",
            "volume": "350",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "Building machines that learn and think like people",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "M"
                    ],
                    "last": "Lake",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "D"
                    ],
                    "last": "Ullman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Gershman",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Behavioral and Brain Sciences",
            "volume": "40",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "The contemporary theory of metaphor",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lakoff",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Metaphor and Thought",
            "volume": "",
            "issn": "",
            "pages": "202--251",
            "other_ids": {}
        },
        "BIBREF67": {
            "ref_id": "b67",
            "title": "The neural theory of metaphor",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lakoff",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "The Cambridge Handbook of Metaphor and Thought",
            "volume": "",
            "issn": "",
            "pages": "17--38",
            "other_ids": {}
        },
        "BIBREF68": {
            "ref_id": "b68",
            "title": "Transforming task representations to perform novel tasks",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "K"
                    ],
                    "last": "Lampinen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "L"
                    ],
                    "last": "Mcclelland",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the National Academy of Sciences",
            "volume": "117",
            "issn": "52",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF69": {
            "ref_id": "b69",
            "title": "Beyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "L\u00e1zaro-Gredilla",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Guntupalli",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "George",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Science Robotics",
            "volume": "4",
            "issn": "26",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "How can we accelerate progress towards human-like linguistic generalization?",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Linzen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "5210--5217",
            "other_ids": {}
        },
        "BIBREF71": {
            "ref_id": "b71",
            "title": "Modeling visual problem solving as analogical reasoning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lovett",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Forbus",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Psychological review",
            "volume": "124",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF72": {
            "ref_id": "b72",
            "title": "Seeing the meaning : Vision meets semantics in solving pictorial analogy problems",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ichien",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Yuille",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Holyoak",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 41st Annual Meeting of the Cognitive Science Society",
            "volume": "",
            "issn": "",
            "pages": "2201--2207",
            "other_ids": {}
        },
        "BIBREF73": {
            "ref_id": "b73",
            "title": "How to build a baby: II. Conceptual primitives",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Mandler",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Psychological Review",
            "volume": "99",
            "issn": "4",
            "pages": "587--604",
            "other_ids": {}
        },
        "BIBREF74": {
            "ref_id": "b74",
            "title": "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Mao",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kohli",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "7th International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF75": {
            "ref_id": "b75",
            "title": "NLP's generalization problem, and how researchers are tackling it. The Gradient",
            "authors": [
                {
                    "first": "Ana",
                    "middle": [],
                    "last": "Marasovi\u0107",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF76": {
            "ref_id": "b76",
            "title": "A Proposal for the Dartmouth Summer Research Project in Artificial Intelligence",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Mccarthy",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Minsky",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Rochester",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "E"
                    ],
                    "last": "Shannon",
                    "suffix": ""
                }
            ],
            "year": 1955,
            "venue": "",
            "volume": "27",
            "issn": "",
            "pages": "12--14",
            "other_ids": {}
        },
        "BIBREF77": {
            "ref_id": "b77",
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mccoy",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pavlick",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Linzen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF78": {
            "ref_id": "b78",
            "title": "Linguistic regularities in continuous space word representations",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "T"
                    ],
                    "last": "Yih",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zweig",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "746--751",
            "other_ids": {}
        },
        "BIBREF79": {
            "ref_id": "b79",
            "title": "Analogy-Making as Perception: A Computer Model",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mitchell",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF80": {
            "ref_id": "b80",
            "title": "Artificial Intelligence: A Guide for Thinking Humans",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mitchell",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Straus, and Giroux",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF81": {
            "ref_id": "b81",
            "title": "Musicat: A model of music perception and expectation",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Nichols",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hofstadter",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the 31st Annual Meeting of the Cognitive Science Society",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF82": {
            "ref_id": "b82",
            "title": "Bongard-LOGO: A new benchmark for human-level concept learning and reasoning",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Mao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Anandkumar",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2010.00763"
                ]
            }
        },
        "BIBREF83": {
            "ref_id": "b83",
            "title": "Attention, similarity, and the identification-categorization relationship",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nosofsky",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "Journal of Experimental Psychology: General",
            "volume": "115",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF84": {
            "ref_id": "b84",
            "title": "Joe Biden positions himself as 'bridge' to next generation at Michigan rally. The Guardian",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Perkins",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF85": {
            "ref_id": "b85",
            "title": "Detecting unseen visual relations using analogies",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Peyre",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sivic",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Laptev",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Schmid",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, ICCV",
            "volume": "",
            "issn": "",
            "pages": "1981--1990",
            "other_ids": {}
        },
        "BIBREF86": {
            "ref_id": "b86",
            "title": "Semantic image retrieval via active grounding of visual situations",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Quinn",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Conser",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Witte",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mitchell",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the International Conference on Semantic Computing",
            "volume": "",
            "issn": "",
            "pages": "172--179",
            "other_ids": {}
        },
        "BIBREF87": {
            "ref_id": "b87",
            "title": "Meta-learning for semi-supervised few-shot classification",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Triantafillou",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ravi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Snell",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Swersky",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Larochelle",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "S"
                    ],
                    "last": "Zemel",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the International Conference on Learning Representations, ICLR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF88": {
            "ref_id": "b88",
            "title": "Early visual cortex as a multiscale cognitive blackboard",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "R"
                    ],
                    "last": "Roelfsema",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "P"
                    ],
                    "last": "De Lange",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Annual Review of Vision Science",
            "volume": "2",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF89": {
            "ref_id": "b89",
            "title": "Principles of categorization",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Rosch",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Concepts: Core Readings",
            "volume": "",
            "issn": "",
            "pages": "189--206",
            "other_ids": {}
        },
        "BIBREF90": {
            "ref_id": "b90",
            "title": "The child as hacker",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Rule",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Piantadosi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Trends in Cognitive Sciences",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF91": {
            "ref_id": "b91",
            "title": "VISALOGY: Answering visual analogy questions",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sadeghi",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Zitnick",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1882--1890",
            "other_ids": {}
        },
        "BIBREF92": {
            "ref_id": "b92",
            "title": "A concept learning algorithm with adaptive search",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Saito",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nakano",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Machine Intelligence 14: Applied Machine Intelligence",
            "volume": "",
            "issn": "",
            "pages": "347--363",
            "other_ids": {}
        },
        "BIBREF93": {
            "ref_id": "b93",
            "title": "Winogrande: An adversarial Winograd schema challenge at scale",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sakaguchi",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "Le"
                    ],
                    "last": "Bras",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bhagavatula",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the National Conference on Artificial Intelligence, AAAI",
            "volume": "",
            "issn": "",
            "pages": "8732--8740",
            "other_ids": {}
        },
        "BIBREF94": {
            "ref_id": "b94",
            "title": "Meta-learning with memory-augmented neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Santoro",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bartunov",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Botvinick",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wierstra",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lillicrap",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "International Conference on Machine Learning (ICML 2016)",
            "volume": "",
            "issn": "",
            "pages": "1842--1850",
            "other_ids": {}
        },
        "BIBREF95": {
            "ref_id": "b95",
            "title": "A simple neural network module for relational reasoning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Santoro",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Raposo",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "G T"
                    ],
                    "last": "Barrett",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Malinowski",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pascanu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Battaglia",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lillicrap",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "4968--4977",
            "other_ids": {}
        },
        "BIBREF96": {
            "ref_id": "b96",
            "title": "Darwin and the political economists: Divergence of character",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Schweber",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "Journal of the History of Biology",
            "volume": "13",
            "issn": "2",
            "pages": "195--289",
            "other_ids": {}
        },
        "BIBREF97": {
            "ref_id": "b97",
            "title": "A cognitive architecture that combines internal simulation with a global workspace",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shanahan",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Consciousness and Cognition",
            "volume": "15",
            "issn": "2",
            "pages": "433--449",
            "other_ids": {}
        },
        "BIBREF98": {
            "ref_id": "b98",
            "title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Smolensky",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Artificial Intelligence",
            "volume": "46",
            "issn": "1-2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF99": {
            "ref_id": "b99",
            "title": "Prototypical networks for few-shot learning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Snell",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Swersky",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zemel",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "4078--4088",
            "other_ids": {}
        },
        "BIBREF100": {
            "ref_id": "b100",
            "title": "Core knowledge",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "S"
                    ],
                    "last": "Spelke",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Kinzler",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Developmental Science",
            "volume": "10",
            "issn": "1",
            "pages": "89--96",
            "other_ids": {}
        },
        "BIBREF101": {
            "ref_id": "b101",
            "title": "Improving generalization for abstract reasoning tasks using disentangled feature representations",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Steenbrugge",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Leroux",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Verbelen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Dhoedt",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1811.04784"
                ]
            }
        },
        "BIBREF102": {
            "ref_id": "b102",
            "title": "Intriguing properties of neural networks",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zaremba",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bruna",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Erhan",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the International Conference on Learning Representations, ICLR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF103": {
            "ref_id": "b103",
            "title": "Feature binding, attention and object perception",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Treisman",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proceedings of the Royal Society, Series B",
            "volume": "6",
            "issn": "",
            "pages": "1295--1306",
            "other_ids": {}
        },
        "BIBREF104": {
            "ref_id": "b104",
            "title": "Conceptual integration in counterfactuals",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Turner",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Fauconnier",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Discourse and Cognition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF105": {
            "ref_id": "b105",
            "title": "The Computer and the Brain",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Neumann",
                    "suffix": ""
                }
            ],
            "year": 1958,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF106": {
            "ref_id": "b106",
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Michael",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Bowman",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 EMNLP Workshop on Blackbox NLP: Analyzing and Interpreting Neural Networks for NLP",
            "volume": "",
            "issn": "",
            "pages": "353--355",
            "other_ids": {}
        },
        "BIBREF107": {
            "ref_id": "b107",
            "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Pruksachatkun",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Nangia",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Michael",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Bowman",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "32",
            "issn": "",
            "pages": "3266--3280",
            "other_ids": {}
        },
        "BIBREF108": {
            "ref_id": "b108",
            "title": "Meta-learning in natural and artificial intelligence",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "X"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2011.13464"
                ]
            }
        },
        "BIBREF109": {
            "ref_id": "b109",
            "title": "Automatic generation of Raven's progressive Matrices",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI",
            "volume": "",
            "issn": "",
            "pages": "903--909",
            "other_ids": {}
        },
        "BIBREF110": {
            "ref_id": "b110",
            "title": "Learning representations that support extrapolation",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "W"
                    ],
                    "last": "Webb",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Dulberg",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "M"
                    ],
                    "last": "Frankland",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "A"
                    ],
                    "last": "Petrov",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "C"
                    ],
                    "last": "O&apos;reilly",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Cohen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2007.05059"
                ]
            }
        },
        "BIBREF111": {
            "ref_id": "b111",
            "title": "Emergent symbols through binding in external memory",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "W"
                    ],
                    "last": "Webb",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sinha",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Cohen",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the International Conference on Learning Representations, ICLR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF112": {
            "ref_id": "b112",
            "title": "Learning and reasoning by analogy",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "H"
                    ],
                    "last": "Winston",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "Communications of the ACM",
            "volume": "23",
            "issn": "12",
            "pages": "689--703",
            "other_ids": {}
        },
        "BIBREF113": {
            "ref_id": "b113",
            "title": "The Scattering Compositional Learner: Discovering Objects, Attributes, Relationships in Analogical Reasoning",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Grosse",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2007.04212"
                ]
            }
        },
        "BIBREF114": {
            "ref_id": "b114",
            "title": "RAVEN: A dataset for relational and analogical visual reasoning",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "S.-C",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "volume": "2019",
            "issn": "",
            "pages": "5312--5322",
            "other_ids": {}
        },
        "BIBREF115": {
            "ref_id": "b115",
            "title": "Learning perceptual inference by contrasting",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "S.-C",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "32",
            "issn": "",
            "pages": "1075--1087",
            "other_ids": {}
        },
        "BIBREF116": {
            "ref_id": "b116",
            "title": "Abstract spatial-temporal reasoning via probabilistic abduction and execution",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF117": {
            "ref_id": "b117",
            "title": "Abstract reasoning with distracting features",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Z.-J",
                    "middle": [],
                    "last": "Zha",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "32",
            "issn": "",
            "pages": "5842--5853",
            "other_ids": {}
        },
        "BIBREF118": {
            "ref_id": "b118",
            "title": "Solving Raven's progressive matrices with neural networks",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Zhuo",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kankanhalli",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2002.01646"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An example of SME's input, in the form of logical propositions. Adapted from Ref. 27. et al.'s illustrative examples, the base gives propositions about the solar system and the target gives propositions about the Rutherford atom.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "A sample RPM problem.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "52 The following are some examples from this domain: If the string abc changes to the string abd, what does the string pqrs change to? If the string abc changes to the string abd, what does the string ppqqrrss change to? If the string abc changes to the string abd, what does the string srqp change to? If the string abc changes to the string abd, what does the string xyz change to? If the string axbxcx changes to the string abc, what does the string pzqzrzsz change to?",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "(a) Illustration of part of Copycat's concept network. (b) Illustration of Copycat's workspace, during a run of the program.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "State of the workspace at six different timesteps during a run of Copycat (adapted from Ref. 80).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Illustration of answer-generation method for RAVEN dataset. Given the correct answer (here, black pentagon at top), each incorrect candidate answer is generated by changing one attribute of the correct answer. Adapted from from Ref.58.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Four sample Bongard Problems (adapted from Foundalis' website. 34 )",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "(a) Human-designed grammar for rule induction. (b) Sample Bongard problem, with rule derivation and corresponding rule expression (\"the left side has objects that contain triangles.\") (Adapted from Ref. 19.) Douglas Hofstadter has written extensively about Bongard problems, starting with his 1979 book G\u00f6del, Escher, Bach: an Eternal Golden Braid, 51 in which he sketched a rough architecture for solving them. (Notably, this sketch became the basis for the Active Symbol Architecture that I described above.)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "A sample ARC task (adapted fromChollet 14 ). Best viewed in color.probabilistic program induction with a neurally inspired model of visual perception and action in a physical robot. The work of Evans et al.24 notably focuses on unsupervised program induction constrained by domain-independent properties of cognition.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": ",39,76 It's not clear that benchmarks such as Balanced-Raven are free from such biases. This, coupled with the lack of transparency about how the networks accomplish their tasks, makes it unclear what these networks have learned, and what features they are basing their decisions on.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This material is based upon work supported by the National Science Foundation under Grant No. 2020103. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the National Science Foundation. This work was also supported by the Santa Fe Institute. I am grateful to Marianna Bolognesi, Andrew Burt, Richard Evans, Ross Gayler, Brenden Lake, Adam Santoro, Wai Keen Vong, and two anonymous reviewers for comments on an earlier version of the manuscript.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        }
    ]
}
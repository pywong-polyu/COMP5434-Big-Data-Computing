{
    "paper_id": "f0de6d5118de3500f97870073ebc92590d9de397",
    "metadata": {
        "title": "HTMOT : Hierarchical Topic Modelling Over Time",
        "authors": [
            {
                "first": "Judicael",
                "middle": [],
                "last": "Poumay",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "ULiege/HEC Liege Rue",
                    "location": {
                        "addrLine": "louvrex 14",
                        "postCode": "4000",
                        "settlement": "Liege",
                        "country": "Belgium"
                    }
                },
                "email": ""
            },
            {
                "first": "Ashwin",
                "middle": [],
                "last": "Ittoo",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "ULiege/HEC Liege Rue",
                    "location": {
                        "addrLine": "louvrex 14",
                        "postCode": "4000",
                        "settlement": "Liege",
                        "country": "Belgium"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Over the years, topic models have provided an efficient way of extracting insights from text. However, while many models have been proposed, none are able to model topic temporality and hierarchy jointly. Modelling time provide more precise topics by separating lexically close but temporally distinct topics while modelling hierarchy provides a more detailed view of the content of a document corpus. In this study, we therefore propose a novel method, HTMOT, to perform Hierarchical Topic Modelling Over Time. We train HTMOT using a new implementation of Gibbs sampling, which is more efficient. Specifically, we show that only applying time modelling to deep sub-topics provides a way to extract specific stories or events while high level topics extract larger themes in the corpus. Our results show that our training procedure is fast and can extract accurate high-level topics and temporally precise sub-topics. We measured our model's performance using the Word Intrusion task and outlined some limitations of this evaluation method, especially for hierarchical models. As a case study, we focused on the various developments in the space industry in 2020.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The amount of data being generated by humanity increases exponentially. About 80% of the unstructured data worldwide is textual and comes from a wide variety of ubiquitous sources, including online news and social media platforms. Buried within those voluminous amounts of texts are meaningful insights, which could help in supporting business decision-making activities. Hence, methods for extracting these insights are becoming more and more valuable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Thematic information constitutes one such type of insight. In NLP, several methods for topic detection from text have been proposed to extract the various themes contained in a corpus [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . These models have been employed for different applications, including fraud detection [11] , environment scanning [12] , understanding employee and customer satisfaction [13, 14] among others [15, 16, 17] .",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 187,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 188,
                    "end": 190,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 191,
                    "end": 193,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 194,
                    "end": 196,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 197,
                    "end": 199,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 200,
                    "end": 202,
                    "text": "6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 203,
                    "end": 205,
                    "text": "7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 206,
                    "end": 208,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 209,
                    "end": 211,
                    "text": "9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 212,
                    "end": 215,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 304,
                    "end": 308,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 332,
                    "end": 336,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 388,
                    "end": 392,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 393,
                    "end": 396,
                    "text": "14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 410,
                    "end": 414,
                    "text": "[15,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 415,
                    "end": 418,
                    "text": "16,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 419,
                    "end": 422,
                    "text": "17]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "By far, Latent Dirichlet Allocation (LDA) [1] is the most commonly employed topic model. However, despite its widespread adoption, it suffers from a number of longstanding limitations. First, LDA requires the number of topics to be defined upfront. This constraint arises from the underlying Dirichlet law used, which defines a distribution over random vectors of finite length. The finite nature of these vectors forces users to pre-define a number of topics to extract.",
            "cite_spans": [
                {
                    "start": 42,
                    "end": 45,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Consequently, users are compelled to experiment with a varying number of topics in order to determine the optimal number for their applications. Additionally, LDA ignores interesting characteristics of topics, in particular, hierarchy and temporality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To overcome the issue of predefining the number of topics, the Hierarchical Dirichlet Process model (HDP) [2] has been proposed. Through the use of Dirichlet Processes, it is capable of deciding the number of topics during training. This is because contrary to Dirichlet laws, a Dirichlet Process defines a distribution over random vectors of infinite length.",
            "cite_spans": [
                {
                    "start": 106,
                    "end": 109,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In practice this provides the ability to determine the number of topics dynamically during training. HDP has been used in many tasks such as speaker diarization [18] , i.e. clustering an input audio stream according to the number of speakers, determined automatically.",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 165,
                    "text": "[18]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "More importantly, HDP provided a solid premise for the development of more advanced topic modelling methods, such as the nested Chinese Restaurant Process (nCRP) [3] and the nested Hierarchical Dirichlet Process (nHDP) [4] .",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 165,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 219,
                    "end": 222,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The latter being an improved version of the former. nHDP was tested on news and Wikipedia articles and evaluated based on a perplexity and a qualitative analysis; both analyses revealing a substantial improvement over LDA and HDP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Moreover, nHDP was trained using Stochastic Variational Inference (SVI) which is substantially faster than the traditional Gibbs Sampling algorithm used. Such hierarchical models enable the extraction of topics and corresponding subtopics, organizing them in a tree-like hierarchy of arbitrary depth and breadth. Similar to HDP, the shape of this tree is defined during training thanks to the use of Dirichlet processes. Extracting topic hierarchies from a corpus provides a more finegrained view of the underlying data and is particularly useful in applications such as ontology learning [19] and research idea recommendation [20] .",
            "cite_spans": [
                {
                    "start": 589,
                    "end": 593,
                    "text": "[19]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 627,
                    "end": 631,
                    "text": "[20]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In parallel, temporal topic models [6, 8, 9, 7] have been proposed to model the evolution and/or localization of topics with respect to time. For instance, in the seminal study of Wang and McCallum [6] , the authors associated each topic with a Beta distribution to model time. They showed that their Topic over Time (ToT) model more accurately pinpointed the occurrence of topics in time, augmenting the results obtained by classical LDA. A KL-divergence analysis further revealed that the topics extracted by their model were more distinct than those extracted by LDA. In essence, incorporating temporal information enable the separation of topics that are lexically similar but temporally distinct. ToT was evaluated on various datasets, such as NIPS articles, e-mails and historical texts. Temporal topic models have been used for tracking trends in scientific articles [21] and events in social media [22] .",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 38,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 39,
                    "end": 41,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 42,
                    "end": 44,
                    "text": "9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 45,
                    "end": 47,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 198,
                    "end": 201,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 874,
                    "end": 878,
                    "text": "[21]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 906,
                    "end": 910,
                    "text": "[22]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Intuitively, incorporating both temporal and hierarchical modelling would yield models that encompass the strengths of both. Furthermore, several applications warrant the incorporation of temporal and hierarchical information in topic models. One such application is that of environment scanning [12] , which is the task of gathering, analyzing and monitoring information that is relevant to an organization to identify future threats and opportunities. It is clear that this task would benefit from having both more detailed topics using hierarchical modelling and more precise topics that are better localized in time using temporal modelling. However, to date, no topics model integrating both temporal and hierarchical information exist, despite the recent advances in topic modelling and in NLP in general.",
            "cite_spans": [
                {
                    "start": 296,
                    "end": 300,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To address this issue, we propose a novel method, HTMOT, for Hierarchical Topic Modelling Over Time. By jointly modelling topic hierarchy and temporality, our model offers the advantages of previous methods, which only focused on a single dimension (i.e. temporality or hierarchy). To the best of our knowledge, our model is the first to jointly model topic hierarchy and temporality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "However, incorporating both temporal and hierarchical information poses a significant challenge during training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "While HTMOT is based on nHDP, it could not be trained using Stochastic Variational Inference (SVI) like its predecessor. This is because SVI requires all distribution to estimate to have a conjugate prior. However, the beta distribution used to model time in ToT does not have a known conjugate prior and therefore SVI could not be used to train HTMOT. To overcome this issue, we resorted to using Gibbs sampling which is known to be slow and would result in an prohibitively long training time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To address this problem, we developed a new implementation of Gibbs sampling for HTMOT. At its core is a novel tree-based data structure, which we call the Infinite Dirichlet Tree. The basic idea is to assign all the words in the corpus to the nodes of the tree which represents topics. More specifically, there is one such tree for the corpus and one for each document. As we will see later, the arrangement of words in the trees implicitly defines a topic-word, topic-time, document-topic, corpus-topic and topic-hierarchy distributions. The Gibbs sampler is then reduced to the sampling of only one distribution (the word-topic distribution) which decides to which nodes each word is assigned to in an iterative manner. This implicitly estimates the others distributions as the sampler constantly re-arranges the words in the tree. Consequently, this implementation of Gibbs sampling provides a massive speed-up compared to traditional implementation. Specifically, our experiments reveal that it is similar to SVI in terms of speed. Our HTMOT model and its novel Gibbs sampling training procedure constitute the core contributions of our work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "As ancillary contributions, we demonstrate how NER could help enhancing the interpretability of topics. Additionally, the theoretical information about Dirichlet Processes necessary to develop this implementation was scattered around the literature. Hence, we also provide a synthetic review of Dirichlet Processes for hierarchical topic modelling.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Finally, we evaluate our method's performance using various methods, including the recent Word Intrusion task [23] .",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 114,
                    "text": "[23]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "It involves human users/annotators detecting an intruder (polluting) word, deliberately introduced in a topic. Given the difficulty in evaluating topics [23] , we provide a critic of the Word Intrusion task, in particular in the context of hierarchical topic modelling, and propose directions for future topic evaluation methods.",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 157,
                    "text": "[23]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our literature review showed that there is no standard dataset for evaluating topic models. Therefore, for our experiments we crawled 62k news articles from DigitalTrends.com between 2015 and 2020. Doing so enabled us to test our method on recent news. As a case study, we focused on the various developments in the space industry in 2020 to show how our HTMOT method can help uncover insights about a particular domain to perform environment scanning. Indeed, the results do provide a better understanding of the many recent developments in the space industry such as the advent of new spacecrafts (Boeing Starliner, NASA Orion), and the heightened interest in asteroid sampling missions. For the Word Intrusion task, 57 people answered our survey which resulted in a 74.83% overall accuracy. More precisely, we show that deeper topics show worst performance: 98.25% for depth 1 topics and 63.13% for depth 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "However, we also measured the confidence of annotators: 92.63% for depth 1 topics while depth 2 topics show 71.13%",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "confidence. Hence, we argue that deeper topics require more knowledge from annotators which is one of the flaw of the Word Intrusion task for hierarchical models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To summarize, our contributions are: The rest of the paper is structured as follow. Section 2 provides an overview of Dirichlet processes and describes current topic modelling studies. Then, in section 3 we present our model by describing Infinite Dirichlet Trees , how temporality was incorporated, and the training procedure. Next, section 4 will present the experimental setup, the evaluation methods, and parameters used. Finally, results are presented in section 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we begin with a synthetic review of Dirichlet processes. Then, we will review the literature on topic models that are related to ours. Finally, we will discuss the various methods used to evaluate topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Background and Related Work"
        },
        {
            "text": "To understand the foundation of our model, it is important to understand Dirichlet Processes. However, the important information about Dirichlet Processes, especially in the context of topic modelling is scattered across different sources in the existing literature. Hence, we provide a synthetic review of the DP from its definition to its important properties with respect to topic modelling. We believe that our synthesis could be valuable for future related research.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dirichlet processes (DPs)"
        },
        {
            "text": "A Dirichlet distribution defines a probability distribution over random vectors of finite dimensions defined such that the sum of their elements is one; Dirichlet processes generalize this idea to infinity. Specifically, a Dirichlet process defines a probability distribution over random vectors of infinite dimensions (for which the sum of their elements is one).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dirichlet processes (DPs)"
        },
        {
            "text": "Formally, let H be a probability distribution (called base distribution) over S and \u03b1 a positive real number. Then, we can define a Dirichlet Process, DP (H, \u03b1), as a stochastic process whose realization X is itself a probability distribution over S, such that for any measurable finite partition of S,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dirichlet processes (DPs)"
        },
        {
            "text": "Using the stick breaking construction (see figure 1 ) we can define X as : 2 DPs have three properties that are interesting for hierarchical topic modelling: discreteness, clustering, and hierarchisation. Discreteness : We can look at a DP as a tool to discretize a distribution. Independent of whether or not H is continuous, the resulting X \u223c DP (H, \u03b1) is defined as a weighted sum of discrete atoms. This discreteness ensures that the probability of a topic is not infinitesimal and leads to the clustering property. Clustering : Let \u03b8 1 , ..., \u03b8 n be a sample of topics drawn from X. Then, if we were to draw from the posterior X|\u03b8 1 , ..., \u03b8 n , we would have a reasonable chance 1 Dir(.) correspond to the Dirichlet law 2 With \u03b4x being the Dirac delta distribution.",
            "cite_spans": [
                {
                    "start": 726,
                    "end": 727,
                    "text": "2",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [
                {
                    "start": 43,
                    "end": 51,
                    "text": "figure 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Dirichlet processes (DPs)"
        },
        {
            "text": "of drawing from \u03b8 1 , ..., \u03b8 n again. Precisely, the posterior is :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dirichlet processes (DPs)"
        },
        {
            "text": "Thus, as the number of observations (n) increases, we start to ignore H and focus on the empirical distribution n i \u03b4 \u03b8 i .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dirichlet processes (DPs)"
        },
        {
            "text": "Hence, we are more likely to choose atoms that were already chosen, and clustering happens. However, we always retain some probability of drawing from H and create a new cluster (topic). In our case, n is equal to the number of words in the corpus. Hierarchisation : If we consider another Dirichlet Process Y \u223c DP (X, \u03b2) with its own concentration parameter \u03b2, then the atoms of Y are exactly the atoms of X \u223c DP (H, \u03b1). This property can be used so that documents and corpus share the same topics (atoms) but with a different distribution over them. Precisely, X will model the corpustopic distribution while multiple Y i can be used to model each document-topic distribution. For more information about",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dirichlet processes (DPs)"
        },
        {
            "text": "DPs, see Ghosal [24] , Teh [25] and Paisley et al. [4] .",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 20,
                    "text": "[24]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 27,
                    "end": 31,
                    "text": "[25]",
                    "ref_id": null
                },
                {
                    "start": 51,
                    "end": 54,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Dirichlet processes (DPs)"
        },
        {
            "text": "We now describe related studies on topic modelling methods. However, given the significant attention that topic modelling has received over the years, we will not provide a comprehensive review, which is out of the scope of our study. Instead, for conciseness, we will focus only on those studies most closely related to ours. For more in-depth reviews see Alghamdi and Alfalqi [26] and Barde and Bainwad [27] . Also, to better present the wide diversity in datasets and evaluation methods, we list them in table 1.",
            "cite_spans": [
                {
                    "start": 378,
                    "end": 382,
                    "text": "[26]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 405,
                    "end": 409,
                    "text": "[27]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Topic Modelling. LDA [1] model was the first popular topic model. It has provided the basis for subsequent models and is still at the crux of many applications. At the core of LDA is a Bayesian generative model, with two Dirichlet distributions, respectively for the document-topic distribution and for the topic-word distribution. These distributions are learnt and optimized via an inference procedure, which enables topics to be detected. The main weakness of LDA is that it requires the user to specify a predefined number of topics to be extracted. However, such information is usually not known in advance. Consequently, LDA requires a long model validation step to determine the number of topics by optimizing some performance metric, such as the perplexity metric.",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 24,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "A more systematic approach to to determine the number of topics is to rely on DPs instead of Dirichlet distributions, as done with HDP Teh et al. [2] . As we discussed earlier, the difference between Dirichlet distributions and processes is that DPs define a distribution over random vectors of infinite length. Hence, for DPs there is no limit to the number of topics. Yet the discrete and finite nature of a corpus forces the collapse of the potentially infinite number of topics into a finite set of clusters of words. Thus, the final number of topic is finite but determined automatically during training.",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 149,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Otherwise, HDP operates on a similar premise to LDA. At the corpus and document level, we have distributions over the topics (DPs) and at the topic level, we have a distribution over the words (Dirichlet distribution). The hierarchisation property of the DPs ensures that the atoms (topics) of the global (corpus) DP are shared by the local (document) DPs; they define different distributions over the same topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Topic models such as LDA have seen many application including fraud detection [11] , environment scanning [12] , understanding employee and customer satisfaction [13, 14] and many other applications [15, 16, 17] . In particular, El",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 82,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 106,
                    "end": 110,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 162,
                    "end": 166,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 167,
                    "end": 170,
                    "text": "14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 199,
                    "end": 203,
                    "text": "[15,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 204,
                    "end": 207,
                    "text": "16,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 208,
                    "end": 211,
                    "text": "17]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Akrouchi et al. [12] used LDA to extract potential weak signals from news articles.",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 20,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Hierarchical Topic Modelling. Methods such as LDA and HDP provide interesting insights into the content of a corpus.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "However, these methods are only capable of extracting the higher order topics. Hence, new methods have been developed to also extract topic hierarchies which provide more fine-grained insights into the content of a corpus.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "One method for hierarchical topic modelling is nHDP [4] . It models topic hierarchy by defining a potentially infinite tree where each node corresponds to a topic. At each level of the tree, we exactly have the HDP model. The difference is that when a word is assigned to a topic during training, there is a chance to go deeper in the tree based on a Bernoulli distribution. If we do go deeper, then we will choose a sub-topic in the same way we choose the parent topic except that now we work with a sub-corpus made up of the tokens assigned to the parent topic.",
            "cite_spans": [
                {
                    "start": 52,
                    "end": 55,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Other topic models have been proposed to model hierarchy [10, 5, 3] . hPAM [10] proposes an acyclical graph structure instead of a tree to model topic hierarchy where high level topics can share low level topics while also modelling topic correlation; while this provides more precise relationships between topics, it is harder to display and navigate. LSHTM [5] recursively applies LDA to the sub-corpus defined by the topics of the previous LDA application; hence, it requires a pre-defined set of parameters to define the shape of the final topic tree. Finally, the nCRP [3] is the predecessor of nHDP and works similarly except that it does not model the document-topic distribution resulting in mono-topic document modelling. Hence, nHDP is more powerful than [5, 3] while keeping a strict tree structure contrary to [10] which is easier to display.",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 61,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 62,
                    "end": 64,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 65,
                    "end": 67,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 75,
                    "end": 79,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 359,
                    "end": 362,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 574,
                    "end": 577,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 765,
                    "end": 768,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 769,
                    "end": 771,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 822,
                    "end": 826,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Extracting topic hierarchies provides a more fine-grained view of the data and is particularly useful in applications such as ontology learning [19] and research idea recommendation [20] . In particular, Wang et al. [20] used Hierarchical Topic Modelling to discover a user's interest from articles read and cross-referenced these interests with current research trends to provide research ideas.",
            "cite_spans": [
                {
                    "start": 144,
                    "end": 148,
                    "text": "[19]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 182,
                    "end": 186,
                    "text": "[20]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 216,
                    "end": 220,
                    "text": "[20]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Temporal Topic Modelling. Other authors have decided to investigate the temporality of topics. This provides information such as when a topic occurred or how it evolved. Understanding the temporality of topics is important, especially for environment scanning where changes in the environment are the most important signals.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "ToT [6] proposes a modified version of LDA by incorporating temporality. Each document/word is associated with a timestamp which are used to fit a beta distribution for each topic. This beta distribution is optimized jointly as the topics are being discovered. The results show topics that are either better localized in time (events with specific dates) or with a clear evolution through time (growth/decline).",
            "cite_spans": [
                {
                    "start": 4,
                    "end": 7,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Other topic models have been proposed to model temporality [9, 7, 8] . MTT [8] creates a binary tree which provides the ability to understand topics at various time scale; deeper nodes correspond to a smaller timescale. DTM [7] slices the corpus by periods; the first slice is processed similarly to LDA and the following slices are processed using the previous one as prior. Finally, DCTM [9] also slices the corpus in period but uses Gaussian processes and SVD instead of LDA based techniques. The advantage of ToT is that it is non-Markovian and it models time as a continuum. Continuity, in particular, is important when mixing ToT with nHDP as we are already building a tree for the topic hierarchy and ToT is the only model which does not require its own discrete structure to model time such as slices or a binary tree.",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 62,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 63,
                    "end": 65,
                    "text": "7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 66,
                    "end": 68,
                    "text": "8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 75,
                    "end": 78,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 224,
                    "end": 227,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 390,
                    "end": 393,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Such Temporal models have been used for tracking trends in scientific articles [21] and events in social media [22] .",
            "cite_spans": [
                {
                    "start": 79,
                    "end": 83,
                    "text": "[21]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 111,
                    "end": 115,
                    "text": "[22]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "In particular, Zhou and Chen [22] uses a ToT-based topic model that also incorporates geoposition information to extract events from social media.",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "text": "[22]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Topic Modelling"
        },
        {
            "text": "Various methods have been used in previous studies to evaluate topic models as we can see in table 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Models Evaluation"
        },
        {
            "text": "Perplexity has been the standard for comparing topic models for a long time. It requires to compute how likely it is that the data would have been generated by the trained topic model. However, this method does not correlate with human judgement [23] . Hence, new methods for evaluating topics have been proposed, but none have provided a new standard.",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 250,
                    "text": "[23]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Topic Models Evaluation"
        },
        {
            "text": "A novel evaluation method is the Word Intrusion task. It involves inserting an intruder word in the list of topic words, then ask people to find this intruder [23] . The idea is that in good topics, the annotators would easily find it. This intruder is selected at random from a pool of words with low probability in the current topic but high probability in some other topic to avoid rare words. With this evaluation method, the final score corresponds to the average classification accuracy made by humans. In, [28] they have shown that this task can be automated with performance close to human annotators.",
            "cite_spans": [
                {
                    "start": 159,
                    "end": 163,
                    "text": "[23]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 513,
                    "end": 517,
                    "text": "[28]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Topic Models Evaluation"
        },
        {
            "text": "Newman et al. [29] proposed topic coherence as a method of topic evaluation. This method consist in computing some similarity score between the top N topic words. Precisely, it is computed as (where w i is more frequent than w j ):",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 18,
                    "text": "[29]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Topic Models Evaluation"
        },
        {
            "text": "i<j score(w i , w j ). Topic coherence is a modular evaluation method as it allows for many different score functions;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Models Evaluation"
        },
        {
            "text": "UCI and UMass being the most popular. Both use the co-occurrence of words but UCI is an extrinsic measure based on Wikipedia articles while UMass is intrinsic and uses the training corpus. However, other score functions such as the cosine similarity of word embeddings can also be used. The topic coherence score of a model is the average coherence score of the topics. In this article we will use UMass as an intrinsic evaluation method and Word Intrusion as an extrinsic one.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Models Evaluation"
        },
        {
            "text": "Finally, all papers listed in table 1 provide a qualitative analysis of the extracted topics. Compared to opaque measures such as coherence and perplexity, qualitatively examining the resulting topics provides a good understanding of the model's performance. However, such an evaluation method is prone to cherry picking especially when many topics are extracted.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Models Evaluation"
        },
        {
            "text": "Model name Corpora ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Topic Models Evaluation"
        },
        {
            "text": "We now describe our method for hierarchical topic modeling over time, HTMOT. We begin by presenting a new type of data structure at the core of HTMOT. We refer to this tree-based structure as an Infinite Dirichlet Tree (IDT). IDT's main purpose is to record the word-topic assignments. In addition, as will be detailed in section 3.3, they are at the basis of our fast implementation of the Gibbs sampling procedure. Next, we describe how temporality was modelled, before finally detailing our novel implementation of Gibbs sampling for HTMOT.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "HTMOT : Hierarchical topic modelling over time"
        },
        {
            "text": "An Infinite Dirichlet Tree (IDT) is an efficient tree-based data structure we developed for HTMOT. There is one such tree for the corpus and one for each document. Each node in these trees represents a topic and each word in the corpus is assigned to a node (see figure 2 ).They are called IDTs to refer to the potentially infinite number of topics provided by the Dirichlet Processes which defines how they grow. Every node of the infinite tree is potentially accessible, they are created when a first word is assigned to them and destroyed automatically when unused. Each node is identified by a finite path in the tree as a sequence of node ids, starting from the root. In the next paragraphs, we will see how these trees are used to model the topic-word, topic-time, document-topic, corpus-topic and topic-hierarchy distributions jointly.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 263,
                    "end": 271,
                    "text": "figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Counting words using Infinite Dirichlet Trees"
        },
        {
            "text": "We have a tree for the corpus and a tree for each document. All words in the corpus are assigned to nodes of the corpus tree while document trees only contains words from their respective document. Hence, the document trees are mutually exclusive subsets of the corpus tree in the sense that adding the content of their nodes would yield the corpus tree. For both kind of trees, each node (topics) will be assigned a different number of words meaning they will differ in size which creates a distribution. Hence, the corpus tree defines a corpus-topic distribution and each document trees define a document-topic distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Counting words using Infinite Dirichlet Trees"
        },
        {
            "text": "Each node represents a topic and each word in the corpus is assigned to a node in the corpus tree and associated document tree (this assignment is later explained in section 3.3.1). More precisely, each topic is represented by at most one node in each tree; a topic may not be represented in a document tree if no word of the associated document has been assigned to that topic. Additionally, each word is associated with the timestamp of its document. The timestamp corresponds to the date of the document and is mapped to a number between 0 and 1 3 such that 0 corresponds to the earliest date of a document in the corpus and 1 corresponds to the latest. Thus, for each node, we can count how many and which words have been assigned to it. This defines a word distribution. Moreover, since each word is associated with a timestamp, we also have a time distribution. Thus, each node defines a topic-word and a topic-time distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Counting words using Infinite Dirichlet Trees"
        },
        {
            "text": "The trees also model the hierarchical distribution of topics. When a word is assigned to a node, it is also assigned to all ancestors of that node. Hence, each word is assigned to a sequence of nodes starting from the root up to the final chosen topic. This creates a hierarchical dependency between the nodes and thus a hierarchical distribution. Hence, there are two types of assignments. First, when a word is assigned to a node as it was the chosen topic for that word; we say the word stops at that node. Second, when a word is assigned to all of the ancestor nodes of the chosen topic (node); we say the word pass through those nodes. This will be important later when discussing word assignments (section 3.3.1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Counting words using Infinite Dirichlet Trees"
        },
        {
            "text": "To speed up training, we also make use of multiple parameters. A Critical Mass threshold (CM) is used to define a minimal size for topic nodes. Given that N is the number of words in the entire corpus, if a node/topic has fewer than CM * N assignments, it won't be considered valid. If a node is not valid, the topic it represents will not be included in the results, and if it stays below the critical mass after seeing the data twice 4 , it will be destroyed automatically. Nodes are also destroyed when they become empty. In any case, words that the now destroyed node may have recorded are simply unassigned; they will be re-assigned in a later iteration. Furthermore, a similar splitting mass threshold is used to decide when nodes are large enough to create children. Finally, nodes create children one at a time and only if all the other current children are valid. These rules constrain the growth of the trees and improve run-time and memory usage. 3 The domain of the beta distribution used 4 This parameter is called the Time To Live (TTL) and was set after empirical observations ",
            "cite_spans": [
                {
                    "start": 958,
                    "end": 959,
                    "text": "3",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1001,
                    "end": 1002,
                    "text": "4",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Counting words using Infinite Dirichlet Trees"
        },
        {
            "text": "Temporality is modelled by associating a topic with a beta distribution as in ToT [6] . However, contrary to ToT, we do not apply temporality to all topics but only deep ones; we choose depths 3 and 4 for our experiments. The reason is that the beta distribution is monomodal (when both of its parameters >1) which is not enough to model the complex temporality of high-level topics.",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 85,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Modelling temporality"
        },
        {
            "text": "Indeed, if we consider the topic of space exploration, many events may occur throughout a given year (especially in recent ones). These multiple occurrences will translate to many peaks in frequency. Therefore, such high-level topics fundamentally have a multimodal distribution. Hence, if we were to apply time modelling to high-level topics, it would separate them into many similar monomodals topics which is undesirable. Conversely, deeper topics (i.e. subtopics)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Modelling temporality"
        },
        {
            "text": "often correspond to precise stories/events, occurring within narrower time frames. Thus, they can be characterized by monomodal distributions, such as the beta distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Modelling temporality"
        },
        {
            "text": "Hence, by mixing temporal and hierarchical modelling and only applying time modelling to deep topics we are able to separate temporally distinct topics at the deepest level while keeping the number of high level topics low enough that they are easily interpretable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Modelling temporality"
        },
        {
            "text": "The parameters of the beta distribution \u03c1 1 i and \u03c1 2 i are computed for a topic i based on the current timestamps assignments (associated with each word assignment). Currently, there exist no known conjugate prior for the beta distribution that is suitable for our application. Hence, we use the method of the moment to estimate these parameters [6] :",
            "cite_spans": [
                {
                    "start": 347,
                    "end": 350,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Modelling temporality"
        },
        {
            "text": "Where t i is the empirical average timestamp assigned to topic i and \u03c3 t i is the empirical variance. Note that the variance is set to have a minimum of 0.0001 to avoid numerical instability and high \u03c1 i values. These parameters are updated each time a word is assigned or unassigned to a topic. Note that non-valid nodes are forced to have a uniform time probability to let them grow unaffected by time distribution until they reach critical mass.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Modelling temporality"
        },
        {
            "text": "Furthermore, to force topics to be more localized in time we modify the estimated parameters. More specifically, when using the beta distribution of a topic in practice, we multiply its parameters \u03c1 i by some constant \u2206; detailed values are provided in the parameter section. Then, we add one to the resulting parameters because parameter values between 0 and 1 lead to a bi-modal regime for the Beta distribution which is not desirable. Finally, we add 0.5 to the Beta Distribution itself. This is because the beta distribution can be valued at near zero for large parameters but this is not desirable as it can lock a topic in time during training since a probability of 0 means no words would be assigned outside of its current time range.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Modelling temporality"
        },
        {
            "text": "Thus, we define a modified version of the beta distribution:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Modelling temporality"
        },
        {
            "text": "Note that ModBetaPDF returns 1 for \u2206 > 1. This is used to disable time for higher level topics as \u2206 values are defined for each depth.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Modelling temporality"
        },
        {
            "text": "Our model (HTMOT) is based in part on nHDP [4] which was trained using stochastic variational inference (SVI).",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 46,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Training HTMOT using Gibbs sampling"
        },
        {
            "text": "However, this training procedure requires each distribution to estimate to have a conjugate prior. The problem is that the beta distribution used to model time does not have a known conjugate prior. Thus, we had to use Gibbs sampling as a substitute training procedure for our model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training HTMOT using Gibbs sampling"
        },
        {
            "text": "Gibbs sampling is advantageous as it eliminates the need for conjugate priors. Moreover, contrary to variational inference it is asymptotically exact and more precise with small datasets [30] . This is important because we care about small sub-topics which are contained in small subsets of the dataset. Finally, Gibbs sampling avoids the problem of mode collapse that is inherent to variational inference. However, its main weakness is that classical implementations are typically prohibitively slow.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 191,
                    "text": "[30]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Training HTMOT using Gibbs sampling"
        },
        {
            "text": "Consequently, we had to develop a novel Gibbs sampling implementation to compete with SVI in terms of speed. for N iterations do 3: for each document in corpus do 4: for each word in document do While classical Gibbs sampling implementation requires sampling from all distributions (see algorithm 1), in the context of topic modelling, it is possible to only draw the word-topic assignment distribution [31] which greatly speed up the process. However, this requires the construction of a data structure tailored to the model to implicitly model the other distributions. This is the role played by our novel Infinite Dirichlet Trees.",
            "cite_spans": [
                {
                    "start": 129,
                    "end": 131,
                    "text": "3:",
                    "ref_id": null
                },
                {
                    "start": 163,
                    "end": 165,
                    "text": "4:",
                    "ref_id": null
                },
                {
                    "start": 403,
                    "end": 407,
                    "text": "[31]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Training HTMOT using Gibbs sampling"
        },
        {
            "text": "As can be seen in figure 3 ), our algorithm consists essentially of three steps. Firstly, unassigning the word from its current topic (and its ancestors) in the corpus and associated document tree. Secondly, draw a topic assignment given the word, document and timestamp based on the corpus and document tree. Thirdly, re-assign the word to the chosen topic (and its ancestors) in the corpus and associated document tree. The initialization procedure of our algorithm is similar expect that it ignores the first step as all words starts unassigned.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 18,
                    "end": 26,
                    "text": "figure 3",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Training HTMOT using Gibbs sampling"
        },
        {
            "text": "As stated in section 3.1, the Infinite Dirichlet Trees model the topic-word, topic-time, document-topic, corpus-topic and topic-hierarchy distribution from their content. Hence, simply by iteratively re-arranging the words through the sampling of the topic assignment distribution, we are implicitly optimizing the aforementioned distributions modelled by the trees (topic-word, topic-time, document-topic, corpus-topic and topic-hierarchy). This is the key to speed up the Gibbs sampling process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training HTMOT using Gibbs sampling"
        },
        {
            "text": "Given that the corpus contains n words, n d of those are part of document d and n w of those are instantiations of the word w. Then, when drawing a topic assignment z j for a word w with timestamp t in document d at a topic depth j we may draw from three possible distributions. These correspond to 1) drawing a node from the document tree (Local DP) 2) drawing a node from the corpus tree (Global DP) 3) create a new node: ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 340,
                    "end": 350,
                    "text": "(Local DP)",
                    "ref_id": null
                }
            ],
            "section": "Sampling topic-word assignments (paths in the trees)"
        },
        {
            "text": "new with probability \u03b2 \u03b2+nw * \u03b1 \u03b1+n d",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sampling topic-word assignments (paths in the trees)"
        },
        {
            "text": "Where, A(k|w) corresponds to the number of words w assigned to topic k or its descendants (corpus tree information). A(k|d) corresponds to the number of words in document d assigned to topic k or its descendants (document tree information). 5 ModBetaPDF corresponds to the probability density function of the modified beta distribution. Finally, the parameters , \u03c6, \u03b2, \u03b1 are priors for the Dirichlet distributions and processes; more details are provided in the parameter section.",
            "cite_spans": [
                {
                    "start": 241,
                    "end": 242,
                    "text": "5",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Sampling topic-word assignments (paths in the trees)"
        },
        {
            "text": "Note that sampling a node from the corpus tree can lead to the creation of a new node in the document tree if that node does not already exist. However, when creating an entirely new node, it is created in both trees (document and corpus).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sampling topic-word assignments (paths in the trees)"
        },
        {
            "text": "Once a topic z j is drawn at depth j, we draw from a Bernoulli with parameter p to decide if we stop or go deeper in the tree:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sampling topic-word assignments (paths in the trees)"
        },
        {
            "text": "Where, P is the weight of the currently selected parent node. C is the weight of all of the children and N is the weight of a potentially new child. These weights are updated each time a word instance is (re)-assigned to a node. V is the vocabulary length and A * (k) is a strict version of A(k) which does not count elements that pass through the node k only those which stops. Finally, \u03b8 1 and \u03b8 2 are the prior for the Bernoulli.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sampling topic-word assignments (paths in the trees)"
        },
        {
            "text": "To summarize, when drawing a topic assignment for a word, we either draw from the document tree (local DP) with probability n d \u03b1+n d or we draw from the corpus tree (global DP) with probability nw \u03b2+nw * ( \u03b1 \u03b1+n d ) or else we create a new topic. Then, we draw from a Bernoulli to decide if we go deeper or not. If we do go deeper, we repeat the same process until we eventually stop. This process is then applied repeatedly too all of the words in the corpus until convergence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sampling topic-word assignments (paths in the trees)"
        },
        {
            "text": "For comparison, the equivalent step for LDA using our implementation of Gibbs sampling would require to sample only from:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sampling topic-word assignments (paths in the trees)"
        },
        {
            "text": "There are a few noticeable differences between HTMOT and nHDP in terms of the training process. First, we do not have a particular initialization protocol. The algorithm just starts with all words not assigned to any topic. Hence, initialization is similar to the rest of the training procedure except that unassignement is not needed. Second, we do not make use of a greedy algorithm to select sub-trees for each document. The tree for each document is created organically as the Gibbs sampler progresses. Our training algorithm is thus simpler and easier to implement.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "How our model differ from nHDP"
        },
        {
            "text": "Our implementation is available on github 6 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "How our model differ from nHDP"
        },
        {
            "text": "To perform our experiments, we crawled 7 62k articles from the Digital Trends 8 archives from 2015 to 2020. This news website is mainly focused on technological news but also contain general news. It includes news about hardware, software and related companies but also space exploration and COVID-19. For all articles, we extracted the text, title, category and timestamp. 6 https://github.com/JudicaelPoumay/HTMOT 7 The crawling was performed using Python with the help of the BeautifulSoup library. 8 https://www.digitaltrends.com/ We discarded articles from the \"deals\" category as they cannot be considered as news articles, but are instead advertisements containing mostly URLs. We also discarded articles containing less than 500 characters. For the remaining articles, we concatenated the title and text.",
            "cite_spans": [
                {
                    "start": 374,
                    "end": 375,
                    "text": "6",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 416,
                    "end": 417,
                    "text": "7",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "Next, we removed common editor's sentences such as \"we strive to help our readers find the best deals on quality products and services, and we choose what we cover carefully and independently..\" and common journalistic phrases such as \"digital trends has reached out to\". Various other pre-processing steps were also applied:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "\u2022 We extracted entities with the following tags using Spacy's NER:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "-Person, Norp, Fac, Org, Gpe, Loc, Product, Event, Work Of Art, Law, Language.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "\u2022 Non-alphabetical characters were discarded.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "\u2022 POS was applied on the rest of the document and only words with the following tags were extracted:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "-ADJ, NOUN, VERB,INTJ, ADV.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "\u2022 Words with the \"PROPN\" POS tag were also extracted to cover some entities that spacy's NER missed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "Finally, infrequent words were removed if they occurred more in one document than in the rest of the corpus.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "A good pre-processing procedure is essential for the interpretability of topics as shown in [32] . We believe interpretability is of primary importance for topic models which is why we extracted entities to help understand topics. The training algorithm will not discriminate between words and entities but the visualization interface does. This means that a topic is no longer displayed as a simple list of words but is instead represented by a list of words and a list of entities. This greatly impacts the interpretability of topics as the entities shows actors in the topic such as personalities and companies.",
            "cite_spans": [
                {
                    "start": 92,
                    "end": 96,
                    "text": "[32]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "We perform our experiments on two datasets, viz: the full dataset of 62K news articles and a subset containing articles only of the previous year (i.e. 2020) with 5k articles. Our aim in doing so was to compare the convergence speed with respect to the dataset size. Additionally, focusing on a specific year provides a more focused and fine-grained analysis for environment scanning. Hence unless otherwise stated, all results presented will be about the subset corresponding to the year 2020.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "Many parameters control the behavior of our model; this section will describe each.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "First, we have the Infinite Dirichlet Trees parameters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 \u03b1 : the rate at which we create new topics in the document trees.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 \u03b2 : the rate at which we create new topics in the corpus tree.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 \u2206 (depth time multipliers) : define for each depth a time coefficient which affects how localized topics are (see section 3.2).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "-For each depth we can set the coefficient to > 1, this disables time for that depth -\u2206 4 defines the time coefficient for depth 4 and deeper depths.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 \u03b8 : how likely we are to create deeper sub topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 \u0398 : the importance of the Bernoulli prior.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "-Precisely, the priors for the Bernoulli distribution are \u03b8 1 = \u03b8 * \u0398 and \u03b8 2 = (1 \u2212 \u03b8) * \u0398.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "Second, we have parameters that regulate the growth of the trees.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 CM (Critical Mass) : the minimum valid topic size; only valid topics are outputted.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 SM (Splitting Mass) : the minimum topic size to create sub-topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "-Both are defined as a percentage of the total number of words in the corpus.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 TTL (Time To Live) : how many pass through the corpus before destroying a non-valid node.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "Third, we have the Dirichlet prior parameters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 \u03c6 : the prior for the topic-word distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 : the prior for the corpus/document-topic distributions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "-Precisely, these distributions refer to the base distributions used by the corpus/document-topic DPs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "Finally, we have training parameters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 The batch size : the number of documents seen per iteration 9 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 Iterations 10 : how many batches we will go through during training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "\u2022 SGI (Stop Growth Iteration) : a point at which node new nodes won't be created.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameters"
        },
        {
            "text": "-Set SGI < Iterations to ensure that the last topic to be created has time to converge. Table 2 defines the value of each parameter for the models we will present in the next section. 9 Its sole purpose is to help graph the various statistics by averaging over the batch. 10 In the code Iterations=Iterations*epochs. Epoch only defines when to save checkpoints. ",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 185,
                    "text": "9",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 272,
                    "end": 274,
                    "text": "10",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [
                {
                    "start": 88,
                    "end": 95,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Parameters"
        },
        {
            "text": "We will now present our results. We will start by providing a statistical analysis of the extracted topics and of the training behaviors of the model trained. Then, we will discuss the results of the Word Intrusion task, its flaws and directions for future topic modelling evaluation methods. Finally, we will examine the various extracted topics qualitatively. First, we will first take a look at topic statistics: time variance, size and coherence (see figure 4 ). While both time variance and coherence look normally distributed (although skewed), size is closer to an exponential distribution. For the topic size, this is the result of both the stick breaking construction of the Dirichlet Processes (DPs) and the fact there Dyads Pearson correlation coefficient Size and coherence 0.25 Coherence and time variance -025 Size and time variance 0.06 Table 3 are many more sub-topics than the larger parent topics. As can be seen in table 3, topic size vs coherence and coherence vs time variance are noticeably correlated. Conversely, size and time variance are not correlated. Hence, topics that are larger or more localized in time tend to be more coherent. For the latter, this is because time localized topics tend to be about a narrower theme with a more specific vocabulary. A similar observation was done in Wang and McCallum [6] .",
            "cite_spans": [
                {
                    "start": 1335,
                    "end": 1338,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 455,
                    "end": 463,
                    "text": "figure 4",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 852,
                    "end": 859,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Results and Discussion"
        },
        {
            "text": "However, for the former this is surprising as we expect smaller more specific sub-topics to be more coherent than larger parent topics which encompass a wider variety of domains. This paradox can be explained by the mathematics of coherence measures. Coherence measures ignore topic-word assignments and use co-occurence for all instances of the words. This implies that if a word is one of the top words (most probable words) of a small and a large topic, coherence measures will use all instances of the word to measure coherence and not just instances that were specifically assigned to that smaller topic. Thus, smaller topics might overshadow by larger ones as many instances used to measure coherence are more likely to belong to the larger topic than the small one. In other words, there will be more situations where that word will occur in the context of the larger topic than smaller one. Using the Umass formula of p(w 1 ,w 2 ) p(w 1 ) * p(w 2 ) , this means the denominator will be disproportionately greater than the numerator. In a similar vein, if a small topic has a very specific vocabulary (rare words) but one of its top words is common this will disproportionately and negatively affect its coherence. For example, let say the topic of \"engine testing\" has the following top 5 words (test,fire, altitude, prototype, engine). While the words (altitude, prototype, engine) are quite specific, the words (test,fire) are much more common.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Interesting Observations"
        },
        {
            "text": "Hence, considering the Umass formula and given that w 1 = \"test\" and w 2 = \"altitude\" then p(w 1 ) >> p(w 2 ) > p(w 1 , w 2 ) and consequently the coherence will drop dramatically. However, subjectively, there is nothing inherently incoherent with this pair of word for the \"engine testing\" topic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Interesting Observations"
        },
        {
            "text": "The foregoing discussion highlights two limitations of the coherence measures. 1) By ignoring the specifics of topicword assignments, larger topics overshadow smaller ones. 2) Topics with which have a mix of rare and common words will inevitably see their coherence score be penalized. These limitations are aggravated by hierarchical models like ours as we extract many small sub-topics which tend to have a mix of common words and more specific rarer words. Hence, coherence measures which are not based directly based on word co-occurrence might be preferable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Interesting Observations"
        },
        {
            "text": "Another interesting observation pertains to the training behavior of the model is also interesting, see figure 5 . The sub-figure (a) presents the number of topics over time. We can see that creation of topics at depth 3 and 4 peaks quickly and then falls; no depth 4 topics remained at the end of the training process. Conversely, the growth of depth 1 and 2 topic were stopped early, i.e. they plateau, based on the SGI parameter (see table 2 ). Sub- figure (b) shows the average KL divergence between sibling topics at different depths. We can see that depth 1 topics are the most distinct. Conversely, depth 3 topics are closer in term of KL divergence; this make sense as they are conceptually related by a parent topic.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 104,
                    "end": 112,
                    "text": "figure 5",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 437,
                    "end": 444,
                    "text": "table 2",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 453,
                    "end": 463,
                    "text": "figure (b)",
                    "ref_id": null
                }
            ],
            "section": "Interesting Observations"
        },
        {
            "text": "Finally, sub-figure (c) shows the evolution in size of depth 1 topics. It can be observed that they converge quickly to a final size in a few thousand iterations but this convergence can be greatly perturbed by the creation of new topics which is another reason for the SGI parameter. We repeated the same experiments on the entire corpus, spanning 2015-2020 (see figure 6 ). We observed that in this setting, fewer topics are created at the first depth. This can be explained by the difference in dataset size. As we can see in the equation 3, the probability of creating a new topic is \u03b2 \u03b2+nw * \u03b1 \u03b1+n d . Hence, as the dataset gets bigger, so does n w (the number of instances of word w in the corpus) and the probability of creating a new topic gets smaller. As can be expected, the convergence speed is slower than when using the 1-year dataset as depicted by the sub-figure (c).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 364,
                    "end": 372,
                    "text": "figure 6",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Interesting Observations"
        },
        {
            "text": "However, the reduction in speed is not proportional to the difference in dataset size, illustrating the efficiency of our novel Gibbs sampling procedure. Specifically, the full 5 year dataset is made up of 62k documents, while the 1 year subset is made up of 5k documents. Thus, assuming a linear convergence rate, one would expect the number of iterations before convergence for the full dataset to be at least 10 times longer than on the 5k subset. However, as can be seen from the sub-figure (c), 3000 iterations were more than enough for the 1-year dataset and at 6000 iterations the 5-year dataset is already flattening. This indicates a sub-linear convergence rate which is highly desirable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Interesting Observations"
        },
        {
            "text": "Considering the overall training time, our Gibbs sampler can go through 100k documents per hour on a single desktop computer 11 . This is comparable to the SVI algorithm used for nHDP (as described by [4] ) in terms of speed : 90k articles per hour 12 [4] . Overall, our model (based on the 1 year dataset) was trained in \u223c 22h but converged after \u223c 14h at 3000 11 Ryzen 5 3600x, 32Go RAM, NVMe SSD 12 No information about hardware is provided, and language environment differs :MATLAB for SVI vs Python for our Gibbs sampler iterations. The algorithmic complexity is linear with respect to the dataset size. However, the depth of the topic trees and thus the growth and regulating parameters for the Infinite Dirichlet trees can greatly impact performance. Nonetheless, as we have seen, convergence rate seems to be sub-linear. Hence, ten times the data does not imply ten times the number of required iterations.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 127,
                    "text": "11",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 201,
                    "end": 204,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 252,
                    "end": 255,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 362,
                    "end": 364,
                    "text": "11",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 399,
                    "end": 401,
                    "text": "12",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Interesting Observations"
        },
        {
            "text": "We applied the Word Intrusion task to evaluate our model. Originally, to asses the quality of a given topic, the Word Intrusion task involves selecting an intruder word from another topic. However, since our model is hierarchical, one could easily bias the Word Intrusion score by choosing words from topics that are most distinct, i.e. non-sibling topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "To avoid this caveat and to prevent the score to be biased in our favor, we only selected intruder words from sibling topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "The reasoning is that we need to evaluate if siblings, in particular, are distinct. For example, when selecting an intruder word for the sub-topic of \"astronomy\", we would choose from one of its siblings such as the \"astronaut\" topic instead of a cousin topic such as the \"Covid-19 vaccines\" topic which would have lead to a much more obvious intruder. (see figure   7 ). The survey used to perform this task was created using Google Forms 13 . The annotators come from a small internet 13 It is also available on github community that centers around sharing and answering surveys 14 . 57 respondents answered the survey over the month of may 2021.",
            "cite_spans": [
                {
                    "start": 487,
                    "end": 489,
                    "text": "13",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 581,
                    "end": 583,
                    "text": "14",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 358,
                    "end": 368,
                    "text": "figure   7",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "Results show 74.83% accuracy in the Word Intrusion task (as defined in section 2.3) on 6 topics in various depths.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "This is on par with performance shown in [23] . However, deeper topics show worst performance: 98.25% for depth 1 topics and 63.13% for depth 2. We have also measured the confidence level of annotators: 92.63% confidence for depth 1 topics while depth 2 topics show 71.13% confidence.",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 45,
                    "text": "[23]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "Our results show that our model outperforms the only other hierarchical model evaluated with Word Intrusion presented in Pujara and Skomoroch [5] as it reports 27% Word Intrusion accuracy. This difference can be explained by the fact that their method requires the manual settings of the size and structure of the extracted topic tree. Specifically, they choose to set the number of topics and sub-topics to be 5. On the other hand, our model creates as many or few topics/sub-topics as it needs which makes it more flexible and adaptable. Moreover, [5] lacks a pre-processing procedure and elements such as numbers end up as topic words. This makes it more difficult for annotators to correctly find the intruder word. Thus, this observation illustrates the benefit of the pre-processing step of our model, which improves the interpretability of the generated topics.",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 145,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 550,
                    "end": 553,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "Comparatively, the performance of non-hierchical models such as LDA, is around 85-90% accuracy; exact figures were not given [23] . For depth 1 topics, our model sees higher performance. Nonetheless, this can be explained by the difference in corpus used as LDA is similar enough to HTMOT for depth 1 topics. Nonetheless, as we have seen for HTMOT the accuracy of the Word Intrusion tasks quickly drops with topic depth.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 129,
                    "text": "[23]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "However, the Word Intrusion task has some noticeable flaws, especially for hierarchical topic model. As we go deeper in the topic tree, sibling topics are increasingly similar. E.g. the sub-topics (astronaut, crew, launch, rocket, space) and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "(launch, rocket, space, satellite, payload) can be understood as being about astronauts and satellites in the wider topic of space. However, we can see that these topics share some important words. In other words, deeper sibling topics essentially share a smaller effective vocabulary. Figure 8 provides another example of this situation where the sub-topic intruder word, i.e. galaxy, is less obvious than that of its parent, i.e. title. Specifically, the deeper we go, the closer the intruder word will be to other words in a given topic. This is problematic because topics can be close (in term of KL divergence) but still distinct. Hence, the Word Intrusion tasks is less reliable for deeper closer topics. We argue this is because it is a topic centric evaluation method and it lacks a global view of the topic tree. Finally, hierarchical topic models can potentially produce hundreds of topics. Thus, creating a survey assessing a significant part of them is not feasible as the survey needs to be small enough for people to be willing to answer it.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 286,
                    "end": 294,
                    "text": "Figure 8",
                    "ref_id": "FIGREF9"
                }
            ],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "The Word Intrusion task has other flaws. It is a binary classification which is a coarse-grained approach and does not indicate the level of confidence of annotators. It would be preferable to ask users to rank words from most likely to least 14 https://www.reddit.com/r/SampleSize/ likely to be an intruder. However, this would be a heavier process. Furthermore, extracted topics can span a wide range of themes. Hence, annotators may not be knowledgeable about all of these topics; thus making it more difficult for them to find the intruder. When running our experiments, one annotator actually remarked on the difficulty of not recognizing some words which hindered their ability to answer the survey. Another study made a similar observation when applying LDA to medical text [33] . Furthermore, this situation is worsened for hierarchical and temporal topic modelling as deeper topics tend to be lexically closer which requires even more knowledge to distinguish.",
            "cite_spans": [
                {
                    "start": 781,
                    "end": 785,
                    "text": "[33]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "In lieu of these issues, we argue that the search for a reliable evaluation method for topic modelling must continue.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "We posit that at least four measures of performance are needed to evaluate topics: coherence, distinguishability, clarity, and stability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "Coherence can be defined as a measure of how much a group of words make sense together and can be evaluated using topic coherence measures; with the aforementioned limitations. Distinguishability can be defined as the ability to distinguish one topic from another; it can be measured using probability distance measures such as KL divergence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "However, as we have discussed earlier, deeper topics will be closer in term of KL divergence but can still be distinct.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "Hence, we need a way to normalize the KL divergence to compare topics across depths. The size of the effective vocabulary of sibling topics might provide a good normalizing factor but more experimentation is needed. Stability can be defined as the variability in topic words through multiple run of the same model on the same corpus; it can be measured by quantifying the word overlap between the same topic through different run of the same model on the same corpus [34] .",
            "cite_spans": [
                {
                    "start": 467,
                    "end": 471,
                    "text": "[34]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "Finally, clarity can be defined as the level of subjective understanding of the theme of a topic and could be measured as the level of annotator agreement in a topic identification task (e.g. inter-annotator scores, such as the Kappa statistics).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "However, this is difficult and still requires a wide range of knowledge from annotators. For now, we believe examining topics manually still remains the best way to understand the performance of a topic model. However, this solution is prone to cherry picking. This is why our data and results are available on github alongside our code so that readers may investigate the results more deeply. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Word Intrusion task and its flaws"
        },
        {
            "text": "Since we modelled topic hierarchy (over time), we deemed it important to provide users with an interface for easy topic exploration and visualization, a snapshot of which is in figure 9 15 . Our interface provides:",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 188,
                    "text": "15",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Examining resulting topics"
        },
        {
            "text": "\u2022 The ability to collapse and expand sub-topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examining resulting topics"
        },
        {
            "text": "\u2022 A representation of the estimated and empirical time distribution (blue and red curves respectively).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examining resulting topics"
        },
        {
            "text": "\u2022 The list of words, entities and the top 5 documents for each topic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examining resulting topics"
        },
        {
            "text": "\u2022 A way to title topics and save them.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examining resulting topics"
        },
        {
            "text": "Now, we will inspect selected topics to illustrate the capabilities of our HTMOT model, i.e. whether it discovers meaningful topics. Specifically, we will focus on the topic of space exploration. Figure 10 presents a depth 1 topic (a) and two of its sub-topics (b,c). We can clearly see that the parent topic (a) is about space, and the two sub-topics are about astronomy (b) and astronauts (c), which are indeed related to the parent topic. This example also illustrates how entities can help interpret and understand these topics. For example, in the astronauts topic, we can see that Bob Behnken, (Doug)",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 196,
                    "end": 205,
                    "text": "Figure 10",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Examining resulting topics"
        },
        {
            "text": "Hurley and SpaceX are important entities. A quick look at the top documents for that topic show that they were the first to fly on a SpaceX rocket. Moreover, in the astronomy topic, Hubble and Spitzer are frequent entities. This is coherent as they are two important low earth orbit telescopes. Other sub-topics of space include satellite launches, rovers, exoplanets, test flights, etc. Now, we will look at the document tree for one document, see figure 12 . This was created by choosing only the topics that were assigned to at least 5% of words in the chosen document. The document in questions is titled \"Astronauts are using VR to train for the Boeing Starliner capsule\". The three main extracted topics are virtual reality applications, space and research. Two children of the topic of space were also assigned to this document: test flights and astronauts. Looking at the title alone, this tree encapsulate well the main themes of this document.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 449,
                    "end": 458,
                    "text": "figure 12",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Examining resulting topics"
        },
        {
            "text": "As we have discussed, we can use HTMOT to perform basic environment scanning in the space industry. We already mentioned the historic launch of Bob Behnken and Doug Hurley. Test flights are another subtopic of space (see figure 13 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 221,
                    "end": 230,
                    "text": "figure 13",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Examining resulting topics"
        },
        {
            "text": "Its sub-topics are interesting as they show the different rockets that are being developed currently: the Boeing's Starliner, SpaceX's Starship, NASA's Space Launch System and the NASA's Orion spacecraft. This shows that the space race has been revived. Asteroid sampling is another sub-topic of space. Here we learn that such an endeavours are becoming more common with two spacecrafts from NASA (Osiris-rex and Lucy) and one from Japan JAXA (Hayabusa2) being mentioned in the top-5 documents and entities. These elements are only a sample of what we learned from the topics ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examining resulting topics"
        },
        {
            "text": "Furthermore, through a sensitivity analysis, we discovered that increasing the parameters \u03b1 and \u03b2 leads to wide and shallow trees. While smaller \u03b1 and \u03b2 lead to narrower and deeper trees. Overall, the same topics were extracted in both cases except that some topics that are depth 2 in the narrow/deep tree become depth 1 in the shallow/wide tree. Hence, a wider/shallower tree would separate astronomy and space exploration topics while a narrower/deeper would group those topics into a larger topic of space. However, it is not always obvious which configuration is better. If we look at the topic of space, we can see that it is mainly about space exploration. Astronomy, while it is related to space, is quite distinct compared to the other sub-topics extracted. Thus, the question arises, at which points two topics are related enough that they should be grouped and at which point are they distinct enough to be separated? We would argue that it is up to the user of the model to decide what kind of structures it would prefer. Narrow/deep trees have fewer topics per branch which might be preferable when trying to understand a corpus while wide/shallow trees require less computation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Moreover, we experimented with the \u2206 parameter. We observed that the width of the time span of the estimated time distribution is proportional the value of \u2206 : dividing \u2206 by two decreases the time span of the topic by two (for \u2206 < 1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Finally, we observed that some depth 3 topics are not well localized in time. All of these have in common that their size is close to the CM parameter. We suspect that topics that are close to this boundary may fluctuate between valid and non-valid state. Since non-valid topics have their time modelling disabled, this might handicap such topic when modelling their time distribution. One solution would be to enable time when the topic size is < CM. This would ensure that barely valid topics won't fluctuate between enabling and disabling time modelling. Nonetheless, it is also important to note that smaller more specific sub-topics do not have to be temporally specific which is another part of the explanation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "We have proposed a new model for topic modelling capable of modelling hierarchy and time jointly as well as a novel implementation of Gibbs sampling for hierarchical topic models. This implementation provides a fast alternative to SVI that makes Gibbs sampling a viable solution for training such complex models. Furthermore, we have discussed the flaws in the Word Intrusion task and coherence measures and pointed the need for a better and multi-factorial evaluation method, especially for hierarchical topic models. Moreover, we have shown how extracting entities can help interpret and understand topics at a deeper level. Finally, we have developed a tool to visualize topics, their hierarchy, temporality as well as their top words, documents and entities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        },
        {
            "text": "To experiment with our model, we performed an environment scanning of the space industry in 2020. Results show recent developments such as the new spacecrafts being developed (SpaceX's starship, Boeing's Starliner, NASA's SLS and NASA's Orion) or the growing number of asteroid sampling missions. Future work includes developing better topic evaluation methods, experimenting with mini-batch stochastic Gibbs sampling to speed up the inference and incorporating topic correlation and/or sentiment analysis for a more complete topic model. Moreover, the beta distribution currently used to model temporality is mono-modal which does not reflect the complex reality of topic evolution. A mixture of beta or a more complex distribution could be used to improve upon the current solution by enabling time for high-level topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Latent dirichlet allocation",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "Y"
                    ],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res",
            "volume": "3",
            "issn": "",
            "pages": "993--1022",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Hierarchical dirichlet processes",
            "authors": [
                {
                    "first": "Y",
                    "middle": [
                        "W"
                    ],
                    "last": "Teh",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Beal",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Blei",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Journal of the American Statistical Association",
            "volume": "101",
            "issn": "",
            "pages": "1566--1581",
            "other_ids": {
                "DOI": [
                    "10.1198/016214506000000302"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Hierarchical topic models and the nested chinese restaurant process",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "L"
                    ],
                    "last": "Griffiths",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Advances in neural information processing systems",
            "volume": "16",
            "issn": "",
            "pages": "17--24",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Nested hierarchical dirichlet processes",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Paisley",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "37",
            "issn": "",
            "pages": "256--270",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2014.2318728"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Large-scale hierarchical topic models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pujara",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Skomoroch",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "NIPS Workshop on Big Learning",
            "volume": "128",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mccallum",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "424--433",
            "other_ids": {
                "DOI": [
                    "10.1145/1150402.1150450"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Proceedings of the 23rd international conference on Machine learning",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Lafferty",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "113--120",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "Nallapati",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ditmore",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Lafferty",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ung",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "520--529",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A non-parametric approach to pair-wise dynamic topic correlation detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Giles",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Eighth IEEE International Conference on Data Mining",
            "volume": "",
            "issn": "",
            "pages": "1031--1036",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Mixtures of hierarchical topics with pachinko allocation",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Mimno",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mccallum",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the 24th international conference on Machine learning",
            "volume": "",
            "issn": "",
            "pages": "633--640",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Leveraging deep learning with lda-based text analytics to detect automobile insurance fraud",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Decision Support Systems",
            "volume": "105",
            "issn": "",
            "pages": "87--95",
            "other_ids": {
                "DOI": [
                    "10.1016/j.dss.2017.11.001"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "End-to-end lda-based automatic weak signal detection in web news",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "El"
                    ],
                    "last": "Akrouchi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Benbrahim",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kassou",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Knowledge-Based Systems",
            "volume": "212",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1016/j.knosys.2020.106650"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Mining the voice of employees: A text mining approach to identifying and analyzing job satisfaction factors from online employee reviews",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Jung",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Suh",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Decision Support Systems",
            "volume": "123",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1016/j.dss.2019.113074"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A text analytics approach for online retailing service improvement: Evidence from twitter",
            "authors": [
                {
                    "first": "N",
                    "middle": [
                        "F"
                    ],
                    "last": "Ibrahim",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Decision Support Systems",
            "volume": "121",
            "issn": "",
            "pages": "37--50",
            "other_ids": {
                "DOI": [
                    "10.1016/j.dss.2019.03.002"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Tracking geographical locations using a geo-aware topic model for analyzing social media data",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lozano",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schreiber",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Brynielsson",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Decision Support Systems",
            "volume": "99",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "The determinants of crowdfunding success: A semantic text analytics approach, Decision Support Systems",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lau",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "91",
            "issn": "",
            "pages": "67--76",
            "other_ids": {
                "DOI": [
                    "10.1016/j.dss.2016.08.001"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Detecting short-term cyclical topic dynamics in the user-generated content and news",
            "authors": [
                {
                    "first": "H.-M",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Decision Support Systems",
            "volume": "70",
            "issn": "",
            "pages": "1--14",
            "other_ids": {
                "DOI": [
                    "10.1016/j.dss.2014.11.006"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "A sticky HDP-HMM with application to speaker diarization",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "B"
                    ],
                    "last": "Fox",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "B"
                    ],
                    "last": "Sudderth",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Willsky",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "The Annals of Applied Statistics",
            "volume": "5",
            "issn": "",
            "pages": "1020--1056",
            "other_ids": {
                "DOI": [
                    "10.1214/10-AOAS395"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Unsupervised terminological ontology learning based on hierarchical topic modeling",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Klabjan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "N"
                    ],
                    "last": "Bless",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE International Conference on Information Reuse and Integration (IRI",
            "volume": "",
            "issn": "",
            "pages": "32--41",
            "other_ids": {
                "DOI": [
                    "10.1109/IRI.2017.18"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Personal research idea recommendation using research trends and a hierarchical topic model",
            "authors": [
                {
                    "first": "H.-C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "T.-T",
                    "middle": [],
                    "last": "Hsu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sari",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Scientometrics",
            "volume": "121",
            "issn": "",
            "pages": "1385--1406",
            "other_ids": {
                "DOI": [
                    "10.1007/s11192-019-03258-x"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Tracking trends: Incorporating term volume into temporal topic models",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "D"
                    ],
                    "last": "Davison",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '11",
            "volume": "",
            "issn": "",
            "pages": "484--492",
            "other_ids": {
                "DOI": [
                    "10.1145/2020408.2020485"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Event detection over twitter social media streams",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "The VLDB Journal",
            "volume": "23",
            "issn": "",
            "pages": "381--400",
            "other_ids": {
                "DOI": [
                    "10.1007/s00778-013-0320-3"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Reading tea leaves: How humans interpret topic models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gerrish",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Boyd-Graber",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Blei",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "22",
            "issn": "",
            "pages": "288--296",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "The dirichlet process, related priors and posterior asymptotics",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Bayesian nonparametrics",
            "volume": "28",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "A survey of topic modeling in text mining",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Alghamdi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Alfalqi",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Journal of Advanced Computer Science and Applications",
            "volume": "6",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.14569/ijacsa.2015.060121"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "An overview of topic modeling methods and tools",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "V"
                    ],
                    "last": "Barde",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Bainwad",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 International Conference on Intelligent Computing and Control Systems (ICICCS)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1109/iccons.2017.8250563"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lau",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Newman",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Baldwin",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "530--539",
            "other_ids": {
                "DOI": [
                    "10.3115/v1/E14-1056"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Automatic evaluation of topic coherence",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Newman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lau",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Grieser",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Baldwin",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT '10",
            "volume": "",
            "issn": "",
            "pages": "100--108",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Variational inference: A review for statisticians",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kucukelbir",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Mcauliffe",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Journal of the American statistical Association",
            "volume": "112",
            "issn": "",
            "pages": "859--877",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Efficient collapsed gibbs sampling for latent dirichlet allocation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Stibor",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of Machine Learning Research, JMLR Workshop and Conference Proceedings",
            "volume": "13",
            "issn": "",
            "pages": "63--78",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "More efficient topic modelling through a noun only approach",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Martin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the Australasian Language Technology Association Workshop",
            "volume": "",
            "issn": "",
            "pages": "111--115",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Evaluating topic model interpretability from a primary care physician perspective",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "W"
                    ],
                    "last": "Arnold",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Speier",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer methods and programs in biomedicine",
            "volume": "124",
            "issn": "",
            "pages": "67--75",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "What is wrong with topic modeling? and how to fix it using search-based software engineering",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Agrawal",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Menzies",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Information and Software Technology",
            "volume": "98",
            "issn": "",
            "pages": "74--88",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "1. A novel method, HTMOT, for hierarchical topic modelling over time 2. A fast Gibbs Sampling implementation based on a novel tree-based data structure 3. Increasing the interpretability of topics through NER extraction 4. Providing a critic of the Word Intrusion task 5. Providing a synthetic review of Dirichlet Processes for hierarchical topic modelling",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Visualisation of the stick breaking construction H can be any distribution but in the context of topic modelling, H is a Dirichlet Distribution. Then, each \u03b8 i corresponds to a topic-word distributions, v is the infinite random vector drawn from the Dirichlet process and each v i corresponds to the probability of each topic. The stick breaking construction defines how the probabilities are distributed over the space of topics S. This construction implies that a small proportion of topics are assigned most of the probability mass. The spread of the probability mass across the topics is controlled by the concentration parameter \u03b1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Example of an IDT with word assignments and time distribution (inside nodes).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Gibbs sampling with Infinite Dirichlet Trees. Repeat for each word of each document until convergence.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Exploratory analysis of the resulting topic time variance, size and coherence.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Training behavior for the 1 year model. The aforementioned results describe the behavior of our model on the subset of 5k news articles for the year 2020.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Training behavior for the 5 year model.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Example of a topic tree with cousins and siblings.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Example of topics with intruder. On the left we have the parent topic and one of its children on the right. The intruder is highlighted in orange. Galaxy is a less obvious intruder than title.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "A snapshot of our output interface",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "shows the topic of astronauts (itself a topic of the space exploration topic, as described above) and its three sub-topics: the return of Chris Cassidy (left), the launch of Bob Behnken and Doug Hurley (center) and the launch of three cosmonauts (right) as interpreted from the words, entities and documents associated with the topic. These topics15 A ready to use interface is available at https://github.com/JudicaelPoumay/HTMOT/tree/mainThe space topic and two of its sub-topics : astronauts and astronomy are depth 3 topics and are well localized in time. The estimated time distribution of the sub-topics matches the timing of the aforementioned events: end of October 2020 (left), end of May 2020 (center) and the start of April 2020 (right).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Examples of depth 3 topics localized in time for the 1y dataset extracted by HTMOT. Thus, by itself our method already provides an efficient way to get insights about a specific domain or corpus.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Example of document topic tree for the document : \"Astronauts are using VR to train for the Boeing Starliner capsule\" .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Test flight and its sub topics and the asteroid sampling topic",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Table 1: Related methods corpora, evaluation methods used and type of topic model.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Parameters used for our model",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This research was sponsored by KPMG Belgium & Luxembourg through the HEC Digital Lab, HEC-Li\u00e8ge, ULi\u00e8ge.The funding source had no involvement at any stage of this research.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        }
    ]
}
{
    "paper_id": "59358d955c647f5ac6600fcc3b90d7adde7c16a8",
    "metadata": {
        "title": "Unsupervised Feature Value Selection Based on Explainability",
        "authors": [
            {
                "first": "Kilho",
                "middle": [],
                "last": "Shin",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Gakushuin University",
                    "location": {
                        "settlement": "Tokyo",
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Kenta",
                "middle": [],
                "last": "Okumoto",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "David",
                "middle": [
                    "Lawrence"
                ],
                "last": "Shepard",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Akira",
                "middle": [],
                "last": "Kusaba",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Kyushu University",
                    "location": {
                        "settlement": "Fukuoka",
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Takako",
                "middle": [],
                "last": "Hashimoto",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chiba University of Commerce",
                    "location": {
                        "settlement": "Chiba",
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Jorge",
                "middle": [],
                "last": "Amari",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Gakushuin University",
                    "location": {
                        "settlement": "Tokyo",
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Keisuke",
                "middle": [],
                "last": "Murota",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Gakushuin University",
                    "location": {
                        "settlement": "Tokyo",
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Junnosuke",
                "middle": [],
                "last": "Takai",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Gakushuin University",
                    "location": {
                        "settlement": "Tokyo",
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Tetsuji",
                "middle": [],
                "last": "Kuboyama",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Gakushuin University",
                    "location": {
                        "settlement": "Tokyo",
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Hiroaki",
                "middle": [],
                "last": "Ohshima",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Hyogo",
                    "location": {
                        "settlement": "Kobe",
                        "country": "Japan"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "The problem of feature selection has been an area of considerable research in machine learning. Feature selection is known to be particularly difficult in unsupervised learning because different subgroups of features can yield useful insights into the same dataset. In other words, many theoretically-right answers may exist for the same problem. Furthermore, designing algorithms for unsupervised feature selection is technically harder than designing algorithms for supervised feature selection because unsupervised feature selection algorithms cannot be guided by class labels. As a result, previous work attempts to discover intrinsic structures of data with heavy computation such as matrix decomposition, and require significant time to find even a single solution. This paper proposes a novel algorithm, named Explainability-based Unsupervised Feature Value Selection (EUFVS), which enables a paradigm shift in feature selection, and solves all of these problems. EUFVS requires only a few tens of milliseconds for datasets with thousands of features and instances, allowing the generation of a large number of possible solutions and select the solution with the best fit. Another important advantage of EUFVS is that it selects feature values instead of features, which can better explain phenomena in data than features. EUFVS enables a paradigm shift in feature selection. This paper explains its theoretical advantage, and also shows its applications in real experiments. In our experiments with labeled datasets, EUFVS found feature value sets that explain labels, and also detected useful relationships between feature value sets not detectable from given class labels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Feature selection is one of the classical problems of machine learning. While many methods have been developed for supervised learning because the problem is relatively easy given the presence of class labels, in unsupervised learning, no such labels are available and the problem is classically hard.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In supervising learning, a target phenomenon is predetermined and is described in a dataset through class labels associated with individual instances. Each instance of the dataset is a vector of values of the same dimensionality, and each dimension is referred to as a feature. The objective of feature selection in supervised learning is to select as few features as possible with high explanatory ability of the target phenomenon. Since it is theoretically evident that fewer features cannot have more explanatory ability, the objective of supervised feature selection is to find an optimal balance to this trade-off between the number of features and the explanatory ability that they bear. The explanatory ability of features, however, can be understood in multiple ways. One typical way is to define it through statistical or information-theoretic indices like correlation coefficients and mutual information. Another is to define it as the potential predictive power of the features, which can be measured by accuracy of prediction by classifiers, when the values of the features and the class labels of instances are input into the classifiers. Different definitions of explanatory ability may lead us to different conclusions to the question of what is the best result of feature selection. In fact, feature selection methods for obtaining high explanatory ability defined through statistical and information-theoretic indices are categorized as filter-type feature selection, while methods belonging to the wrapper-type and embedded-type feature selection aim to realize high predictive performance for particular classifiers. Nevertheless, it is common in any case that the target phenomenon is given and unchanging, and instances' class labels play a critical role in feature selection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In contrast, unsupervised feature selection operates without a definite solution or source of truth, because a target phenomenon is not pre-defined. What counts as the \"right\" result in unsupervised feature selection is unclear. We refer to this as the indefiniteness problem. It is as if we were traveling without knowing our destination, and had to decide if we reached the right destination when we arrived.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "One possible solution to the indefiniteness problem is using clustering to generate pseudo-labels for each instance. Clustering is the process of categorizing instances based on their similarity. For example, when instances are plotted as points in a Euclidean space, similarity between two instances can be defined as the Euclidean distance between the corresponding points. By assuming that clusters define class labels, we can reduce unsupervised feature selection to supervised feature selection. Eventually, some unsupervised feature selection algorithms proposed in the literature first determine pseudo-labels through clustering and then apply supervised feature selection to explain the pseudo-labels [8, 9, 12] .",
            "cite_spans": [
                {
                    "start": 709,
                    "end": 712,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 713,
                    "end": 715,
                    "text": "9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 716,
                    "end": 719,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Using clustering to generate pseudo-class labels, however, does not solve the indefiniteness problem, because diverse definitions of similarity exist, and different definitions lead to different sets of selected features. For example, the L \u221e distance between (x 1 ,...,x n ) and (y 1 ,...,y n ) is identical to max{|x i \u2212 y i | | i = 1,...,n} and evidently yields a totally different similarity measure than the Euclidean distance does. This issue also occurs for other methods known in the literature like methods to select features so as to preserve manifold structures [4, 7, 25] and data-specific structures [21, 22] .",
            "cite_spans": [
                {
                    "start": 573,
                    "end": 576,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 577,
                    "end": 579,
                    "text": "7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 580,
                    "end": 583,
                    "text": "25]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 613,
                    "end": 617,
                    "text": "[21,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 618,
                    "end": 621,
                    "text": "22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In principle, the indefiniteness problem cannot be solved, as shown by the following thought experiment. A DNA array of human beings determines a sequence of genes, and each gene corresponds to an individual biological function. Given a particular biological function, for example, a particular genetic disease, identifying the gene that causes the function is nothing more than supervised feature selection: each gene is a feature. Unsupervised feature selection for a DNA array requires to identify some gene without specifying a particular biological function, and we see that, in theory, a great number of right answers exists.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Thus, in this paper, we accept the indefiniteness problem as an inherent limitation of unsupervised feature selection and propose a new approach to address this issue.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The key is the development of an unsupervised feature selection algorithm with the following properties:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-High time efficiency -Hyperparameters for selecting different features By leveraging such an algorithm, we can run many iterations of the algorithm with different hyperparameter values and can obtain many different answers (sets of features). From these results, we choose the most appropriate answer according to our purpose.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In fact, the main contribution of this paper is to propose a novel algorithm, namely, Explainability-based Unsupervised Feature Value Selection (EUFVS), which is both highly efficient and has hyperparameters for selecting different feature sets. In fact, EUFVS requires only a few tens of milliseconds to obtain a single answer for a dataset with thousands of features and thousands of instances. EUFVS is based on the algorithm presented in [19] . For example, EUFVS selects feature values instead of features. This idea was initially introduced in [19] and yields the advantages of more concrete and more efficient interpretation of selection results. On the other hand, EUFVS has two key differences:",
            "cite_spans": [
                {
                    "start": 442,
                    "end": 446,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 550,
                    "end": 554,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-EUFVS is theoretically based on the novel concept of explainability; -EUFVS takes two hyper-parameters rather than a single hyper-parameter, which change the search space of the algorithm in two independent directions, and as a result, can output a wider range of answers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This paper is organized as follows. Section 2 introduces some mathematical notations and explains some mathematical concepts used in this paper. Section 3 compares supervised and unsupervised feature selection in more detail, and Sect. 4 explains the advantages of feature value selection over feature selection. In Sect. 5, we introduce the concept of explainability and our algorithm, EUFVS. Section 6 is devoted to reporting the results of our experiments to evaluate effectiveness of our algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, a dataset D is a set of instances, and F denotes the entire set of the features that describe D. A feature f \u2208 F is a function f : D \u2192 R( f ), where R( f ) denotes the range of f , which is a finite set of values.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "More formally, we canonically determine a probability space as follows and can view features as random variables. We define a sample space \u03a9 as \u220f f \u2208F R( f ) and a \u03c3-algebra \u03a3 as P(\u03a9), the power set of \u03a9. Then, the dataset D introduces an empirical probability measure p : P(\u03a9) \u2192 [0, 1]: For an element v v v \u2208 \u03a9, p({v v v}) is determined by the ratio of the number of occurrences of v v v in D to the size of D, that is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "Evidently, the triplet (\u03a9, \u03a3, p) determines a probability space, which is also known as an empirical probability space. Furthermore, we identify a feature f with the projection \u03c0 f : \u03a9 \u2192 R( f ), and hence, we can view f as a random variable. Thus, we can view f in two different ways, as a function f : D \u2192 R( f ) and as a function f :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": ". This is natural, however, because the support of a multiset D is a subset of \u03a9.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "Moreover, we identify a finite set of features { f 1 ,..., f n } \u2286 F with the product",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "Under this definition, the probability distribution for a feature set is identical to the joint probability distribution of the random variables (features) involved.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "By viewing feature sets, say S and T , as random variables, we can apply many useful information theoretical indices to S and T . Such indices include information entropy H(S), mutual information I(S; T ), normalized mutual information NMI(S; T ), Bayesian risk Br(S; T ) and complementary Bayesian risk Br(S; T ). In general, these indices are defined as follows for arbitrary random variables X and Y :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "Br(X;Y ) = 1 \u2212 Br(X;Y ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "In the equations above, we assume that the range R(X) and R(Y ) of random variables X and Y are finite, but however, these indices can be defined for more general settings: A random variable X : \u03a9 \u2192 R(X) is a measurable function from a probability space (\u03a9, \u03a3, p) to a measure space (R(X), A,\u00b5), and a Radon-Nikodym",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "f d\u00b5 for any A \u2208 A. Therefore, the information entropy",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "The Shannon information (or information content) of an event observing a value x for a random variable X is defined by \u2212 log 2 Pr[X = x]. It is interpreted as the quantity of information that the event carries, and the information entropy H(X) is the mean across all the possible observables of X. Moreover, when we simultaneously observe",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "quantifies the overlap of Shannon information between the events of X = x and Y = y. The mutual information I(X;Y ) is the mean of the overlap, and therefore, quantifies overall correlational relation between observables of X and Y . In fact, we have:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "is either 0 or 1 for any x and y.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "The The inequality below describes the relationship between I(X;Y ) and Br(X;Y ) [17] :",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 85,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "In the remainder of this paper, we suppose that D is a dataset described by a feature set F , which consists of only categorical features. Furthermore, unless otherwise noted, a feature f is supposed to be a member of F , and a feature set S is a subset of F .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formalization and Notations"
        },
        {
            "text": "The most significant difference between feature selection in supervised learning and feature selection in unsupervised learning lies in whether class labels can be used as effective guides when selecting features. We will first review the literature on supervised feature selection. The literature shows that the following four principles are commonly considered in designing supervised feature selection algorithms:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Feature Selection Vs. Unsupervised Feature Selection"
        },
        {
            "text": "-Maintaining high class relevance; -Reducing the number of selected features; -Reducing the internal redundancy of selected features; -Reducing the information entropy of selected features.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Feature Selection Vs. Unsupervised Feature Selection"
        },
        {
            "text": "In the following illustration, we assume that S is a feature set selected by any feature selection algorithm from the entire feature set F that describes a dataset D. We also let C denote the random variable that yields class labels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Feature Selection Vs. Unsupervised Feature Selection"
        },
        {
            "text": "The class relevance of S represents the extent to which the features of S correlate to class labels and can typically be measured by the mutual information I(S;C). In fact, I(S;C) quantifies the part of the information content H(C) of C that is also born by S. And hence, the class relevance I(S;C) of S cannot exceed the entire information content",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Feature Selection Vs. Unsupervised Feature Selection"
        },
        {
            "text": "On the other hand, the purpose of feature selection is indeed to reduce the number of features to be used for explaining class labels. By its nature, the class relevance of selected features is a monotonically-increasing function with respect to the inclusion relation. In fact, for mutual information, we have I(T ;C) \u2264 I(S;C), if T \u2286 S. Therefore, the most fundamental problem of supervised feature detection can be stated as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Feature Selection Vs. Unsupervised Feature Selection"
        },
        {
            "text": "The fundamental problem of supervised feature selection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Feature Selection Vs. Unsupervised Feature Selection"
        },
        {
            "text": "Eliminate the maximum number of features while minimizing the resulting reduction of class relevance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Feature Selection Vs. Unsupervised Feature Selection"
        },
        {
            "text": "We have two important categories of features to eliminate or not to select.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Feature Selection Vs. Unsupervised Feature Selection"
        },
        {
            "text": "ing class labels. A feature f with small mutual information I( f ;C) is irrelevant. Redundant features, on the other hand, bear content information that is mostly covered by the remaining features. For example, we suppose that S is a set of features selected",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Irrelevant features bear only a small amount of information content useful for explain-"
        },
        {
            "text": "is also sufficiently small.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Irrelevant features bear only a small amount of information content useful for explain-"
        },
        {
            "text": "In the literature, the well-known feature selection algorithm MRMR (Minimum Redundancy and Maximum Relevance) [11] tries to eliminate irrelevant features and redundant features. To determine a feature f to add to the tentative solution S, it intends to evaluate the index of",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 114,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Irrelevant features bear only a small amount of information content useful for explain-"
        },
        {
            "text": "which quantifies a balance between contribution to class relevance and increase of redundancy by adding f to S. Computing b( f , S) is, however, costly, and MRMR uses the following approximation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Irrelevant features bear only a small amount of information content useful for explain-"
        },
        {
            "text": "(",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Irrelevant features bear only a small amount of information content useful for explain-"
        },
        {
            "text": "Algorithm 1 describes MRMR. The asymptotic time complexity of MRMR is estimated by O(k 2 |F ||D|). MRMR is one of the most well-known feature selection algorithms and in fact has been not only intensively studied but also used widely in practice [2, 5, 10, 13, 14, 20, 23, 23, 26] . CFS [6] is another feature selection algorithm that is widely used in practice. Algorithm 1. MRMR [11] . Require: A dataset D described by F \u222a {C}; and k < |F |. Ensure: A feature subset S \u2286 F with |S| = k.",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 249,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 250,
                    "end": 252,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 253,
                    "end": 256,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 257,
                    "end": 260,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 261,
                    "end": 264,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 265,
                    "end": 268,
                    "text": "20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 269,
                    "end": 272,
                    "text": "23,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 273,
                    "end": 276,
                    "text": "23,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 277,
                    "end": 280,
                    "text": "26]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 287,
                    "end": 290,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 381,
                    "end": 385,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Irrelevant features bear only a small amount of information content useful for explain-"
        },
        {
            "text": "It is also based on the same principle as MRMR but uses a different formula than Eq. (2) to evaluate a balance of class relevance and interior redundancy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Irrelevant features bear only a small amount of information content useful for explain-"
        },
        {
            "text": "These algorithms, however, encounter the problem of ignoring feature interaction [24] . We say that two or more features mutually interact when each individual feature has only low class relevance, but the group of these features has high class relevance. The aforementioned algorithms, which only evaluate the information entropy of individual features, cannot detect mutual feature interaction, and are likely to discard interacting features, which can result in a loss of class relevance.",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 85,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Irrelevant features bear only a small amount of information content useful for explain-"
        },
        {
            "text": "Zhao et al. [24] pointed out the importance of this issue and proposed INTER-ACT, the first algorithm that evaluates feature interaction and realizes practical timeefficiency at the same time. INTERACT has led to the development of many algorithms including LCC [15, 16] , which improve INTERACT in both accuracy (when used with classifiers) and time-efficiency. INTERACT and LCC use the complementary Bayesian risk Br(S;C) to measure class relevance of S. Equation (1) describes the correlational relation between Br(S;C) and I(S;C). LCC takes a single hyper-parameter t, which specifies a lower limit of class relevance of the output feature set. Algorithm 2 describes the algorithm of LCC. Also, CWC [15, 18] is equivalent to LCC with t = 1. [15, 16] . ",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 16,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 262,
                    "end": 266,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 267,
                    "end": 270,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 703,
                    "end": 707,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 708,
                    "end": 711,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 745,
                    "end": 749,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 750,
                    "end": 753,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Irrelevant features bear only a small amount of information content useful for explain-"
        },
        {
            "text": "Let j = arg min{ j | j \u2208 [1, i + 1], Br(S \\ F [ j, i];C) \u2265 tBr(F ;C)}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 2. LCC"
        },
        {
            "text": "6: end while 7: Return S.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "Unlike MRMR, INTERACT and LCC evaluate class relevance of S by Br(S;C) without using approximation based on evaluation of individual features. By this, they not only can eliminate irrelevant and redundant features but also can incorporate feature interaction into feature selection. Although computing Br(S;C) is more costly than using the approximation, LCC drastically improves time efficiency by taking advantage of binary search when searching a feature f j to select in Step 4 of Algorithm 2. Due to this, the asymptotic time complexity of LCC is O(|F ||D| log |F |), and its practical time efficiency is significantly high.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "The principle of reducing information entropy is loosely related to the principle of reducing number of features, although they are not equivalent to each other. Explanation of phenomena using fewer features is more understandable for humans, while explanation using features with smaller entropy is more efficient from a information-theoretical point of view. From the left chart of Fig. 1 , we first notice that CWC has selected a reasonably small number of features for all the datasets. For example, while the dataset DOROTHEA originally includes 100,000 features, CWC has selected only 28 features.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 384,
                    "end": 390,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "5:"
        },
        {
            "text": "The middle chart of Fig. 1 shows that the dataset except DOROTHEA and NOVA include many redundant features and only a few irrelevant features. In fact, we see only small losses of the information entropy between the entire features F (blue) and the selected features S (orange). In contrast, CWC eliminates many irrelevant features from DOROTHEA and NOVA: The gaps between the information entropy of F and S are large for these datasets. By the nature of CWC, there is no loss in mutual information. It is interesting to note that the normalized mutual information scores of S are better than F for some of the datasets (the right chart of Fig. 1 ). In particular, the extents of improvement for DOROTHEA and NOVA are significant. Since the normalized mutual information quantifies the extent of the identity between two random variables, the selected features S can explain class labels more confidently than the entire features F . This emphasizes the importance of feature selection in addition to the advantages in model construction and the improved efficiency of post-feature-selection learning.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 20,
                    "end": 26,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 640,
                    "end": 646,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "5:"
        },
        {
            "text": "So far, we have shown the four most important guiding principles in designing supervised feature selection algorithms. Among them, the principle of maintaining high class relevance cannot be used in unsupervised feature selection, because we do not have class labels in unsupervised learning. The issue here lies in the fact that there is a trade-off between maintaining high class relevance and each of the last three principles, and supervised feature selection selects features that realize an appropriate balance between class relevance and the other indices. Without class relevance, the other three indices do not mutually constrain, and it turns out that selecting no features is always the optimal answer. Thus, the most important question for realizing successful unsupervised feature selection is how to find one or more principles that constrain the other three principles without leveraging class labels. We propose an answer for this question in Sect. 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "The advantages of feature value selection over feature selection are two-fold:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Value Selection Vs. Feature Selection"
        },
        {
            "text": "1. Feature value selection is likely to explain the same phenomena using factors with less information content. This means that the explanation is more efficient and more accurate. 2. We sometimes use the term modeling to indicate selecting of a small number of effective explanatory variables from a larger pool of possible variables to explain objective variables. Using feature values as explanatory variables improves the concreteness of the explanation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Value Selection Vs. Feature Selection"
        },
        {
            "text": "For formalization, we first introduce a binarized feature and a binarized feature set. The purpose is to reduce feature value selection to feature selection by converting a feature value into a binary vector using one-hot encoding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "More Efficient Explanation of Phenomena"
        },
        {
            "text": "As explained in Sect. 2, a binarized feature v @ f \u2208 S b is canonically viewed as a random variable defined over the sample space \u220f v @ f \u2208S b Z 2 , where D determines an empirical probability measure. Z 2 denotes {0, 1}. In particular, we can convert a dataset D into a new dataset D b , which consists of the same instances but is described by F b .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "More Efficient Explanation of Phenomena"
        },
        {
            "text": "Thus, we can equate feature value selection on a dataset D to feature selection on D b .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "More Efficient Explanation of Phenomena"
        },
        {
            "text": "The entire features F of the dataset of the next example can explain class labels necessarily and sufficiently. We see that we can also select feature values S \u2282 F b that completely explain the class labels as well. Although the cardinality of S is the same as that of F , H(S) is significantly smaller than H(F ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "More Efficient Explanation of Phenomena"
        },
        {
            "text": "We consider n features f 1 ,..., f n , whose values are arbitrary b-dimensional binary vectors (b-bit-long natural numbers), that is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 1."
        },
        {
            "text": "For an instance x \u2208 D, we determine its class label C(x) by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 1."
        },
        {
            "text": "where L is an odd number that determines the number of classes. To illustrate, we further assume that f i is independent of any other f j , and the associated probability distribution is uniform. Because the class labels of x rely on all of f 1 (x),..., f n (x), the answer of feature selection for this dataset is unique and must be { f 1 ,..., f n }. For the same reason, the answer of feature value selection must be {0 @ f 1 ,...,0 @ f n }. While it is evident that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 1."
        },
        {
            "text": "holds, the information entropy H( f 1 ,. .., f n ) = nb is significantly greater than",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 23,
                    "end": 40,
                    "text": "entropy H( f 1 ,.",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Example 1."
        },
        {
            "text": "when b is not small. This means that, 0 @ f 1 ,...,0 @ f n can explain the class C with the same accuracy but significantly more efficiently than f 1 ,..., f n .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 1."
        },
        {
            "text": "The following general mathematical results justify the result of Example 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 1."
        },
        {
            "text": ", we have the following for arbitrary w w w \u2208 R(S b ) and u u u \u2208 R(T ):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1. For disjoint feature sets S and T in F , H(S, T"
        },
        {
            "text": "Hence, the assertion follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1. For disjoint feature sets S and T in F , H(S, T"
        },
        {
            "text": "The following corollaries to Theorem 1 explain Example 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1. For disjoint feature sets S and T in F , H(S, T"
        },
        {
            "text": "Proof. Theorem 1 implies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Corollary 2. For feature subsets S and T in F , I(S; T ) = I(S b ; T ) holds."
        },
        {
            "text": "By ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I(S; T ) = H(S) + H(T ) \u2212 H(S, T ) = H(S b ) + H(T ) \u2212 H(S b , T ) = I(S b ; T )."
        },
        {
            "text": "Feature value selection explains how features contribute to the determination of class labels more clearly. Even if a feature f is selected through feature selection, not all of the possible values of f necessarily contribute to the determination equally. In particular, only a small portion of values may be useful for explaining class labels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "More Concrete Modeling"
        },
        {
            "text": "For example, an Intrusion Protection System (IPS) tries to detect a small portion of packets generated for malicious purposes out of the large volume of packets that are transmitted in networks. Based on the information of the detected malicious packets, IPS tries to take effective measures to protect a system. To a packet, multiple headers of protocols such as TCP, IP and IEEE 802.x are attached. The information born by these headers is the main source of information for IPS. For example, a TCP header includes a Destination Port field, and a value of this field usually specifies what application will receive this packet and will execute particular functions as a result of the reception. Since malicious attackers target particular vulnerable applications, knowing what potion numbers are correlated to malicious attacks will allow an IPS to take more accurate countermeasures than only knowing that values of the destination port field.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "More Concrete Modeling"
        },
        {
            "text": "The basis of our proposed algorithm was presented in [19] . We now explain our improvements to this algorithm using explainability.",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 57,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Fast Unsupervised Feature Selection"
        },
        {
            "text": "To review, [19] provides a useful basis for developing an efficient unsupervised feature value selection algorithm:",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 15,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Fast Unsupervised Feature Selection"
        },
        {
            "text": "-It leverages the principle that every instance must be explained by at least one selected feature value. This principle constrains the minimization of the three remaining factors (feature value count, internal redundancy, and information entropy, as discussed in Sect. 3) and guarantees that at least one meaningful solution exists. -It incorporates the algorithmic framework of LCC [16, 18] by leveraging binary search, which gives it significantly high time efficiency; -It contains one hyperparameter for excluding feature values below a threshold of information entropy;",
            "cite_spans": [
                {
                    "start": 270,
                    "end": 272,
                    "text": "3)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 384,
                    "end": 388,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 389,
                    "end": 392,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Fast Unsupervised Feature Selection"
        },
        {
            "text": "We build on this algorithm by adding two features:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fast Unsupervised Feature Selection"
        },
        {
            "text": "1. We introduce the concept of explainability as a substitute for the concept of class relevance that plays a central role in supervised feature selection. 2. We add two hyperparameters: the minimum of collective explainability across all selected feature values, and a minimum explainability for each individual feature value.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fast Unsupervised Feature Selection"
        },
        {
            "text": "Supervised learning provides an effective guide for feature selection in the form of class relevance scores. There are several measures of class relevance: MRMR [26] and CFS [6] use mutual information I(S;C) following [3] , while INTERACT [24] and LCC [16] deploy the complementary Bayesian risk Br(S;C). On the other hand, unsupervised feature selection has no class labels to measure the relevance of features to class labels. As a substitute, then, we introduce explainability. In [19] , the support of a set of feature values is defined as follows:",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 165,
                    "text": "[26]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 174,
                    "end": 177,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 218,
                    "end": 221,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 239,
                    "end": 243,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 252,
                    "end": 256,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 484,
                    "end": 488,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Explainability-Based Unsupervised Feature Selection"
        },
        {
            "text": "The support supp D (S) consists of the instances that possess at least one feature value included in S, or, in other words, are explained by the feature values in S.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 3. ([19]). For S \u2286 F b , the support of S is defined by"
        },
        {
            "text": "Having defined explainability, we can formally define \u03be-explainability-based unsupervised feature value selection as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 4. The explainability of S is determined by"
        },
        {
            "text": "Given an unlabeled dataset D described by a feature set F and the lower limit \u03be of explainability, find S \u2286 F b that minimizes H(S) or |S|, or both if possible, subject to the condition of X D (S) \u2265 \u03be.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "As explained in Sect. 3, although the information entropy H(S) and the size |S| are loosely correlated, minimizing one does not necessarily mean minimizing the other. Also, H(S) is important from an explanation efficiency point of view, while |S| affects the understandability of the obtained model by humans. Thus, the aforementioned formalization leaves some ambiguity in terms of objective functions, but however, this does not significantly matter in practice, because finding exact solutions to the problem of \u03be-EUFVS is likely to be computationally impossible. When solving it approximately, the aforementioned loose correlation between H(S) and |S| helps us reach a reasonable balance between them.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "We see how explainability performs as a substitute for class relevance using Fig. 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 77,
                    "end": 83,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "To illustrate, we assume that F b consists of only four values We will start at the top node of F b and will search the node that minimizes H(S) and/or |S| by following directed edges downward. If the search space is the entire Hasse diagram, it is evident that we can stop when we reach the bottom node that represents the empty set / 0. This solution is indeed trivial and meaningless. Thus, we need an appropriate restriction on the search space. For supervised feature selection, the principle of maintaining high class relevance narrows down the search space, because small feature sets can have only low class relevance (for example, I( / 0;C) = 0 holds), and such nodes are eliminated from the search space. As a result, we can reach a non-trivial meaningful node in the search space. The condition that the explainability X D (S) is no smaller than the predetermined threshold \u03be has the same effect. Like mutual information, the explainability index is monotonous with respect to the inclusion relation of feature value sets: if T \u2286 S \u2286 F b , X D (T ) \u2264 X D (S) holds. This means that, if S is out of the search space, that is, X D (S) < \u03be holds, any T \u2286 S is out of the search space as well. In particular, supp D ( / 0) = / 0 and X D ( / 0) = 0 hold. Chart (b) of Fig. 3 depicts this.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1274,
                    "end": 1280,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "The sets T \u2286 F b with X D (T ) < \u03be are displayed in red, and we see that there is more than one minimal selection S in the sense that X D (S) \u2265 \u03be holds but X D (T ) < \u03be holds for arbitrary T S. All of these minimal nodes S comprise the set of candidate solutions to the \u03be-EUFVS problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "As the threshold \u03be for the entire explainability X D (S) increases, the resulting search space becomes narrower, and the border has a higher altitude. In other words, the threshold \u03be moves the border of the search space in the vertical direction.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "In addition to the threshold \u03be, we introduce a different threshold t for individual explainability X D (v) for individual feature value v \u2208 F b . This threshold t constrains the search space so that a node in the space includes only feature values v whose individual explainability X D (v) is not smaller than t. In contrast to the threshold \u03be to collective explainability, this threshold has the effect of moving the border of the search space in the horizontal direction. For example, in Fig. 3(c) , we assume that",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 490,
                    "end": 499,
                    "text": "Fig. 3(c)",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": ". Then, the subgraph displayed in blue is the search space determined in combination with \u03be.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "The introduction of this threshold t can be justified as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "-For example, if F includes a feature f , which yields a unique identifier for each instance of D, the support of any feature value v @ f is a singleton, and hence, its individual explainability is positive but minimum. Evidently, selecting a unique identifier v @ f is of no help for understanding the dataset. Although this example is extreme, in general, a feature value whose support is a very small set of instances lacks generality, and it is not desirable to include it in selection. -As already explained, the threshold t for individual explainability moves the border of the search space in the horizontal direction, while the threshold \u03be for entire explainability does in the vertical direction. By combining these two thresholds, we can move the border of the search space in both the vertical and horizontal directions, and hence, we will have multiplicative flexibility to define the range of solutions to the EUFVS problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "At last of this subsection, we note the relation between X D (v) and H(v). Since v is a feature value, and therefore, is binary as a random variable, we have H(v) . The algorithm presented in [19] takes a threshold to H(v) as a hyperparameter. When we assume that X D (v) \u2264 1 2 , these two definitions of hyperparameters are equivalent to each other.",
            "cite_spans": [
                {
                    "start": 192,
                    "end": 196,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [
                {
                    "start": 158,
                    "end": 162,
                    "text": "H(v)",
                    "ref_id": null
                }
            ],
            "section": "\u03be-Explainability-Based Unsupervised Feature Value Selection (\u03be-EUFVS)."
        },
        {
            "text": "Algorithm 3 describes the algorithm that we propose in this paper. Due to the monotonicity property of X D (S) \u2264 X D (T ) for S \u2286 T , we can take advantage of a binary search to find the next feature value to leave in S (Step 5). As a result, the algorithm is significantly fast as shown in Sect. 6.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Algorithm"
        },
        {
            "text": "Require: An unlabeled dataset D described by F ; a threshold \u03be \u2208 1 2 , 1 ; a threshold t \u2208 0, 1 2 . The time complexity of Algorithm 3 can be estimated as follows: the complexity of computing X D (v i ) and ",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 97,
                    "text": "1 2",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Algorithm 3. Explainability-based Unsupervised Feature Value Selection (EUFVS)."
        },
        {
            "text": "We conducted three experiments with EUFVS. The first assessed the basic performance of EUFVS, while the second and third applied EUFVS to real-world data, specifically tweets and electricity consumption.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Empirical Performance Evaluation"
        },
        {
            "text": "To measure EUFVS's performance compared to other algorithms, we tried it on the 11 datasets from well-known machine learning challenges shown in Fig. 2 . We used well-known datasets to ensure our results were comparable to other algorithms tested on these datasets. Our goal was to discover how accurately and quickly EUFVS could build feature value sets that explained these datasets' labels, with the labels removed.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 145,
                    "end": 151,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Basic Performance Evaluation"
        },
        {
            "text": "In the experiment, we set the threshold on collective explainability to \u03be = 1 and changed the threshold on individual explainability t on each iteration so that the maximum value would not exceed 5% of the total number of instances in each datsets. Figure 4 describes the runtime of Algorithm 3 in milliseconds for three typical datasets: KDD-20% with significantly many instances, DOROTHEA with significantly many features, and GISETTE with both many instances and many features (Fig. 2) . The scores include only the search time. The runtime was under 100 milliseconds for all datasets, except for when we used very small thresholds. The longest run was GISETTE with a threshold of t = 0, which took only 2,500 ms. Selection Performance. Several affinities appear in the results of nine of these eleven datasets. We will describe these using the examples of GISETTE and SYLVA (The left and middle columns of Fig. 5 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 249,
                    "end": 257,
                    "text": "Figure 4",
                    "ref_id": "FIGREF10"
                },
                {
                    "start": 480,
                    "end": 488,
                    "text": "(Fig. 2)",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 910,
                    "end": 916,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF12"
                }
            ],
            "section": "Basic Performance Evaluation"
        },
        {
            "text": "1. All 11 datasets consisted of labelled data, which provided a ground truth to test against. Our goal was to see how well EUFVS could produce feature sets that explained the labels without the labels for guidance, so we removed the labels from the datasets. Even so, it found feature value sets that explain the labels well. In fact, I(S;C) remains close to I(F ;C), until t exceeds a certain limit. This property is significant evidence that our algorithm has an excellent ability to select appropriate feature values, because the dataset labels are a perfect summary of the datasets. 2. When t exceeds the said limit, I(S;C) rapidly decreases. In other words, the different selected feature value sets represent different views of the datasets. The evaluation result of KDD-20% are also interesting. KDD-20% is a dataset of network packet headers gathered by intrusion detection software. Each instance is labelled as either \"normal\" or \"anomalous\". Unlike in the other datasets, the score of H(S) moves around half of H(F ), while I(S;C) remains close to I(F ;C). In fact, KDD-20% and ADA are the only datasets that could exhibit higher NMI(S;C) than NMI(F ;C). With high I(S;C) and low H(S), the feature values selected could have good classification capability when used with a classifier. Also, it is surprising that the number of feature values selected is smaller than 30, when they show the highest score of NMI(S;C). The figure is significantly lower than the 225 feature values that CWC selects for this dataset, and hence, could provide a much more interpretable model. Classification Performance. The experimental results on the (normalized) mutual information scores for the selection results of EUFVS imply that the selection results can accurately predict the dataset's class labels even though the inputs into EUFVS do not include them.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Runtime Performance."
        },
        {
            "text": "To investigate their potential classification performance, we ran experiments with the three typical classifiers: CART, Na\u00efve Bayes (NB) and Supprt Vector Machine (C-SVM) classifiers. When we use the SVM classifier, we project instances to points in higher-dimensional spaces using the RBF kernel.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Runtime Performance."
        },
        {
            "text": "We evaluate classification performance using averaged accuracy scores obtained through five-fold cross-validation. Optimal hyperparameter values are determined using grid search based on five-fold cross-validation scores on training data, executed at each fold execution to compute a single accuracy score. For comparison, we also run the same experiments on feature value sets selected by MRMR [11] . Figure 6 describes the results for the three typical datasets of GISETTE, SYLVA and KDD-20%. From the charts, we can observe the following.",
            "cite_spans": [
                {
                    "start": 395,
                    "end": 399,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 402,
                    "end": 410,
                    "text": "Figure 6",
                    "ref_id": "FIGREF13"
                }
            ],
            "section": "Runtime Performance."
        },
        {
            "text": "-For four deatasets including GISETTE, the accuracy scores for EUFVS encompass a relatively wide range, and some of them are compatible with the results of MRMR. -For six datasets including SYLVA, the accuracy scores for the selection results of EUFVS varies within a relatively narrow range, and are sometimes better and sometimes worse than those for MRMR. Overall, their classification performance appear to be compatible with the selection results of MRMR. -The results for KDD-20% is surprising, since the accuracy scores significantly better than MRMR. To be precise, for all classifiers and all feature value sets selected by MRMR, the accuracy scores fall within the range between 0.5 and 0.6. This will make us conclude that the selection results of MRMR are not useful for the purpose of classification. In contrast, the accuracy scores for EUFVS distributes in a narrow range around the value of 0.9. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Runtime Performance."
        },
        {
            "text": "For the experiments using the 11 labeled datasets, we investigate the differences between the feature value sets selected by EUFVS for different threshold values t. For this purpose, we leverage the distance derived from the Jaccard coefficient and the k-means algorithm, a distance-based clustering algorithm. The Jaccard index between two sets S and T is defined by J(S, T ) = |S\u2229T | |S\u222aT | . It is known that J(S, T ) is positive definite, and hence, d J (S, T ) = 2 \u2212 2J(S, T ) is identical to the Euclid distance in some Euclidean space (reproducing kernel Hilbert space) between the projections of S and T in the space. This is derived from the well-known cosine formula. In particular, when a finite number of sets S 1 ,...,S n are given, they can be projected into a common n-dimensional Euclidean space, and their coordinates in the space is computed as follows. We let J = [J(S i , S j )] i, j be the Gram matrix. Since the Jaccard index is positive definite, the Schur decomposition of J is as follows with \u03bb i \u2265 0:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Phase Transition by Change of Threshold."
        },
        {
            "text": "Here, we let S 1 ,...,S n be the feature value sets selected by EUFVS for n different thresholds. Because we can concretely project them into an n-dimensional space, we can apply the k-means clustering algorithms to the projections in plural times for different cluster count k.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Phase Transition by Change of Threshold."
        },
        {
            "text": "The upper row of Fig. 7 shows the transition of the silhouette coefficient and the Davies Bouldin index as k increases, and based on it, we determine the optimal k for each dataset. Note that a greater silhouette coefficient and a smaller Davies Bouldin index indicate better clustering. At the same time, We also prefer to use the smallest possible k. With these constraints, we determine k = 15 for GISETTE, k = 7 for SYLVA and k = 11 for KDD-20% as optimal cluster counts.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 17,
                    "end": 23,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF14"
                }
            ],
            "section": "Phase Transition by Change of Threshold."
        },
        {
            "text": "With these values of k determined individually for the three datasets, the charts in the lower row of Fig. 7 depict the distributions of plots of the feature value sets in a three-dimensional space after reducing the dimensionality by MDS. For SYLVA and KDD-20%, we see that clustering has performed well, and clusters correspond exactly to consecutive intervals of thresholds. This implies that changing threshold parameter values results in a continuous change of viewpoint over these datasets. For GISETTE, no clear correspondence between clusters and thresholds was found.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 102,
                    "end": 108,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF14"
                }
            ],
            "section": "Phase Transition by Change of Threshold."
        },
        {
            "text": "This experiment shows an example of applying EUFVS to the analysis of real Twitter data. The data used in this experiment include 24,142 tweets posted on March 1, 2020 from 9:00 PM to 9:30 PM. This dataset is a set of tweets sent by users during the time who were tweeting about the COVID-19 pandemic. All of the users tweeted at least once about COVID-19, but not all of the tweets in the data set were about COVID-19. We extracted keywords from each tweets using the MeCab morphological analyzer, resulting in a matrix of 24,142 tweets (instances) \u00d7 49,342 unique words (features) as a feature table.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments on Analysis of Twitter Data"
        },
        {
            "text": "We performed feature selection using EUFVS multiple times with different parameter settings. Then, the UFVS results were compressed into two dimensions by TSNE (Manhattan distance) and were clustered by DBSCAN. Figure 8 shows the clustering results with different parameter settings: (a) \u03be = 1.0,t = 0; (b) \u03be = 0.95,t = 20; (c) \u03be = 0.9,t = 40; and (d) \u03be = 0.95,t = 80. Figure 8(a) shows the clustering result with \u03be = 1.0 and t = 0. After EUFVS, 3154 features remained. Several relatively large clusters can be seen. We have observed some clusters that represent COVID-19-related topics. Figure 8 (b) shows the clustering result with \u03be = 0.95 and t = 20. After EUFVS, 2519 features remained. The set of Tweets was clearly divided into right and left sides. They may represent the characteristics of some Tweets groups. In fact, coronavirusrelated clusters can be observed on the right side. Figure 8 (c) (1173 features, \u03be = 0.90, t = 40) is also divided into two major groups, showing a trend similar to that of Fig. 8(b) . On the other hand, Fig. 8 (d) (486 features, \u03be = 0.95, t = 80) shows a pattern similar to Fig. 8(a) . In addition, the ring-shaped Tweet set can be seen in Figs. 8(a) and (d). The significance of these clusters will be a subject of future research.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 211,
                    "end": 219,
                    "text": "Figure 8",
                    "ref_id": null
                },
                {
                    "start": 369,
                    "end": 380,
                    "text": "Figure 8(a)",
                    "ref_id": null
                },
                {
                    "start": 588,
                    "end": 596,
                    "text": "Figure 8",
                    "ref_id": null
                },
                {
                    "start": 891,
                    "end": 899,
                    "text": "Figure 8",
                    "ref_id": null
                },
                {
                    "start": 1012,
                    "end": 1021,
                    "text": "Fig. 8(b)",
                    "ref_id": null
                },
                {
                    "start": 1043,
                    "end": 1049,
                    "text": "Fig. 8",
                    "ref_id": null
                },
                {
                    "start": 1114,
                    "end": 1123,
                    "text": "Fig. 8(a)",
                    "ref_id": null
                }
            ],
            "section": "Experiments on Analysis of Twitter Data"
        },
        {
            "text": "Changing the parameter settings of EUFVS allows us to view the set of tweets from different perspectives. Our future work will explore these applications.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments on Analysis of Twitter Data"
        },
        {
            "text": "(c) (d) Fig. 8 . Clustering of tweets.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 8,
                    "end": 14,
                    "text": "Fig. 8",
                    "ref_id": null
                }
            ],
            "section": "Experiments on Analysis of Twitter Data"
        },
        {
            "text": "In this experiment, we apply EUFVS to analyzing power consumption in university classrooms. The on/off binary data of electricity consumption in each classroom is recorded in 30-min. time slots over one semester of 15 weeks. By concatenating daily slices (9 am to 6 pm) of this data, we obtain a table where one instance is one classroom on one day, and each feature is a one-hour time slot from 9 am to 6 pm. The data size is 3782 instances and 19 features. We performed a number of feature selection trials using EUFVS with varying parameter settings of \u03be and t. Here, we show two examples. Figures 9 includes 3D visualizations of the data after feature selection by EUFVS. For a dimensionality reduction algorithm, we used UMAP with the Euclidean distance. Figure 9 (a) shows the results of EUFVS with the parameter settings of \u03be = 1.0 and t = 1660. Thirteen features remained in this parameter setting. There are two dense clusters on the right and center of the figure, and a spreading, string-like cluster on the left side of the figure. The present data include classrooms in two buildings. Classrooms in the same building tend to cluster together, which reflects different power consumption patterns in different buildings. Figure 9 (b) shows the results of EUFVS with \u03be = 0.95 and t = 2000. Five features remain in this parameter setting. Two dense clusters similar to those in Fig. 9 (a) can be found, but the spreading, string-like cluster disappears. On the other hand, the structures of the two dense clusters of Fig. 9 (b) are easily visible. Figure 10 shows an enlarged view of the two dense clusters. These clusters also have a string-like structure. Furthermore, the structure is found to be sequentially linked from the first week to the 15th week. The instances are plotted in different colors for each day of the week, and the same color appears periodically, which is a manifestation of that.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 593,
                    "end": 614,
                    "text": "Figures 9 includes 3D",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 760,
                    "end": 768,
                    "text": "Figure 9",
                    "ref_id": "FIGREF15"
                },
                {
                    "start": 1232,
                    "end": 1240,
                    "text": "Figure 9",
                    "ref_id": "FIGREF15"
                },
                {
                    "start": 1387,
                    "end": 1393,
                    "text": "Fig. 9",
                    "ref_id": "FIGREF15"
                },
                {
                    "start": 1526,
                    "end": 1532,
                    "text": "Fig. 9",
                    "ref_id": "FIGREF15"
                },
                {
                    "start": 1557,
                    "end": 1566,
                    "text": "Figure 10",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Experiments on Analysis of Electricity Consumption Data"
        },
        {
            "text": "As described above, by changing the parameter settings, we can obtain the visualizations suitable for the spreading, string-like cluster and suitable for the dense clusters. This is due to the design of EUFVS, which allows us to check a large number of various solutions through lightweight calculations. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments on Analysis of Electricity Consumption Data"
        },
        {
            "text": "The intractability of unsupervised feature selection is caused by the indefiniteness problem: any dataset can contain multiple theoretically-correct solutions, but there is no metric for picking the best one. Instead of attempting to find a metric, EUFVS accepts the indefiniteness problem and works around it by speeding up the feature selection process. Because EUFVS is a fast, tunable algorithm, it empowers a human being to select the best result from many options.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Learning boolean concepts in the presence of many irrelevant features",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Almuallim",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "G"
                    ],
                    "last": "Dietterich",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Artif. Intell",
            "volume": "69",
            "issn": "1-2",
            "pages": "279--305",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "mRMR+ and CFS+ feature selection algorithms for high-dimensional data",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Angulo",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Appl. Intell",
            "volume": "49",
            "issn": "5",
            "pages": "1954--1967",
            "other_ids": {
                "DOI": [
                    "10.1007/s10489-018-1381-1"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Using mutual information for selecting features in supervised neural net learning",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Battiti",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "IEEE Trans. Neural Netw",
            "volume": "5",
            "issn": "4",
            "pages": "537--550",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Unsupervised feature selection for multi-cluster data",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2010)",
            "volume": "",
            "issn": "",
            "pages": "333--342",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Minimum redundancy feature selection from microarray gene expression data",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings of the 2003 IEEE Bioinformatics Conference. CSB2003",
            "volume": "",
            "issn": "",
            "pages": "523--528",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Correlation-based feature selection for discrete and numeric class machine learning",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Hall",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "ICML 2000",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Laplacian score for feature selection",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Niyogi",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Advances in Neural Information Processing Systems (NIPS 2005)",
            "volume": "",
            "issn": "",
            "pages": "507--514",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Clustering-guided sparse structural learning for unsupervised feature selection",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "26",
            "issn": "9",
            "pages": "2138--2150",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Consensus guided unsupervised feature selection",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI 2016)",
            "volume": "",
            "issn": "",
            "pages": "1874--1880",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Metaheuristic approach for an enhanced MRMR filter method for classification using drug response microarray data",
            "authors": [
                {
                    "first": "N",
                    "middle": [
                        "S"
                    ],
                    "last": "Mohamed",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zainudin",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "A"
                    ],
                    "last": "Othman",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Expert Syst. Appl",
            "volume": "90",
            "issn": "",
            "pages": "224--231",
            "other_ids": {
                "DOI": [
                    "10.1016/j.eswa.2017.08.026"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Feature selection based on mutual information: criteria of maxdependency, max-relevance and min-redundancy",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "27",
            "issn": "8",
            "pages": "1226--1238",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Robust unsupervised feature selection",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Qian",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhai",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of 23rd International Joint Conference on Artificial Intelligence (IJCAI 2013)",
            "volume": "",
            "issn": "",
            "pages": "1621--1627",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Minimum redundancy maximum relevance feature selection approach for temporal gene expression data",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Radovic",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ghalwash",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Filipovic",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Obradovic",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "BMC Bioinform",
            "volume": "18",
            "issn": "1",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1186/s12859-016-1423-9"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "A new maximum relevance-minimum multicollinearity (mrmmc) method for feature selection and ranking",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Senawi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Billings",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Pattern Recognit",
            "volume": "67",
            "issn": "",
            "pages": "47--61",
            "other_ids": {
                "DOI": [
                    "10.1016/j.patcog.2017.01.026"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Consistency measures for feature selection: a formal definition, relative sensitivity comparison, and a fast algorithm",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Fernandes",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Miyazaki",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "22nd International Joint Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "1491--1497",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "sCWC/sLCC: highly scalable feature selection algorithms",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kuboyama",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hashimoto",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shepard",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Information",
            "volume": "8",
            "issn": "4",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Consistency-based feature selection",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "13th International Conference on Knowledge-Based and Intelligent Information & Engineering System",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Super-CWC and super-LCC: super fast feature selection algorithms",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kuboyama",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hashimoto",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shepard",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Big Data",
            "volume": "",
            "issn": "",
            "pages": "61--67",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "A fast algorithm for unsupervised feature value selection",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Okumoto",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shepard",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kuboyama",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hashimoto",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ohshima",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "12th International Conference on Agents and Artificial Intelligence (ICAART 2020)",
            "volume": "",
            "issn": "",
            "pages": "203--213",
            "other_ids": {
                "DOI": [
                    "10.5220/0008981702030213"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "An improved maximum relevance and minimum redundancy feature selection algorithm based on normalized mutual information",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "T"
                    ],
                    "last": "Vinh",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "D"
                    ],
                    "last": "Thang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "K"
                    ],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "IEEE/IPSJ International Symposium on Applications and the Internet",
            "authors": [],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1109/SAINT.2010.50"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Unsupervised feature selection on networks: a generative view",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI 2016)",
            "volume": "",
            "issn": "",
            "pages": "2215--2221",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Multi-view unsupervised feature selection by cross-diffused matrix alignment",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of 2017 International Joint Conference on Neural Networks (IJCNN 2017)",
            "volume": "",
            "issn": "",
            "pages": "494--501",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Gene selection algorithm by combining reliefF and mRMR",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "BCM Genomics",
            "volume": "9",
            "issn": "2",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Searching for interacting features",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of International Joint Conference on Artificial Intelligence (IJCAI 2007)",
            "volume": "",
            "issn": "",
            "pages": "1156--1161",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Spectral feature selection for supervised and unsupervised learning",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the 24th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1151--1157",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Maximum relevance and minimum redundancy feature selection methods for a marketing machine learning platform",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Anand",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Springer Nature Switzerland AG 2021 A. P. Rocha et al. (Eds.): ICAART 2020, LNAI 12613, pp. 421-444, 2021. https://doi.org/10.1007/978-3-030-71158-0_20",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "normalized mutual information NMI(X;Y ), on the other hand, is defined by the harmonic mean of I(X;Y ) H(X) and I(X;Y ) H(Y ) , and hence, takes values in [0, 1]. We have: -NMI(X;Y ) = 0, if, and only if, X is independent of Y ; -NMI(X;Y ) = 1, if, and only if, X and Y are isomorphic as random variables. Bayesian risk Br(X;Y ) also quantifies correlation of X to Y , which takes values in 0, |R(Y )|\u22121 |R(Y )| . In contrast to mutual information, a smaller value of Br(X;Y ) indicates a tighter correlation. This is why we use Br(X;Y ) = 1 \u2212 Br(X;Y ) in some cases. In particular, we have: -Br(X;Y ) = 0, if, and only if, Y is totally dependent on X; -Br(X;Y ) = 1, if, and only if, Y is totally dependent on X.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Require: A dataset D described by a feature set F \u222a {C}; and a threshold t \u2208 [0, 1]. Ensure: A minimal feature subset S \u2286 F with Br(S;C) \u2265 tBr(F ;C). 1: Number the features of F so that f 1 ,..., f |F | are in a decreasing order of NMI( f i ;C). 2: Let S = F and i = n |F | . 3: while i \u2265 1 do 4:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Before and after of supervised feature selection.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Eleven datasets used in our experiment.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "shows the results of feature selection by CWC when applied to the 11 datasets described inFig. 2and below. These datasets are relatively large and encompass a wide range in terms of the numbers of features and instances. They are taken from the literature: five from NIPS 2003 Feature Selection Challenge, five from WCCI 2006 Performance Prediction Challenge, and one from KDD-Cup Challenge. Since our interest in this paper lies in the selection of categorical features, the continuous features included in the datasets are categorized into five equally-long intervals before feature selection. The instances of all of the datasets are annotated with binary labels.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "the monotonicity properties of information entropy and mutual information, if S \u2282 S b , we have H(S ) \u2264 H(S) and I(S ; T ) \u2264 I(S; T ). Example 1 is the case where H(S ) H(S) holds, while I(S ; T ) = I(S; T ) holds, for S = { f 1 ,..., f n }, S = {0 @ f 1 ,...,0 @ f n } and T = {C}.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Search space of EUFVS. The chart (a) depicts the Hasse diagram of F b , which is a directed graph (V H , E H ) such that V H is the power set of F b , and (S, T ) \u2208 V H \u00d7 V H is in E H , if, and only if, S \u2283 T and |S| \u2212 |T | = 1 hold. The height of a plot of S \u2286 F b represents the magnitude of H(S).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Number the feature values of S so that S = {v 1 ,...,v |S| } and X D (v i ) \u2265 X D (v j ) for i < j. 3: Let l = 0 and S = S. 4: while l < |S| do 5: Let k = max{ j | X D (S \\ S[l + 1, j]) \u2265 \u03be, j = l,...,|S|} by binary search. 6: Let S = S \\ S[l + 1, k] and l = k + 1. 7: end while 8: return S.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "\u03be can be investigated in O(|D|)-time, and the average complexity to execute the while loop is estimated by O((log 2 |F b |) 2 \u00b7 |D|).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Runtime in milliseconds for different t values (x-axis).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "As t increases, I(S;C) and H(S) synchronously decrease. This implies that our algorithm eliminates non-redundant and relevant feature values after it has eliminated all the redundant feature values. 4. H(S) remains very close to its upper bound of H(F ) (the orange line) until t reaches the said limit. By contrast, the number of feature values selected decreases rapidly immediately when t increases. This may imply that an overwhelming majority of feature values v with small H(v) are redundant. 5. The number of the selected feature values approaches the number of the features selected by CWC (the green line). This implies that approximately one value for each feature selected by CWC is truly relevant to class labels.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Comparison in H(S), I(S;C), NMI(C; S) and |S| for typical three datasets.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Comparison in accuracy by CART, NB and SVM classifiers for typical three datasets.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Clustering of selected feature sets based on the jaccard index.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "UMAP 3D view.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Enlarged views of two dense clusters inFig.9.",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements. This work was partially supported by the Grant-in-Aid for Scientific Research (JSPS KAKENHI Grant Numbers 16K12491 and 17H00762) from the Japan Society for the Promotion of Science.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}
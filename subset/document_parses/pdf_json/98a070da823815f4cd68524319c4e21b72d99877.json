{
    "paper_id": "98a070da823815f4cd68524319c4e21b72d99877",
    "metadata": {
        "title": "Bringing A Robot Simulator to the SCAMP Vision System",
        "authors": [
            {
                "first": "Yanan",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {},
                "email": "yanan.liu@bristol.ac.uk"
            },
            {
                "first": "Jianing",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {},
                "email": "jianing.chen@manchester.ac.uk"
            },
            {
                "first": "Laurie",
                "middle": [],
                "last": "Bose",
                "suffix": "",
                "affiliation": {},
                "email": "lauriebose@gmail.com"
            },
            {
                "first": "Piotr",
                "middle": [],
                "last": "Dudek",
                "suffix": "",
                "affiliation": {},
                "email": "p.dudek@manchester.ac.uk"
            },
            {
                "first": "Walterio",
                "middle": [],
                "last": "Mayol-Cuevas",
                "suffix": "",
                "affiliation": {},
                "email": "walterio.mayol-cuevas@bristol.ac.uk"
            }
        ]
    },
    "abstract": [
        {
            "text": "This work develops and demonstrates the integration of the SCAMP-5d vision system into the CoppeliaSim robot simulator, creating a semi-simulated environment. By configuring a camera in the simulator and setting up communication with the SCAMP python host through remote API, sensor images from the simulator can be transferred to the SCAMP vision sensor, where on sensor image processing such as CNN inference can be performed. SCAMP output is then fed back into CoppeliaSim. This proposed platform integration enables rapid prototyping validations of SCAMP algorithms for robotic systems. We demonstrate a car localisation and tracking task using this proposed semi-simulated platform, with a CNN inference on SCAMP to command the motion of a robot. We made this platform available online.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The SCAMP visual system is a smart sensor supporting in-sensor processing. The direct analogue electronic current signal process and calculation on the pixel processor array (PPA) enable low-power consumption, parallel, and efficient computing without external hardware. With these features, it is increasingly being integrated with robots for various applications [1] - [5] . However, it is often time-consuming and difficult to prototype ideas using real robotic platforms, especially during the COVID-19 pandemic period. To improve experimental flexibility, we integrate a comprehensive robot simulator CoppeliaSim [6] and SCAMP python host to test and validate ideas rapidly. CoppeliaSim is a robot environment simulator where each agent can be controlled via remote API [7] . Its simulated sensor readings can be transferred to other independent platforms written in python, C/C++, or 1 . Robot simulation environment. The SCAMP is 'mounted' under the drone facing the ground. Real-time image can be seen from the vision sensor with a resolution of 256\u00d7256 which is set the same as that of a SCAMP. The floor is designed with disrupting texture for CNN localisation. Note that the current version of SCAMP-5d only supports gray-scale images; hence CNN inference on SCAMP only relies on gray texture from the scene. the proposed semi-simulation platform, we implemented a convolutional neural network (CNN) [8] , [9] on the SCAMP processing the imported camera images for localisation purpose from the robot simulator where the camera is mounted under a drone.",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 368,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 371,
                    "end": 374,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 618,
                    "end": 621,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 775,
                    "end": 778,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1411,
                    "end": 1414,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1417,
                    "end": 1420,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 890,
                    "end": 891,
                    "text": "1",
                    "ref_id": null
                }
            ],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "A SCAMP vision system is connected to a computer via USB through scamp5d interface library. And SCAMP host [10] 1 is a GUI executed on the computer to interact with the vision system and visualise the data sent back from the device. This work develops a scamp python module for the python GUI based on previous C/C++ host libraries, to make the SCAMP host easier to connect to third party software. With this method, the host visualisation and remote API can be codesigned on the scamp python module. The configuration of remote API on the CoppeliaSim can be found from 2 .",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 111,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "II. SCAMP PYTHON HOST AND THE SEMI-SIMULATED PLATFORM"
        },
        {
            "text": "To evaluate the CNN performance on SCAMP working with a robot-related applications, this paper developed a semisimulated platform in which a real SCAMP-5d visual system collaborates with Coppeliasim robotics simulation environment through remote API 3 . That means environment setups and sensor image collection is performed in the simulator while the real SCAMP is in charge of CNN inference with sensor images from the simulator and outputting useful information to the simulator (Fig. 2) .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 482,
                    "end": 490,
                    "text": "(Fig. 2)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "II. SCAMP PYTHON HOST AND THE SEMI-SIMULATED PLATFORM"
        },
        {
            "text": "This work trained a binarized CNN which introduces batch norm and use both binary weights and activations to reduce the error caused by continuous analogue electronic current computing [11] . A car localisation and tracking task are exploited based on the proposed neural network structure. SCAMP visual system is suitable for mobile robot platform due to its low power consumption, lightweight and in-sensor machine vision computing ability without using or communicating with external hardware. This experiment takes advantage of the SCAMP-5d visual system to localise a mobile vehicle moving in the 2D simulated environment, using this location information to guide a drone to track the vehicle. Fig. 4 . Selected images for training. Although from the simulation environment the localisation seems simple, the inputs for SCAMP is the grayscale images with a low resolution of 64\u00d764, which is a challenging task for SCAMP localisation. ",
            "cite_spans": [
                {
                    "start": 185,
                    "end": 189,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 699,
                    "end": 705,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "III. EXPERIMENTS ON PLATFORM A. binarized CNN for car localisation from a drone"
        },
        {
            "text": "The localisation training dataset is collected from the simulation environment, by placing the robot at a series of positions within the map, with different orientations under the view-field of a camera. An image is recorded once there is a change in the robot pose or camera pose. With this method, a dataset of 104,000 training images and 19,200 testing images was collected. To simulate the vibration and tilting of a flying drone, random noise is introduced into the camera pose, and this can also be regarded as a type of data augmentation that benefits CNN training. This data collection process is less-time consuming and cheap to validate the effectiveness of a proposed CNN on SCAMP-5d before performing testing in the real world.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "1) Localisation Dataset:"
        },
        {
            "text": "2) CNN inference for robot localisation: This localisation task is regarded as a classification task by splitting the x and the y axis into eight labels, giving 64 possible positions to localise the robot. As for the CNN training, the training loss for backpropagation is the loss summation of x and y as shown in Fig. 3 . The final testing accuracy for localisation is around 93%, which conservatively, only counts the correct predictions of the CNN. In a practical situation, a close prediction to the ground truth should still allow tracking to proceed without significant difficulty. Fig. 7 visualises a sequence of 8 frames with CNN inference on SCAMP where the prediction possibility distribution can be seen from light blue curve along x and y axis, the final prediction is obtained with two highest possibilities from two curves along axes. Fig. 6 . Neuron activations after the first fully connected layer between SCAMP and PyTorch simulation. Due to binary activations and bit counting, we mitigate the error issue caused by using AREG for signal processing, hence getting a similar activation value from simulation to a real SCAMP. The complete localisation and tracking are processed frame by frame, and the instructions to pilot the drone is generated using the PID control to minimise the distance between the robot ground truth position and the predicted robot position.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 314,
                    "end": 320,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 588,
                    "end": 594,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 849,
                    "end": 855,
                    "text": "Fig. 6",
                    "ref_id": null
                }
            ],
            "section": "1) Localisation Dataset:"
        },
        {
            "text": "To demonstrate the CNN inference on SCAMP in terms of accuracy, Fig. 5 and Fig. 6 compares the binary activation layer and first fully-connected layer between PyTorch and SCAMP, which shows a high similarity between the PyTorch simulation and SCAMP hardware implementation. The noise mainly results from the analogue signal information process, and it is inevitable due to the current method of manufacturing such hardware. However, this phenomenon could be effectively alleviated in the next generation of the SCAMP vision systems. To further validate the performance of CNN localisation on SCAMP, a chaotic trajectory is pre-set in the simulator for the robot to move along. The drone trajectory is plotted with guidance from SCAMP CNN inference. Fig. 8 shows the comparison among the ground truth robot course, CNN on PyTorch guided drone course and CNN on SCAMP guided drone course. Finally, we make our SCAMP python host, CoppeliaSim model and its configuration available online: https:// github.com/ yananliusdu/ scamp5d interface.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 64,
                    "end": 70,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 75,
                    "end": 81,
                    "text": "Fig. 6",
                    "ref_id": null
                },
                {
                    "start": 749,
                    "end": 755,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "1) Localisation Dataset:"
        },
        {
            "text": "In this work, we proposed a semi-simulated platform where a real SCAMP interacts with the robot simulator via remote API for a rapid prototype validation. The SCAMP CNN inference results with the simulated sensor readings can instruct the motion of an agent in the proposed platform. More applications related to the SCAMP and robots integration can be explored based on the developed platform.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. CONCLUSION"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Proximity estimation using vision features computed on sensor",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Carey",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dudek",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA)",
            "volume": "",
            "issn": "",
            "pages": "2689--2695",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Agile reactive navigation for a non-holonomic mobile robot using a pixel processor array",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bose",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Greatwood",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Richardson",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Carey",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dudek",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Mayol-Cuevas",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IET Image Processing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "https:/ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12158"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Tracking control of a uav with a parallel visual processor",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Greatwood",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bose",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Richardson",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Mayol-Cuevas",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Carey",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dudek",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "volume": "",
            "issn": "",
            "pages": "4248--4254",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Visual odometry using pixel processor arrays for unmanned aerial systems in gps denied environments",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mcconville",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bose",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Clarke",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Mayol-Cuevas",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Greatwood",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Carey",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dudek",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Richardson",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Frontiers in Robotics and AI",
            "volume": "7",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Weighted node mapping and localisation on a pixel processor array",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Castillo-Elizalde",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bose",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Mayol-Cuevas",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "V-rep: A versatile and scalable robot simulation framework",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Rohmer",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "P"
                    ],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Freese",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "2013 IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "volume": "",
            "issn": "",
            "pages": "1321--1326",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Pyrep: Bringing v-rep to deep robot learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "James",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Freese",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Davison",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.11176"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "High-speed light-weight cnn inference via strided convolutions on a pixel processor array",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bose",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Carey",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dudek",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Mayol-Cuevas",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "The 31st British Machine Vision Conference (BMVC 2020",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Fully embedding fast convolutional networks on pixel processor arrays",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bose",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dudek",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Carey",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "W"
                    ],
                    "last": "Mayol-Cuevas",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "European Conference on Computer Vision -ECCV 2020",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Scamp5d vision system and development framework",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Carey",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dudek",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 12th International Conference on Distributed Smart Cameras",
            "volume": "",
            "issn": "",
            "pages": "1--2",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Near-sensor and in-sensor computing",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chai",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature Electronics",
            "volume": "3",
            "issn": "11",
            "pages": "664--671",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Semi-simulated platform by integrating the SCAMP with Coppeliasim Robot Simulator. This platform takes advantage of the SCAMP parallel computation ability and the rich simulation scenes in the simulator, where applications can be exploited and validated virtually.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The CNN architecture for car localisation. A shared convolutional layer is used for object 2D localisation on the SCAMP, where label x and y using identical convolutional layer but different fully-connected layers.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Binary activations comparison after image convolution on SCAMP and PyTorch simulation. White and yellow dots represent '1's while the dark area is '-1's. This shows the similarity of binary activations after the first convolutional layer.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Robot localisation result visualisation using SCAMP CNN inference results along with consecutive frames. The light blue prediction curves are plotted along x and y axis with possibilities for each label. With the largest possibilities for each axis (red straight line), the final localisation prediction (pink circle) is plotted. The full experimental video for robot localisation and tracking can be seen from https:// youtu.be/ semthdfXH5A.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Tracking trajectories with a drone. There are three paths: the pre-set robot trajectory as the groundtruth, drone tracking trajectory with CNN on PyTorch guidance, and drone tracking trajectory with guidance from SCAMP CNN inference.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Matlab through several communication protocols. Based on This work is accepted by ICRA2021 workshop On and Near-sensor Vision Processing, from Photons to Applications (ONSVP). This work was supported by UK EPSRC EP/M019454/1, EP/M019284/1, EPSRC Centre for Doctoral Training in Future Autonomous and Robotic Systems: FARSCOPE and China Scholarship Council (No. 201700260083).",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
{
    "paper_id": "d54345e23d283e9c1379d0c9d37f933087bf6917",
    "metadata": {
        "title": "IndoorCare: Low-Cost Elderly Activity Monitoring System through Image Processing",
        "authors": [
            {
                "first": "Daniel",
                "middle": [],
                "last": "Fuentes",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Polytechnic Institute of Leiria",
                    "location": {
                        "postCode": "2411-901",
                        "settlement": "Leiria",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Lu\u00eds",
                "middle": [],
                "last": "Correia",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Polytechnic Institute of Leiria",
                    "location": {
                        "postCode": "2411-901",
                        "settlement": "Leiria",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Nuno",
                "middle": [],
                "last": "Costa",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Polytechnic Institute of Leiria",
                    "location": {
                        "postCode": "2411-901",
                        "settlement": "Leiria",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Ars\u00e9nio",
                "middle": [],
                "last": "Reis",
                "suffix": "",
                "affiliation": {
                    "laboratory": "INESC TEC, University of Tr\u00e1s-os-Montes e Alto Douro",
                    "institution": "",
                    "location": {
                        "addrLine": "Quinta de Prados",
                        "postCode": "5001-801",
                        "settlement": "Vila Real",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Jos\u00e9",
                "middle": [],
                "last": "Ribeiro",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Polytechnic Institute of Leiria",
                    "location": {
                        "postCode": "2411-901",
                        "settlement": "Leiria",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Carlos",
                "middle": [],
                "last": "Rabad\u00e3o",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Polytechnic Institute of Leiria",
                    "location": {
                        "postCode": "2411-901",
                        "settlement": "Leiria",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Barroso",
                "suffix": "",
                "affiliation": {
                    "laboratory": "INESC TEC, University of Tr\u00e1s-os-Montes e Alto Douro",
                    "institution": "",
                    "location": {
                        "addrLine": "Quinta de Prados",
                        "postCode": "5001-801",
                        "settlement": "Vila Real",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Ant\u00f3nio",
                "middle": [],
                "last": "Pereira",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Polytechnic Institute of Leiria",
                    "location": {
                        "postCode": "2411-901",
                        "settlement": "Leiria",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "",
                "middle": [],
                "last": "Munoz-Organero",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Citation: Fuentes, D.; Correia, L.; Costa, N.; Reis, A.; Ribeiro, J.; Rabad\u00e3o, C.; Barroso, J.; Pereira, A.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Portugal has an aging population with a tendency to increase [1] , particularly, in rural areas, where the migration of the active population to large urban centers, in search of better job opportunities and quality of life, has led the remaining resident population in rural environments, mostly elderly, suffering social exclusion, and often ending up living in isolation.",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 64,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction and Motivation"
        },
        {
            "text": "As the economic factor is sometimes a barrier for technology adoption, especially by the elderly population with limited financial resources [2] , cost-effective solutions to monitor the elderly and the isolated population in general have been researched for a while. The Ambient Assisted Living Joint Programme [3] is a European initiative and is an example of the needs that exist in support for the elderly. Since then, more and more research has been carried out [4] , and every year new solutions appear make their contribution.",
            "cite_spans": [
                {
                    "start": 141,
                    "end": 144,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 312,
                    "end": 315,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 467,
                    "end": 470,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction and Motivation"
        },
        {
            "text": "In this article, a low-cost solution for monitoring the movement of elderly people living alone in their homes, based on the AAL paradigm, is presented with the name IndoorCare. The system is based on a distributed architecture, where low-cost IoT devices acquire and process video images to export non-personal/private data through a gateway to a server. Then, the server aggregates all this information and makes it available, in a simple way, to the user or caregiver. This proposed solution is based in technologies increasingly used in the area of smart everything [5] and provides a non-invasive monitoring system to a caregiver or a family member. The IndoorCare system records the person's physical movements over time and allows that information to be analyzed later by the caregiver to assess the person's health. One other advantage of a solution such as this is to allow the detection of any anomalous situations, such as emergencies or possible falls, in time.",
            "cite_spans": [
                {
                    "start": 570,
                    "end": 573,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction and Motivation"
        },
        {
            "text": "The paper is organized as follows: Section 2 presents an overview of the related work; Section 3 describes the solution's architecture; Section 4 presents the prototype that was developed to validate the concept; Section 5 presents the system's evaluation and optimizations; and Section 6 presents the work's general conclusions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction and Motivation"
        },
        {
            "text": "There are several solutions and projects focused on the detection, spatial location, and monitoring of the daily activity of people in indoor environments. In this section, we discuss some of the founding technologies for this type of solution, namely image analysis and infrared sensors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "There are several articles that analyze the AAL solutions that have been appearing in recent years [6, 7] and some that expose the challenges that must be resolved in the future [8] , especially in a post COVID-19 era [9] . There are also recent studies that focus on the analyses and comparison, in various ways, of the created applications and architectures of recent AAL solutions, exposing the trends in the solutions' implementation that most works have followed [10] . On a more practical level, there are several interesting implementations that have used various methods to monitor and interact with older people; these solutions typically use IoT devices to perform this monitoring [11] , whether using sensors attached to the person [12] , monitoring furniture [13] , using video systems that analyze in real time what is happening [14, 15] , using face recognition to detect people and who they are [16] , or even through the analyses of the sound [17] . The use of computer vision together with artificial intelligence is an increasingly common practice, using the best that these technologies allow to better monitor the elderly in their homes [18] . In addition, there is a growing need to transfer the information processing from data centers to the periphery of the systems, namely to the source where the data is acquired, to reduce the traffic sent by the equipment at the edge. This concept is called fog computing, and in addition to being a great advantage for computer vision solutions, it is also already being used in monitoring solutions for the elderly that use wearable devices, among others [19] .",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 102,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 103,
                    "end": 105,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 178,
                    "end": 181,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 218,
                    "end": 221,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 468,
                    "end": 472,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 691,
                    "end": 695,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 743,
                    "end": 747,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 771,
                    "end": 775,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 842,
                    "end": 846,
                    "text": "[14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 847,
                    "end": 850,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 910,
                    "end": 914,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 959,
                    "end": 963,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1157,
                    "end": 1161,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1619,
                    "end": 1623,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The extraction of information based on image analysis is a relatively recent topic that has enabled the development of technologies that allow the automatization of the information gathering process. Image recognition solutions, such as the Open-Source Computer Vision Library (OpenCV) [20] , combined with ubiquitous computing, using microcomputers, such as Raspberry Pi [21] or Arduino [22] , allow the creation of environments that can act intelligently, according to the information extracted and collected.",
            "cite_spans": [
                {
                    "start": 286,
                    "end": 290,
                    "text": "[20]",
                    "ref_id": null
                },
                {
                    "start": 372,
                    "end": 376,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 388,
                    "end": 392,
                    "text": "[22]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The OpenCV software library uses image analysis to recognize the various types of information in an image, such as: detection of hand gestures, as set out in [23] ; human facial recognition [24] , where in this particular paper [25] the authors implemented a prototype using the Java CV library [26] , which analyzes the camera stream from a IP security camera and detects human presence; and recognition and extraction of vehicle registration information [27] or surveillance security systems [28] , in which the authors coupled common web cameras to devices of small processing power, e.g., Raspberry Pi, which acquires the image from the camera and uses a cloud platform to process the image for movement detection.",
            "cite_spans": [
                {
                    "start": 158,
                    "end": 162,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 190,
                    "end": 194,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 228,
                    "end": 232,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 295,
                    "end": 299,
                    "text": "[26]",
                    "ref_id": null
                },
                {
                    "start": 456,
                    "end": 460,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 494,
                    "end": 498,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The research in the area of human monitoring and human location has resulted in several interesting works, such as the one presented in [29] , where the authors propose a system that uses two modes of monitoring, inside the residence (indoor) and outside the residence (outdoor). For the detection of the indoor position, the users must wear RFID (Radio-Frequency Identification) tags, which are detected and read whenever the user enters a new division, similarly to the RFID tagging systems used in logistic solutions to track items. For the detection of the outdoor position, the user must wear a GPS (Global Positioning System) device for position tracking. The GPS mode (outdoor) is activated automatically whenever the user leaves the room three meters away.",
            "cite_spans": [
                {
                    "start": 136,
                    "end": 140,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In ref. [30] , the authors used infrared (IR) sensors to calculate the number of people inside a building, installing sensors on the doors' tops to detect transit movement between rooms, so they could calculate how many people were in each division.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In the work developed in ref. [31] , the authors propose a solution that addresses some of the problems enunciated in this work. A system is proposed that constantly monitors the security of a home. It uses several Raspberry Pi devices, connected to surveillance cameras, and uses the OpenCV library for image analysis. The system can detect various types of events, such as opening and closing doors and windows, movement in the rooms, and breaking windows. Table 1 summarizes and explains why the solutions previously presented are considered interesting for the development of this solution. Table 1 . Comparison between the solutions presented in the related work section.",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 34,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [
                {
                    "start": 459,
                    "end": 466,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 595,
                    "end": 602,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Related Work"
        },
        {
            "text": "Case Study Why Was Chosen [23] Hand gesture detection Image data extraction using OpenCV [24] Video processing on Raspberry PI OpenCV on Raspberry PI [25] Human facial recognition OpenCV on Raspberry PI [27] Extraction of vehicle information OpenCV on Raspberry PI [28] Surveillance system Image acquisition on Raspberry PI [29] Indoor/outdoor person detection Uses RFID tags to detect humans [30] Indoor human detection Uses infrared sensor to detect humans [31] Indoor monitoring Uses IoT devices with AI IndoorCare",
            "cite_spans": [
                {
                    "start": 26,
                    "end": 30,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 89,
                    "end": 93,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 150,
                    "end": 154,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 203,
                    "end": 207,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 265,
                    "end": 269,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 324,
                    "end": 328,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 393,
                    "end": 397,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 459,
                    "end": 463,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Reference"
        },
        {
            "text": "Indoor human monitoring Uses IoT devices with Computer Vision",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reference"
        },
        {
            "text": "The work that identifies most of the requirements for this intended solution is ref. [31] , mainly because of the image analysis using computer vision, artificial intelligence, and IoT devices. Although the solution works as intended, according to the authors, it requires too many processing resources from the IoT devices, which means that a robust IoT device must be used, thus increasing the solution's cost. The objective of this work is to develop a solution that can monitor a person's movements inside the house, in the various rooms, also using computer vision, as in ref. [31] , using IoT devices, while keeping the cost reasonably low.",
            "cite_spans": [
                {
                    "start": 85,
                    "end": 89,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 582,
                    "end": 586,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Reference"
        },
        {
            "text": "The solution IndoorCare, proposed in this article, is based on some principles used in other solutions, namely using only one device per room for human detection [29] , detecting the presence of people through motion capture analysis [30] , and using low-cost microcomputers to analyze and process the collected information [31] . The system has a distributed and multi-agent architecture [32] , which implements the client-server model, having a gateway module to ensure information security, as presented in Figure 1 . The architecture comprises three modules, highlighted in Figure 1 , where each module has a specific agent, a software-based entity, which performs the various tasks on the equipment to ensure proper operation of the system:",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 166,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 234,
                    "end": 238,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 324,
                    "end": 328,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 389,
                    "end": 393,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [
                {
                    "start": 510,
                    "end": 518,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 578,
                    "end": 586,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "IndoorCare System Architecture"
        },
        {
            "text": "The monitoring module, which translates to the several IoT devices at home that capture images and pre-process the data obtained from the images, in order to send this information to the presentation module.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "The gateway module, responsible for ensuring WiFi network availability and data communication security from the clients to the server (presentation module).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "The presentation module, which is the server that receives the data from the monitoring modules. It provides a presentation layer for the users (caregivers) to visually perceive the dynamics of the elderly person's activities inside the house over time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "The option to use image acquisition equipment and computer vision, instead of infrared sensors, although the latter in theory are cheaper, was because with computer vision it is possible to analyze several subzones within the same zone with a single IoT device, while with affordable infrared sensors, typically, one sensor is necessary to detect motion in each subzone. Although there are infrared sensors with this capability, such as the temperature detection cameras used to detect possible cases of COVID-19 [33] , these are not low-cost IoT devices as their cost is in the range of thousands of euros per device.",
            "cite_spans": [
                {
                    "start": 513,
                    "end": 517,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "In Figure 2 are detailed all the components of each of the modules and how they communicate with each other. The architecture comprises three modules, highlighted in Figure 1 , where each module has a specific agent, a software-based entity, which performs the various tasks on the equipment to ensure proper operation of the system:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 166,
                    "end": 174,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "\u2022"
        },
        {
            "text": "The monitoring module, which translates to the several IoT devices at home that capture images and pre-process the data obtained from the images, in order to send this information to the presentation module.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "The gateway module, responsible for ensuring WiFi network availability and data communication security from the clients to the server (presentation module).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "The presentation module, which is the server that receives the data from the monitoring modules. It provides a presentation layer for the users (caregivers) to visually perceive the dynamics of the elderly person's activities inside the house over time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "The option to use image acquisition equipment and computer vision, instead of infrared sensors, although the latter in theory are cheaper, was because with computer vision it is possible to analyze several subzones within the same zone with a single IoT device, while with affordable infrared sensors, typically, one sensor is necessary to detect motion in each subzone. Although there are infrared sensors with this capability, such as the temperature detection cameras used to detect possible cases of COVID-19 [33] , these are not low-cost IoT devices as their cost is in the range of thousands of euros per device.",
            "cite_spans": [
                {
                    "start": 513,
                    "end": 517,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "The IoT devices present in the elderly person's home are responsible for processing the information they acquire; namely, image analysis through OpenCV, generating processed data ready to be sent to the server. This functionality is in line with the Edge Computing concept [34] , where the processing is executed close to the data source, in this case at the home of the elderly, being a paradigm increasingly used in the IoT universe and in smart systems.",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 277,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Monitoring Module (IoT Device)"
        },
        {
            "text": "The monitoring module works as a black box system that receives video feeds, and outputs the extracted data from the image analysis in the form of movement events. No other information is fed to the user/caregiver. This module encompasses the devices installed in the distinct areas (zones) of the elderly home to perform the movement detection. The software agent defined for this module and presented in Figure 2 is responsible for the image acquisition and hotspot calculation, using the available resources on the IoT device. A hotspot is defined as an area where movement is detected in the image, and for which the device must compare several image frames to be able to confirm if there is movement in a zone or not.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 406,
                    "end": 414,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Monitoring Module (IoT Device)"
        },
        {
            "text": "In Figure 2 are detailed all the components of each of the modules and how they communicate with each other. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Monitoring Module (IoT Device)"
        },
        {
            "text": "The IoT devices present in the elderly person's home are responsible for processing the information they acquire; namely, image analysis through OpenCV, generating processed data ready to be sent to the server. This functionality is in line with the Edge Computing concept [34] , where the processing is executed close to the data source, in this case at the home of the elderly, being a paradigm increasingly used in the IoT universe and in smart systems.",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 277,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Monitoring Module (IoT Device)"
        },
        {
            "text": "The monitoring module works as a black box system that receives video feeds, and outputs the extracted data from the image analysis in the form of movement events. No other information is fed to the user/caregiver. This module encompasses the devices installed in the distinct areas (zones) of the elderly home to perform the movement detection. The software agent defined for this module and presented in Figure 2 is responsible for the image acquisition and hotspot calculation, using the available resources on the IoT device. A hotspot is defined as an area where movement is detected in the image, and for which the device must compare several image frames to be able to confirm if there is movement in a zone or not.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 406,
                    "end": 414,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Monitoring Module (IoT Device)"
        },
        {
            "text": "Each equipment has a unique identifier (ID) that identifies with which zone the device is associated. The ID is also used to identify the hotspot's information in the server's database and establish a relation between the device and its location in the house.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Monitoring Module (IoT Device)"
        },
        {
            "text": "This module is responsible for the confidentiality and integrity of the data transmission between the monitoring module and the presentation module. The module's agent, also presented in Figure 2 , ensures that all communications go through an encrypted tunnel, from the network access point to the server. The agent guarantees the data encryption, as well as the server's address validation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 187,
                    "end": 195,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Gateway Module (Gateway)"
        },
        {
            "text": "This module acts as the server element of the client-server model in the communication and receives data from the clients, which are represented by the monitoring module. The data are saved, processed, and presented to the caregiver user. The software agent in this module, as presented in Figure 2 , implements the features for data reception, decryption, saving, and presentation on a web platform. The user interaction is minimalistic and Each equipment has a unique identifier (ID) that identifies with which zone the device is associated. The ID is also used to identify the hotspot's information in the server's database and establish a relation between the device and its location in the house.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 290,
                    "end": 298,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Presentation Module (Server)"
        },
        {
            "text": "This module is responsible for the confidentiality and integrity of the data transmission between the monitoring module and the presentation module. The module's agent, also presented in Figure 2 , ensures that all communications go through an encrypted tunnel, from the network access point to the server. The agent guarantees the data encryption, as well as the server's address validation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 187,
                    "end": 195,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Gateway Module (Gateway)"
        },
        {
            "text": "This module acts as the server element of the client-server model in the communication and receives data from the clients, which are represented by the monitoring module. The data are saved, processed, and presented to the caregiver user. The software agent in this module, as presented in Figure 2 , implements the features for data reception, decryption, saving, and presentation on a web platform. The user interaction is minimalistic and the agent basically combines the data streams from the several clients into a unique event feed.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 290,
                    "end": 298,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Presentation Module (Server)"
        },
        {
            "text": "To create the event stream from the data, the agent uses the subzones, as previously defined in the server's configuration, to check whether there is movement within them, using the hotspots sent by the clients. A subzone corresponds to a part of the image (the entire zone) acquired by a particular IoT device and corresponds to a specific area inside the elderly person's home.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Presentation Module (Server)"
        },
        {
            "text": "This module provides a web portal for the caregiver to monitor the elderly and browse the daily activities inside the house, via Northbound [35] access. It also provides communication using Application Programming Interfaces (API) [36] via Southbound [35] for communication with the IoT devices and gateways.",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 144,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 231,
                    "end": 235,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 251,
                    "end": 255,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Presentation Module (Server)"
        },
        {
            "text": "As seen previously in Figure 1 , there are three different types of communications:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 22,
                    "end": 30,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Communication"
        },
        {
            "text": "\u2022 Caregiver with the server; \u2022 Gateway with the server; \u2022 IoT devices with the server. In Northbound communication, between the caregiver and the server, as it typically occurs in a web environment (via the Internet), the most suitable communication protocol will be HTTP (Hypertext Transfer Protocol), in its secure version (HTTPS) [37] . This is one of the most used protocols for accessing online platforms and is widely used in the IoT environment for the same purpose.",
            "cite_spans": [
                {
                    "start": 333,
                    "end": 337,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Communication"
        },
        {
            "text": "In Southbound communication, between the server, gateways, and IoT devices, since it is a communication between IoT and network devices (if supported by the hardware), several protocols focused on the IoT environment can be used:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Communication"
        },
        {
            "text": "\u2022 HTTP (Hypertext Transfer Protocol): The most used client-server communication protocol on the Web which is also widely used in the IoT world due to its simplicity and efficiency in delivering information. \u2022 COAP (Constrained Application Protocol): A communication protocol designed for devices that have limited processing capabilities, much like HTTP, but that uses much less data to send messages. \u2022 MQTT (Message Queuing Telemetry Transport): One of the lightest communication protocols, it uses the Publisher/Subscriber model to exchange messages and is widely used in scenarios where network connectivity is not ideal.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Communication"
        },
        {
            "text": "These are just a few examples of communication protocols that can be implemented in this architecture, with HTTP still being one of the most used [38] .",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 150,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Communication"
        },
        {
            "text": "In this section is presented the prototype developed to validate the proposed architecture. Low-cost IoT devices, widely used by the community, were used to implement the solution to validate the fulfilment of the objectives set out in the previous section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implemented Prototype"
        },
        {
            "text": "In Figure 3 , the general architecture of the prototype is illustrated, showing the modules and devices used. The implemented prototype incorporates all the modules described in the architecture to demonstrate the intended functionality with the proposed system. Links at the right side are the interaction that takes place between the server, gateways and IoT devices (Southbound). The link at the left side represents the user/caregiver interaction with the web platform to access the IndoorCare system (Northbound). It should be noted that in this prototype, at the gateway level, only the basic static mechanisms were implemented for the system to work correctly, namely the VPN connection and server address validation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 3",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Implemented Prototype"
        },
        {
            "text": "The equipment selection for this prototype project considered the costs in order to keep the solution effective and as low cost as possible, targeted at people with modest economic resources.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equipment Used"
        },
        {
            "text": "For the client IoT devices, we opted to use Single Board Computers (SBC) with an Operating System (OS) based in Linux, specifically the from the Raspberry PI family [39] , due to its low price and good technical characteristics. To capture and analyze the images, we chose the Raspberry Pi Zero W [40] combined with a Fisheye 160\u00b0 camera (including a 5 V 2.1 A power supply and Micro SD card), as presented in Figure 4 . The total cost per device was around EUR 45 + VAT. The implemented prototype incorporates all the modules described in the architecture to demonstrate the intended functionality with the proposed system. Links at the right side are the interaction that takes place between the server, gateways and IoT devices (Southbound). The link at the left side represents the user/caregiver interaction with the web platform to access the IndoorCare system (Northbound). It should be noted that in this prototype, at the gateway level, only the basic static mechanisms were implemented for the system to work correctly, namely the VPN connection and server address validation.",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 169,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 297,
                    "end": 301,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [
                {
                    "start": 410,
                    "end": 418,
                    "text": "Figure 4",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Equipment Used"
        },
        {
            "text": "The equipment selection for this prototype project considered the costs in order to keep the solution effective and as low cost as possible, targeted at people with modest economic resources.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equipment Used"
        },
        {
            "text": "For the client IoT devices, we opted to use Single Board Computers (SBC) with an Operating System (OS) based in Linux, specifically the from the Raspberry PI family [39] , due to its low price and good technical characteristics. To capture and analyze the images, we chose the Raspberry Pi Zero W [40] combined with a Fisheye 160 \u2022 camera (including a The equipment selection for this prototype project considered the costs in order to keep the solution effective and as low cost as possible, targeted at people with modest economic resources.",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 169,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 297,
                    "end": 301,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Equipment Used"
        },
        {
            "text": "For the client IoT devices, we opted to use Single Board Computers (SBC) with an Operating System (OS) based in Linux, specifically the from the Raspberry PI family [39] , due to its low price and good technical characteristics. To capture and analyze the images, we chose the Raspberry Pi Zero W [40] combined with a Fisheye 160\u00b0 camera (including a 5 V 2.1 A power supply and Micro SD card), as presented in Figure 4 . The total cost per device was around EUR 45 + VAT. This equipment is reasonably compact and has a set of ideal characteristics, such as a single-core processor at 1 GHz, 512 MB of RAM, and built-in WiFi, thus enabling the creation of client equipment capable of collecting, processing, and sending images to the system server over a WiFi network. It should be noted that initially the cameras used were normal Raspberry Pi Camera Modules [41] , which were replaced by fisheye cameras only after testing the system, as described in Section 5.",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 169,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 297,
                    "end": 301,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 859,
                    "end": 863,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [
                {
                    "start": 410,
                    "end": 418,
                    "text": "Figure 4",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Equipment Used"
        },
        {
            "text": "For the server equipment, to receive, store, process, and present data on a web portal, we opted for the Raspberry Pi 3 B [42] (including a 5 V 2.1 A power supply and a Micro SD card). The total cost was around EUR 50 + VAT. The characteristics of this equipment, despite being an IoT device, meet the requirements for server equipment, as it has a quad-core processor at 1.4 GHz and 1 GB of RAM. Although this device has a reasonable performance, in a real or production environment, a more robust computational node should be used, namely a dedicated server (PC) or an online VPS (Virtual Private Server).",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 126,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Equipment Used"
        },
        {
            "text": "For network gateway equipment, we chose a Mikrotik Routerboard RB951Ui-2ND [43] , mainly because of the possibility to create internal scripting for network management, and the ability of this scripting to communicate with platforms via REST API. This device had a total cost around EUR 30 + VAT. This equipment can work as a WiFi access point for the client devices, allowing the creation of a secure bridge Virtual Private Network (VPN) [44] between client and server equipment, thus ensuring client-server end-to-end confidentiality.",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 79,
                    "text": "[43]",
                    "ref_id": null
                },
                {
                    "start": 439,
                    "end": 443,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [],
            "section": "Equipment Used"
        },
        {
            "text": "One major concern is the device intrusion that can lead to the visualization of the images captured by the camera by unauthorized persons. A way to guarantee the privacy of the residents is by physically blurring the lens of the equipment. Figure 5 shows the differences between a focused and an unfocused lens, and it is possible to notice that in the image with the lens out of focus, objects and people are not perceptible, thus ensuring the privacy required by the GDPR [45] . to-end confidentiality.",
            "cite_spans": [
                {
                    "start": 474,
                    "end": 478,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [
                {
                    "start": 240,
                    "end": 248,
                    "text": "Figure 5",
                    "ref_id": "FIGREF11"
                }
            ],
            "section": "Equipment Used"
        },
        {
            "text": "One major concern is the device intrusion that can lead to the visualization of the images captured by the camera by unauthorized persons. A way to guarantee the privacy of the residents is by physically blurring the lens of the equipment. Figure 5 shows the differences between a focused and an unfocused lens, and it is possible to notice that in the image with the lens out of focus, objects and people are not perceptible, thus ensuring the privacy required by the GDPR [45] . ",
            "cite_spans": [
                {
                    "start": 474,
                    "end": 478,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [
                {
                    "start": 240,
                    "end": 248,
                    "text": "Figure 5",
                    "ref_id": "FIGREF11"
                }
            ],
            "section": "Equipment Used"
        },
        {
            "text": "To acquire the location of an individual person from video camera images, it is necessary to analyze and extract information from the images. We used the OpenCV software library [20] and the ImUtils library [46] to recognize movement in the images and Python [47] as the programming language for the software agent. It is possible to compare two images, one that serves as base reference for comparison and the other to check for changes, converting the captured images to arrays of pixels and comparing the different values of their respective positions. To perform the comparison, the absolute value in the subtraction of each of the respective pixels is obtained, thus creating an image that presents the differences found in the pixel array, which in this case shows the complete changes that occurred between the images. Then, a threshold is applied to the resultant image, by defining a change limit between pixels, where the pixels below the threshold ",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 182,
                    "text": "[20]",
                    "ref_id": null
                },
                {
                    "start": 207,
                    "end": 211,
                    "text": "[46]",
                    "ref_id": null
                },
                {
                    "start": 259,
                    "end": 263,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "System Operation"
        },
        {
            "text": "To acquire the location of an individual person from video camera images, it is necessary to analyze and extract information from the images. We used the OpenCV software library [20] and the ImUtils library [46] to recognize movement in the images and Python [47] as the programming language for the software agent. It is possible to compare two images, one that serves as base reference for comparison and the other to check for changes, converting the captured images to arrays of pixels and comparing the different values of their respective positions. To perform the comparison, the absolute value in the subtraction of each of the respective pixels is obtained, thus creating an image that presents the differences found in the pixel array, which in this case shows the complete changes that occurred between the images. Then, a threshold is applied to the resultant image, by defining a change limit between pixels, where the pixels below the threshold are discarded and those above are saved, to create an image, commonly known as threshold, which contains only the pixels where there is a significant difference or, in this case, movement detection.",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 182,
                    "text": "[20]",
                    "ref_id": null
                },
                {
                    "start": 207,
                    "end": 211,
                    "text": "[46]",
                    "ref_id": null
                },
                {
                    "start": 259,
                    "end": 263,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "System Operation"
        },
        {
            "text": "In Figure 6 , on the right side, the threshold which corresponds to the movement detected on the left side of the image is displayed, with the movement area defined by a blue rectangle. are discarded and those above are saved, to create an image, commonly known as threshold, which contains only the pixels where there is a significant difference or, in this case, movement detection. In Figure 6 , on the right side, the threshold which corresponds to the movement detected on the left side of the image is displayed, with the movement area defined by a blue rectangle. To effectively calculate the threshold, as stated in [24] , it is necessary at an early stage to convert the color image to a grayscale image, so the only differentiating factor is the pixel brightness. Then, it is necessary to blur the image so that there are no sudden changes in the pixel tones. Figure 7 shows the different types of blurs supported by OpenCV: Gaussian Blur, Median Blur, and Normalized Block Blur [48] . To effectively calculate the threshold, as stated in [24] , it is necessary at an early stage to convert the color image to a grayscale image, so the only differentiating factor is the pixel brightness. Then, it is necessary to blur the image so that there are no sudden changes in the pixel tones. Figure 7 shows the different types of blurs supported by OpenCV: Gaussian Blur, Median Blur, and Normalized Block Blur [48] . To effectively calculate the threshold, as stated in [24] , it is necessary at an early stage to convert the color image to a grayscale image, so the only differentiating factor is the pixel brightness. Then, it is necessary to blur the image so that there are no sudden changes in the pixel tones. Figure 7 shows the different types of blurs supported by OpenCV: Gaussian Blur, Median Blur, and Normalized Block Blur [48] . Each blur type uses a different approach, yielding different results. The tests performed consisted of acquiring images where there was always the same human movement, walking from one end of the room to the other. Several threshold values were tested with the different types of blurs, which led to the following conclusion:",
            "cite_spans": [
                {
                    "start": 624,
                    "end": 628,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 989,
                    "end": 993,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1049,
                    "end": 1053,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1414,
                    "end": 1418,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1474,
                    "end": 1478,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1839,
                    "end": 1843,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                }
            ],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 6",
                    "ref_id": "FIGREF14"
                },
                {
                    "start": 388,
                    "end": 396,
                    "text": "Figure 6",
                    "ref_id": "FIGREF14"
                },
                {
                    "start": 870,
                    "end": 878,
                    "text": "Figure 7",
                    "ref_id": "FIGREF15"
                },
                {
                    "start": 1295,
                    "end": 1303,
                    "text": "Figure 7",
                    "ref_id": "FIGREF15"
                },
                {
                    "start": 1720,
                    "end": 1728,
                    "text": "Figure 7",
                    "ref_id": "FIGREF15"
                }
            ],
            "section": "System Operation"
        },
        {
            "text": "\u2022 Median Blur and Normalized Block give less false positives in the motion detection; \u2022 Gaussian Blur detects more movement, as it provides more image detail after blurring.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Operation"
        },
        {
            "text": "In the prototype, Median Blur was used, but any other blur could be used as well. Figure 8 describes the algorithm implemented by the Monitoring Agent to collect and compare images, find hotspots, and send them to the server (explained in the communication subsection). The device starts up and initially acquires an image to use as a base. In the following instant, the device acquires another image, and then creates a threshold for it. It analyzes if there is movement or not and if so, creates a hotspot entry and saves it into the log. Every 30 s the IoT device tries to upload all the hotspots it finds in that period of time. In the prototype, Median Blur was used, but any other blur could be used as well. Figure 8 describes the algorithm implemented by the Monitoring Agent to collect and compare images, find hotspots, and send them to the server (explained in the communication subsection). The device starts up and initially acquires an image to use as a base. In the following instant, the device acquires another image, and then creates a threshold for it. It analyzes if there is movement or not and if so, creates a hotspot entry and saves it into the log. Every 30 s the IoT device tries to upload all the hotspots it finds in that period of time. On the server side, the Presentation Agent receives and inserts the data into a database, after which it is processed and presented to the user/caregiver. To detect movement in an area, it is necessary to create sub-areas that will work as baselines for comparison with the detected hotspots by the devices. The server prototype provides the management feature to define and manage zones and subzones, as exemplified in Figure 9 . On the server side, the Presentation Agent receives and inserts the data into a database, after which it is processed and presented to the user/caregiver. To detect movement in an area, it is necessary to create sub-areas that will work as baselines for comparison with the detected hotspots by the devices. The server prototype provides the management feature to define and manage zones and subzones, as exemplified in Figure 9 . On the server side, the Presentation Agent receives and inserts the data into a database, after which it is processed and presented to the user/caregiver. To detect movement in an area, it is necessary to create sub-areas that will work as baselines for comparison with the detected hotspots by the devices. The server prototype provides the management feature to define and manage zones and subzones, as exemplified in Figure 9 . This management feature allows the administrator/system installer to create the zones and the respective subzones that the caregiver want to supervise. It should be noted This management feature allows the administrator/system installer to create the zones and the respective subzones that the caregiver want to supervise. It should be noted that to be able to acquire an image of the IoT device used as a monitoring module, a physical action on the equipment is required, namely the junction of two GPIO pins to activate the device's configuration mode. In Figure 10 are displayed two zones used in the system's prototype and its subzones, each one identified by an ID. Any hotspot detected within one of the delimited areas corresponds to movement in that subzone. that to be able to acquire an image of the IoT device used as a monitoring module, a physical action on the equipment is required, namely the junction of two GPIO pins to activate the device's configuration mode. In Figure 10 are displayed two zones used in the system's prototype and its subzones, each one identified by an ID. Any hotspot detected within one of the delimited areas corresponds to movement in that subzone. Following the hotspot detection, this information must be transmitted to the caregiver through a simple and effective interface, mainly because if the interface is too complex, the caregiver may not feel comfortable using it. An example of a simplistic visual interface is the timeline feed of events shown in Figure 11 , which is a summary of the events that occurred each day at a certain time. The timeline provides a perception feed of the activity in the elderly home spaces under monitoring, by combining the feeds from the zones into a unique feed, formatted as a timeline grid. For the human caregiver/user, it is very simple and effective to check for specific events [49] and general activity in the house. In the timeline, each blue vertical stripe represents a hotspot detection in that respective subzone, signaling that movement was detected at that time in that area. Following the hotspot detection, this information must be transmitted to the caregiver through a simple and effective interface, mainly because if the interface is too complex, the caregiver may not feel comfortable using it. An example of a simplistic visual interface is the timeline feed of events shown in Figure 11 , which is a summary of the events that occurred each day at a certain time. The timeline provides a perception feed of the activity in the elderly home spaces under monitoring, by combining the feeds from the zones into a unique feed, formatted as a timeline grid. For the human caregiver/user, it is very simple and effective to check for specific events [49] and general activity in the house. In the timeline, each blue vertical stripe represents a hotspot detection in that respective subzone, signaling that movement was detected at that time in that area.",
            "cite_spans": [
                {
                    "start": 4428,
                    "end": 4432,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 5311,
                    "end": 5315,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [
                {
                    "start": 82,
                    "end": 90,
                    "text": "Figure 8",
                    "ref_id": "FIGREF20"
                },
                {
                    "start": 715,
                    "end": 723,
                    "text": "Figure 8",
                    "ref_id": "FIGREF20"
                },
                {
                    "start": 1686,
                    "end": 1694,
                    "text": "Figure 9",
                    "ref_id": "FIGREF23"
                },
                {
                    "start": 2117,
                    "end": 2125,
                    "text": "Figure 9",
                    "ref_id": "FIGREF23"
                },
                {
                    "start": 2548,
                    "end": 2556,
                    "text": "Figure 9",
                    "ref_id": "FIGREF23"
                },
                {
                    "start": 3117,
                    "end": 3126,
                    "text": "Figure 10",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 3542,
                    "end": 3551,
                    "text": "Figure 10",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 4061,
                    "end": 4070,
                    "text": "Figure 11",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 4944,
                    "end": 4953,
                    "text": "Figure 11",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "System Operation"
        },
        {
            "text": "interface is the timeline feed of events shown in Figure 11 , which is a summary of the events that occurred each day at a certain time. The timeline provides a perception feed of the activity in the elderly home spaces under monitoring, by combining the feeds from the zones into a unique feed, formatted as a timeline grid. For the human caregiver/user, it is very simple and effective to check for specific events [49] and general activity in the house. In the timeline, each blue vertical stripe represents a hotspot detection in that respective subzone, signaling that movement was detected at that time in that area. Figure 11 . Event timeline. Figure 11 . Event timeline.",
            "cite_spans": [
                {
                    "start": 417,
                    "end": 421,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [
                {
                    "start": 50,
                    "end": 59,
                    "text": "Figure 11",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 623,
                    "end": 632,
                    "text": "Figure 11",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 651,
                    "end": 660,
                    "text": "Figure 11",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "System Operation"
        },
        {
            "text": "With the timeline display, the caregiver can follow the daily life of the elderly and check his routine in a simple and non-intrusive way.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Operation"
        },
        {
            "text": "To be able to send the \"converted images\" transformed into data to the server, it is necessary for the client to be able to structure this information in such a way that it will be well interpreted at the destination. XML (Extensible Markup Language) [50] is a markup language that allows structuring information in a simple and easily readable way by human beings. It is one of the standards used in the communication of information between information systems and has great flexibility, allowing the creation of the most varied message structures. Another format also widely used in information communication is JSON (JavaScript Object Notation) [50] , a compact message format that has less overhead than XML and has been also widely implemented in the industry.",
            "cite_spans": [
                {
                    "start": 251,
                    "end": 255,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 648,
                    "end": 652,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "Communication"
        },
        {
            "text": "In this prototype, we chose to use XML only because it allows easier reading of messages and facilitates the query of logs, but JSON could also be used. The messages sent in XML from the IoT devices to the server have the following fields:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Communication"
        },
        {
            "text": "\u2022 datetime: date and time of registration of hotspots, will be grouped in intervals of 30 s for better organization on the server. with the x and y coordinates of a hotspot detected at that moment, there may be several at the same moment. Figure 12 presents an example of one of these messages, sent periodically to the server. ferent resolutions of different cameras). \u2022 matrixheight: the height scale of the matrix used in this device (to normalize the different resolutions of different cameras). \u2022 hotspot: with the x and y coordinates of a hotspot detected at that moment, there may be several at the same moment. Figure 12 presents an example of one of these messages, sent periodically to the server. Client devices calculate hotspots and store this information in a log to be sent every 30 s. In case of communication failure, the Monitoring Agents themselves save the information that was not successfully sent to the server in the log and in the next iteration they try to resend all the pending information.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 239,
                    "end": 248,
                    "text": "Figure 12",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 619,
                    "end": 628,
                    "text": "Figure 12",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Communication"
        },
        {
            "text": "Initially, it was decided to encrypt the data using symmetric encryption on the clients, but this required unnecessary processing by the IoT devices, so the solution was to delegate this task to the gateway, which would be responsible for creating the VPN bridge with the server and ensure information security and confidentiality. Client devices calculate hotspots and store this information in a log to be sent every 30 s. In case of communication failure, the Monitoring Agents themselves save the information that was not successfully sent to the server in the log and in the next iteration they try to resend all the pending information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Communication"
        },
        {
            "text": "Initially, it was decided to encrypt the data using symmetric encryption on the clients, but this required unnecessary processing by the IoT devices, so the solution was to delegate this task to the gateway, which would be responsible for creating the VPN bridge with the server and ensure information security and confidentiality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Communication"
        },
        {
            "text": "One of the advantages of the way the system is designed and implemented is that there is a history of every hotspot detected, and so the processing of movement in the subzones is carried out in the server, and new subzones can be added or rearranged long after the system's first initialization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Movement Data History"
        },
        {
            "text": "In Figure 13 is presented an example of how this works. On the left side there are five subzones defined and all the hotspots registered since the system startup; on the right side there is a new subzone defined (2F) after the system initialization. Because a hotspot history exists for each zone, every movement that occurred in that subzone, even before its creation, can be fully visualized in the timeline. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "text": "Figure 13",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Movement Data History"
        },
        {
            "text": "One of the advantages of the way the system is designed and implemented is that there is a history of every hotspot detected, and so the processing of movement in the subzones is carried out in the server, and new subzones can be added or rearranged long after the system's first initialization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Movement Data History"
        },
        {
            "text": "In Figure 13 is presented an example of how this works. On the left side there are five subzones defined and all the hotspots registered since the system startup; on the right side there is a new subzone defined (2F) after the system initialization. Because a hotspot history exists for each zone, every movement that occurred in that subzone, even before its creation, can be fully visualized in the timeline. Doing this allows the visualization of all the movement in the new/rearranged subzones, which were not contemplated in the system before, because all the data history related to the detected movement of the entire zone is saved.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "text": "Figure 13",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Movement Data History"
        },
        {
            "text": "Due to the current COVID-19 pandemic, it was not possible to carry out tests in real situations with the elderly. All tests performed were simulated in the same house division/area, with specific tests focused on the correct functioning of each module. During the tests, some optimizations were made, namely in the agent present in the IoT devices. Doing this allows the visualization of all the movement in the new/rearranged subzones, which were not contemplated in the system before, because all the data history related to the detected movement of the entire zone is saved.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests and Optimizations"
        },
        {
            "text": "Due to the current COVID-19 pandemic, it was not possible to carry out tests in real situations with the elderly. All tests performed were simulated in the same house division/area, with specific tests focused on the correct functioning of each module. During the tests, some optimizations were made, namely in the agent present in the IoT devices.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tests and Optimizations"
        },
        {
            "text": "The tests performed on the IoT devices consisted of analyzing the code of the agent developed in python and its ability to perform the necessary operations, namely:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Client Testing"
        },
        {
            "text": "\u2022 Acquire images from a camera connected to the IoT device; \u2022 Process the image using OpenCV for motion detection; \u2022 Creation of hotspots for later upload to the server; \u2022 Sending collected hotspots to the server.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Client Testing"
        },
        {
            "text": "In Figure 14 is shown the output of the Monitoring Agent, while in debug mode, displaying the image coordinates, where the movement was detected, and the XML message generated to be sent to the server. Another test involving the IoT devices was to verify if the hotspots generated by the devices were accurate or not; that is, if the motion detected by the devices was consistent with the motion points that appeared on the server's timeline. In Figure 15 is shown the timeline of the event stream, as displayed by the server, showing movement detection, while on the right side of the figure are shown the outputs of the equipment and the zones they are monitoring. Another test involving the IoT devices was to verify if the hotspots generated by the devices were accurate or not; that is, if the motion detected by the devices was consistent with the motion points that appeared on the server's timeline. In Figure 15 is shown the timeline of the event stream, as displayed by the server, showing movement detection, while on the right side of the figure are shown the outputs of the equipment and the zones they are monitoring.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "text": "Figure 14",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 446,
                    "end": 455,
                    "text": "Figure 15",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 911,
                    "end": 920,
                    "text": "Figure 15",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Client Testing"
        },
        {
            "text": "Another test involving the IoT devices was to verify if the hotspots generated by the devices were accurate or not; that is, if the motion detected by the devices was consistent with the motion points that appeared on the server's timeline. In Figure 15 is shown the timeline of the event stream, as displayed by the server, showing movement detection, while on the right side of the figure are shown the outputs of the equipment and the zones they are monitoring. The tests performed on the system confirmed that the points collected by the device and the movements present in the timeline matched. The tests performed on the system confirmed that the points collected by the device and the movements present in the timeline matched.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 244,
                    "end": 253,
                    "text": "Figure 15",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Client Testing"
        },
        {
            "text": "The tests carried out on the server focused on the reception and processing of data from the servers and their presentation to the user/caregiver, including:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Server Testing"
        },
        {
            "text": "Reception of the hotspot in the IoT devices; \u2022 Hotspot data processing and timeline generation; \u2022 Creation and editing of zones and subzones.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "In Figure 16 is shown an example of how the subzone creation tool of the server was tested; this was accomplished by creating subzones with specific x and y limits and by sending static hotspots generated manually on the IoT device with the corners of the subzone, to verify that the server was placing the hotspot point in the correct pixels on the image and thus generating movement correctly in that subzone. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "text": "Figure 16",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "\u2022"
        },
        {
            "text": "The tests carried out on the server focused on the reception and processing of data from the servers and their presentation to the user/caregiver, including:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Server Testing"
        },
        {
            "text": "\u2022",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Server Testing"
        },
        {
            "text": "Reception of the hotspot in the IoT devices; \u2022 Hotspot data processing and timeline generation; \u2022 Creation and editing of zones and subzones.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Server Testing"
        },
        {
            "text": "In Figure 16 is shown an example of how the subzone creation tool of the server was tested; this was accomplished by creating subzones with specific x and y limits and by sending static hotspots generated manually on the IoT device with the corners of the subzone, to verify that the server was placing the hotspot point in the correct pixels on the image and thus generating movement correctly in that subzone. This test, in particular, served to verify if there was any deviation in the hotspot calculation due to the scale applied to the different image sizes of different types of cameras. Different image capture resolutions were used to see if the same movement coincided in the same subzone, a result that was confirmed at the end.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "text": "Figure 16",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Server Testing"
        },
        {
            "text": "The timeline event feed is a key element of the system, for which were conducted This test, in particular, served to verify if there was any deviation in the hotspot calculation due to the scale applied to the different image sizes of different types of cameras. Different image capture resolutions were used to see if the same movement coincided in the same subzone, a result that was confirmed at the end.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Timeline Interpretation"
        },
        {
            "text": "The timeline event feed is a key element of the system, for which were conducted some tests to verify if an ordinary person (after very brief training) could understand the information, as presented, and perceive the events that might have generated those data. Due to the current pandemic situation, the tests were executed with only five persons simulating caregivers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Timeline Interpretation"
        },
        {
            "text": "A test protocol was designed under which the subjects received a hypothetical timeline of the event feed of a day in a hypothetical house. While visualizing the timeline, the subjects were questioned about what they perceived had happened in the house during the day. In Figure 17 is shown a timeline, created for testing purposes only, in which specific numbered points correspond to specific events. The events were then presented, but not numbered, and the subjects had to match the event with the event number on the timeline. These events were as follows: The results, with a test group of five individuals, are quite positive, with all the individuals confirming that they were able to perceive what happened by reading the timeline. The only exceptions were events 1 and 5, which are very similar in the timeline, and two of the five individuals misinterpreted these two.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 271,
                    "end": 280,
                    "text": "Figure 17",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Timeline Interpretation"
        },
        {
            "text": "During the tests on IoT devices, it was noticed that when using an outline rectangle for the movement detection, as the example in Figure 18 shows, the calculation of hotspots sometimes covered two or more subzones, leading to quite a few false positives in subzones where the movement was not happening. The events were then presented, but not numbered, and the subjects had to match the event with the event number on the timeline. These events were as follows:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 131,
                    "end": 140,
                    "text": "Figure 18",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Hotspot Detection Optimization"
        },
        {
            "text": "\u2022 \"Mr. Jo\u00e3o spent the morning watching television on the sofa and then went to lunch.\"; \u2022 \"Mr. Jo\u00e3o went to drink water in the kitchen.\"; \u2022 \"Mr. Jo\u00e3o went to the bathroom.\"; \u2022 \"Mr. Jo\u00e3o was watching TV for almost 2 h.\"; \u2022 \"Someone knocked on the door and Mr. Jo\u00e3o went to see who it was.\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hotspot Detection Optimization"
        },
        {
            "text": "The results, with a test group of five individuals, are quite positive, with all the individuals confirming that they were able to perceive what happened by reading the timeline. The only exceptions were events 1 and 5, which are very similar in the timeline, and two of the five individuals misinterpreted these two.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hotspot Detection Optimization"
        },
        {
            "text": "During the tests on IoT devices, it was noticed that when using an outline rectangle for the movement detection, as the example in Figure 18 shows, the calculation of hotspots sometimes covered two or more subzones, leading to quite a few false positives in subzones where the movement was not happening. two of the five individuals misinterpreted these two.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 131,
                    "end": 140,
                    "text": "Figure 18",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Hotspot Detection Optimization"
        },
        {
            "text": "During the tests on IoT devices, it was noticed that when using an outline rectangle for the movement detection, as the example in Figure 18 shows, the calculation of hotspots sometimes covered two or more subzones, leading to quite a few false positives in subzones where the movement was not happening. To reduce the number of false positives, it was decided to create a central point in the motion detection rectangle that would represent the midpoint of all the movement that To reduce the number of false positives, it was decided to create a central point in the motion detection rectangle that would represent the midpoint of all the movement that occurred in that specific area of the image. By performing this optimization, and after several tests, it was concluded that when using this midpoint technique, the number of false positives decreased significantly, creating a timeline with much less scattered movement points.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 131,
                    "end": 140,
                    "text": "Figure 18",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Hotspot Detection Optimization"
        },
        {
            "text": "Another advantage of this optimization was the significant decrease (about 50%) in the network traffic to send a hotspot data message, mainly because the messages are in XML and the overhead becomes much smaller, when, in this case, only a pair of X + Y coordinates are transmitted per movement, instead of the two pairs of coordinates to send the rectangle.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hotspot Detection Optimization"
        },
        {
            "text": "The analysis of whether there is movement or not is performed by comparing an image with a previous image (base image), in order to verify if there are differences between them. There is a problem when the base image no longer corresponds to the actual scenario, and small changes were introduced due to non-motion pixel changes. These changes, although not caused by motion, were detected as motion because the pixels in the image changed since the base image.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Automatic Background Adaptation"
        },
        {
            "text": "One of the identified problems was the constant change in the environment that the client was analyzing, either due to the presence of new objects or changes in lighting scenario, such as a light being turned on. To address this problem, in the IoT device was implemented a compensation algorithm that modified the baseline image when required. In Figure 19 , two of the problems encountered while testing the system are presented. The first one is when an object enters the background scenario, and it does not exist in the base image. The second is when the scenario lighting changes, and all the pixels change, creating movement in the entire image. client was analyzing, either due to the presence of new objects or changes in lighting scenario, such as a light being turned on. To address this problem, in the IoT device was implemented a compensation algorithm that modified the baseline image when required. In Figure 19 , two of the problems encountered while testing the system are presented. The first one is when an object enters the background scenario, and it does not exist in the base image. The second is when the scenario lighting changes, and all the pixels change, creating movement in the entire image. After several tests, an algorithm was created based on the work of [51] , to optimize movement detection, that changed the baseline image based on two conditions:",
            "cite_spans": [
                {
                    "start": 1290,
                    "end": 1294,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [
                {
                    "start": 348,
                    "end": 357,
                    "text": "Figure 19",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 918,
                    "end": 927,
                    "text": "Figure 19",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Automatic Background Adaptation"
        },
        {
            "text": "When no significant movement is detected for more than X seconds (X is variable), which allows the baseline image to be updated to ambient lighting throughout the day. \u2022 When significant movement is detected for more than Y seconds (Y is variable), which happens in at least three different cases: when there is a sudden change in the ambient lighting, when new objects are introduced in the scenario, and when there is real movement in the image.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "Regarding the second assumption, when a new base image is created, and if the movement detection continues, then it is because there is real movement detected. In case of a change in lighting or a new object in the scene, after the new base image is created the movement stops. After several tests, an algorithm was created based on the work of [51] , to optimize movement detection, that changed the baseline image based on two conditions:",
            "cite_spans": [
                {
                    "start": 345,
                    "end": 349,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "When no significant movement is detected for more than X seconds (X is variable), which allows the baseline image to be updated to ambient lighting throughout the day. \u2022 When significant movement is detected for more than Y seconds (Y is variable), which happens in at least three different cases: when there is a sudden change in the ambient lighting, when new objects are introduced in the scenario, and when there is real movement in the image.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "Regarding the second assumption, when a new base image is created, and if the movement detection continues, then it is because there is real movement detected. In case of a change in lighting or a new object in the scene, after the new base image is created the movement stops.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022"
        },
        {
            "text": "During the initial development of the system, cameras with regular lenses were used, with an about 72 \u2022 viewing angle, which greatly limited the area to be monitored, especially if viewed from above (ceiling of the room). Later, fisheye lenses, with an approximately 160 \u2022 viewing angle, were installed on the IoT devices, allowing a much larger monitoring area. In Figure 20 , there is a lens comparison with normal lenses (72 \u2022 ) on the left and fisheye lenses (160 \u2022 ) on the right. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 366,
                    "end": 375,
                    "text": "Figure 20",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Cameras with Fisheye Lens"
        },
        {
            "text": "During the initial development of the system, cameras with regular lenses were used, with an about 72\u00b0 viewing angle, which greatly limited the area to be monitored, especially if viewed from above (ceiling of the room). Later, fisheye lenses, with an approximately 160\u00b0 viewing angle, were installed on the IoT devices, allowing a much larger monitoring area. In Figure 20 , there is a lens comparison with normal lenses (72\u00b0) on the left and fisheye lenses (160\u00b0) on the right. The equipment is positioned exactly in the same place and the only difference is the camera lens. On the right side, the amount of area is much bigger, which allows a better use of the image for motion detection. With this type of lens, it is also possible to place the equipment on the ceiling of the room and monitor the entire area, as seen in Figure 8 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 364,
                    "end": 373,
                    "text": "Figure 20",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 827,
                    "end": 835,
                    "text": "Figure 8",
                    "ref_id": "FIGREF20"
                }
            ],
            "section": "Cameras with Fisheye Lens"
        },
        {
            "text": "With this optimization, and after several tests, the fisheye lens installed in the ceiling proved to be the ideal place to position the IoT device and monitor the room. This setup also produced fewer false positives in the movement detection in the subzones, mainly because the line of sight between the camera and the subzone is less likely to be obstructed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cameras with Fisheye Lens"
        },
        {
            "text": "To test the system's performance in real motion detection, various sizes of minimum The equipment is positioned exactly in the same place and the only difference is the camera lens. On the right side, the amount of area is much bigger, which allows a better use of the image for motion detection. With this type of lens, it is also possible to place the equipment on the ceiling of the room and monitor the entire area, as seen in Figure 8 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 431,
                    "end": 439,
                    "text": "Figure 8",
                    "ref_id": "FIGREF20"
                }
            ],
            "section": "Movement Detection Performance"
        },
        {
            "text": "With this optimization, and after several tests, the fisheye lens installed in the ceiling proved to be the ideal place to position the IoT device and monitor the room. This setup also produced fewer false positives in the movement detection in the subzones, mainly because the line of sight between the camera and the subzone is less likely to be obstructed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Movement Detection Performance"
        },
        {
            "text": "To test the system's performance in real motion detection, various sizes of minimum motion area were tested to check how this would influence the system's execution. Figure 21 shows the test scenario with a defined sub-area (a) to detect the movement of the door (b). Three sizes of minimum area were used to detect the same movement of the door (c), which were 20 px, 50 px, and 100 px. These sizes were defined in particular for this test and another set could be defined; the purpose was only to check whether the minimum detection area influenced the detection of the same movement.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 166,
                    "end": 175,
                    "text": "Figure 21",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Movement Detection Performance"
        },
        {
            "text": "use of the image for motion detection. With this type of lens, it is also possible to place the equipment on the ceiling of the room and monitor the entire area, as seen in Figure 8 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 173,
                    "end": 181,
                    "text": "Figure 8",
                    "ref_id": "FIGREF20"
                }
            ],
            "section": "Movement Detection Performance"
        },
        {
            "text": "With this optimization, and after several tests, the fisheye lens installed in the ceiling proved to be the ideal place to position the IoT device and monitor the room. This setup also produced fewer false positives in the movement detection in the subzones, mainly because the line of sight between the camera and the subzone is less likely to be obstructed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Movement Detection Performance"
        },
        {
            "text": "To test the system's performance in real motion detection, various sizes of minimum motion area were tested to check how this would influence the system's execution. Figure  21 shows the test scenario with a defined sub-area (a) to detect the movement of the door (b). Three sizes of minimum area were used to detect the same movement of the door (c), which were 20 px, 50 px, and 100 px. These sizes were defined in particular for this test and another set could be defined; the purpose was only to check whether the minimum detection area influenced the detection of the same movement. The graph in Figure 22 shows the results obtained from the tests carried out. As can be seen, when using a smaller motion detection area, the system can recognize the same motion/movement in that subzone more often than when the area is larger. The graph in Figure 22 shows the results obtained from the tests carried out. As can be seen, when using a smaller motion detection area, the system can recognize the same motion/movement in that subzone more often than when the area is larger. This result is expected and is reflected in previous tests, since the detection of the changed pixels is sometimes very fragmented, creating several small detection areas that are discarded if the minimum size of these to be considered valid is too high.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 166,
                    "end": 176,
                    "text": "Figure  21",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 601,
                    "end": 610,
                    "text": "Figure 22",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 846,
                    "end": 855,
                    "text": "Figure 22",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Movement Detection Performance"
        },
        {
            "text": "In this work, an effective and low-cost indoor monitoring system was proposed to help caregivers take care of the elderly by monitoring their daily lives from a distance. This system brings the advantages of knowing where the elderly person is and the activity dynamics in the house, while fully respecting the elderly person's privacy, thus creating a daily movement record of the elderly person.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        },
        {
            "text": "The test results of the prototype show that it is possible to use low-price and lowperformance IoT equipment, namely a Raspberry Pi Zero W, to build a system that performs monitoring in a specific zone in the house and its associated subzones. In addition, the tests also indicate that the usage of the timeline event feed model is very effective to display the activity inside a home and that it is very simple to interact with.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        },
        {
            "text": "As future work, an optimization that can be made in terms of processing on the Raspberry Pi is the implementation of gray areas, i.e., areas that will never have points of interest for motion detection. The detections that happen in these subzones (e.g., reflections) are not sent to the server, leading to less false positives and a smaller amount of information to be sent to the server. Other improvements in this type of system include the This result is expected and is reflected in previous tests, since the detection of the changed pixels is sometimes very fragmented, creating several small detection areas that are discarded if the minimum size of these to be considered valid is too high.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        },
        {
            "text": "In this work, an effective and low-cost indoor monitoring system was proposed to help caregivers take care of the elderly by monitoring their daily lives from a distance. This system brings the advantages of knowing where the elderly person is and the activity dynamics in the house, while fully respecting the elderly person's privacy, thus creating a daily movement record of the elderly person.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        },
        {
            "text": "The test results of the prototype show that it is possible to use low-price and lowperformance IoT equipment, namely a Raspberry Pi Zero W, to build a system that performs monitoring in a specific zone in the house and its associated subzones. In addition, the tests also indicate that the usage of the timeline event feed model is very effective to display the activity inside a home and that it is very simple to interact with.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        },
        {
            "text": "As future work, an optimization that can be made in terms of processing on the Raspberry Pi is the implementation of gray areas, i.e., areas that will never have points of interest for motion detection. The detections that happen in these subzones (e.g., reflections) are not sent to the server, leading to less false positives and a smaller amount of information to be sent to the server. Other improvements in this type of system include the implementation of automatic alerts, which could be of two types: a \"Non-movement alert\", which would inform the caregiver when something abnormal happens in the elderly's routine; or a \"Too long alert\", which would be used to inform the caregiver that, after movement was detected in an specific area, it suddenly stopped, informing the caregiver that something may be happening. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Proje\u00e7\u00f5es de Popula\u00e7\u00e3o Residente em Portugal. Portal do INE",
            "authors": [
                {
                    "first": "Instituto",
                    "middle": [],
                    "last": "Nacional De Estat\u00edsticas",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "D\u00e9fice Social e Pobreza Relativa: Uma an\u00e1lise da Adequa\u00e7\u00e3o Do Bem-Estar e da Seguran\u00e7a Econ\u00f3mica em Portugal",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Pereirinha",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pereira",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "The ambient assisted living joint programme",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Vodjdani",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 2008 2nd Electronics System-Integration Technology Conference",
            "volume": "",
            "issn": "",
            "pages": "1--2",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "An\u00e1lisis del programa ambient assisted living joint programme (AAL) en el periodo",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pinazo-Hernandis",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "P"
                    ],
                    "last": "Puente",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "2",
            "issn": "",
            "pages": "38--50",
            "other_ids": {
                "DOI": [
                    "10.21892/01239813.95"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Beyond 'smart-only'cities: Redefining the 'smart-everything'paradigm",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Streitz",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "J. Ambient. Intell. Humaniz. Comput",
            "volume": "10",
            "issn": "",
            "pages": "791--812",
            "other_ids": {
                "DOI": [
                    "10.1007/s12652-018-0824-1"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Ambient Assisted living system's models and architectures: A survey of the state of the art",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Abtoy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Touhafi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Tahiri",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "J. King Saud Univ. -Comput. Inf. Sci",
            "volume": "2020",
            "issn": "",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Situation awareness in ambient assisted living for smart healthcare",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "N"
                    ],
                    "last": "Alkhomsan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Hossain",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "M M"
                    ],
                    "last": "Rahman",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Masud",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Access",
            "volume": "5",
            "issn": "",
            "pages": "20716--20725",
            "other_ids": {
                "DOI": [
                    "10.1109/ACCESS.2017.2731363"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Internet of things for ambient assisted living: Challenges and future opportunities",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Gu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 International conference on cyber-enabled distributed computing and knowledge discovery (CyberC)",
            "volume": "",
            "issn": "",
            "pages": "354--357",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Ambient assisted living: Identifying new challenges and needs for digital technologies and service innovation",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vimarlund",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "M"
                    ],
                    "last": "Borycki",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "W"
                    ],
                    "last": "Kushniruk",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Avenberg",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Yearb. Med. Inform",
            "volume": "30",
            "issn": "",
            "pages": "6051--6071",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Reference Architectures, Platforms, and Pilots for European Smart and Healthy Living-Analysis and Comparison",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Grguric",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ortega-Gil",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Markakis",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Pozdniakov",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kloukinas",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Medrano-Gil",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Gaeta",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Fico",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Koloutsou",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "10",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/electronics10141616"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "A review of internet of things technologies for ambient assisted living environments",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Maskeli\u016bnas",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dama\u0161evi\u010dius",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Segal",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Future Internet",
            "volume": "11",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/fi11120259"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Usability of Smartbands by the Elderly Population in the Context of Ambient Assisted Living Applications",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Correia",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Fuentes",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ribeiro",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Costa",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Reis",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Rabad\u00e3o",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Barroso",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Pereira",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "10",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/electronics10141617"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Smart sensory furniture based on WSN for ambient assisted living",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Bleda",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "J"
                    ],
                    "last": "Fern\u00e1ndez-Luque",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rosa",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zapata",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Maestre",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Sens. J",
            "volume": "17",
            "issn": "",
            "pages": "5626--5636",
            "other_ids": {
                "DOI": [
                    "10.1109/JSEN.2017.2721434"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Low-cost automatic ambient assisted living system",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Malekmohamadi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Moemeni",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Orun",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "K"
                    ],
                    "last": "Purohit",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)",
            "volume": "",
            "issn": "",
            "pages": "693--697",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Web platform architecture for ambient assisted living",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Stefan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Aldea",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "S"
                    ],
                    "last": "Nechifor",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "J. Ambient Intell. Smart Environ",
            "volume": "10",
            "issn": "",
            "pages": "35--47",
            "other_ids": {
                "DOI": [
                    "10.3233/AIS-170470"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Non-obstructive authentication in AAL environments",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Almeidaa",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Costaa",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Limaa",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Novaisc",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Workshop Proceedings of the 7th International Conference on Intelligent Environments",
            "volume": "",
            "issn": "",
            "pages": "63--73",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Real-time distributed architecture for remote acoustic elderly monitoring in residential-scale ambient assisted living scenarios",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Navarro",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Vida\u00f1a-Vila",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "M"
                    ],
                    "last": "Alsina-Pag\u00e8s",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Herv\u00e1s",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Sensors",
            "volume": "18",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/s18082492"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Computer vision for ambient assisted living: Monitoring systems for personalized healthcare and wellness that are robust in the real world and accepted by users, carers, and society",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Colantonio",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Coppini",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Giorgi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Morales",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Pascali",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Computer Vision for Assistive Healthcare",
            "volume": "",
            "issn": "",
            "pages": "147--182",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "A fog-based emergency system for smart enhanced living environments",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Nikoloudakis",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Panagiotakis",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Markakis",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pallis",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Mastorakis",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "X"
                    ],
                    "last": "Mavromoustakis",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dobre",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Cloud Comput",
            "volume": "3",
            "issn": "",
            "pages": "54--62",
            "other_ids": {
                "DOI": [
                    "10.1109/MCC.2016.118"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "The Raspberry Pi Foundation. Raspberry Pi",
            "authors": [],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Gesture recognition using Open-CV",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "B"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mishra",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Qadeer",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 7th International Conference on Communication Systems and Network Technologies (CSNT)",
            "volume": "",
            "issn": "",
            "pages": "167--171",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Embedded video processing on Raspberry Pi",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Arva",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Fryza",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 27th International Conference Radioelektronika (RADIOELEKTRONIKA)",
            "volume": "",
            "issn": "",
            "pages": "1--4",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Reconhecimento Facial Em Imagens Capturadas Por C\u00e2meras Digitais De Rede",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "K"
                    ],
                    "last": "Okabe",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Carro",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Colloq. Exactarum",
            "volume": "7",
            "issn": "",
            "pages": "106--119",
            "other_ids": {
                "DOI": [
                    "10.5747/ce.2015.v07.n1.e111"
                ]
            }
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Computer vision based vehicle detection for toll collection system using embedded Linux",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Suryatali",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "B"
                    ],
                    "last": "Dharmadhikari",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 International Conference on Circuits, Power and Computing Technologies",
            "volume": "",
            "issn": "",
            "pages": "1--7",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "IoT based smart surveillance security system using raspberry Pi",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Patil",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ambatkar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kakde",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 International Conference on Communication and Signal Processing (ICCSP)",
            "volume": "",
            "issn": "",
            "pages": "344--348",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Human tracking in certain indoor and outdoor area by combining the use of RFID and GPS",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Hutabarat",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hendry",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Pranoto",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kurniawan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob), Wireless and Mobile (APWiMob)",
            "volume": "",
            "issn": "",
            "pages": "59--62",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Monitoring Indoor People Presence in Buildings Using Low-Cost Infrared Sensor Array in Doorways",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Perra",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Losito",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Pirino",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Moradpour",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Gatto",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Sensors",
            "volume": "21",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/s21124062"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Motion activated security camera using raspberry Pi",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "N K"
                    ],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Natraj",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "P"
                    ],
                    "last": "Jacob",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 International Conference on Communication and Signal Processing (ICCSP)",
            "volume": "",
            "issn": "",
            "pages": "1598--1601",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Edge of things: The big picture on the integration of edge, IoT and the cloud in a distributed computing environment",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "El-Sayed",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sankar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Prasad",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Puthal",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mohanty",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "T"
                    ],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Access",
            "volume": "6",
            "issn": "",
            "pages": "1706--1717",
            "other_ids": {
                "DOI": [
                    "10.1109/ACCESS.2017.2780087"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Opportunities and challenges for the building monitoring systems in the age-pandemic of COVID-19: Review and prospects",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "N S"
                    ],
                    "last": "Al-Humairi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "A A"
                    ],
                    "last": "Kamal",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Innov. Infrastruct. Solut",
            "volume": "6",
            "issn": "",
            "pages": "1--10",
            "other_ids": {
                "DOI": [
                    "10.1007/s41062-020-00454-0"
                ]
            }
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Edge computing: A survey",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "Z"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hakak",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Yaqoob",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Future Gener. Comput. Syst",
            "volume": "97",
            "issn": "",
            "pages": "219--235",
            "other_ids": {
                "DOI": [
                    "10.1016/j.future.2019.02.050"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Software-defined networking (SDN): A reference architecture and open APIs",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "K"
                    ],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "H"
                    ],
                    "last": "Nam",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "J"
                    ],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 2012 International Conference on ICT Convergence (ICTC)",
            "volume": "",
            "issn": "",
            "pages": "360--361",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Design of Application Programming Interfaces (APIs)",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Cwalina",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "M"
                    ],
                    "last": "Abrams",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Moore",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Anderson",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pizzo",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "R A"
                    ],
                    "last": "Brigham",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "430",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "The cost of the \"s\" in https",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Naylor",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Finamore",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Leontiadis",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Grunenberger",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mellia",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Munaf\u00f2",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Papagiannaki",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Steenkiste",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 10th ACM International on Conference on emerging Networking Experiments and Technologies",
            "volume": "",
            "issn": "",
            "pages": "133--140",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "A survey of communication protocols for internet of things and related challenges of fog and cloud computing integration",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dizdarevi\u0107",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Carpio",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jukan",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Masip-Bruin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACM Comput. Surv. (CSUR)",
            "volume": "51",
            "issn": "",
            "pages": "1--29",
            "other_ids": {
                "DOI": [
                    "10.1145/3292674"
                ]
            }
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "The Raspberry Pi Foundation. Raspberry Pi Products",
            "authors": [],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "The Raspberry Pi Foundation",
            "authors": [],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Raspberry Pi Camera Module V2",
            "authors": [],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "The Raspberry Pi Foundation",
            "authors": [],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Virtual private networks",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Venkateswaran",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "IEEE Potentials",
            "volume": "20",
            "issn": "",
            "pages": "11--15",
            "other_ids": {
                "DOI": [
                    "10.1109/45.913204"
                ]
            }
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Taxonom\u00eda de informaci\u00f3n personal de salud para garantizar la privacidad de los individuos",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lozoya-De-Diego",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "T"
                    ],
                    "last": "Villalba-De-Benito",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Arias-Pou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "26",
            "issn": "",
            "pages": "293--302",
            "other_ids": {
                "DOI": [
                    "10.3145/epi.2017.mar.16"
                ]
            }
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Create Your Own Internet of Things: A survey of IoT platforms",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "S"
                    ],
                    "last": "Kapoor",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Consum. Electron. Mag",
            "volume": "6",
            "issn": "",
            "pages": "57--68",
            "other_ids": {
                "DOI": [
                    "10.1109/MCE.2016.2640718"
                ]
            }
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "OpenCV: Smoothing Images",
            "authors": [
                {
                    "first": "Opencv",
                    "middle": [],
                    "last": "Team",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Visualization using timelines",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "M"
                    ],
                    "last": "Karam",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Proceedings of the 1994 ACM SIGSOFT International Symposium on Software Testing and Analysis",
            "volume": "",
            "issn": "",
            "pages": "125--137",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Comparison of JSON and XML data interchange formats: A case study",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Nurseitov",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Paulson",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Reynolds",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Izurieta",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "9",
            "issn": "",
            "pages": "157--162",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Adaptive change detection for real-time surveillance applications",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Huwer",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Niemann",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Proceedings of the Third IEEE International Workshop on Visual Surveillance",
            "volume": "",
            "issn": "",
            "pages": "37--46",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "IndoorCare conceptual architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "IndoorCare conceptual architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "IndoorCare detailed architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "IndoorCare detailed architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Prototype conceptual architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Prototype conceptual architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "supply and Micro SD card), as presented inFigure 4. The total cost per device was around EUR 45 + VAT.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Raspberry PI Zero W #1 (a) and #2 (b).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Raspberry PI Zero W #1 (a) and #2 (b).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Focused (a) and unfocused (b) lens.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Focused (a) and unfocused (b) lens.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Motion detection in the image.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "Some types of image blurring.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Motion detection in the image.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "Motion detection in the image.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Some types of image blurring.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Some types of image blurring.Each blur type uses a different approach, yielding different results. The tests performed consisted of acquiring images where there was always the same human movement, walking from one end of the room to the other. Several threshold values were tested with the different types of blurs, which led to the following conclusion:\u2022 Median Blur and Normalized Block give less false positives in the motion detection; \u2022 Gaussian Blur detects more movement, as it provides more image detail after blurring.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "Client operation algorithm.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "Client operation algorithm.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "Client operation algorithm.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "Management of zones and subzones.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "Management of zones and subzones.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF26": {
            "text": "Two zones and several subzones defined for the prototype.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF27": {
            "text": "Two zones and several subzones defined for the prototype.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF28": {
            "text": "Example of an XML message used.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF29": {
            "text": "Example of an XML message used.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF31": {
            "text": "Hotspot's history.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF32": {
            "text": "Hotspot's history.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF33": {
            "text": "Monitoring Agent output in debug mode.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF34": {
            "text": "Monitoring Agent output in debug mode.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF35": {
            "text": "Timeline of events and the respective devices.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF36": {
            "text": "Timeline of events and the respective devices.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF38": {
            "text": "Subzone creation testing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF39": {
            "text": "Subzone creation testing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF40": {
            "text": "Timeline interpretation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF41": {
            "text": "\"Mr. Jo\u00e3o spent the morning watching television on the sofa and then went to lunch.\"; \u2022 \"Mr. Jo\u00e3o went to drink water in the kitchen.\"; \u2022 \"Mr. Jo\u00e3o went to the bathroom.\"; \u2022 \"Mr. Jo\u00e3o was watching TV for almost 2 h.\"; \u2022 \"Someone knocked on the door and Mr. Jo\u00e3o went to see who it was.\".",
            "latex": null,
            "type": "figure"
        },
        "FIGREF42": {
            "text": "Timeline interpretation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF43": {
            "text": "Movement detection optimization.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF44": {
            "text": "Movement detection optimization.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF45": {
            "text": "Creation of the new base image.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF46": {
            "text": "Creation of the new base image.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF48": {
            "text": "Angle viewing of lens: 72\u00b0 (Left) vs. 160\u00b0 (Right).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF49": {
            "text": "Angle viewing of lens: 72 \u2022 (Left) vs. 160 \u2022 (Right).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF50": {
            "text": "Movement detection of a closed (a) and open door (b) and the used movement area sizes (c).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF51": {
            "text": "Movement detection of a closed (a) and open door (b) and the used movement area sizes (c).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF53": {
            "text": "Movement detection with multiple minimum area sizes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF54": {
            "text": "Movement detection with multiple minimum area sizes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF55": {
            "text": "Contributions: Conceptualization, D.F., L.C. and A.P.; data curation, D.F. and L.C.; formal analysis, A.R., C.R., J.B. and A.P.; funding acquisition, A.R., J.B., C.R. and A.P.; investigation, D.F. and L.C.; methodology, A.P.; resources, A.R., C.R., J.B., J.R., N.C. and A.P.; software, D.F. and L.C.; supervision, A.R., J.B., N.C. and A.P.; validation, N.C., A.R., J.R., C.R., J.B. and A.P.; writing-original draft, D.F., L.C., N.C., A.R. and A.P.; writing-review and editing, D.F., L.C., J.R., A.R., C.R., J.B. and A.P. All authors have read and agreed to the published version of the manuscript.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF56": {
            "text": "This work was supported by Project \"Digitalization of end-of-line distributed testers for antennas (\"D-EoL-TA)\", operation number: POCI-01-0247-FEDER-049698, financed by the Program COMPETE 2020, Portugal 2020, by National Funds through the Portuguese funding agency, FCT-Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia, within project UIDB/04524/2020, and was partially supported by Portuguese National funds through FITEC-Programa Interface, with reference CIT \"INOV-INESC Inova\u00e7\u00e3o-Financiamento Base\" and by Portuguese Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia-FCT, I.P., under the project UIDB/50014/2020. Institutional Review Board Statement: Not applicable. Informed Consent Statement: Not applicable. Data Availability Statement: Not applicable.",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "Center for the facilities granted in the implementation of part of this work, in the context of the Smart IoT Ecosystems research line and the Mobile Computing Laboratory of the School of Technology and Management of the Polytechnic of Leiria. The authors also acknowledge the authorship of some of the images used in some of the visual content created using the tool \"diagrams.net, accessed on 20 January 2021\" and the free content available in \"iconfinder.com, accessed on 20 January 2021\", \" pixabay.com, accessed on 20 January 2021\", and \"flaticon.com, accessed on 20 January 2021\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments: The authors acknowledge the Computer Science and Communication Research"
        },
        {
            "text": "The authors declare no conflict of interest.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conflicts of Interest:"
        }
    ]
}
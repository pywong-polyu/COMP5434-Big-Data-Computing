{
    "paper_id": "a6274ef6c17826a0ffdf1b56abb873e70854d36f",
    "metadata": {
        "title": "Political audience diversity and news reliability in algorithmic ranking",
        "authors": [
            {
                "first": "Saumya",
                "middle": [],
                "last": "Bhadani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of South Florida",
                    "location": {
                        "settlement": "Tampa",
                        "region": "FL",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Shun",
                "middle": [],
                "last": "Yamaya",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Stanford University",
                    "location": {
                        "settlement": "Stanford",
                        "region": "CA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Alessandro",
                "middle": [],
                "last": "Flammini",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Indiana University",
                    "location": {
                        "settlement": "Bloomington",
                        "region": "IN",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Filippo",
                "middle": [],
                "last": "Menczer",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Indiana University",
                    "location": {
                        "settlement": "Bloomington",
                        "region": "IN",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Giovanni",
                "middle": [
                    "Luca"
                ],
                "last": "Ciampaglia",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of South Florida",
                    "location": {
                        "settlement": "Tampa",
                        "region": "FL",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Brendan",
                "middle": [],
                "last": "Nyhan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Dartmouth College",
                    "location": {
                        "settlement": "Hanover",
                        "region": "NH",
                        "country": "USA"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Newsfeed algorithms frequently amplify misinformation and other low-quality content. How can social media platforms more effectively promote reliable information? Existing approaches are difficult to scale and vulnerable to manipulation. In this paper, we propose using the political diversity of a website's audience as a quality signal. Using news source reliability ratings from domain experts and web browsing data from a diverse sample of 6,890 U.S. citizens, we first show that websites with more extreme and less politically diverse audiences have lower journalistic standards. We then incorporate audience diversity into a standard collaborative filtering framework and show that our improved algorithm increases the trustworthiness of websites suggested to users -especially those who most frequently consume misinformation -while keeping recommendations relevant. These findings suggest that partisan audience diversity is a valuable signal of higher journalistic standards that should be incorporated into algorithmic ranking decisions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Concerns continue to grow about the prevalence of misinformation on social media platforms [31, 50] , including during the recent COVID-19 pandemic [51] . These types of content often exploit people's tendency to prefer pro-attitudinal information [23] , which can be exacerbated by platform content recommendations [5, 6] . In this paper, we explore a possible algorithmic approach to mitigate the spread of misinformation and promote content with higher journalistic standards online.",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 95,
                    "text": "[31,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 96,
                    "end": 99,
                    "text": "50]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 148,
                    "end": 152,
                    "text": "[51]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 248,
                    "end": 252,
                    "text": "[23]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 316,
                    "end": 319,
                    "text": "[5,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 320,
                    "end": 322,
                    "text": "6]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Social media platform recommendation algorithms frequently amplify bias in human consumption decisions. Though the information diets of Americans are less slanted in practice than many assume, the people who consume the most political news are most affected by the tendency toward selective exposure [17] . As a result, the news audience is far more polarized than the public as a whole [10, 19] . Although the prevalence of so-called \"fake news\" online is rather limited and concentrated among relatively narrow audiences [2, 3, [16] [17] [18] 20] , content that generally appeals to these tendencies -which does include low-quality or false news -may generate high levels of readership or engagement [50], prompting algorithms that seek to maximize engagement to distribute them more widely.",
            "cite_spans": [
                {
                    "start": 300,
                    "end": 304,
                    "text": "[17]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 387,
                    "end": 391,
                    "text": "[10,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 392,
                    "end": 395,
                    "text": "19]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 523,
                    "end": 526,
                    "text": "[2,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 527,
                    "end": 529,
                    "text": "3,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 530,
                    "end": 534,
                    "text": "[16]",
                    "ref_id": null
                },
                {
                    "start": 535,
                    "end": 539,
                    "text": "[17]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 540,
                    "end": 544,
                    "text": "[18]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 545,
                    "end": 548,
                    "text": "20]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Prior research indicates that existing recommendation algorithms tend to promote items that have already achieved popularity [13, 38] . This bias may have several effects on the consumption of low-quality and false news. First, sorting the news by engagement (either predicted or achieved) can exacerbate polarization by increasing in-group bias and discouraging consumption among outgroup members [47] . Second, it may contribute to information cascades, amplifying differences in rankings from small variations or random fluctuations and degrading the overall quality of information consumed by users [8, 12, 24, 32, 43] . Third, exposure to engagement metrics makes users more likely to share and less likely to fact-check highly engaging content from low-credibility sources, increasing vulnerability to misinformation [4] . Finally, popularity bias in recommendation systems can create socio-algorithmic vulnerabilities to threats such as automated amplifiers, which exploit algorithmic content rankings to spread low-quality and inflammatory content to like-minded audiences [45, 48] .",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 129,
                    "text": "[13,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 130,
                    "end": 133,
                    "text": "38]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 398,
                    "end": 402,
                    "text": "[47]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 603,
                    "end": 606,
                    "text": "[8,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 607,
                    "end": 610,
                    "text": "12,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 611,
                    "end": 614,
                    "text": "24,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 615,
                    "end": 618,
                    "text": "32,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 619,
                    "end": 622,
                    "text": "43]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 823,
                    "end": 826,
                    "text": "[4]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "text": "[45,",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 1086,
                    "end": 1089,
                    "text": "48]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Given the speed and scale of social media, assessing directly the quality of every piece of content or the behavior of each user is infeasible. Online platforms are instead seeking to include signals about news quality in their content recommendation algorithms [9, 15] , for example by extracting information from trusted publishers [26] or by means of linguistic patterns analysis [27, 40] . More generally, a vast literature examines how to assess the credibility of online sources [7, 22] and the reputations of individual online users [1, 14] , which could in principle bypass the problem",
            "cite_spans": [
                {
                    "start": 262,
                    "end": 265,
                    "text": "[9,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 266,
                    "end": 269,
                    "text": "15]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 334,
                    "end": 338,
                    "text": "[26]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 383,
                    "end": 387,
                    "text": "[27,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 388,
                    "end": 391,
                    "text": "40]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 485,
                    "end": 488,
                    "text": "[7,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 489,
                    "end": 492,
                    "text": "22]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 544,
                    "end": 547,
                    "text": "14]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "of checking each individual piece of content. Unfortunately, many of these methods are hard to scale to large groups and/or depend upon context-specific information about the type of content being generated. For example, methods for assessing the credibility of content on Wikipedia often assume content is organized as a wiki. As a result, they are not easily applied to news content recommendations on social media platforms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Another approach is to try to evaluate the quality of articles directly [54] , but scaling such an approach would likely be costly and cause lags in the evaluation of novel content. Similarly, while crowdsourced website evaluations have been shown to be generally reliable in distinguishing between high and low quality news sources [39] , the robustness of such signals to manipulation is yet to be demonstrated.",
            "cite_spans": [
                {
                    "start": 72,
                    "end": 76,
                    "text": "[54]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 333,
                    "end": 337,
                    "text": "[39]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Building on the literature about the benefits of diversity at the group level [25, 46] , we propose using the partisan diversity of the audience of a news source as a signal of its quality. This approach has two key advantages. First, audience partisan diversity can be computed at scale given that information about the partisanship of users is available or can be inferred in a reliable manner. Second, because diversity is a property of the audience and not of its level of engagement, it is less susceptible to manipulation if one can detect inauthentic partisan accounts [44, 49, 52, 53] . These two conditions (inferring partisanship reliably and preventing abuse by automated amplification/deception) could easily be met by the major social media platforms, which have routine access to a wealth of signals about their users and their authenticity.",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 82,
                    "text": "[25,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 83,
                    "end": 86,
                    "text": "46]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 576,
                    "end": 580,
                    "text": "[44,",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 581,
                    "end": 584,
                    "text": "49,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 585,
                    "end": 588,
                    "text": "52,",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 589,
                    "end": 592,
                    "text": "53]",
                    "ref_id": "BIBREF51"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We evaluate the merits of our proposed approach using data from two sources: a comprehensive data set of web traffic history from 6,890 Americans, collected along with surveys of self-reported partisan information from respondents in the YouGov Pulse survey panel, and a data set of 3,765 news source reliability scores compiled by trained experts in journalism and provided by News-Guard [37] . We first establish that domain pageviews are not associated with overall news reliability, highlighting the potential problem with algorithmic recommendation systems that rely on popularity and related metrics of engagement. We next define measures of audience partisan diversity and show that these measures correlate with news reliability better than popularity does.",
            "cite_spans": [
                {
                    "start": 389,
                    "end": 393,
                    "text": "[37]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Finally, we study the effect of incorporating audience partisan diversity into algorithmic ranking decisions. When we create a variant of the standard collaborative filtering algorithm that explicitly takes audience partisan diversity into account, our new algorithm provides more trustworthy recommendations than the standard approach with only a small loss of relevance, suggesting that reliable sources can be recommended without the risk of jeopardizing user experience.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "These results demonstrate that diversity in audience partisanship can serve as a useful signal of news reliability at the domain level, a finding that has important implications for the design of content recommendation algorithms used by online platforms. Although the news recommendation technologies deployed by platforms are more sophisticated than the approach tested here, our results highlight a fundamental weakness of algorithmic ranking methods that prioritize content that generates engagement and suggest a new metric that could help improve the reliability of the recommendations that are provided to users.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "To motivate our study, we first demonstrate that the popular news content that algorithmic recommendations often highlight is not necessarily reliable. To do so, we assess the relationship between source popularity and news reliability. We measure source popularity using the YouGov Pulse traffic data. Due to skew in audience size among domains, we transform these data to a logarithmic scale. In practice, we measure the popularity of a source in two ways: as the (log of) number of users, and as the (logged) number of visits, or pageviews. News reliability is instead measured using NewsGuard scores (see Methods A). Fig. 1 shows that the popularity of a news Domains for which we have NewsGuard reliability scores [37] are shaded in blue (where darker shades equal lower scores). Domains with no available score are plotted in gray.",
            "cite_spans": [
                {
                    "start": 719,
                    "end": 723,
                    "text": "[37]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [
                {
                    "start": 621,
                    "end": 627,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Popularity does not predict news reliability"
        },
        {
            "text": "source is at best weakly associated with its reliability. At the user level (left pane), the overall Pearson correlation is r = 0.03 (two-sided p = 0.36). At the pageview level (right pane), r = 0.05 (two-sided p = 0.12). The association between the two variables remains weak even if we divide sources based on their partisanship. When measuring popularity at the user level, websites that have a predominantly Democratic audience have a significant positive association (r = 0.09, twosided p = 0.02), but for websites with a Republican audience the correlation is negative and not significant at conventional standards (r = \u22120.12, two-sided p = 0.06). A similar pattern holds at the pageview level: a weak positive association for websites with predominantly Democratic audiences (r = 0.08, two-sided p = 0.02) and a negative but not significant association for those with predominantly Republican audiences (r = \u22120.06, two-sided p = 0.34). Overall, these results suggest the strength of association between the two variables is quite weak.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Popularity does not predict news reliability"
        },
        {
            "text": "In contrast, we observe that sites with greater audience partisan diversity tend to have higher NewsGuard scores while those with lower levels of diversity, and correspondingly more homogeneous partisan audiences, tend to have lower reliability scores. As our primary metric of diversity, we selected from a range of alternative definitions (see Methods B) the variance of the partisanship distribution. Fig. 2 As Fig. 2 indicates, unreliable websites with very low NewsGuard scores are concentrated in the tails of the distribution, where partisanship is most extreme and audience partisan diversity is, by necessity, very low. This relationship is not symmetrical: low-reliability websites (whose markers are darker shades of blue in the figure) are especially concentrated in the right tail, which corresponds to websites with largely Republican audiences. The data in Fig. 2 also suggests that the reliability of a website may be associated not just with the variance of the distribution of audience partisanship slants, but also with its mean. To account for this, we first compute the coefficient of partial correlation between NewsGuard reliability scores and the variance of audience partisanship given the mean audience partisanship of each website. Compared with popularity, we find a stronger (and significant) correlation regardless of whether mean partisanship and audience partisan diversity are calculated by weighting individual audience members equally (user level, left panel: partial correlation r = 0.38, two-sided p < 10 \u22124 ) or by how often they visited a given site (pageview level, right panel: partial correlation r = 0.22, two-sided p < 10 \u22124 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 404,
                    "end": 410,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 414,
                    "end": 420,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 872,
                    "end": 878,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Audience partisan diversity is a signal for high-reliability news"
        },
        {
            "text": "Aside from mean partisanship, a related, but potentially distinct, confounding factor is the extremity of the partisanship slants distribution (i.e., the distance of the average partisanship of a website visitor on a 1-7 scale from the midpoint of 4, which represents a true independent). We thus computed partial correlation coefficients again, but instead keep the ideological extremity of website audiences constant instead of the mean. Our results are consistent using this approach (user level: r = 0.26, p < 10 \u22124 ; pageview level: r = 0.15, p < 10 \u22124 ; both tests are two-sided).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Audience partisan diversity is a signal for high-reliability news"
        },
        {
            "text": "We study the diversity-reliability relationship in more detail in Fig. 3 , which differentiates be-tween websites with audiences that are mostly Republican and those with audiences that are mostly Democratic. Consistent with what we report above, Fig. 3 shows that audience partisan diversity is positively associated with news reliability. Again, this relationship holds both when individual audience members are weighted equally (user level, left panel) and when they are weighted by their number of accesses (pageview level, right panel), though the association is stronger at the user level (standardized OLS coefficient: \u03b2 = 6.67 (0.58) at user level; \u03b2 = 3.91 (0.71) at pageview level). In addition, we find that the relationship is stronger for sites whose average visitor identifies as a Republican (standardized OLS coefficient of Republican domains: \u03b2 = 13.1 (1.59) at user level; \u03b2 = 9.61 (2.20) at pageview level) versus those whose average visitor identifies as a Democrat In fact, the association between diversity and Newsguard reliability scores is consistent even when controlling for popularity (user level: r = 0.34, two-sided p < 10 \u22124 ; pageview level: r = 0.17, twosided p < 10 \u22124 ), suggesting that diversity could contribute to detecting quality over and above the more typical popularity metrics used by social media algorithms. However, the previous analysis of Fig. 3 shows that the overall relationship masks significant heterogeneity between websites with mostly Republican or Democratic audiences. To tease apart the contributions of popularity from those of partisanship, we estimate a full multivariate regression model. After controlling for both popularity and political orientation, we find qualitatively similar results. Full regression tables can be found in Supplementary Materials.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 66,
                    "end": 72,
                    "text": "Fig. 3",
                    "ref_id": null
                },
                {
                    "start": 247,
                    "end": 253,
                    "text": "Fig. 3",
                    "ref_id": null
                },
                {
                    "start": 1388,
                    "end": 1394,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Audience partisan diversity is a signal for high-reliability news"
        },
        {
            "text": "As mentioned before, variance in audience partisanship is not the only possible way to define audience partisan diversity; alternative definitions can be used (e.g., entropy; see Methods B). As a robustness check, we therefore consider a range of alternative definitions of audience partisan diversity and obtain results that are qualitatively similar to the ones presented here, though results are strongest for variance (see Supplementary Materials).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Audience partisan diversity is a signal for high-reliability news"
        },
        {
            "text": "To understand the potential effects of incorporating audience partisan diversity into algorithmic recommendations, we next consider how recommendations from a standard user-based collaborative filtering (CF) algorithm [29, 41] change if we include audience partisan diversity as an additional signal. We call this modified version of the algorithm CF+D, which stands for Collaborative Filtering + Diversity (see Methods C for formal definition).",
            "cite_spans": [
                {
                    "start": 218,
                    "end": 222,
                    "text": "[29,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 223,
                    "end": 226,
                    "text": "41]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Audience partisan diversity produces trustworthy, relevant recommendations"
        },
        {
            "text": "In classic CF, users are presented with recommendations drawn from a set of items (in this case, web domains) that have been \"rated\" highly by those other users whose tastes are most similar to theirs. Lacking explicit data about how a user would \"rate\" a given web domain, we use a quantity derived from the number of user pageviews to a domain (based on TF-IDF; see also Methods C) as the rating.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Audience partisan diversity produces trustworthy, relevant recommendations"
        },
        {
            "text": "To evaluate our method, we follow a standard supervised learning workflow. We first divide web traffic data for each user in the YouGov Pulse panel into training and testing sets by domain (see Methods D). We then compute similarities in traffic patterns between users for all domains in the training set (not just news websites) and use the computed similarities to predict the aforementioned domain-level pageviews metric on the test set. The domains that receive the highest predicted ratings (i.e., expected TF-IDF-transformed pageviews) are then selected as recommendations. As a robustness check, we obtain consistent results if we split the data longitudinally instead of randomly (i.e., as a forecasting exercise; see Supplementary Materials for details).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Audience partisan diversity produces trustworthy, relevant recommendations"
        },
        {
            "text": "Note that if a user has not visited a domain, then the number of visits for that domain will be zero. In general, due to the long tail in user interests [13] , we cannot infer that the user has a negative preference toward a website just because they have not visited it. The user may simply be unaware of the site. We therefore follow standard practice in the machine learning literature in only evaluating recommendations for content for which we have ratings (i.e., visits in the test set), though in practice actual newsfeed algorithms rank items from a broader set of inputs, which typically includes content the user may not have seen (for example, content shared by friends [5] ).",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 157,
                    "text": "[13]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 681,
                    "end": 684,
                    "text": "[5]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Audience partisan diversity produces trustworthy, relevant recommendations"
        },
        {
            "text": "To produce recommendations for a given user, we consider all the domains visited by the user in the test set for which ratings are available from one or more respondents in a neighborhood of most similar users (domains with no neighborhood rating are discarded since neither CF nor CF+D can make a prediction for them; see Methods C) and for which we have a NewsGuard score (i.e., a reliability score). We then rank those domains by their rating computed using either CF or CF+D.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Audience partisan diversity produces trustworthy, relevant recommendations"
        },
        {
            "text": "This process produces a ranked list of news domains and reliability scores from both the standard CF algorithm and the CF+D algorithm, which has been modified to incorporate the audience partisan diversity signal. We evaluate these lists using two different measures of trustworthiness which are computed for the top k domains in each list: the mean score (a number in the 0-100 range) and the proportion of domains with a score of 60 or higher, which NewsGuard classifies as indicating that a site \"generally adheres to basic standards of credibility and transparency\" [37] This baseline does not include any local information about user-user similarities, and thus can be seen as a \"global\" measure of popularity with no contribution due to user personalization (see",
            "cite_spans": [
                {
                    "start": 570,
                    "end": 574,
                    "text": "[37]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Audience partisan diversity produces trustworthy, relevant recommendations"
        },
        {
            "text": "We observe in Fig. 4 that the trustworthiness of recommendations produced by CF+D is sig- nificantly better than standard CF recommendations, global popularity recommendations, and baseline statistics from user behavior. In particular, CF produces less trustworthy rankings than both the recommendations based on global popularity and on user visits (for small values of k the difference is within the margin of error). In contrast, CF+D produces rankings that are more trustworthy than CF and either baseline (global popularity or actual visits) across different levels of k. These results suggest that audience partisan diversity can provide a valuable signal to improve the reliability of algorithmic recommendations.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 14,
                    "end": 20,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Methods E)."
        },
        {
            "text": "Of course, the above exercise would be meaningless if our proposed algorithm recommended websites that do not interest users. Because CF+D alters the set of recommended domains to prioritize those visited by more diverse partisan audiences, it may be suggesting sources that offer counter-attitudinal information or that users do not find relevant. In this sense, CF+D could represent an audience-based analogue of the topic diversification strategy from the recommender systems literature [55] . If so, a loss of predictive ability would be expected. To provide intuition about the contribution of popularity in recommendations, the left panel of Fig. 5 also shows the precision of the na\u00efve baseline obtained by ranking items by their global popularity. This baseline outperform CF and CF+D but at the price of providing the same set of recommendations to all users (i.e., the results are not personalized) and of providing recommendations of lower trustworthiness (Fig. 4) . Note that the RMSE cannot be computed for this baseline because this metric requires knowledge of the rating of a domain, not just of its relative ranking.",
            "cite_spans": [
                {
                    "start": 490,
                    "end": 494,
                    "text": "[55]",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 967,
                    "end": 975,
                    "text": "(Fig. 4)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 648,
                    "end": 654,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Methods E)."
        },
        {
            "text": "Our results are generally encouraging. In both cases, precision is low and RMSE is high for low values of k, but error levels start to stabilize around k = 10, which suggests that making correct recommendations for shorter lists (i.e., k < 10) is more challenging than for longer ones. Moreover, when we compare CF+D with CF, accuracy declines slightly for CF+D relative to CF but the difference is not statistically significant for all but small values of k, suggesting that CF+D is still capable of producing relevant recommendations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods E)."
        },
        {
            "text": "Re-ranking items by diversity has minimal effects on predictive accuracy, but how does it affect user satisfaction? The recommendations produced by CF+D would be useless if users did not find them engaging. Unfortunately, we lack data about user satisfaction in the YouGov panel -our primary metric (log number of website visits) cannot be interpreted as a pure measure of satisfaction (other factors of course shape the decision by users in the YouGov panel to visit a website, including social media recommendations themselves).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods E)."
        },
        {
            "text": "However, it is possible that more accurate recommendations will result in higher user satisfaction. In the revised version, we therefore quantify the significance of the observed drop in accuracy due to re-ranking by diversity. More specifically, we simulated the sampling distribution of the precision of recommendation after re-ranking by re-shuffling domain labels in the list of ratings produced by CF+D. This procedure allows us to calculate the probability of a drop in precision as small as the observed one due to random chance alone. Compared with this null model, we find that our results lead to significantly higher precision -most random re-rankings of the same magnitude as the one produced by CF+D would result in lower precision than what we observe.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods E)."
        },
        {
            "text": "We report the results of this additional analysis in the Supplementary Materials (Fig. S8 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 81,
                    "end": 89,
                    "text": "(Fig. S8",
                    "ref_id": "FIGREF9"
                }
            ],
            "section": "Methods E)."
        },
        {
            "text": "The results above demonstrate that incorporating audience partisan diversity can increase the trustworthiness of recommended domains while still providing users with relevant recommendations. However, we know that exposure to unreliable news outlets varies dramatically across the population. For instance, exposure to untrustworthy content is highly concentrated among a narrow subset of highly active news consumers with heavily slanted information diets [16, 20] . We therefore take advantage of the survey and behavioral data available on participants in the Pulse panel to consider how CF+D effects vary by individual partisanship (self-reported via survey), behavioral measures such as volume of news consumption activity and information diet slant, and contextual factors that are relevant to algorithm performance such as similarity with other users.",
            "cite_spans": [
                {
                    "start": 457,
                    "end": 461,
                    "text": "[16,",
                    "ref_id": null
                },
                {
                    "start": 462,
                    "end": 465,
                    "text": "20]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Audience partisan diversity mitigates selective exposure to misinformation"
        },
        {
            "text": "In this section, we again produce recommendations using either CF or CF+D and measure their difference in trustworthiness with respect to a baseline based on user visits (specifically the ranking by TF-IDF-normalized number of visits v; see Methods C). However, we analyze the results differently than those reported above. Rather than considering recommendations for lists of varying length k, we create recommendations for different subgroups based on the factors of interest and compare how the effects of the CF+D approach vary between those groups.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Audience partisan diversity mitigates selective exposure to misinformation"
        },
        {
            "text": "To facilitate comparisons in performance between subgroups that do not depend on list length k, we define a new metric to summarize the overall trustworthiness of the ranked lists obtained with CF and CF+D over all possible values of k. Since users tend to pay less attention to items ranked lower in the list [28] , it is reasonable to assume that lower-ranked items ought to contribute less to the overall trustworthiness of a given ranking.",
            "cite_spans": [
                {
                    "start": 310,
                    "end": 314,
                    "text": "[28]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Audience partisan diversity mitigates selective exposure to misinformation"
        },
        {
            "text": "Let us now consider probabilistic selections from two different rankings, represented by random variables X and X , where X is the random variable of the ranking produced by one of the two recommendation algorithms (either CF or CF+D) and X is the selection from the baseline ranking based on user visits. Using a probabilistic discounting method (see Eq. 8 in Method H), we compute the expected change in trustworthiness Q from switching the selection from X to X,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Audience partisan diversity mitigates selective exposure to misinformation"
        },
        {
            "text": "where the expectations of Q(X) and Q(X ) are taken with regard to the respective rankings (see Applying Eq. 1, we find that CF+D substantially increases trustworthiness for users who tend to visit sources that lean conservative ( Fig. 6(a) ) and for those who have the most polarized information diets (in either direction; see Fig. 6 (c)), two segments of users who are especially likely to be exposed to unreliable information [2, 16, 20] . In both cases, CF+D achieves the greatest improvement among the groups where CF reduces the trustworthiness of recommendations the most, which highlights the pitfalls of algorithmic recommendations for vulnerable audiences and the benefits of prioritizing sources with diverse audiences in making recommendations to those users.",
            "cite_spans": [
                {
                    "start": 429,
                    "end": 432,
                    "text": "[2,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 433,
                    "end": 436,
                    "text": "16,",
                    "ref_id": null
                },
                {
                    "start": 437,
                    "end": 440,
                    "text": "20]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [
                {
                    "start": 230,
                    "end": 239,
                    "text": "Fig. 6(a)",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 328,
                    "end": 334,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Audience partisan diversity mitigates selective exposure to misinformation"
        },
        {
            "text": "Note that even though the YouGov sample includes self-reported information on both party ID and partisanship of respondents. We use only the former ( Fig. 6 (b)) for stratification to avoid circularity given the definition of CF+D, which relies on the latter. In Figs 6(a) and 6(c), we instead stratify on an external measure of news diet slant (calculated from a large sample of social media users; see Methods I).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 150,
                    "end": 156,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Audience partisan diversity mitigates selective exposure to misinformation"
        },
        {
            "text": "We also observe that CF+D has strong positive effects for users who identify as Republicans or lean Republican (Fig. 6(b) ) and for those who are the most active news consumers in terms of both total consumption ( Bars represent the standard error of the mean of each stratum. Change in trustworthiness \u2206Q based on scores from NewsGuard [37] .",
            "cite_spans": [
                {
                    "start": 337,
                    "end": 341,
                    "text": "[37]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [
                {
                    "start": 111,
                    "end": 121,
                    "text": "(Fig. 6(b)",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Audience partisan diversity mitigates selective exposure to misinformation"
        },
        {
            "text": "nearest neighbor similarities, we find that CF+D results in improvements for the users whose browsing behavior is most similar to others in their neighborhood and who might thus be most at risk of \"echo chamber\" effects ( Fig. 6(f) ). Finally, when we group users by the trustworthiness of the domains they visit, we find that the greatest improvements from the CF+D algorithm occur for users who are exposed to the least trustworthy information (Fig. 6(g) ). By contrast, the standard CF algorithm often recommends websites that are less trustworthy than those that respondents actually visit (\u2206Q < 0).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 222,
                    "end": 231,
                    "text": "Fig. 6(f)",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 446,
                    "end": 456,
                    "text": "(Fig. 6(g)",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Audience partisan diversity mitigates selective exposure to misinformation"
        },
        {
            "text": "The findings presented here suggest that the ideological diversity of the audience of a news source is a reliable indicator of its journalistic quality. To obtain these findings, we combined source reliability ratings compiled by expert journalists with traffic data from the YouGov Pulse panel. Of course, we are not the first to study the information diets of Internet users. Prior work has leveraged Web traffic data to pursue related topics such as identifying potential dimensions of bias of news sources [38, 42] , designing methods to present diverse political opinions [34, 35] , and measuring the prevalence of filter bubbles [10] . Unlike these studies, however, we focus on how to promote exposure to trustworthy information rather than seeking to quantify or reduce different sources of bias.",
            "cite_spans": [
                {
                    "start": 510,
                    "end": 514,
                    "text": "[38,",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 515,
                    "end": 518,
                    "text": "42]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 577,
                    "end": 581,
                    "text": "[34,",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 582,
                    "end": 585,
                    "text": "35]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 635,
                    "end": 639,
                    "text": "[10]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "DISCUSSION"
        },
        {
            "text": "A number of limitations must be acknowledged. First, our current methodology, which is based on reliability ratings compiled at the level of individual sources, does not allow us to evaluate the quality of specific articles that participants saw. However, even a coarse signal about source quality could still be useful for ranking a newsfeed given that information about reliability is more widely available at the publisher level than the article level. Another limitation is that our data lack information about actual engagement. Though we show that our re-ranking procedure is associated with a minimal loss in predictive accuracy, it remains an open question whether diversity-based rankings lead not just to higher exposure with trustworthy content, but also to more engagement with it. More research is needed to tease apart the causal link between political attitudes, readership, engagement, and information quality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "DISCUSSION"
        },
        {
            "text": "Our work has a number of implications for the integrity of the online information ecosystem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "DISCUSSION"
        },
        {
            "text": "First, our findings suggest that search engines and social media platforms should consider including audience diversity to their existing set of news quality signals. Such a change could be especially valuable for domains for which we lack other signals about their quality like source reliability ratings compiled by experts. Media ratings systems such as NewsGuard could also benefit from adopting our diversity metric, for example to help screen and prioritize domains for manual evaluation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "DISCUSSION"
        },
        {
            "text": "Critics may raise concerns that such a change in ranking criteria would result in unfair outcomes, for example by reducing exposure to content by certain partisan groups but not others. To see whether ranking by diversity leads to any differential treatment for different partisan news sources, we compute the rate of false positives due to re-ranking by diversity. Here the false positive rate is defined as the conditional probability that CF+D does not rank a trustworthy domain among the top k recommendations while CF does for both left-and right-leaning domains. To determine whether a domain is trustworthy we rely on the classification provided by NewsGuard (i.e. the domain has a reliability score \u2265 60). Fig. 7 shows the rate of false positives as a function of k of both left-and right-leaning domains averaged over all users. Despite some small differences, especially",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 714,
                    "end": 720,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "DISCUSSION"
        },
        {
            "text": "for low values of k, we find no consistent evidence that this change would produce systematically differential treatment across partisan groups.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "DISCUSSION"
        },
        {
            "text": "Another concern is the possibility of abuse. For example, an attacker could employ a number of automated accounts to collectively engage with an ideologically diverse set of sources. This inauthentic, ideologically diverse audience could then be used to push specific content the attacker wants to promote atop the rankings of a recommender system. Similarly, an attacker who wanted to demote a particular content could craft an inauthentic audience with low diversity. Fortunately, there is a vast literature on the topic of how to defend recommender systems against such \"shilling\" attacks [21, 30] and platforms already collect a wealth of signals to detect and remove inauthentic coordinated behavior of this kind. Future work should investigate the feasibility of creating trusted social media audiences that are modeled on existing efforts in marketing research using panels of consumers. We hope that our result stimulates further research in this area.",
            "cite_spans": [
                {
                    "start": 592,
                    "end": 596,
                    "text": "[21,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 597,
                    "end": 600,
                    "text": "30]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "DISCUSSION"
        },
        {
            "text": "Our analysis combines two sources of data. The first is the NewsGuard News Website Reliability Index [37] , a list of web domain reliability ratings compiled by a team of professional journalists and news editors. The data that we licensed for research purposes includes scores of 3,765 web domains on a 100-point scale based on a number of journalistic criteria such as editorial responsibility, accountability, and financial transparency. 1 NewsGuard categorizes web domains into four main groups: \"Green\" domains, which have a score of 60 or more points and are considered reliable;",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 105,
                    "text": "[37]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "A. Data"
        },
        {
            "text": "\"Red\" domains, which score less than 60 points and are considered unreliable; \"Satire\" domains, which should not be regarded as news sources regardless of their score; and \"Platform\" domains like Facebook or YouTube that primarily host content generated by users. The mean reliability score for domains in the data is 69.6; the distribution of scores is shown in Fig. 8 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 363,
                    "end": 369,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF9"
                }
            ],
            "section": "A. Data"
        },
        {
            "text": "The second data source is the YouGov Pulse panel, a sample of U.S.-based Internet users whose web traffic was collected in anonymized form with their prior consent. This traffic data was collected during seven periods between October 2016 and March 2019 (see Table I We perform a number of pre-processing steps on this data. We combine all waves into a single sample. We pool web traffic for each domain that received thirty or more unique visitors. Finally, we use the self-reported partisanship of the visitors (on a seven-point scale from an online survey)",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 259,
                    "end": 266,
                    "text": "Table I",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "A. Data"
        },
        {
            "text": "to estimate mean audience partisanship and audience partisan diversity, which we estimate using ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Data"
        },
        {
            "text": "To measure audience partisan diversity, first define N j as the count of participants who visited a web domain and reported their political affiliation to be equal to j for j = 1, . . . , 7 (where 1 = strong Democrat and 7 = strong Republican). The total number of participants who visited the domain is thus N = j N j , and the fraction of participants with a partisanship value of j is p j = N j /N . Denote the partisanship of the i-th individual as s i . We calculate the following metrics to measure audience partisan diversity: The above metrics all capture the idea that the partisan diversity of the audience of a web domain should be reflected in the distribution of its traffic across different partisan groups. Each weighs the contribution of each individual person who visits the domain equally; they can thus be regarded as user-level measures of audience partisan diversity. However, the volume and content of web browsing activity is highly heterogeneous across internet users [19, 33] , with different users recording different numbers of pageviews to the same website. To account for this imbalance, we also compute the pageview-level, weighted variants of the above audience partisan diversity metrics where, instead of treating all visitors equally, each individual visitor is weighted by the number of pageviews they made to any given domain.",
            "cite_spans": [
                {
                    "start": 992,
                    "end": 996,
                    "text": "[19,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "text": "33]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "B. Definition of audience partisan diversity"
        },
        {
            "text": "As a robustness check, we compare the strength of association of each of these metrics to news reliability in the Supplementary Materials. We find that all variants correlate with news reliability, but the relationship is strongest for variance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Definition of audience partisan diversity"
        },
        {
            "text": "In general, a recommendation algorithm takes a set of users U and a set of items D and learns a function f : U \u00d7 D \u2192 R that assigns a real value to each user-item pair (u, d) representing the interest of user u in item d. This value denotes the estimated rating that user u will give to item d. In the context of the present study, D is a set of news sources identified by their web domains (e.g., nytimes.com, wsj.com), so from now on we will refer to d \u2208 D interchangeably as either a web domain or a generic item.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "Collaborative filtering is a classic recommendation algorithm in which some ratings are provided as input and unknown ratings are predicted based on those known input ratings. In particular, the user-based CF algorithm, which we employ here, seeks to provide the best recommendations for users by learning from others with similar preferences. CF therefore requires a user-domain matrix where each entry is either known or needs to be predicted by the algorithm. Once the ratings are predicted, the algorithm creates a ranked list of domains for each user that are sorted in descending order by their predicted ratings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "To test the standard CF algorithm and our modified CF+D algorithm, we first construct a user-domain matrix V from the YouGov Pulse panel. The YouGov Pulse dataset does not provide user ratings of domains, so we instead count the number of times \u03c0 u,d \u2208 Z + a user u has visited a domain d (i.e., pageviews) and use this variable as a proxy [28] . Because this quantity is known to follow a very skewed distribution, we compute the rating as the TF-IDF of the pageview counts:",
            "cite_spans": [
                {
                    "start": 340,
                    "end": 344,
                    "text": "[28]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "where \u03c0 = u d \u03c0 u,d is the total number of visits. Note that if a user has never visited a particular domain, then v u,d = 0. Therefore, if we arrange all the ratings into a user-domain matrix V \u2208 R |U |\u00d7|D| , such that (V ) u,d = v u,d , we will obtain a sparse matrix. The goal of any recommendation task is to complete the user-domain matrix by predicting the missing ratings, which in turn allows us to recommend new web domains to users that may not have seen them. In this case, however, we lack data on completely unseen domains. To test the validity of our methods, we therefore follow the customary practice in machine learning of setting aside some data to be used purely for testing (see Methods D).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "Having defined V , the next step of the algorithm is to estimate the similarity between each pair of users. To do so, we use either the Pearson correlation coefficient or the Kendall rank correlation of their user vectors; i.e., their corresponding row vectors in V (i.e., zeroes included). For example, if \u03c4 (\u00b7, \u00b7) \u2208 [\u22121, 1] denotes the Kendall rank correlation coefficient between two sets of observations, then the corresponding coefficient of similarity between u \u2208 U and u \u2208 U can be defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "where V u , V u \u2208 R 1\u00d7|U | are the row vectors of u and u , respectively. A similar definition can be used for Pearson's correlation coefficient in place of \u03c4 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "These similarity coefficients are in turn used to calculate the predicted ratings. In the standard user-based CF, the predicted rating of a user u for a domain d is calculated as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "where N u d \u2286 U is the set of the n = 10 most similar users to u who have also rated d (i.e., the neighbors of u), v u ,d is the observed rating (computed with Eq. 2) that neighboring user u has given to domain d,v u andv u are the average ratings of u and u across all domains they visited, respectively, and sim(u, u ) is the similarity coefficient (computed with Eq. 3) between users u and u based on either the Pearson or the Kendall correlation coefficient.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "Having defined the standard CF in Eq. 4, we now define our variant CF+D, which incorporates audience partisan diversity of domain d \u2208 D as a re-ranking signal in the following way:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "where g (\u03b4 d ) is the re-ranking term of domain d, obtained by plugging the audience partisan diversity \u03b4 d (for example, we use the variance of the distribution of self-reported partisan slants of its visitors,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "into a standard logistic function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "In Eq. 6, parameters a, \u03c8, and t generalize the upper asymptote, inverse growth rate, and location of the standard logistic function, respectively. For the results reported in this study we empirically estimate the location as t =\u03b4, the average audience partisan diversity across all domains, which corresponds to the value of\u03b4 = 4.25 since we measure diversity as the variance of the distribution of self-reported partisan slants. For the remaining parameters, we choose a = 1, \u03c8 = 1. As a robustness check, we re-ran all analyses with a larger value of a and obtained qualitatively similar results (available upon reasonable request).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Incorporating audience partisan diversity into collaborative filtering recommendations"
        },
        {
            "text": "To evaluate both recommendation algorithms, we follow a standard supervised learning workflow. We use precision and root mean squared error (RMSE), two standard metrics used to measure the relevance and accuracy of predicted ratings in supervised learning settings. We define these two metrics elsewhere (see Methods G). Here, we instead describe the workflow we followed to evaluate the recommendation methods. Since our approach is based on supervision, we need to designate some of the user ratings (i.e., the number of visits to each domain, which are computed using Eq. 2) as ground truth to compute performance metrics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Supervised learning evaluation workflow"
        },
        {
            "text": "For each user, we randomly split the domains they visited into a training set (70%) and a testing set (30%). This splitting varies by user: the same domain could be included in the training set of a user and in the testing set of another. Then, given any two users, their training set ratings are used to compute user-user similarities using Eq. 3 (which is based on Kendall's rank correlation coefficient; a similar formula can be defined using Pearson's correlation). If, in computing user-user similarities with Eq. 3, a domain is present for a user but not for the other, then the latter rating is assumed to be zero regardless of whether the domain is present in testing or not. This assumption, which follows standard practice in collaborative filtering algorithm, ensures that there is no leaking of information between the test and training sets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Supervised learning evaluation workflow"
        },
        {
            "text": "Finally, using either Eq. 4 or Eq. 5, we predict ratings for domains in the test set and compare them with the TF-IDF of the actual visit counts in the data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Supervised learning evaluation workflow"
        },
        {
            "text": "We also generate ranked lists for users based on global domain popularity (user-level) as an additional baseline recommendation technique. All the domains are initially assigned a rank (global popularity rank) according to their user-level popularity, which is calculated from the training set views. Then, the domains in the test set of each user are ranked according to their global popularity ranks to generate the recommendations. This method does not include any personalization as the rank of a domain for a particular user does not depend on other similar users but depends on the whole population. In particular, if two users share the same two domains in testing, their relative ranking is preserved, even if the two users visited different domains in training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Recommendation based on global popularity"
        },
        {
            "text": "In addition to standard metrics of accuracy (precision and RMSE; see Methods G), we define a new metric called trustworthiness to measure the news reliability of the recommended domains.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F. Trustworthiness metrics"
        },
        {
            "text": "It is calculated using NewsGuard scores in two ways: either using the numerical scores or the set of binary indicators for whether a site meets or exceeds the threshold score of 60 defined by NewsGuard as indicating that a site is generally trustworthy [37] . Let d 1 , d 2 , . . . , d k be a ranked list of domains. Using numerical scores, the trustworthiness is the average:",
            "cite_spans": [
                {
                    "start": 253,
                    "end": 257,
                    "text": "[37]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "F. Trustworthiness metrics"
        },
        {
            "text": "where Q(d) \u2208 [0, 100] denotes the NewsGuard reliability score of d \u2208 D.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F. Trustworthiness metrics"
        },
        {
            "text": "If instead we use the binary indicator of trustworthiness provided by Newsguard, then the trustworthiness of domains in a list is defined as the fraction of domains that meet or exceed the threshold score. Note that, unlike precision and RMSE, the trustworthiness of a list of recommendations does not use information on the actual ratings v u,d . Instead, using Eq. 7, we compute the trustworthiness of the domains in the test set ranked in decreasing order of user visits v u,d . We then compare the trustworthiness of the rankings obtained with either CF or CF+D against the trustworthiness of this baseline.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F. Trustworthiness metrics"
        },
        {
            "text": "Given a user u, let us consider a set D of web domains for which |D| = D. For each domain d \u2208 D, we have three pieces of information: the two predicted ratingsv CF u,d andv CF+D u,d produced by CF and CF+D and the actual rating v u,d (defined elsewhere; see Methods C). In the following, we omit the subscript u of the user, which is fixed throughout, and the CF/CF+D superscript unless it is not obvious from context.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G. Accuracy metrics"
        },
        {
            "text": "Let us consider a given recommendation method (either CF or CF+D) and denote with r(d) (respectively, r (d)) the rank of d when the domains are sorted by decreasing order of recommendation and actual ratings, respectively. Given a recommendation list length 0 < k \u2264 D, let us define the set of predicted domains as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G. Accuracy metrics"
        },
        {
            "text": "and the set of actual domains as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G. Accuracy metrics"
        },
        {
            "text": "Then the precision for a given value of k is given by the fraction of correctly predicted domains:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G. Accuracy metrics"
        },
        {
            "text": "Similarly, the root mean squared error for a given value of k between the two ranked lists of ratings is computed as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G. Accuracy metrics"
        },
        {
            "text": "where \u03c1 : [D] \u2192 D (respectively \u03c1 ) is the inverse function of r(\u00b7) (respectively, r (\u00b7)); that is, the function that maps ranks back to their domain by the recommendation method (respectively, by actual visits). Note that, in the summation, \u03c1(r) and \u03c1 (r) do not generally refer to the same web domain: the averaging is over the two ranked lists of ratings, not over the set of domains in common between the two lists.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "G. Accuracy metrics"
        },
        {
            "text": "To measure the effect of CF+D on the trustworthiness of rankings, we must select a particular list length k. Although Fig. 4 shows improvements for all values of k, one potential problem when stratifying on different groups of users is that the results could depend on the particular choice of k. To avoid dependence on k, we consider a probabilistic model of a hypothetical user visiting web domains from a ranked list of recommendations and define overall trustworthiness as the expected value of the trustworthiness of domains selected from that list (i.e., discounted by probability of selection).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 118,
                    "end": 124,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "H. Discounting via ranking"
        },
        {
            "text": "Let us consider a universe of domains D as the set of items to rank. Inspired by prior approaches on stochastic processes based on ranking [11] , we consider a discounting method that posits that the probability of selecting domain d \u2208 D from a given ranked recommendation list decays as a power law of its rank in the list:",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 143,
                    "text": "[11]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "H. Discounting via ranking"
        },
        {
            "text": "where X \u2208 D is a random variable denoting the probabilistic outcome of the selection from the ranked list, r d \u2208 N is the rank of a generic d \u2208 D, and \u03b1 \u2265 0 is the exponent of power-law decay (when \u03b1 = 0, all domains are equally likely; when \u03b1 > 0, top-ranked domains are more likely to be selected).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H. Discounting via ranking"
        },
        {
            "text": "This procedure allows us to compute, for any given user, the effect of a recommendation method (either CF or CF+D) simply as the difference between its expected trustworthiness and the trustworthiness of the ranking obtained by sorting the domains visited by the user in decreasing order of pageviews (see Eq. 1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H. Discounting via ranking"
        },
        {
            "text": "In practice, to compute Eq. 1, let d 1 , d 2 , . . . , d k and d 1 , d 2 , . . . , d k be two ranked lists of domains, d r , d r \u2208 D \u2200r = 1, . . . , k, generated by a recommendation algorithm and by actual user pageviews, respectively, and let us denote with Q(d) the NewsGuard reliability score of d \u2208 D (see Methods F).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H. Discounting via ranking"
        },
        {
            "text": "Recall that Eq. 8 specifies the probability of selecting a given domain d \u2208 D from a particular ranked list as a function of its rank. Even though any pair of equally-ranked domains will be different across these two lists (that is, d r = d r in general), their probability will be the same because Eq. 8",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H. Discounting via ranking"
        },
        {
            "text": "only depends on r. We can thus calculate the expected improvement in trustworthiness as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H. Discounting via ranking"
        },
        {
            "text": "where P (r) is the probability of selecting a domain with rank r from Eq. (8), which we computed setting \u03b1 = 1. The raw data that support the findings of this study are available from NewsGuard Technology, Inc. but restrictions apply to the availability of these data, which were used under license for the current study and thus cannot be made publicly available. However, data are available from the authors upon reasonable request subject to licensing from NewsGuard. The data used in this study were current as of November 12, 2019 and do not reflect NewsGuard's regular updates of the data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "H. Discounting via ranking"
        },
        {
            "text": "\"Political audience diversity and news reliability in algorithmic ranking \"",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supplementary Materials for Bhadani et al.,"
        },
        {
            "text": "We repeat the analysis of Fig. 3 for all diversity metrics (see Methods B) and summarize the results in Table S1 . For each metric, we estimate the degree of linear association with news quality using the Pearson correlation coefficient. We also report the R 2 coefficient of determination and the two-sided p-value of the F-statistic as a measure of significance of the fit. And finally, we",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 26,
                    "end": 32,
                    "text": "Fig. 3",
                    "ref_id": null
                },
                {
                    "start": 104,
                    "end": 112,
                    "text": "Table S1",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "S1. ALTERNATIVE DEFINITIONS OF AUDIENCE DIVERSITY"
        },
        {
            "text": "show the partial correlation coefficient by controlling the mean partisanship and the extremity of domains. Each metric is positively correlated with quality at the user level, but we find that the relationship is strongest for variance of audience partisanship. At the pageview level, however, the association disappears for all metrics but variance, which still produces a modest correlation. Fig. 3 in the main text we show the relationship between NewsGuard reliability scores of news domains and audience partisan diversity, via linear regression. In Tables S2-S3 we report the associated summary tables for both the user and pageview level, respectively. To ensure the regression coefficients and associated errors can be comparable across datasets, we standardize all independent variables prior to fitting the models to the data. In this sense, they represent a true forecasting exercise. Despite a slightly larger loss of precision relative to CF (compare the left panel of Fig. 5 in the main text with the left panel of Fig. S7 ), our results remain qualitatively consistent with those shown in the main text. For the prior Figs. 4, 5, S1, S2, S3 and S4, the data for each user are randomly split into a training (70%) and",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 395,
                    "end": 401,
                    "text": "Fig. 3",
                    "ref_id": null
                },
                {
                    "start": 556,
                    "end": 578,
                    "text": "Tables S2-S3 we report",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 983,
                    "end": 989,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1030,
                    "end": 1037,
                    "text": "Fig. S7",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "S1. ALTERNATIVE DEFINITIONS OF AUDIENCE DIVERSITY"
        },
        {
            "text": "testing set (30%), so that, for any given user, there is no overlap between the two sets. Note that each user is split independently of the others, so a given domain can appear in the training set of one user and in the testing set of another. Instead, in Figs. S6 and S7, the data of traffic that took place before a fixed boundary date (which is identical for all users) form the training set, and those that took place after form the testing set. This means that the same domain can occur in both the training and the testing set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "S1. ALTERNATIVE DEFINITIONS OF AUDIENCE DIVERSITY"
        },
        {
            "text": "Data collection for the YouGov Pulse panel took place in 7 different time periods (see Table I adding a term that depends on diversity (see Eq. 5), we simulate this process by simply shuffling the diversity terms among the items before ranking them. This procedure ensures that we consider only lists obtained by shifting the ratings by the same amount of CF+D. Fig. S8 shows the sampling distribution of the precision of re-rankings of the same magnitude as those of CF+D using this process for k = 1 and k = 10. To sample from this distribution, we rank domains using the ratings computed from Eq. 4. We then compute in a separate labeled vector the diversity term g(\u03b4 d ) obtained using the logistic function (Eq. 6), reshuffle the labels at random, obtaining for each term a new label d , and finally apply the reshuffled term g(\u03b4 d ) as in Eq. 5. We then re-rank based on the new ratings and compute the precision of the ranked list. This reshuffling is carried out separately for each user with at least k domains in their testing set. The precision is then averaged over all users. This procedure is repeated 1,000 times to obtain the sampling distribution. Finally, we compute a one-tailed p-value by finding the proportion of samples that have a precision higher than the observed value for CF+D.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 87,
                    "end": 94,
                    "text": "Table I",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 362,
                    "end": 369,
                    "text": "Fig. S8",
                    "ref_id": "FIGREF9"
                }
            ],
            "section": "S1. ALTERNATIVE DEFINITIONS OF AUDIENCE DIVERSITY"
        },
        {
            "text": "e. Stratification analysis without discounting. Fig. S9 -S15 show the results of the stratification analysis without using the discounting model. from Bakshy et al. [5] ) and by length of ranked list k. In this and the following plots, bars represent the standard error of the mean. Change in trustworthiness \u2206Q based on scores from NewsGuard [37] .",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 168,
                    "text": "[5]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 343,
                    "end": 347,
                    "text": "[37]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [
                {
                    "start": 48,
                    "end": 55,
                    "text": "Fig. S9",
                    "ref_id": null
                }
            ],
            "section": "S1. ALTERNATIVE DEFINITIONS OF AUDIENCE DIVERSITY"
        },
        {
            "text": "FIG. S14. Effect of CF and CF+D versus baseline by average user-user similarity with nearest n = 10 neighbors in training set (terciles) and by length of ranked list k. Change in trustworthiness \u2206Q based on scores from NewsGuard [37] .",
            "cite_spans": [
                {
                    "start": 229,
                    "end": 233,
                    "text": "[37]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "S1. ALTERNATIVE DEFINITIONS OF AUDIENCE DIVERSITY"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Social media and fake news in the 2016 election",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Allcott",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gentzkow",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Journal of Economic Perspectives",
            "volume": "31",
            "issn": "2",
            "pages": "211--247",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Evaluating the fake news problem at the scale of the information ecosystem",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Allen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Howland",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mobius",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rothschild",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Watts",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Science Advances",
            "volume": "",
            "issn": "14",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Exposure to social engagement metrics increases vulnerability to misinformation",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Avram",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Micallef",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Patil",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Harvard Kennedy School Misinformation Review",
            "volume": "",
            "issn": "5",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Exposure to ideologically diverse news and opinion on Facebook",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Bakshy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Messing",
                    "suffix": ""
                },
                {
                    "first": "Adamic",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Science",
            "volume": "348",
            "issn": "6239",
            "pages": "1130--1132",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Neutral bots reveal political bias on social media",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Pacheco",
                    "suffix": ""
                },
                {
                    "first": "K.-C",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "A survey on trust modeling",
            "authors": [
                {
                    "first": "J.-H",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "Adali",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ACM Comput. Surv",
            "volume": "48",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "How algorithmic popularity bias hinders or promotes quality",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "L"
                    ],
                    "last": "Ciampaglia",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nematzadeh",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Flammini",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Scientific Reports",
            "volume": "8",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Prioritizing original news reporting on facebook",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Fb",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Filter bubbles, echo chambers, and online news consumption",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Flaxman",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Goel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Rao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Public opinion quarterly",
            "volume": "80",
            "issn": "S1",
            "pages": "298--320",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Scale-free network growth by ranking",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Fortunato",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Flammini",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Physical Review Letters",
            "volume": "96",
            "issn": "21",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "The few-get-richer: A surprising consequence of popularity-based rankings?",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Germano",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "G\u00f3mez",
                    "suffix": ""
                },
                {
                    "first": "Le",
                    "middle": [],
                    "last": "Mens",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The World Wide Web Conference, WWW '19",
            "volume": "",
            "issn": "",
            "pages": "2764--2770",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Anatomy of the long tail: Ordinary people with extraordinary tastes",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Goel",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Broder",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Gabrilovich",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM '10",
            "volume": "",
            "issn": "",
            "pages": "201--210",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Computing and applying trust in web-based social networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Golbeck",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Surfacing useful and relevant content -how news works",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Google",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Avoiding the echo chamber about echo chambers: Why selective exposure to like-minded political news is less prevalent than you think",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Guess",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Lyons",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Nyhan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Reifler",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Less than you think: Prevalence and predictors of fake news dissemination on Facebook",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Guess",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Nagler",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tucker",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Science Advances",
            "volume": "5",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "(almost) everything in moderation: New evidence on americans' online media diets",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Guess",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Exposure to untrustworthy websites in the 2016 US election",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Guess",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Nyhan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Reifler",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature Human Behaviour",
            "volume": "4",
            "issn": "5",
            "pages": "472--480",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Shilling attacks against recommender systems: a comprehensive survey",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Gunes",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kaleli",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bilge",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Polat",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Artificial Intelligence Review",
            "volume": "42",
            "issn": "4",
            "pages": "767--799",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "TweetCred: Real-Time Credibility Assessment of Content on Twitter",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kumaraguru",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Castillo",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Meier",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "228--243",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Feeling validated versus being correct: a meta-analysis of selective exposure to information",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hart",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Albarrac\u00edn",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "H"
                    ],
                    "last": "Eagly",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Brechan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Lindberg",
                    "suffix": ""
                },
                {
                    "first": "Merrill",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Psychological Bulletin",
            "volume": "135",
            "issn": "4",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Disentangling the effects of social signals",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hogg",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lerman",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "2",
            "issn": "",
            "pages": "189--208",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Groups of diverse problem solvers can outperform groups of highability problem solvers",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Page",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proceedings of the National Academy of Sciences",
            "volume": "101",
            "issn": "46",
            "pages": "16385--16389",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Factoring Fact-Checks: Structured Information Extraction from Fact-Checking Articles",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Baumgartner",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ittycheriah",
                    "suffix": ""
                },
                {
                    "first": "Yu",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1592--1603",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Linguistic signals under misinformation and fact-checking: Evidence from user comments on social media",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wilson",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. ACM Hum.-Comput. Interact",
            "volume": "2",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Accurately interpreting clickthrough data as implicit feedback",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Joachims",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Granka",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hembrooke",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Gay",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "SIGIR Forum",
            "volume": "51",
            "issn": "1",
            "pages": "4--11",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Grou-pLens: Applying collaborative filtering to usenet news",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Konstan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "N"
                    ],
                    "last": "Miller",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Maltz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "L"
                    ],
                    "last": "Herlocker",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "R"
                    ],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Riedl",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Commun. ACM",
            "volume": "40",
            "issn": "3",
            "pages": "77--87",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Shilling recommender systems for fun and profit",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "K"
                    ],
                    "last": "Lam",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Riedl",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proceedings of the 13th International Conference on World Wide Web, WWW '04",
            "volume": "",
            "issn": "",
            "pages": "393--402",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "The science of fake news",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lazer",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Baum",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Benkler",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Berinsky",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Greenhill",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Metzger",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Nyhan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pennycook",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rothschild",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schudson",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sloman",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Sunstein",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Thorson",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Watts",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zittrain",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Science",
            "volume": "359",
            "issn": "6380",
            "pages": "1094--1096",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Opinion cascades and the unpredictability of partisan polarization",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Macy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Deri",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ruch",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Tong",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Science Advances",
            "volume": "",
            "issn": "8",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Identifying web browsing trends and patterns",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Montgomery",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Faloutsos",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Computer",
            "volume": "34",
            "issn": "7",
            "pages": "94--95",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Encouraging reading of diverse political viewpoints with a browser widget",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Munson",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Resnick",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "International AAAI Conference on Web and Social Media, ICWSM '13",
            "volume": "",
            "issn": "",
            "pages": "419--428",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Presenting Diverse Political Opinions: How and How Much",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Munson",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Resnick",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1457--1466",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Entropy and inference, revisited",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Nemenman",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Shafee",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Bialek",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, NIPS'01",
            "volume": "",
            "issn": "",
            "pages": "471--478",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Rating process and criteria",
            "authors": [
                {
                    "first": "Inc",
                    "middle": [],
                    "last": "Newsguard",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Retrieved from Internet Archive",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Quantifying biases in online information exposure",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Nikolov",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lalmas",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Flammini",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of the Association for Information Science and Technology",
            "volume": "70",
            "issn": "3",
            "pages": "218--229",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Fighting misinformation on social media using crowdsourced judgments of news source quality",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pennycook",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "G"
                    ],
                    "last": "Rand",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the National Academy of Sciences",
            "volume": "116",
            "issn": "",
            "pages": "2521--2526",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Truth of varying shades: Analyzing language in fake news and political fact-checking",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Rashkin",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "Y"
                    ],
                    "last": "Jang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Volkova",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "2931--2937",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "GroupLens: An open architecture for collaborative filtering of netnews",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Resnick",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Iacovou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Suchak",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bergstrom",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Riedl",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Proceedings of the 1994 ACM Conference on Computer Supported Cooperative Work, CSCW '94",
            "volume": "",
            "issn": "",
            "pages": "175--186",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Media bias monitor: Quantifying biases of social media news outlets at large-scale",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ribeiro",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Henrique",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Benevenuto",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chakraborty",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kulshrestha",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Babaei",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Gummadi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media",
            "volume": "",
            "issn": "",
            "pages": "290--299",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Experimental study of inequality and unpredictability in an artificial cultural market",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Salganik",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Dodds",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Watts",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Science",
            "volume": "311",
            "issn": "5762",
            "pages": "854--856",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Detection of novel social bots by ensembles of specialized classifiers",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sayyadiharikandeh",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Varol",
                    "suffix": ""
                },
                {
                    "first": "K.-C",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Flammini",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proc. 29th ACM International Conference on Information & Knowledge Management (CIKM)",
            "volume": "",
            "issn": "",
            "pages": "2725--2732",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "The spread of low-credibility content by social bots",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "L"
                    ],
                    "last": "Ciampaglia",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Varol",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Flammini",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Nature Communications",
            "volume": "9",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "The wisdom of polarized crowds",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Teplitskiy",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Duede",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Evans",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Nature Human Behaviour",
            "volume": "3",
            "issn": "4",
            "pages": "329--336",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Sorting the news: How ranking by popularity polarizes our politics",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shmargad",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Klar",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Political Communication",
            "volume": "37",
            "issn": "3",
            "pages": "423--446",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Bots increase exposure to negative and inflammatory content in online social systems",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Stella",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ferrara",
                    "suffix": ""
                },
                {
                    "first": "De",
                    "middle": [],
                    "last": "Domenico",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the National Academy of Sciences",
            "volume": "115",
            "issn": "49",
            "pages": "12435--12440",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Online human-bot interactions: Detection, estimation, and characterization",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Varol",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ferrara",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Davis",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Flammini",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. Eleventh Intl AAAI Conference on Web and Social Media, ICWSM '17",
            "volume": "",
            "issn": "",
            "pages": "280--289",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "The spread of true and false news online",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vosoughi",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "Aral",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Science",
            "volume": "359",
            "issn": "6380",
            "pages": "1146--1151",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Prevalence of low-credibility information on Twitter during the COVID-19 outbreak",
            "authors": [
                {
                    "first": "K.-C",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Torres-Lugo",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proc. Fourteenth Intl AAAI Conference on Web and Social Media, ICWSM '20",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Arming the public with artificial intelligence to counter social bots",
            "authors": [
                {
                    "first": "K.-C",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Varol",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "A"
                    ],
                    "last": "Davis",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ferrara",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Flammini",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Human Behavior and Emerging Technologies",
            "volume": "1",
            "issn": "1",
            "pages": "48--61",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Scalable and generalizable social bot detection through data selection",
            "authors": [
                {
                    "first": "K.-C",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Varol",
                    "suffix": ""
                },
                {
                    "first": "P.-M",
                    "middle": [],
                    "last": "Hui",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Menczer",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proc. 34th AAAI Conf. on Artificial Intelligence (AAAI)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "A structured response to misinformation: Defining and annotating credibility indicators in news articles",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "X"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ranganathan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Metz",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Appling",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "Sehat",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Gilmore",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "B"
                    ],
                    "last": "Adams",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Vincent",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Robbins",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Bice",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hawke",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Karger",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "X"
                    ],
                    "last": "Mina",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Companion Proc. of The Web Conference 2018, WWW '18",
            "volume": "",
            "issn": "",
            "pages": "603--612",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Improving recommendation lists through topic diversification",
            "authors": [
                {
                    "first": "C.-N",
                    "middle": [],
                    "last": "Ziegler",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "M"
                    ],
                    "last": "Mcnee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Konstan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lausen",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of the 14th International Conference on World Wide Web, WWW '05",
            "volume": "",
            "issn": "",
            "pages": "22--32",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Right: proportion of domains labeled as 'trustworthy,' also by NewsGuard. Actual visits v are normalized using TF-IDF (see Methods C). Each bin represents the average computed on the topk recommendations for all users in the YouGov panel with \u2265 k recommendations in their test sets. Bars represent the standard error of the mean",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Fig",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "S6",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "37",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Right: RMSE (root mean squared error) of predicted pageviews for top k ranked domains by length of ranked list k (lower is better). Each bin represents the average computed on the top-k recommendations of all users with \u2265 k recommendations in their test sets. Bars represent the standard error of the mean",
            "authors": [
                {
                    "first": "",
                    "middle": [
                        "S7"
                    ],
                    "last": "Fig",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "In this figure, both CF and CF+D compute the similarity between users using the Kendall \u03c4 correlation coefficient (see Methods C)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Distribution of precision obtained after re-ranking the domains, by means of re-shuffling the diversity signal values g(\u03b4 d ) from the CF+D ratings calculation (see Eq. 5 and Eq. 6). The re-shuffling was repeated 1, 000 times. The two distributions correspond to different values of k. The (one-sided) p-values are 0.002 (k = 1) and 0.021 (k = 10)",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Fig",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "S8",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Effect of CF and CF+D versus baseline by self-reported party ID from YouGov Pulse responses as measured on a 7-point scale (1-3: Democrats including people who lean Democrat but do not identify as Democrats, 4: Independents",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Fig",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "S10",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "5--7",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Effect of CF and CF+D versus baseline by absolute slant of visited domains (terciles using scores from Bakshy et al.) and by length of ranked list k. Change in trustworthiness \u2206Q based on scores from NewsGuard",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Fig",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "S11",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "37",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Effect of CF and CF+D versus baseline by total online activity (TF-IDF",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Fig",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "S12",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "terciles) and by length of ranked list k. Change in trustworthiness \u2206Q based on scores from NewsGuard",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "37",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Effect of CF and CF+D versus baseline by distinct number of domains visited (terciles) and by length of ranked list k. Change in trustworthiness \u2206Q based on scores from NewsGuard",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Fig",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "S13",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "37",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "Effect of CF and CF+D versus baseline by baseline trustworthiness of domains visited by users (terciles) and by length of ranked list k. Change in trustworthiness \u2206Q based on scores from NewsGuard",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Fig",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "S15",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "37",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Relationship between audience size and news reliability by domain. Reliability scores provided byNewsGuard[37].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Average audience partisanship versus variance. Left panel: user level. Right panel: pageview level.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "(standardized OLS coefficient of Democrat domains: \u03b2 = 2.77 (0.54) at user level; \u03b2 = 0.68 (0.61) at pageview level), which is consistent with Fig. 2. These results are not affected by popularity. Partisan diversity is weakly correlated with popularity, regardless of the operational definition of either measure (see Supplementary Materials).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Trustworthiness of recommended domains by length of ranked list k. Left: Trustworthiness based on scores from NewsGuard [37]. Right: proportion of domains labeled as 'trustworthy', also by NewsGuard. Actual visits v are normalized using TF-IDF (see Methods C). Global popularity is overall domain popularity (see Methods E). Each bin represents the average computed on the top-k recommendations for all users in the YouGov panel with \u2265 k recommendations in their test sets. Bars represent the standard error of the mean. The values of k are capped so that each bin has \u2265 100 users in it (see Supplementary Materials for plot with all values of k). In this figure, both CF and CF+D compute the similarity between users using the Kendall \u03c4 correlation coefficient (see Methods C). We obtain qualitatively similar results using the Pearson correlation coefficient (see Supplementary Materials). (see Methods F). By varying the number of top domains k, we can evaluate how trustworthiness changes as the length of the list of recommendations increases. In Fig. 4 we plot the trustworthiness of the recommended domains as a function of k. We restrict values of k to 1-28, the values for which there are at least 100 users in each bin (plots spanning the full range are available in Supplementary Materials). Each panel compares the average trustworthiness of domains ranked by CF and CF+D with two baselines. The first is the trustworthiness of websites users visited in the test set ranked by their TF-IDF-transformed number of visits (i.e., pageviews). This baseline captures the trustworthiness of the websites that users in the YouGov Pulse panel actually visited after adjusting for the fact that more popular websites tend to attract more visits in general. The second baseline is the trustworthiness of recommendations produced according to the overall popularity of domains.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Accuracy of domain recommendations by length of ranked list k. Left: Precision (proportion of correctly ranked sites) by length of ranked list k (higher is better). Right: RMSE (root mean squared error) of predicted pageviews for top k ranked domains by length of ranked list k (lower is better). Each bin represents the average computed on the top-k recommendations of all users with \u2265 k recommendations in their test sets. Bars represent the standard error of the mean. The values of k are capped so that each bin has \u2265 100 users in it (see Supplementary Materials for plot with all values of k). In this figure, both CF and CF+D compute the similarity between users using the Kendall \u03c4 correlation coefficient (see Methods C). We obtain qualitatively similar results using the Pearson correlation coefficient (see Supplementary Materials).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "compares the accuracy of CF+D in predicting user visits to domain in the test set with that of CF. To evaluate accuracy, we compute both the fraction of correctly predicted domains (precision) and root mean squared error (RMSE) as a function of the number of recommended domains k (see Methods G for definitions). Note that precision improves with k (left panel) by definition -as k grows, we are comparing an increasingly large set of recommendations with a list of fixed size. Because each bin averages over users with at least k domains in their test set, when k reaches the maximum size of the recommendation list we can make, the precision necessarily becomes 100%. Note that the plots in Fig. 5 do not reach this level -they include only bins with at least 100 users in them -but trend upward with k. (In the Supplementary Materials, we show plots that include results for all values of k.) As with precision, RMSE declines with k (right panel) since we focus progressively on users with longer lists and thus more training data. Like in the left panel, each bin in the right panel averages over users with at least k domains in their test set. Unlike precision, however, RMSE is more prone to producing outliers because it does not depend on the relative ranking of item ratings but instead on their magnitude. This difference is reflected in the sudden drop in the error bars for the RMSE at k = 27 due to the presence of a single user with a maximum list length of 26 domains in testing. We manually checked the data of this user and found that the training set included only domains visited infrequently, leading to large errors. Removing this outlier eliminated the observed change.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Methods H). A value of \u2206Q > 0 indicates that algorithmic recommendations are more trustworthy than what users actually accessed. If \u2206Q < 0, the trustworthiness of a ranked list is lower than the baseline from user visits. (To ensure that the results below are not affected by the discounting method we employ, we report qualitatively similar results obtained without any discounting for a selection of values of k in the Supplementary Materials.)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "d)) and number of distinct sources (Fig. 6(e)). Furthermore, since the two recommendation schemes considered here (CF and CF+D) are predicated on identifying similar users according to their tastes and behaviors, we also segment the users of the YouGov sample according to the degree of similarity with their nearest neighbors (identified based on Kendall's rank correlation coefficient between user vectors; see Methods C). Stratifying on the average of FIG. 6. Effect of CF and CF+D (versus actual visits baseline) on trustworthiness by user characteristics and behavior. (a) Ideological slant of visited domains (terciles using scores from Bakshy et al. [5]). (b) Selfreported party ID from YouGov Pulse responses as measured on a 7-point scale (1-3: Democrats including people who lean Democrat but do not identify as Democrats, 4: Independents, 5-7: Republicans including people who lean Republican but do not identify as Republicans). (c) Absolute slant of visited domains (terciles using scores from Bakshy et al.). (d) Total online activity (TF-IDF-transformed pageviews; terciles).(e) Distinct number of domains visited (terciles). (f) Average user-user similarity with nearest n = 10 neighbors in training set (terciles) (g) Trustworthiness of domains visited by users (in training set; terciles).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Probability that a trustworthy domain (NewsGuard score \u2265 60) is not recommended by CF+D but is recommended by CF for left-and right-leaning domains. Each point is the average over a sample of users. The shaded regions represent the values of k for which the difference is not statistically significant at standard levels (\u03b1 = 0.05, Welch's t-tests with Bonferroni correction for n = 28; all tests are two-sided).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Distribution of NewsGuard scores (N = 3,726) by trustworthiness rating. Domains that score below 60 points (i.e., untrustworthy) on the rubric used by NewsGuard[37] are shown in white. Those that score 60 or above are shown in green. The bin width is 5; the bin containing score 60 also includes a few domains with lower scores. The dashed line indicates the average score in the data.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "where s is average partisanship;Shannon's entropy: S = \u2212 p(j) log p(j), where p(j) is estimated in the following three dif-ferent ways: (i ) p(j) = p j (maximum likelihood); (ii ) p(j) = N j +\u03b1N +7\u03b1 (mean of the posterior distribution of Dirichlet prior with \u03b1 = 1); and (iii ) the method of Nemenman et al. [36], which uses a mixture of Dirichlet priors (NSB prior). Complementary Maximum Probability: 1 \u2212 max j {p j }; Complementary Gini: 1\u2212G where G is the Gini coefficient of the count distribution {N j } j=1...7 .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "we use the self-reported partisanship of respondents in the YouGov Pulse panel as the basis for our diversity signal (see Methods B). To avoid the circular reasoning in stratifying on the same source of data, Fig. 6(a) and Fig. 6(c) group these users according to the slant of their actual news consumption, which may not necessarily reflect their self-reported partisanship (e.g., a self-reported Democrat might access mostly conservative-leaning websites). We determined this latter metric using an external classification originally proposed by Bakshy et al. [5], who estimated the slant of 500 web domains focused on hard news topics. In practice, Bakshy et al. based their classification on how hard news from those domains were shared on Facebook by users who self-identified as liberal or conservative in their profile. For almost all domains, Bakshy et al. reported a value s \u2208 [\u22121, 1] with a value of s = +1 for domains that are shared almost exclusively by conservatives, and a value of s = \u22121 for those shared almost exclusively by liberals. (These values could technically vary over [\u22122, 2] but only 1% of domains fell outside [\u22121, 1] using the measurement approach described by Bakshy et al. [5].) In Fig. 6(c), respondents are grouped according to the absolute slant |s| of the visited domains where a value of |s| = 0 denotes domains with a perfectly centrist slant and a value of |s| = 1 indicates domains with extreme liberal or conservative slants (i.e., they are almost exclusively shared by one group and not the other). AUTHOR CONTRIBUTIONS ETHICS STATEMENT This study was reviewed by the IRB under protocols #HUM00161944 (University of Michigan) and #STUDY000433 (University of South Florida). CODE AND DATA AVAILABILITY Data necessary to reproduce the findings in the manuscript are available, in aggregated and anonymized format, at https://github.com/glciampaglia/InfoDiversity/ along with the associated source code. To reproduce the findings in the Supplementary Materials, additional code and data are available upon reasonable request.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "simplicity we considered only 3 waves (the first three). Figs. S6 and S7 show the analysis performed on the first wave of data collection, which took place between October 7 and November 14, 2016, and we split the data using November 1, 2016 as boundary. We find qualitatively similar results for the second and third waves. (Data available upon reasonable request to the authors.) d. Resampling. To estimate the significance of the observed drop in precision of CF+D, we simulate the process of re-ranking a list of items. Recommendations are obtained in this context by sorting items by their predicted rating. Since CF+D simply shifts the rating of each item by FIG. S1. Trustworthiness of recommended domains by length of ranked list k, for all values of k. Left: Trustworthiness based on scores from NewsGuard [37]. Right: proportion of domains labeled as 'trustworthy,' also by NewsGuard. Actual visits v are normalized using TF-IDF (see Methods C). Each bin represents the average computed on the top-k recommendations for all users in the YouGov panel with \u2265 k recommendations in their test sets. Bars represent the standard error of the mean. In this figure, both CF and CF+D compute the similarity between users using the Kendall \u03c4 correlation coefficient (see Methods C).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Accuracy of domain recommendations by length of ranked list k, for all values of k. Left: Precision (proportion of correctly ranked sites) by length of ranked list k (higher is better). Right: RMSE (root mean squared error) of predicted pageviews for top k ranked domains by length of ranked list k (lower is better). Each bin represents the average computed on the top-k recommendations of all users with \u2265 k recommendations in their test sets. Bars represent the standard error of the mean. In the last bin (k = 73) precision is 100% for all users. In this figure, both CF and CF+D compute the similarity between users using the Kendall \u03c4 correlation coefficient (see Methods C). FIG. S3. Trustworthiness of recommended domains by length of ranked list k, for all values of k. Left: Trustworthiness based on scores from NewsGuard [37]. Right: proportion of domains labeled as 'trustworthy,' also by NewsGuard. Actual visits v are normalized using TF-IDF (see Methods C). All results represent averages computed for all users in the YouGov panel. Bars represent the standard error of the mean. In this figure, both CF and CF+D compute the similarity between users using the Pearson correlation coefficient.38 FIG. S4. Accuracy of domain recommendations by length of ranked list, for all values of k. Left: Precision (proportion of correctly ranked sites) by length of ranked list k (higher is better). Right: RMSE (root mean squared error) of predicted pageviews for top k ranked domains by length of ranked list k (lower is better). Each bin represents the average computed on the top-k recommendations of all users with \u2265 k recommendations in their test sets. Bars represent the standard error of the mean. In the last bin (k = 30) precision is 100% for all users. Bars represent the standard error of the mean. In this figure, both CF and CF+D compute the similarity between users using the Pearson correlation coefficient. FIG. S5. Number of users with k domains in the test set for neighborhoods (the set of the n = 10 most similar users to a given user) computed using the correlation coefficient of Kendall (solid line) and Pearson (dashed line). In general, Pearson leads to shorter lists of recommendations. FIG. S9. Effect of CF and CF+D versus baseline by ideological slant of visited domains (terciles using scores",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "shows how NewsGuard scores vary with both mean audience partisanship and the variance in audience partisanship.FIG. 3. Relationship between audience partisan diversity and news reliability for websites whose average visitor is a Democrat or a Republican. Left panel: variance computed at user level. Right panel: variancecomputed at pageview level. News reliability scores from NewsGuard[37].",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "). A total of 6,890 participants provided data. Overall, this group is diverse and resembles the U.S. population on key seeTable Ifor details by sample collection period). Note that, to be eligible for the study, participants in the YouGov Pulse panel had to be 18+ years of age, so the reported dimensions should be interpreted as being conditional on this extra eligibility criterion.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "YouGov Pulse respondent data summary Note: The participants for each data collection period were different. Some participants took part in multiple waves but overlap was small. different measures described next and evaluated in the Supplementary Materials.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Relationship between audience partisan diversity and news quality.S2. REGRESSION OF NEWSGUARD SCORES ON WEBSITE AUDIENCE VARIANCE",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Relationship between NewsGuard scores and user-level partisan audience diversity Note: Standard errors in parentheses. * p<0.1; * * p<0.05; * * * p<0.01 TABLE S3. Relationship between NewsGuard scores and pageview-level partisan audience diversity Note: Standard errors in parentheses. * p<0.1; * * p<0.05; * * * p<0.01 S3. CORRELATIONS BETWEEN DOMAIN POPULARITY AND AUDIENCE DIVERSITYInTable S4we show the Pearson correlation coefficients between the popularity of a domain and its diversity. We operationalize the popularity of website as either its audience size (i.e., number of unique users) or its traffic (number of pageviews). For our diversity measures, we rely on the user-level and pageview-level partisanship variance.Overall, domain popularity is very weakly correlated with the variance of audience partisanship regardless of how we choose to operationalize each measure. Recall that in our original analysis we show that domain popularity is largely uncorrelated with quality (as proxied by NewsGuard scores). Together, these findings suggest that audience partisan diversity is associated with quality of news independent of the variation caused by domain popularity.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Pearson correlation coefficients between domain diversity and popularity Variance (rows) / Popularity (columns) N Unique users N Pageviews User-level 0.04 (p = 1.39 \u00d7 10 \u22125 ) 0.0093 (p = 0.31) Pageview-level 0.062 (p = 2.1 \u00d7 10 \u221211 ) 0.019 (p = 0.038) Furthermore, we estimate multivariate regressions interacting our diversity measures with an indicator of whether the website has a predominantly Democratic or Republican website with the following model: Reliability = \u03b2 0 + \u03b2 1 (Diversity measure) + \u03b2 2 (Conservative website dummy)+ \u03b2 3 (Logged audience size) + \u03b2 4 (Diversity measure \u00d7 Conservative website dummy) We estimate two separate regression models, using our two operational diversity measures: userlevel and pageview-level partisanship variance. At the user level, after controlling for audience size, a one-standard deviation increase in diversity is associated with a 2.91 point increase in NewsGuard reliability for websites with predominantly Democratic audiences, and with a 10.8 point increase for websites with predominantly Republican audiences. Both estimates are statistically significant. At the pageview level, we find that a one-standard deviation increase is associated with a 0.68 point increase (statistically indistinguishable from zero) in NewsGuard reliability for Democratic websites, and with a 9.24 point increase for Republican websites. In summary, both methods indicate that our diversity measure is a good predictor of journalistic quality, independent of audience size. This relationship is especially strong for websites with predominantly Republican audiences.S4. ROBUSTNESS CHECKS a. No minimum frequency capping. Figs. S1 and S2 are the analogous of Figs. 4 and 5 from the main text, but unlike the plots in the main text, which capped the range of k to include only bins with a minimum frequency, the plots here show all possible values of k. b. Alternative similarity metric based on rank correlation. Figs. S3 and S4 also show the results of analyses analogous to those in Figs. 4 and 5, but unlike the plots in the main text, which used the Kendall rank correlation coefficient to compute the similarity between users, the plots here show the results obtained using the Pearson correlation coefficient. Moreover, the plots here show all possible values of k, without the aforementioned cap. To get a better sense sense of this difference, Fig. S5 shows the distribution of the number of users as a function of the length of the ranked list k. We observe that Pearson tends to produce smaller recommendation lists than Kendall. c. Longitudinal analysis. Figs. S6 and S7 show the results of an analysis analogous to those in Figs. 4 and 5, but in which training and testing sets are split longitudinally instead of randomly.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "We thank NewsGuard for licensing the data and acknowledge Andrew Guess and Jason Reifler, Nyhan's coauthors on the research project that generated the web traffic data used in this study.We are also grateful to organizers, chairs and participants of the News Quality in the Platform Era ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "ACKNOWLEDGEMENTS"
        }
    ]
}
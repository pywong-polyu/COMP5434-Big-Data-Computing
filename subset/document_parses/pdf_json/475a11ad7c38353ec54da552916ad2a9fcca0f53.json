{
    "paper_id": "475a11ad7c38353ec54da552916ad2a9fcca0f53",
    "metadata": {
        "title": "Multi-task UNet: Jointly Boosting Saliency Prediction and Disease Classification on Chest X-ray Images",
        "authors": [
            {
                "first": "Hongzhi",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of British Columbia",
                    "location": {
                        "settlement": "Vancouver",
                        "country": "Canada"
                    }
                },
                "email": "hzhu@ece.ubc.ca"
            },
            {
                "first": "Robert",
                "middle": [],
                "last": "Rohling",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of British Columbia",
                    "location": {
                        "settlement": "Vancouver",
                        "country": "Canada"
                    }
                },
                "email": "rohling@ece.ubc.ca"
            },
            {
                "first": "Septimiu",
                "middle": [],
                "last": "Salcudean",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of British Columbia",
                    "location": {
                        "settlement": "Vancouver",
                        "country": "Canada"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Human visual attention has recently shown its distinct capability in boosting machine learning models. However, studies that aim to facilitate medical tasks with human visual attention are still scarce. To support the use of visual attention, this paper describes a novel deep learning model for visual saliency prediction on chest X-ray (CXR) images. To cope with data deficiency, we exploit the multi-task learning method and tackles disease classification on CXR simultaneously. For a more robust training process, we propose a further optimized multi-task learning scheme to better handle model overfitting. Experiments show our proposed deep learning model with our new learning scheme can outperform existing methods dedicated either for saliency prediction or image classification. The code used in this paper is available at https://github.com/hz-zhu/MT-UNet.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Recent work in machine learning and computer vision have demonstrated advantages of integrating human attention with artificial neural network models, as studies show that many machine vision tasks, i.e., image segmentation, image captioning, object recognition, etc., can benefit from adding human visual attention (Liu and Milanova, 2018) .",
            "cite_spans": [
                {
                    "start": 316,
                    "end": 340,
                    "text": "(Liu and Milanova, 2018)",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Visual attention is the ability inherited in biological visual systems to selectively recognize regions or features on scenes relevant to a specific task (Borji et al., 2012) , where \"bottom-up\" attention (also called exogenous attention) focuses on physical properties in the visual input that are salient and distinguishable, and \"top-down\" attention (also called endogenous attention) generally refers to mental strategies adopted by the visual systems to accomplish the intended visual tasks (Paneri and Gregoriou, 2017) . Early research on saliency prediction aims to understand attentions triggered by visual features and patterns, and thus \"bottom-up\" attention is the research focus (Borji et al., 2012) . More recent attempts, empowered by interdisciplinary efforts, start to study both \"bottom-up\" and \"top-down\" attentions, and therefore the terms, saliency prediction and visual attention prediction, are used interchangeably (Sun et al., 2021) . In this paper, we use the term saliency prediction as the prediction of human visual attentions allocations when viewing 2D images, containing both \"bottom-up\" and \"top-down\" attentions. 2D heatmap is usually used to represent human visual attention distribution. Note that saliency prediction studied in this paper is different from neural network's saliency/attention which can be visualized through class activation mapping (CAM) by Zhou et al. (2016) and other methods (Simonyan et al., 2013; Fu et al., 2019; Selvaraju et al., 2016) . With the establishment of several benchmark datasets, data driven approaches demonstrated major advancements in saliency prediction (review in Borji (2019) and ). However, saliency prediction for natural scenes is the primary focus, and more needs to be done in the medical domain. Hence, we intend to study the saliency prediction for examining chest X-ray (CXR) images, one of the most common radiology tasks worldwide.",
            "cite_spans": [
                {
                    "start": 154,
                    "end": 174,
                    "text": "(Borji et al., 2012)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 496,
                    "end": 524,
                    "text": "(Paneri and Gregoriou, 2017)",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 691,
                    "end": 711,
                    "text": "(Borji et al., 2012)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 938,
                    "end": 956,
                    "text": "(Sun et al., 2021)",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 1395,
                    "end": 1413,
                    "text": "Zhou et al. (2016)",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 1432,
                    "end": 1455,
                    "text": "(Simonyan et al., 2013;",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 1456,
                    "end": 1472,
                    "text": "Fu et al., 2019;",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1473,
                    "end": 1496,
                    "text": "Selvaraju et al., 2016)",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1642,
                    "end": 1654,
                    "text": "Borji (2019)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "CXR imaging is commonly used for the diagnosis of cardio and/or respiratory abnormalities; it is capable of identifying multiple conditions through a single shot, i.e., COVID-19, pneumonia, heart enlargement, etc. (\u00c7 all\u0131 et al., 2021) . There exists multiple public CXR datasets (Irvin et al., 2019; Wang et al., 2017) . However, the creation of large comprehensive medical datasets is labour intensive, and requires significant medical resources which are usually scarce (Castro et al., 2020) . Consequently, medical datasets are rarely as abundant as that for non-medical fields. Thus, machine learning approaches applied on medical datasets need to address the problem of data scarcity. In this paper, we exploit the multitask learning for solution.",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 235,
                    "text": "COVID-19, pneumonia, heart enlargement, etc. (\u00c7 all\u0131 et al., 2021)",
                    "ref_id": null
                },
                {
                    "start": 280,
                    "end": 300,
                    "text": "(Irvin et al., 2019;",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 301,
                    "end": 319,
                    "text": "Wang et al., 2017)",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 473,
                    "end": 494,
                    "text": "(Castro et al., 2020)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Multi-task learning is known for its inductive transfer characteristics that can drive strong representation learning and generalization of each component task (Caruana, 1997) . Therefore, multi-task learning methods partially alleviates some of the major shortcomings in deep learning, i.e., high demands for data sufficiency and heavy computation loads (Crawshaw, 2020) . However, to apply multi-task learning methods successfully, challenges still exist, which can be the proper selection of component tasks, the architecture of the network, the optimization of the training schemes and many others (Zhang and Yang, 2021; Crawshaw, 2020) . This paper investigates the proper configuration of a multi-task learning model that can tackle visual saliency prediction and image classification simultaneously.",
            "cite_spans": [
                {
                    "start": 160,
                    "end": 175,
                    "text": "(Caruana, 1997)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 355,
                    "end": 371,
                    "text": "(Crawshaw, 2020)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 602,
                    "end": 624,
                    "text": "(Zhang and Yang, 2021;",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 625,
                    "end": 640,
                    "text": "Crawshaw, 2020)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The main contributions of this paper are: 1) development of a new deep convolutional neural network (DCNN) architecture for CXR image saliency prediction and classification based on UNet (Ronneberger et al., 2015) , and 2) proposal of an optimized multi-task learning scheme that handles overfitting. Our method aims to outperform the state-of-theart networks dedicated either for saliency prediction or image classification.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 213,
                    "text": "(Ronneberger et al., 2015)",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "DCNN is the leading machine learning method applied to saliency prediction (Pan et al., 2016; K\u00fcmmerer et al., 2016; Jia and Bruce, 2020; Kroner et al., 2020) . Besides, transfer learning with pre-trained networks was observed to boost the performance of saliency prediction (Oyama and Yamanaka, 2017; K\u00fcmmerer et al., 2016; Oyama and Yamanaka, 2018) . A majority of DCNN approaches are for natural scene saliency prediction, and so far, only a few studied the saliency prediction for medical images. By Cai et al. (2018) , the generative adversarial network is used to predict expert sonographer's saliency when performing standard fetal head plane detection on ultrasound (US) images. However, the saliency prediction is used as a secondary task to assist the primary detection task, and thus, the saliency prediction performance failed to outperform benchmark prediction methods in several key metrics. Similarly, by Karargyris et al. (2021) , as a proof-of-concept study, the gaze data is used as an auxiliary task for CXR image classification, and the performance of saliency prediction is not reported in the study.",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 93,
                    "text": "(Pan et al., 2016;",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 94,
                    "end": 116,
                    "text": "K\u00fcmmerer et al., 2016;",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 117,
                    "end": 137,
                    "text": "Jia and Bruce, 2020;",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 138,
                    "end": 158,
                    "text": "Kroner et al., 2020)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 275,
                    "end": 301,
                    "text": "(Oyama and Yamanaka, 2017;",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 302,
                    "end": 324,
                    "text": "K\u00fcmmerer et al., 2016;",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 325,
                    "end": 350,
                    "text": "Oyama and Yamanaka, 2018)",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 504,
                    "end": 521,
                    "text": "Cai et al. (2018)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 920,
                    "end": 944,
                    "text": "Karargyris et al. (2021)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Saliency prediction with deep learning"
        },
        {
            "text": "Public datasets for CXR images enabled data driven approaches for automatic image analysis and diagnosis (Serte et al., 2020; . Advancements in standardized image classification networks, i.e., ResNet (He et al., 2016) , DenseNet (Huang et al., 2017) , and EfficientNet (Tan and Le, 2019) , facilitate CXR image classification. Yet, CXR image classification remains challenging, as CXR images are noisy, and may contain subtle features that are difficult to recognize even by experts (\u00c7 all\u0131 et al., 2021; Khan et al., 2021) .",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 125,
                    "text": "(Serte et al., 2020;",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 201,
                    "end": 218,
                    "text": "(He et al., 2016)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 230,
                    "end": 250,
                    "text": "(Huang et al., 2017)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 270,
                    "end": 288,
                    "text": "(Tan and Le, 2019)",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 484,
                    "end": 505,
                    "text": "(\u00c7 all\u0131 et al., 2021;",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 506,
                    "end": 524,
                    "text": "Khan et al., 2021)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "CXR image classification with deep learning"
        },
        {
            "text": "As stated in Section 1, component task selection, network architecture design, and training scheme are key factors for multi-task learning. We select the classification task together with the saliency prediction based on the fact that attention patterns are task specific (Karessli et al., 2017) . Radiologists are likely to exhibit distinguishable visual behaviors when different patient conditions are shown on CXR images (McLaughlin et al., 2017) . This section introduces our multi-task UNet (MT-UNet) architecture, and derives a better multi-task training scheme for saliency prediction and image classification. Figure 1 shows the architecture of the proposed MT-UNet. The network takes CXR images, x \u2208 R 1\u00d7H\u00d7W , where H and W are image dimensions, as input, and produces two outputs, predicted saliency y s \u2208 R 1\u00d7H\u00d7W , and predicted classification y c \u2208 R C , where C is the number of classes. As the ground truth for y s is human visual attention distribution, represented as a 2D matrix whose elements are non-negative and sum to 1, y s is normalized by Softmax before output from MT-UNet. Softmax is also applied to y c before output so that the classification outcome can be interpreted as class probability. For the simplicity of notation, batch dimensions are neglected. The proposed MT-UNet is derived from standard UNet architecture (Ronneberger et al., 2015) . As a well-known image-to-image deep learning model, the UNet structure has been adopted for various tasks. For example, the UNet is appended with additional structures for visual scene understanding (Jha et al., 2020) , the features from the bottleneck (middle of the UNet) are extracted for image classification tasks (Karargyris et al., 2021) , and by combining UNet with Pyramid Net (Lin et al., 2017) , features at different depth are aggregated for enhanced segmentation (Moradi et al., 2019) . What's more, the encoder-decoder structure of UNet is utilized for multi-task learning, where the encoder structure is used to learn representative features, along with designated decoder structures or classification heads for image reconstruction, segmentation, and/or classification (Zhou et al., 2021; Amyar et al., 2020) . In our design, we apply classification heads (shaded in light green in Figure 1 ), which are added not only to the bottleneck but also the ending part of the UNet architecture. This additional classification specific structures aggregates middle and higher-level features for classification, exploiting features learnt at different depths. The attention heads perform global average pooling operations to the 4D tensors, followed by concatenation, and two linear transforms (dense layers) with dropout (rate=25%) in the middle to produce classification outcomes. The MT-UNet belongs to the hard parameter sharing structure in multi-task learning, where different tasks share the same trainable parameters before branched out to each tasks' specific parameters (Vandenhende et al., 2021) . Having more trainable parameters in task specific structures may improve the performance for that task at a cost of introducing additional parameters and increasing computational load (Crawshaw, 2020; Vandenhende et al., 2021) . In our design, we wish to avoid heavy structures with lots of task specific parameters, and therefore, task specific structures are minimized. In Figure 1 , we use yellow and green shades to denote network structures dedicated for saliency prediction and classification, respectively.",
            "cite_spans": [
                {
                    "start": 272,
                    "end": 295,
                    "text": "(Karessli et al., 2017)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 424,
                    "end": 449,
                    "text": "(McLaughlin et al., 2017)",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 1348,
                    "end": 1374,
                    "text": "(Ronneberger et al., 2015)",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 1576,
                    "end": 1594,
                    "text": "(Jha et al., 2020)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1696,
                    "end": 1721,
                    "text": "(Karargyris et al., 2021)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1763,
                    "end": 1781,
                    "text": "(Lin et al., 2017)",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 1853,
                    "end": 1874,
                    "text": "(Moradi et al., 2019)",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 2162,
                    "end": 2181,
                    "text": "(Zhou et al., 2021;",
                    "ref_id": null
                },
                {
                    "start": 2182,
                    "end": 2201,
                    "text": "Amyar et al., 2020)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 2964,
                    "end": 2990,
                    "text": "(Vandenhende et al., 2021)",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 3177,
                    "end": 3193,
                    "text": "(Crawshaw, 2020;",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 3194,
                    "end": 3219,
                    "text": "Vandenhende et al., 2021)",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [
                {
                    "start": 618,
                    "end": 626,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 2275,
                    "end": 2283,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 3368,
                    "end": 3376,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Multi-task Learning Method"
        },
        {
            "text": "Balancing the losses between tasks in a multi-task training process has a direct impact on the training outcome (Vandenhende et al., 2021) . There exist multi-task training schemes (Kendall et al., 2018; Chen et al., 2018; Guo et al., 2018; Sener and Koltun, 2018) , and among which, we adopt the uncertainty based balancing scheme (Kendall et al., 2018) with the modification proposed in (Liebel and K\u00f6rner, 2018) . Hence, the loss function is:",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 138,
                    "text": "(Vandenhende et al., 2021)",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 181,
                    "end": 203,
                    "text": "(Kendall et al., 2018;",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 204,
                    "end": 222,
                    "text": "Chen et al., 2018;",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 223,
                    "end": 240,
                    "text": "Guo et al., 2018;",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 241,
                    "end": 264,
                    "text": "Sener and Koltun, 2018)",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 332,
                    "end": 354,
                    "text": "(Kendall et al., 2018)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 389,
                    "end": 414,
                    "text": "(Liebel and K\u00f6rner, 2018)",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Multi-task Training Scheme"
        },
        {
            "text": "where L s and L c are loss values for y s and y c , respectively; \u03c3 s > 0 and \u03c3 c > 0 are trainable scalars estimating the uncertainty of L s and L c , respectively; \u03c3 s and \u03c3 c are initialized to 1; ln(\u03c3 s +1) and ln(\u03c3 c +1) are regularizing terms to avoid arbitrary decrease of \u03c3 s and \u03c3 c . With Equation (1), we know that \u03c3 values can dynamically weigh losses of different amplitudes during training, and loss with low uncertainty (small \u03c3 value) is prioritized in the training process. L > 0. Given y s and y c with their ground truth\u0233 s and\u0233 c , respectively, the loss functions are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-task Training Scheme"
        },
        {
            "text": "where H(Q, R) = \u2212\u03a3 n i Q i ln(R i ) stands for cross entropy of two discrete distributions Q and R, both with n elements; H(Q) = H(Q, Q) stands for the entropy, or self cross entropy, of discrete distribution Q. L s is the Kullback-Leibler divergence (KLD) loss, and L c is the cross-entropy loss. By observing Equation (2) and Equation (3), we know that only the cross entropy terms, H(\u00b7, \u00b7), generate gradient when updating network parameters, as the term \u2212H(\u0233 s ) in L s is a constant and has zero gradient. Therefore, we extend the method in (Kendall et al., 2018) , and use 1 \u03c3 2 to scale a KLD loss (L s ) as that for a cross-entropy loss (L c ).",
            "cite_spans": [
                {
                    "start": 546,
                    "end": 568,
                    "text": "(Kendall et al., 2018)",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Multi-task Training Scheme"
        },
        {
            "text": "Although the training scheme in Equation (1) yields many successful applications, overfitting for multi-task networks still can jeopardize the training process, especially for small datasets (Wang et al., 2020) . Multiple factors can cause overfitting, among witch, learning rate, r > 0, shows the most significant impact (Li et al., 2019) . Also, r generally has significant influences on the training outcome (Smith, 2018), making it one of the most important hyper-parameters for a training process. When training MT-UNet, r is moderated by several factors. The first factor is the use of an optimizer. Many optimizers, i.e., Adam (Kingma and Ba, 2014) and RMSProp (Tieleman et al., 2012) , deploy the momentum mechanism or its variants, which can adaptively adjust the effective learning rate, r e , during training. As a learning rate scheduler is often used for more efficient training, it is the second factor to influence r. The influence of r from a learning rate scheduler can be adaptive, i.e., reduce learning rate on plateau (RLRP), or more arbitrary, i.e., cosine annealing with warm restarts (Loshchilov and Hutter, 2016) . By observing Equation (1), we know that an uncertainly estimator \u03c3 for a loss L also serves as a learning rate adaptor for L, which is the third factor. More specifically, given a loss value L with learning rate r, the effective learning rate for parameters with a scaled loss value L \u03c3 2 is r \u03c3 2 . Decreasing r upon overfitting can alleviate its effects (Smith, 2018; Duffner and Garcia, 2007) , but Equation (1) leads to increased learning rate upon overfitting, further worsening the training process. This happens because training loss decreases when overfitting occurs, reducing its variance at the same time. Thus, \u03c3 decreases accordingly, which increases the effective learning rate, thus creating a vicious circle of overfitting. More detailed mathematical derivation is presented in Appendix A. This phenomenon can be observed in Figure 2 , where changes of losses and \u03c3 values during a training process following Equation (1) are presented. We can see from Figure 2 (a), at epoch 40, after an initial decrease in both the training and validation losses, the training loss start to acceleratedly decrease while the validation loss start to amplify, which is a vicious circle of overfitting. A RLRP scheduler can halt the vicious circle by resetting the model parameters to a former epoch and reducing r. Yet, even with reduced r, a vicious circle of overfitting can remerge in later epochs. (1) To alleviate overfitting, we propose the use of the following equations to replace Equation (1):",
            "cite_spans": [
                {
                    "start": 191,
                    "end": 210,
                    "text": "(Wang et al., 2020)",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 322,
                    "end": 339,
                    "text": "(Li et al., 2019)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 668,
                    "end": 691,
                    "text": "(Tieleman et al., 2012)",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 1107,
                    "end": 1136,
                    "text": "(Loshchilov and Hutter, 2016)",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 1495,
                    "end": 1508,
                    "text": "(Smith, 2018;",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 1509,
                    "end": 1534,
                    "text": "Duffner and Garcia, 2007)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [
                {
                    "start": 1979,
                    "end": 1987,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 2107,
                    "end": 2115,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Multi-task Training Scheme"
        },
        {
            "text": "The essence of Equations (4) and (5) is to fix the uncertainty term for one loss in Equation (1) to 1, so that the flexibility in changing effective learning rate is reduced. With the uncertainty term fixed for one component loss, Equations (4) and (5) demonstrate the ability to alleviate overfitting and stabilize the training processing. It is worth noting that Equations (4) and (5) cannot be used interchangeably. We need to test both equations to check which can achieve better performances, as depending on the dataset and training process, overfitting can occur of different severity in all component tasks. In this study, training process with Equation (5) achieves the best performance. Ablation study of this method is presented in Section 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-task Training Scheme"
        },
        {
            "text": "We use the \"chest X-ray dataset with eye-tracking and report dictation\" (Karargyris et al., 2021) shared via PhysioNet (Moody et al., 2000) in this study. The dataset was derived from the MIMIC-CXR dataset (Johnson et al., 2019a,b) with additional gaze tracking and dictation from an expert radiologist. 1083 CXR images are included in the dataset, and accompanying each image, there are tracked gaze data; a diagnostic label (either normal, pneumonia, or enlarged heart); segmentation of lungs, mediastinum, and aortic knob; and radiologist's audio with dictation. The CXR images in the dataset are in resolutions of various sizes, i.e., 3056 \u00d7 2044, and we down sample and/or pad each image to 640 \u00d7 416. A GP3 gaze tracker by Gazepoint (Vancouver, Canada) was used for the collection of gaze data. The tracker has an accuracy of around 1\u00b0of visual angle, and has a 60 Hz sampling rate (Zhu et al., 2019) . Several metrics have been used for the evaluation of saliency prediction performances, and they can be classified into location-based metrics and distribution-based metrics (Bylinskii et al., 2018) . Due to the tracking inaccuracy of the GP3 gaze tracker, location-based metrics is not suited for this study. Therefore, in this paper, we follow suggestions in (Bylinskii et al., 2018) and use KLD for performance evaluation. We also include histogram similarity (HS), and Pearson's correlation coefficient (PCC) for reference purposes. For the evaluation of classification performances, we use the area under curve (AUC) metrics for multi-class classifications (Hand and Till, 2001; Fawcett, 2006) , and the classification accuracy (ACC) metrics. We also include the AUC metrics for each class: normal, enlarged heart, and pneumonia, denoted as AUC-Y1, AUC-Y2, and AUC-Y3, respectively. In this paper, all metrics values are presented as median statistics followed by standard deviations behind the \u00b1 sign. Metrics with up-pointing arrow \u2191 indicates greater values reflect better performances, and vise versa. Best metrics are emboldened.",
            "cite_spans": [
                {
                    "start": 72,
                    "end": 97,
                    "text": "(Karargyris et al., 2021)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 119,
                    "end": 139,
                    "text": "(Moody et al., 2000)",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 206,
                    "end": 231,
                    "text": "(Johnson et al., 2019a,b)",
                    "ref_id": null
                },
                {
                    "start": 888,
                    "end": 906,
                    "text": "(Zhu et al., 2019)",
                    "ref_id": null
                },
                {
                    "start": 1082,
                    "end": 1106,
                    "text": "(Bylinskii et al., 2018)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1269,
                    "end": 1293,
                    "text": "(Bylinskii et al., 2018)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1570,
                    "end": 1591,
                    "text": "(Hand and Till, 2001;",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1592,
                    "end": 1606,
                    "text": "Fawcett, 2006)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Dataset and Evaluation Methods"
        },
        {
            "text": "In this subsection, we compare the performance of MT-UNet, with benchmark networks for CXR image classification and saliency prediction. Detailed training settings are presented in Appendix B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Benchmark comparison"
        },
        {
            "text": "For CXR image classification, the benchmark networks are chosen from the top performing networks for CXR image classification examined in (El Asnaoui et al., 2021) , which are ResNet50 (He et al., 2016) and Inception-ResNet v2 (abbreviated as IRNetV2 in this paper) (Szegedy et al., 2017) . Following Karargyris et al. (2021) , we also include a state-of-the-art general purpose classification network: EfficientNetV2-S (abbreviated as EffNetV2-S) (Tan and Le, 2021) for comparison. For completeness, classification using standard UNet with additional classification head (denoted as UNetC) is included. Results are presented in Table 1 , and We can see that MT-UNet outperforms the other classification networks.",
            "cite_spans": [
                {
                    "start": 138,
                    "end": 163,
                    "text": "(El Asnaoui et al., 2021)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 185,
                    "end": 202,
                    "text": "(He et al., 2016)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 266,
                    "end": 288,
                    "text": "(Szegedy et al., 2017)",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 301,
                    "end": 325,
                    "text": "Karargyris et al. (2021)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 448,
                    "end": 466,
                    "text": "(Tan and Le, 2021)",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [
                {
                    "start": 629,
                    "end": 636,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Benchmark comparison"
        },
        {
            "text": "For CXR image saliency prediction, comparison was conducted with 3 state-of-the-art saliency prediction models, which are SimpleNet (Reddy et al., 2020) , MSINet (Kroner et al., 2020) and VGGSSM (Cao et al., 2020) . Saliency prediction using standard UNet (denoted as UNetS) is also included for reference. Table 2 shows the result, where MT-UNet outperforms the rest. Visual comparisons for saliency prediction results are presented through Table 4 in Appendix C.",
            "cite_spans": [
                {
                    "start": 132,
                    "end": 152,
                    "text": "(Reddy et al., 2020)",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 162,
                    "end": 183,
                    "text": "(Kroner et al., 2020)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 195,
                    "end": 213,
                    "text": "(Cao et al., 2020)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 307,
                    "end": 314,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 442,
                    "end": 449,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Benchmark comparison"
        },
        {
            "text": "MT-UNet UNetC EffNetv2-S IRNetv2 ResNet50",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "ACC \u2191 0.670 \u00b1 0.018 0.593 \u00b1 0.009 0.640 \u00b1 0.037 0.640 \u00b1 0.017 0.613 \u00b1 0.013 AUC \u2191 0.843 \u00b1 0.012 0.780 \u00b1 0.006 0.826 \u00b1 0.015 0.824 \u00b1 0.014 0.816 \u00b1 0.010 AUC-Y1 \u2191 0.864 \u00b1 0.014 0.841 \u00b1 0.007 0.852 \u00b1 0.013 0.862 \u00b1 0.016 0.845 \u00b1 0.015 AUC-Y2 \u2191 0.912 \u00b1 0.008 0.840 \u00b1 0.003 0.901 \u00b1 0.015 0.897 \u00b1 0.011 0.896 \u00b1 0.015 AUC-Y3 \u2191 0.711 \u00b1 0.027 0.597 \u00b1 0.018 0.653 \u00b1 0.017 0.633 \u00b1 0.036 0.622 \u00b1 0.022 Table 1 : Performance comparison between classification models.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 389,
                    "end": 396,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Metrics"
        },
        {
            "text": "MT-UNet UNetS SimpleNet MSINet VGGSSM KLD \u2193 0.726 \u00b1 0.004 0.750 \u00b1 0.002 0.758 \u00b1 0.009 0.748 \u00b1 0.003 0.743 \u00b1 0.007 PCC \u2191 0.569 \u00b1 0.004 0.552 \u00b1 0.002 0.545 \u00b1 0.008 0.557 \u00b1 0.002 0.561 \u00b1 0.005 HS \u2191 0.548 \u00b1 0.001 0.540 \u00b1 0.001 0.541 \u00b1 0.002 0.545 \u00b1 0.001 0.545 \u00b1 0.003 Table 2 : Performance comparison between saliency prediction models.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 265,
                    "end": 272,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Metrics"
        },
        {
            "text": "To validate the modified multi-task learning scheme, ablation study is performed. The multi-task learning schemes following Equations (1), (4) and (5) are compared, and they are denoted as MTLS1, MTLS2, and MTLS3, respectively. Please note that the bestperforming MTLS3 is used for benchmark comparison in Section 5.1. Figure 3 in Appendix C shows the training process for MTLS2 and MTLS3. With Figures 2 and 3 , we can see that overfitting occurs both for MTLS1 and MTLS2, but the overfitting is reduced in MTLS3.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 319,
                    "end": 327,
                    "text": "Figure 3",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 395,
                    "end": 410,
                    "text": "Figures 2 and 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Ablation study"
        },
        {
            "text": "The training processes shown in Figures 2 and 3 are with optimized hyper-parameters. The resulting performances are compared in Table 3 in Appendix C. We can see that MTLS3 outperforms the rest learning schemes both in classification and in saliency prediction.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 32,
                    "end": 47,
                    "text": "Figures 2 and 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 128,
                    "end": 135,
                    "text": "Table 3",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Ablation study"
        },
        {
            "text": "To validate the effects of using classification head that aggregates features from different depths, we create ablated versions of MT-UNet that use features from either the bottleneck or the top layer of the MT-UNet for classification, denoted as MT-UNetB and MT-UNetT, respectively. Results are presented in Table 3 in Appendix C. We can see that MT-UNet generally performs better than MT-UNetT and MT-UNetB.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 309,
                    "end": 316,
                    "text": "Table 3",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Ablation study"
        },
        {
            "text": "In this paper, we build the MT-UNet model and propose a further optimized multi-tasking learning scheme for saliency prediction and disease classification with CXR images. While a multi-task learning model has the potential of enhancing the performances for all component tasks, a proper training scheme is one of the key factors to fully unveil its potentiality. As shown in Table 3 , MT-UNet with the standard multi-task learning scheme may barely outperform existing models for saliency prediction or image classification.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 376,
                    "end": 383,
                    "text": "Table 3",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "Several future work could be done to improve this study. The first would be the expansion of the gaze tracking dataset for medical images. So far, only 1083 CXR images are publicly available with radiologist's gaze behavior, limiting extensive studies of gazetracking assisted machine learning methods in the medical field. Also, more dedicated studies on multi-task learning methods, especially for small datasets, can be helpful for medical machine learning tasks. Overfitting and data deficiency are the lingering challenges encountered by many studies. A better multi-task learning method may handle these challenges more easily.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "published the multi-modal chest X-ray dataset for this research. This research is supported by Compute Canada and the Natural Sciences and Engineering Research Council of Canada (NSERC).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Yue Zhou, Houjin Chen, Yanfeng Li, Qin Liu, Xuanang Xu, Shu Wang, Pew-Thian Yap, and Dinggang Shen. Multi-task learning for segmentation and classification of tumors in 3d automated breast ultrasound images. Medical Image Analysis, 70:101918, 2021.",
            "cite_spans": [
                {
                    "start": 4,
                    "end": 80,
                    "text": "Zhou, Houjin Chen, Yanfeng Li, Qin Liu, Xuanang Xu, Shu Wang, Pew-Thian Yap,",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Hongzhi Zhu, Septimiu E Salcudean, and Robert N Rohling. A novel gaze-supported multimodal human-computer interaction for ultrasound machines. International journal of computer assisted radiology and surgery, 14 (7):1107-1115, 2019.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Let L \u2265 0 be the loss for a task, T , and \u03c3 > 0 be the variance estimator for L used in Equation (1). Therefore, the loss for T following Equation (1) can be expressed as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Mathmatical deriviation of vicious circle for overfitting"
        },
        {
            "text": "The partial derivative of L with respect to \u03c3 is:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Mathmatical deriviation of vicious circle for overfitting"
        },
        {
            "text": "During a gradient based optimization process, to minimize L, \u03c3 converges to the equilibrium value (\u03c3 remains unchanged after gradient descend) which is achieved when \u2202L \u2202\u03c3 = 0. Therefore, the following equation holds when \u03c3 is at its equilibrium value, denoted as\u03c3:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Mathmatical deriviation of vicious circle for overfitting"
        },
        {
            "text": "which is calculated by letting \u2202L \u2202\u03c3 = 0. Let f (\u03c3) = L,\u03c3 > 0, we can calculate that:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Mathmatical deriviation of vicious circle for overfitting"
        },
        {
            "text": "Therefore, we know that f (\u03c3) is strictly monotonically increasing with respect to\u03c3, and hence the inverse function of f (\u03c3), f \u22121 (\u00b7), exists. More specifically, we have:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Mathmatical deriviation of vicious circle for overfitting"
        },
        {
            "text": "As a pair of inverse functions share the same monotonicity, we know that\u03c3 = f \u22121 (L) is also strictly monotonically increasing. Thus, when L decreases due to overfitting, we know that\u03c3 will decrease accordingly, forcing \u03c3 to decrease. The decreased \u03c3 leads to an increase in the effective learning rate for T , forming a vicious circle of overfitting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A. Mathmatical deriviation of vicious circle for overfitting"
        },
        {
            "text": "We use the Adam optimizer with default parameters (Kingma and Ba, 2014) and the RLRP scheduler for all the training processes. The RLRP scheduler reduces 90% of the learning rate when validation loss stops improving for P consecutive epochs, and reset model parameters to an earlier epoch when the network achieves the best validation loss. All training and testing are performed with the PyTorch framework (Paszke et al., 2019) . Hyper-parameters for optimizations are learning rate r, and P in RLRP scheduler. The dataset is randomly partitioned into 70%, 10% and 20% subsections for training, validation and testing, respectively. The random data partitioning process preserves the balanced dataset characteristic, and all classes have equal share in all sub-datasets. All the results presented in this paper are based on at least 5 independent trainings with same hyper-parameters. NVIDIA V100 and A100 GPUs (Santa Clara, USA) were used. Table 4 : Visualization of predicted saliency distributions. The ground truth and predicted saliency distributions are overlaid over CXR images. Jet colormap is used for saliency distributions where warmer (red and yellow) colors indicate higher concentration of saliency and colder (green and blue) colors indicate lower concentration of saliency.",
            "cite_spans": [
                {
                    "start": 407,
                    "end": 428,
                    "text": "(Paszke et al., 2019)",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [
                {
                    "start": 942,
                    "end": 949,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Appendix B. Training settings"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Multi-task deep learning based ct imaging analysis for covid-19 pneumonia: Classification and segmentation",
            "authors": [
                {
                    "first": "Amine",
                    "middle": [],
                    "last": "Amyar",
                    "suffix": ""
                },
                {
                    "first": "Romain",
                    "middle": [],
                    "last": "Modzelewski",
                    "suffix": ""
                },
                {
                    "first": "Hua",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Su",
                    "middle": [],
                    "last": "Ruan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Computers in Biology and Medicine",
            "volume": "126",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Saliency prediction in the deep learning era: Successes and limitations",
            "authors": [
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Borji",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE transactions on pattern analysis and machine intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study",
            "authors": [
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Borji",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Dicky",
                    "suffix": ""
                },
                {
                    "first": "Laurent",
                    "middle": [],
                    "last": "Sihite",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Itti",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Transactions on Image Processing",
            "volume": "22",
            "issn": "1",
            "pages": "55--69",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "What do different evaluation metrics tell us about saliency models?",
            "authors": [
                {
                    "first": "Zoya",
                    "middle": [],
                    "last": "Bylinskii",
                    "suffix": ""
                },
                {
                    "first": "Tilke",
                    "middle": [],
                    "last": "Judd",
                    "suffix": ""
                },
                {
                    "first": "Aude",
                    "middle": [],
                    "last": "Oliva",
                    "suffix": ""
                },
                {
                    "first": "Antonio",
                    "middle": [],
                    "last": "Torralba",
                    "suffix": ""
                },
                {
                    "first": "Fr\u00e9do",
                    "middle": [],
                    "last": "Durand",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE transactions on pattern analysis and machine intelligence",
            "volume": "41",
            "issn": "",
            "pages": "740--757",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Multi-task sonoeyenet: detection of fetal standardized planes assisted by generated sonographer attention maps",
            "authors": [
                {
                    "first": "Yifan",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "Harshita",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "Pierre",
                    "middle": [],
                    "last": "Chatelain",
                    "suffix": ""
                },
                {
                    "first": "J Alison",
                    "middle": [],
                    "last": "Noble",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International conference on medical image computing and computer-assisted intervention",
            "volume": "",
            "issn": "",
            "pages": "871--879",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Deep learning for chest x-ray analysis: A survey",
            "authors": [
                {
                    "first": "Ecem",
                    "middle": [],
                    "last": "Erdi \u00c7 All\u0131",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sogancioglu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bram Van Ginneken",
                    "suffix": ""
                },
                {
                    "first": "Keelin",
                    "middle": [],
                    "last": "Kicky G Van Leeuwen",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Murphy",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Medical Image Analysis",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Aggregated deep saliency prediction by selfattention network",
            "authors": [
                {
                    "first": "Ge",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Qing",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Kang-Hyun",
                    "middle": [],
                    "last": "Jo",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Conference on Intelligent Computing",
            "volume": "",
            "issn": "",
            "pages": "87--97",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Multitask learning. Machine learning",
            "authors": [
                {
                    "first": "Rich",
                    "middle": [],
                    "last": "Caruana",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "28",
            "issn": "",
            "pages": "41--75",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Causality matters in medical imaging",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Daniel",
                    "suffix": ""
                },
                {
                    "first": "Ian",
                    "middle": [],
                    "last": "Castro",
                    "suffix": ""
                },
                {
                    "first": "Ben",
                    "middle": [],
                    "last": "Walker",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Glocker",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature Communications",
            "volume": "11",
            "issn": "1",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
            "authors": [
                {
                    "first": "Zhao",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Vijay",
                    "middle": [],
                    "last": "Badrinarayanan",
                    "suffix": ""
                },
                {
                    "first": "Chen-Yu",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Rabinovich",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "794--803",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Multi-task learning with deep neural networks: A survey",
            "authors": [
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Crawshaw",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2009.09796"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "An online backpropagation algorithm with validation error-based adaptive learning rate",
            "authors": [
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Duffner",
                    "suffix": ""
                },
                {
                    "first": "Christophe",
                    "middle": [],
                    "last": "Garcia",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "International Conference on Artificial Neural Networks",
            "volume": "",
            "issn": "",
            "pages": "249--258",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Automated methods for detection and classification pneumonia based on x-ray images using deep learning",
            "authors": [
                {
                    "first": "Khalid",
                    "middle": [],
                    "last": "El Asnaoui",
                    "suffix": ""
                },
                {
                    "first": "Youness",
                    "middle": [],
                    "last": "Chawki",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Idri",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Artificial Intelligence and Blockchain for Future Cybersecurity Applications",
            "volume": "",
            "issn": "",
            "pages": "257--284",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "An introduction to roc analysis",
            "authors": [
                {
                    "first": "Tom",
                    "middle": [],
                    "last": "Fawcett",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Pattern recognition letters",
            "volume": "27",
            "issn": "8",
            "pages": "861--874",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Multicam: Multiple class activation mapping for aircraft recognition in remote sensing images",
            "authors": [
                {
                    "first": "Kun",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Yue",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Zhirui",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Menglong",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "Xian",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Remote Sensing",
            "volume": "11",
            "issn": "5",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Dynamic task prioritization for multitask learning",
            "authors": [
                {
                    "first": "Michelle",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Albert",
                    "middle": [],
                    "last": "Haque",
                    "suffix": ""
                },
                {
                    "first": "De-An",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Serena",
                    "middle": [],
                    "last": "Yeung",
                    "suffix": ""
                },
                {
                    "first": "Li",
                    "middle": [],
                    "last": "Fei-Fei",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
            "volume": "",
            "issn": "",
            "pages": "270--287",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A simple generalisation of the area under the roc curve for multiple class classification problems",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [
                        "J"
                    ],
                    "last": "Hand",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Till",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Machine learning",
            "volume": "45",
            "issn": "2",
            "pages": "171--186",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Xiangyu",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shaoqing",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Densely connected convolutional networks",
            "authors": [
                {
                    "first": "Gao",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Zhuang",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Laurens",
                    "middle": [],
                    "last": "Van Der Maaten",
                    "suffix": ""
                },
                {
                    "first": "Kilian Q",
                    "middle": [],
                    "last": "Weinberger",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "4700--4708",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison",
            "authors": [
                {
                    "first": "Jeremy",
                    "middle": [],
                    "last": "Irvin",
                    "suffix": ""
                },
                {
                    "first": "Pranav",
                    "middle": [],
                    "last": "Rajpurkar",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Ko",
                    "suffix": ""
                },
                {
                    "first": "Yifan",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Silviana",
                    "middle": [],
                    "last": "Ciurea-Ilcus",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Chute",
                    "suffix": ""
                },
                {
                    "first": "Henrik",
                    "middle": [],
                    "last": "Marklund",
                    "suffix": ""
                },
                {
                    "first": "Behzad",
                    "middle": [],
                    "last": "Haghgoo",
                    "suffix": ""
                },
                {
                    "first": "Robyn",
                    "middle": [],
                    "last": "Ball",
                    "suffix": ""
                },
                {
                    "first": "Katie",
                    "middle": [],
                    "last": "Shpanskaya",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the AAAI conference on artificial intelligence",
            "volume": "33",
            "issn": "",
            "pages": "590--597",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Mt-unet: A novel u-net based multi-task architecture for visual scene understanding",
            "authors": [
                {
                    "first": "Ankit",
                    "middle": [],
                    "last": "Jha",
                    "suffix": ""
                },
                {
                    "first": "Awanish",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "Shivam",
                    "middle": [],
                    "last": "Pande",
                    "suffix": ""
                },
                {
                    "first": "Biplab",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "Subhasis",
                    "middle": [],
                    "last": "Chaudhuri",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "2020 IEEE International Conference on Image Processing (ICIP)",
            "volume": "",
            "issn": "",
            "pages": "2191--2195",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Eml-net: An expandable multi-layer network for saliency prediction",
            "authors": [
                {
                    "first": "Sen",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "B"
                    ],
                    "last": "Neil",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bruce",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Image and Vision Computing",
            "volume": "95",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "W"
                    ],
                    "last": "Alistair",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tom",
                    "suffix": ""
                },
                {
                    "first": "Seth",
                    "middle": [
                        "J"
                    ],
                    "last": "Pollard",
                    "suffix": ""
                },
                {
                    "first": "Nathaniel",
                    "middle": [
                        "R"
                    ],
                    "last": "Berkowitz",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Greenbaum",
                    "suffix": ""
                },
                {
                    "first": "Chih-Ying",
                    "middle": [],
                    "last": "Matthew P Lungren",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Roger",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [],
                    "last": "Mark",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Horng",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Scientific data",
            "volume": "6",
            "issn": "1",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "W"
                    ],
                    "last": "Alistair",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tom",
                    "suffix": ""
                },
                {
                    "first": "Nathaniel",
                    "middle": [
                        "R"
                    ],
                    "last": "Pollard",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Greenbaum",
                    "suffix": ""
                },
                {
                    "first": "Chihying",
                    "middle": [],
                    "last": "Matthew P Lungren",
                    "suffix": ""
                },
                {
                    "first": "Yifan",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "Zhiyong",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Roger",
                    "suffix": ""
                },
                {
                    "first": "Seth",
                    "middle": [
                        "J"
                    ],
                    "last": "Mark",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [],
                    "last": "Berkowitz",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Horng",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1901.07042"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Creation and validation of a chest x-ray dataset with eye-tracking and report dictation for ai development. Scientific data",
            "authors": [
                {
                    "first": "Alexandros",
                    "middle": [],
                    "last": "Karargyris",
                    "suffix": ""
                },
                {
                    "first": "Satyananda",
                    "middle": [],
                    "last": "Kashyap",
                    "suffix": ""
                },
                {
                    "first": "Ismini",
                    "middle": [],
                    "last": "Lourentzou",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Joy",
                    "suffix": ""
                },
                {
                    "first": "Arjun",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "Shafiq",
                    "middle": [],
                    "last": "Tong",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Abedin",
                    "suffix": ""
                },
                {
                    "first": "Vandana",
                    "middle": [],
                    "last": "Beymer",
                    "suffix": ""
                },
                {
                    "first": "Elizabeth",
                    "middle": [
                        "A"
                    ],
                    "last": "Mukherjee",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Krupinski",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "8",
            "issn": "",
            "pages": "1--18",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Gaze embeddings for zero-shot image classification",
            "authors": [
                {
                    "first": "Nour",
                    "middle": [],
                    "last": "Karessli",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "4525--4534",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
            "authors": [
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Kendall",
                    "suffix": ""
                },
                {
                    "first": "Yarin",
                    "middle": [],
                    "last": "Gal",
                    "suffix": ""
                },
                {
                    "first": "Roberto",
                    "middle": [],
                    "last": "Cipolla",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "7482--7491",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Intelligent pneumonia identification from chest x-rays: A systematic literature review",
            "authors": [
                {
                    "first": "Wasif",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "Nazar",
                    "middle": [],
                    "last": "Zaki",
                    "suffix": ""
                },
                {
                    "first": "Luqman",
                    "middle": [],
                    "last": "Ali",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Adam: A method for stochastic optimization",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Diederik",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.6980"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Contextual encoderdecoder network for visual saliency prediction",
            "authors": [
                {
                    "first": "Alexander",
                    "middle": [],
                    "last": "Kroner",
                    "suffix": ""
                },
                {
                    "first": "Mario",
                    "middle": [],
                    "last": "Senden",
                    "suffix": ""
                },
                {
                    "first": "Kurt",
                    "middle": [],
                    "last": "Driessens",
                    "suffix": ""
                },
                {
                    "first": "Rainer",
                    "middle": [],
                    "last": "Goebel",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Neural Networks",
            "volume": "129",
            "issn": "",
            "pages": "261--270",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Deepgaze ii: Reading fixations from deep features trained on object recognition",
            "authors": [
                {
                    "first": "Matthias",
                    "middle": [],
                    "last": "K\u00fcmmerer",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Thomas",
                    "suffix": ""
                },
                {
                    "first": "Matthias",
                    "middle": [],
                    "last": "Wallis",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bethge",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1610.01563"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Research on overfitting of deep learning",
            "authors": [
                {
                    "first": "Haidong",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Jiongcheng",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Xiaoming",
                    "middle": [],
                    "last": "Guan",
                    "suffix": ""
                },
                {
                    "first": "Binghao",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "Yuting",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "Xinglong",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "15th International Conference on Computational Intelligence and Security (CIS)",
            "volume": "",
            "issn": "",
            "pages": "78--81",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Accuracy of deep learning for automated detection of pneumonia using chest x-ray images: a systematic review and meta-analysis",
            "authors": [
                {
                    "first": "Yuanyuan",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Zhenyan",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Cong",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Qiang",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "Samireh",
                    "middle": [],
                    "last": "Badrigilan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Computers in Biology and Medicine",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Auxiliary tasks in multi-task learning",
            "authors": [
                {
                    "first": "Lukas",
                    "middle": [],
                    "last": "Liebel",
                    "suffix": ""
                },
                {
                    "first": "Marco",
                    "middle": [],
                    "last": "K\u00f6rner",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1805.06334"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection",
            "authors": [
                {
                    "first": "Tsung-Yi",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Piotr",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "2117--2125",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Visual attention in deep learning: a review",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Milanova",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int Rob Auto J",
            "volume": "4",
            "issn": "3",
            "pages": "154--155",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "authors": [
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Loshchilov",
                    "suffix": ""
                },
                {
                    "first": "Frank",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1608.03983"
                ]
            }
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Computing eye gaze metrics for the automatic assessment of radiographer performance during x-ray image interpretation",
            "authors": [
                {
                    "first": "Laura",
                    "middle": [],
                    "last": "Mclaughlin",
                    "suffix": ""
                },
                {
                    "first": "Raymond",
                    "middle": [],
                    "last": "Bond",
                    "suffix": ""
                },
                {
                    "first": "Ciara",
                    "middle": [],
                    "last": "Hughes",
                    "suffix": ""
                },
                {
                    "first": "Jonathan",
                    "middle": [],
                    "last": "Mcconnell",
                    "suffix": ""
                },
                {
                    "first": "Sonyia",
                    "middle": [],
                    "last": "Mc-Fadden",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International journal of medical informatics",
            "volume": "105",
            "issn": "",
            "pages": "11--21",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Physionet: A research resource for studies of complex physiologic and biomedical signals",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Gb Moody",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Mark",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Goldberger",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Computers in Cardiology",
            "volume": "27",
            "issn": "",
            "pages": "179--182",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Mfp-unet: A novel deep learning based approach for left ventricle segmentation in echocardiography",
            "authors": [
                {
                    "first": "Shakiba",
                    "middle": [],
                    "last": "Moradi",
                    "suffix": ""
                },
                {
                    "first": "Azin",
                    "middle": [],
                    "last": "Mostafa Ghelich Oghli",
                    "suffix": ""
                },
                {
                    "first": "Isaac",
                    "middle": [],
                    "last": "Alizadehasl",
                    "suffix": ""
                },
                {
                    "first": "Niki",
                    "middle": [],
                    "last": "Shiri",
                    "suffix": ""
                },
                {
                    "first": "Mehrdad",
                    "middle": [],
                    "last": "Oveisi",
                    "suffix": ""
                },
                {
                    "first": "Majid",
                    "middle": [],
                    "last": "Oveisi",
                    "suffix": ""
                },
                {
                    "first": "Jan",
                    "middle": [],
                    "last": "Maleki",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dhooge",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Physica Medica",
            "volume": "67",
            "issn": "",
            "pages": "58--69",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Fully convolutional densenet for saliency-map prediction",
            "authors": [
                {
                    "first": "Taiki",
                    "middle": [],
                    "last": "Oyama",
                    "suffix": ""
                },
                {
                    "first": "Takao",
                    "middle": [],
                    "last": "Yamanaka",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "4th IAPR Asian Conference on Pattern Recognition (ACPR)",
            "volume": "",
            "issn": "",
            "pages": "334--339",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Influence of image classification accuracy on saliency map estimation",
            "authors": [
                {
                    "first": "Taiki",
                    "middle": [],
                    "last": "Oyama",
                    "suffix": ""
                },
                {
                    "first": "Takao",
                    "middle": [],
                    "last": "Yamanaka",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "CAAI Transactions on Intelligence Technology",
            "volume": "3",
            "issn": "3",
            "pages": "140--152",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Shallow and deep convolutional networks for saliency prediction",
            "authors": [
                {
                    "first": "Junting",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Elisa",
                    "middle": [],
                    "last": "Sayrol",
                    "suffix": ""
                },
                {
                    "first": "Xavier",
                    "middle": [],
                    "last": "Giro-I Nieto",
                    "suffix": ""
                },
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Mcguinness",
                    "suffix": ""
                },
                {
                    "first": "Noel E O&apos;",
                    "middle": [],
                    "last": "Connor",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "598--606",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Top-down control of visual attention by the prefrontal cortex. functional specialization and long-range interactions",
            "authors": [
                {
                    "first": "Sofia",
                    "middle": [],
                    "last": "Paneri",
                    "suffix": ""
                },
                {
                    "first": "Georgia",
                    "middle": [
                        "G"
                    ],
                    "last": "Gregoriou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Frontiers in neuroscience",
            "volume": "11",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "authors": [
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Paszke",
                    "suffix": ""
                },
                {
                    "first": "Sam",
                    "middle": [],
                    "last": "Gross",
                    "suffix": ""
                },
                {
                    "first": "Francisco",
                    "middle": [],
                    "last": "Massa",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Lerer",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Bradbury",
                    "suffix": ""
                },
                {
                    "first": "Gregory",
                    "middle": [],
                    "last": "Chanan",
                    "suffix": ""
                },
                {
                    "first": "Trevor",
                    "middle": [],
                    "last": "Killeen",
                    "suffix": ""
                },
                {
                    "first": "Zeming",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Natalia",
                    "middle": [],
                    "last": "Gimelshein",
                    "suffix": ""
                },
                {
                    "first": "Luca",
                    "middle": [],
                    "last": "Antiga",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in neural information processing systems",
            "volume": "32",
            "issn": "",
            "pages": "8026--8037",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Tidying deep saliency prediction architectures",
            "authors": [
                {
                    "first": "Navyasri",
                    "middle": [],
                    "last": "Reddy",
                    "suffix": ""
                },
                {
                    "first": "Samyak",
                    "middle": [],
                    "last": "Jain",
                    "suffix": ""
                },
                {
                    "first": "Pradeep",
                    "middle": [],
                    "last": "Yarlagadda",
                    "suffix": ""
                },
                {
                    "first": "Vineet",
                    "middle": [],
                    "last": "Gandhi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "10241--10247",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "authors": [
                {
                    "first": "Olaf",
                    "middle": [],
                    "last": "Ronneberger",
                    "suffix": ""
                },
                {
                    "first": "Philipp",
                    "middle": [],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "Thomas",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Conference on Medical image computing and computer-assisted intervention",
            "volume": "",
            "issn": "",
            "pages": "234--241",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Why did you say that",
            "authors": [
                {
                    "first": "Abhishek",
                    "middle": [],
                    "last": "Ramprasaath R Selvaraju",
                    "suffix": ""
                },
                {
                    "first": "Ramakrishna",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Vedantam",
                    "suffix": ""
                },
                {
                    "first": "Devi",
                    "middle": [],
                    "last": "Cogswell",
                    "suffix": ""
                },
                {
                    "first": "Dhruv",
                    "middle": [],
                    "last": "Parikh",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Batra",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1611.07450"
                ]
            }
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Multi-task learning as multi-objective optimization",
            "authors": [
                {
                    "first": "Ozan",
                    "middle": [],
                    "last": "Sener",
                    "suffix": ""
                },
                {
                    "first": "Vladlen",
                    "middle": [],
                    "last": "Koltun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1810.04650"
                ]
            }
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Deep learning in medical imaging: A brief review",
            "authors": [
                {
                    "first": "Sertan",
                    "middle": [],
                    "last": "Serte",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Serener",
                    "suffix": ""
                },
                {
                    "first": "Fadi",
                    "middle": [],
                    "last": "Al-Turjman",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Transactions on Emerging Telecommunications Technologies",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "authors": [
                {
                    "first": "Karen",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "Andrea",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1312.6034"
                ]
            }
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Leslie",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Smith",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1803.09820"
                ]
            }
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Visual saliency prediction using multi-scale attention gated network",
            "authors": [
                {
                    "first": "Yubao",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Mengyang",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Shaojing",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Multimedia Systems",
            "volume": "",
            "issn": "",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
            "authors": [
                {
                    "first": "Christian",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "Sergey",
                    "middle": [],
                    "last": "Ioffe",
                    "suffix": ""
                },
                {
                    "first": "Vincent",
                    "middle": [],
                    "last": "Vanhoucke",
                    "suffix": ""
                },
                {
                    "first": "Alexander",
                    "middle": [
                        "A"
                    ],
                    "last": "Alemi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Thirty-first AAAI conference on artificial intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "authors": [
                {
                    "first": "Mingxing",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "Quoc",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "6105--6114",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Smaller models and faster training",
            "authors": [
                {
                    "first": "Mingxing",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Quoc",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Efficientnetv2",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2104.00298"
                ]
            }
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning",
            "authors": [
                {
                    "first": "Tijmen",
                    "middle": [],
                    "last": "Tieleman",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "",
            "volume": "4",
            "issn": "",
            "pages": "26--31",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Multi-task learning for dense prediction tasks: A survey",
            "authors": [
                {
                    "first": "Simon",
                    "middle": [],
                    "last": "Vandenhende",
                    "suffix": ""
                },
                {
                    "first": "Stamatios",
                    "middle": [],
                    "last": "Georgoulis",
                    "suffix": ""
                },
                {
                    "first": "Wouter",
                    "middle": [],
                    "last": "Van Gansbeke",
                    "suffix": ""
                },
                {
                    "first": "Marc",
                    "middle": [],
                    "last": "Proesmans",
                    "suffix": ""
                },
                {
                    "first": "Dengxin",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Luc",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "What makes training multi-modal classification networks hard?",
            "authors": [
                {
                    "first": "Weiyao",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Du",
                    "middle": [],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "Matt",
                    "middle": [],
                    "last": "Feiszli",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "12695--12705",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Revisiting video saliency prediction in the deep learning era",
            "authors": [
                {
                    "first": "Wenguan",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Jianbing",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Jianwen",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "Ming-Ming",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "Haibin",
                    "middle": [],
                    "last": "Ling",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Borji",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE transactions on pattern analysis and machine intelligence",
            "volume": "43",
            "issn": "",
            "pages": "220--237",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weaklysupervised classification and localization of common thorax diseases",
            "authors": [
                {
                    "first": "Xiaosong",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Yifan",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "Le",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Zhiyong",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Mohammadhadi",
                    "middle": [],
                    "last": "Bagheri",
                    "suffix": ""
                },
                {
                    "first": "Ronald M",
                    "middle": [],
                    "last": "Summers",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "2097--2106",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "A survey on multi-task learning",
            "authors": [
                {
                    "first": "Yu",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Qiang",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Learning deep features for discriminative localization",
            "authors": [
                {
                    "first": "Bolei",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Aditya",
                    "middle": [],
                    "last": "Khosla",
                    "suffix": ""
                },
                {
                    "first": "Agata",
                    "middle": [],
                    "last": "Lapedriza",
                    "suffix": ""
                },
                {
                    "first": "Aude",
                    "middle": [],
                    "last": "Oliva",
                    "suffix": ""
                },
                {
                    "first": "Antonio",
                    "middle": [],
                    "last": "Torralba",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "2921--2929",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF1": {
            "text": "MT-UNet architecture. The solid blocks represent 3D tensors, R F \u00d7H\u00d7W , where F , H, and W denote feature (channel), height and width dimensions, respectively. The solid circles represent 1D tensors. Arrows denote operations to the tensors. Numbers above some of the solid blocks stand for the number features in tensors.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Training process visualization with Equation",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "0.730 \u00b1 0.007 0.738 \u00b1 0.006 0.726 \u00b1 0.004 0.730 \u00b1 0.003 0.734 \u00b1 0.007 CC \u2191 0.566 \u00b1 0.005 0.563 \u00b1 0.005 0.569 \u00b1 0.004 0.568 \u00b1 0.003 0.561 \u00b1 0.007 HS \u2191 0.547 \u00b1 0.002 0.545 \u00b1 0.002 0.548 \u00b1 0.001 0.548 \u00b1 0.001 0.544 \u00b1 0.003 ACC \u2191 0.649 \u00b1 0.041 0.638 \u00b1 0.019 0.670 \u00b1 0.018 0.653 \u00b1 0.013 0.649 \u00b1 0.011 AUC \u2191 0.832 \u00b1 0.019 0.832 \u00b1 0.010 0.843 \u00b1 0.012 0.836 \u00b1 0.009 0.847 \u00b1 0.008 AUC-Y1 \u2191 0.859 \u00b1 0.014 0.861 \u00b1 0.015 0.864 \u00b1 0.014 0.859 \u00b1 0.007 0.883 \u00b1 0.005 AUC-Y2 \u2191 0.906 \u00b1 0.016 0.913 \u00b1 0.005 0.912 \u00b1 0.008 0.907 \u00b1 0.011 0.910 \u00b1 0.006 AUC-Y3 \u2191 0.682 \u00b1 0.035 0.672 \u00b1 0.010 0.711 \u00b1 0.027 0.694 \u00b1 0.023 0.695 \u00b1 0.025",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Multi",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Ablation study performance comparison.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "We would like to thank physionet.org for providing the open platform for dataset sharing, and we also like to express our gratitude to contributors who collected, organised and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        }
    ]
}
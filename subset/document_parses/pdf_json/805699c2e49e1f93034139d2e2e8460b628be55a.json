{
    "paper_id": "805699c2e49e1f93034139d2e2e8460b628be55a",
    "metadata": {
        "title": "RetinaFaceMask: A Single Stage Face Mask Detector for Assisting Control of the COVID-19 Pandemic",
        "authors": [
            {
                "first": "Xinqi",
                "middle": [],
                "last": "Fan",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Mingjie",
                "middle": [],
                "last": "Jiang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Coronavirus 2019 has made a significant impact on the world. One effective strategy to prevent infection for people is to wear masks in public places. Certain public service providers require clients to use their services only if they properly wear masks. There are, however, only a few research studies on automatic face mask detection. In this paper, we proposed RetinaFaceMask, the first high-performance single stage face mask detector. First, to solve the issue that existing studies did not distinguish between correct and incorrect mask wearing states, we established a new dataset containing these annotations. Second, we proposed a context attention module to focus on learning discriminated features associated with face mask wearing states. Third, we transferred the knowledge from the face detection task, inspired by how humans improve their ability via learning from similar tasks. Ablation studies showed the advantages of the proposed model. Experimental findings on both the public and new datasets demonstrated the stateof-the-art performance of our model. * These two authors contributed equally to this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The authors are with the",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "According to the World Health Organization (WHO), coronavirus disease 2019 (COVID-19) has infected over 79.2 million individuals and caused over 1.7 million fatalities until the end of 2020 [1] . Numerous computer-assisted approaches have been developed to aid in the fight against COVID-19, including automatic detection of COVID-19 cases based on X-ray or computed tomography (CT) images [2] , [3] , COVID-19 trend prediction [4] , and analysis of human reactions to COVID-19 [5] . It is, however, more critical for individuals to protect themselves from the COVID-19 virus. Fortunately, the study [6] demonstrated that surgical face masks can help limit coronavirus dissemination. At the moment, the WHO recommends that people wear face masks if they have respiratory symptoms or are caring for someone who does [7] . Additionally, several public service providers require users to use services only while wearing masks [8] . Therefore, automatic face mask detection has emerged as a critical computer vision task for assisting the worldwide community, but research on this is limited.",
            "cite_spans": [
                {
                    "start": 190,
                    "end": 193,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 390,
                    "end": 393,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 396,
                    "end": 399,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 428,
                    "end": 431,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 478,
                    "end": 481,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 600,
                    "end": 603,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 815,
                    "end": 818,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 923,
                    "end": 926,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Face mask detection entails both the localization of faces and the recognition of mask wearing states, which we define the states as no mask wearing and mask wearing in general. Due to the requirements of healthcare, we further classified the states of mask wearing into correct and incorrect mask wearing states. In one aspect, the face mask detection problem is similar to face detection [9] , as localizing the face is a critical subtask. In another perspective, the problem is closely related to general object detection [10] , where each state can be treated as a distinct class. As shown in Fig. 1 , the challenges of face mask detection include a variety of in-the-wild situations with a complex background, confused faces without masks where faces may be obscured by other objects, a variety of mask types with different shapes and colors, and incorrect mask wearing cases.",
            "cite_spans": [
                {
                    "start": 390,
                    "end": 393,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 525,
                    "end": 529,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [
                {
                    "start": 597,
                    "end": 603,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Typically, traditional object detectors are built on handcrafted feature extractors. The Viola Jones detector utilized the Haar feature in conjunction with the integral image approach [11] , whilst other studies utilized a variety of feature extractors, including the histogram of oriented gradients (HOG), the scale-invariant feature transform (SIFT), and others [12] . Recently, object detectors based on deep learning demonstrated superior performance and have dominated the development of new object detectors. Without relying on prior knowledge to construct feature extractors, deep learning can learn the features in an end-to-end manner [13] . There are two types of deep learning based object detectors: onestage and two-stage detectors. One-stage detectors, such as you only look once (YOLO) [14] and single shot detector (SSD) [15] , detected objects using a single neural network. The advantage of SSD is that it detects objects using multiscale feature maps. By contrast, two-stage detectors, such as region-based convolutional neural network (R-CNN) [16] and faster R-CNN [17] , employed two networks to conduct a coarse-to-fine detection. RetinaFace [18] , a dedicated face mask detector, used a multi-scale detection architecture similar to SSD but included a feature pyramid network (FPN) to fuse high and low level semantic information to increase detection performance. Additionally, numerous approaches for studying face mask detection were created. According to the timeline, the initial version of this work, RetinaFaceMask (also known as RetinaMask), can be considered as the first attempt to introduce the face mask detection work. Li et al. [19] increased the robustness of face mask detection by implementing a mix-up and multi-scale technique based on YOLOv3. To enhance the post-processing of YOLOv3 for face mask detection, a distance intersection over union (IoU) non-maximum suppression (NMS) approach was utilized [20] . However, these algorithms either ignore all possible face mask wearing states that occur in real healthcare applications, or report performance only on limited datasets.",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 188,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 364,
                    "end": 368,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 644,
                    "end": 648,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 801,
                    "end": 805,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 837,
                    "end": 841,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1063,
                    "end": 1067,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1164,
                    "end": 1168,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1665,
                    "end": 1669,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1945,
                    "end": 1949,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "In this paper, we proposed a novel single stage face mask detector, RetinaFaceMask, which is able to detect face masks and contribute to public healthcare. We made the following contributions in this study:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 By reannotating the current MAsked FAces (MAFA) dataset used for masked face analysis, we created a new dataset MAsked FAces for Face Mask Detection (MAFA-FMD). The new annotation includes three distinct mask wearing states: no mask wearing, correct mask wearing, and incorrect mask wearing, which is more realistic in terms of contributing to public health. MAFA-FMD contains around 56,000 annotations. \u2022 To focus on learning discriminated features associated with face mask wearing states, we proposed a novel context attention module (CAM). The module can extract more useful context features, and concentrate on those that are critical for face mask wearing states. \u2022 Inspired by how humans enhance their skills via the use of knowledge gained from other tasks, we used transfer learning (TL) to transfer the knowledge learned from face detection tasks. Experimentally, we demonstrated that face detection and face mask detection are highly correlated, and the feature learned from the former is useful for the latter task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Ablation studies showed the effectiveness of the CAM and TL, since they can boost the mean average precision (mAP) by a large margin. Experimental results on the public dataset AIZOO demonstrated that RetinaFaceMask achieved the state-of-the-art result, and a 4% increase compared to the baseline method. RetinaFaceMask also had the best performance on the MAFA-FMD dataset, which contains three distinct mask wearing states and is notoriously difficult.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The remainder of this paper is structured as follows. Section II illustrates the established dataset. Section III presents the proposed RetinaFaceMask. Section IV discusses the used datasets, experiment settings, results, and discussion. Finally, Section V concludes the paper and outlines future work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Gu et al. prepared the original MAFA dataset from the Internet using the Flickr, Google, and Bing search engines [21] . The dataset contains 35,806 images with a minimum length of 80 pixels. The annotations of the dataset have locations of faces, mask types, etc. Each image was annotated by two individuals and verified by another. More details of MAFA can be found in [21] . However, the original MAFA annotations do not address the requirements for face mask detection in healthcare settings. Therefore, we relabelled the MAFA dataset with three different mask wearing states, \"no mask wearing\", \"correct mask wearing\", and \"incorrect mask wearing\", and named it MAFA-FMD. The procedure for relabeling is as follows. First, we generated reference annotations from original annotations. In detail, we kept all box annotations, and converted \"simple\", \"complex\" mask types as correct mask wearing, \"body\", \"hybrid\" mask types as no mask wearing states. Second, we applied RetinaFaceMask trained on the AIZOO dataset to do inference on MAFA, and recorded all predictions as another reference. Finally, three professional persons manually revised all reference box coordinates and class annotations, and used LabelImg to relabel new faces as well. When identifying masks, we considered disposable medical masks, medical surgical masks, medical protective masks, dusk masks, gas masks, respirators as valid masks. In addition, cloth masks were also regarded as valid ones, since it is also advised by the centers for disease control and prevention (CDC) [22] . Certain masks that do not completely enclose the mouth and nose were deemed invalid. For example, those who wear traditional Chinese veils were considered no mask wearing cases, despite the fact that they resemble some forms of masks.",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 117,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 370,
                    "end": 374,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1552,
                    "end": 1556,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "II. THE MAFA-FMD DATASET"
        },
        {
            "text": "The major differences between the original MAFA and the MAFA-FMD are summarized in Table I . In terms of the total number of annotated faces, MAFA contains 39,485 annotated faces, while MAFA-FMD has 56,084 ones, which is around 16,000 more than that of MAFA. For face types, MAFA does not annotate faces without any masks, but MAFA-FMD contains both masked and unmasked faces. In addition, the mask types have been reclassified to mask wearing states as \"no mask wearing\", \"correct masking wearing\", \"incorrect mask wearing\" with the corresponding numbers 26,463, 28,233, and 1,388 for each class. The imbalanced label distribution shows a long-tailed problem for this in-the-wild dataset. Furthermore, MAFA-FMD includes blurred faces, which were not included in the original MAFA annotation. The number of low resolution (smaller than 32 \u00d7 32 resolution) annotations has been increased from approximately 1,000 in MAFA to more than 4,000 in MAFA-FMD. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 83,
                    "end": 90,
                    "text": "Table I",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "II. THE MAFA-FMD DATASET"
        },
        {
            "text": "The architecture of the proposed RetinaFaceMask is shown in Fig. 2 . To cope with the diverse scenes in face mask detection, a strong feature extraction network ResNet50 is used as the backbone network. C 1 , C 2 , C 3 , C 4 and C 5 denote the intermediate output feature maps of the backbone's layers conv1, conv2 x, conv3 x, conv4 x and conv5 x used in the original ResNet50 [23] . These feature maps are generated by convolutions with distinct receptive fields, allowing for the detection of objects of varying sizes. At this point, we have established the general structure for our multi-scale detection model. However, one disadvantage of shallow layers is that their outputs lack sufficient high-level semantic information, which might result in poor detection performance. To address this, an FPN has been adopted, and the details are as follows. First, we apply a 3\u00d73 convolution on C 5 to obtain P 5 . Then, we upsample P 5 using nearest interpolation to the same size as C 4 , and merge the upsampled P 5 and channel-adjusted C 4 with an element-wise addition. Likewise, we obtain P 3 from P 4 and C 3 . In addition, we also proposed a light-weighted version of RetinaFaceMask (RetinaFaceMask-Light) by using the backbone of MobileNetV1 for running on embedded devices efficiently. C 3 , C 4 and C 5 for RetinaFaceMask-Light are yielded from the last convolution blocks with the original output sizes 28 \u00d7 28, 14 \u00d7 14, and 7 \u00d7 7 in [24] .",
            "cite_spans": [
                {
                    "start": 377,
                    "end": 381,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1442,
                    "end": 1446,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [
                {
                    "start": 60,
                    "end": 66,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "A. Network Architecture"
        },
        {
            "text": "In comparison to face detection, face mask detection requires both the localization of faces and the discrimination of distinct mask wearing states. To focus on learning more discriminated features for mask wearing states, we proposed a CAM as shown in Fig. 3 . First, to enhance the context feature extraction, we employ three parallel subbranches consisting of one 3 \u00d7 3 convolution, two 3 \u00d7 3 convolutions, and three 3 \u00d7 3 convolutions. Equivalently, these branches correspond to 3 \u00d7 3, 5 \u00d7 5 and 7 \u00d7 7 receptive fields. Then, inspired by [25] , we apply channel and spatial attention to focus on both channel and spatial important features associated with face mask wearing states. The channel attention block on the input P \u2208 R D\u00d7H\u00d7W can be calculated as",
            "cite_spans": [
                {
                    "start": 542,
                    "end": 546,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [
                {
                    "start": 253,
                    "end": 259,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "B. Context Attention Module"
        },
        {
            "text": "where \u039b c is the channel attention map; sigmoid function \u03c3 normalizes the output to (0, 1); F M LP denotes for a threelayer multi-layer perception; H GAP and H GM P are global average pooling and global maximum pooling. Similarly, the attention map \u039b s yielded by the spatial attention block is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Context Attention Module"
        },
        {
            "text": "where denotes a 2D convolution; K 3\u00d73 is a 3 \u00d7 3 kernel; \u2295 stands for the channel concatenation; H CAP and H CM P are channel average pooling and channel maximum pooling.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Context Attention Module"
        },
        {
            "text": "The uncontrolled and diverse in-the-wild scenes make feature learning difficult. One possible solution is to collect and annotate more data for training. In RetinaFaceMask, we proposed to mimic the human learning process by transferring knowledge from face detection to help face mask detection. According to [26] , [27] , TL has aided in feature learning as long as these tasks have a correlation. Therefore, in our work, we transfer the knowledge learned on a large scale face detection dataset Wider Face, which consists of 32,203 images and 393,703 annotated faces [28] to enhance the feature extraction ability for FMD.",
            "cite_spans": [
                {
                    "start": 309,
                    "end": 313,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 316,
                    "end": 320,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 569,
                    "end": 573,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "C. Transfer Learning"
        },
        {
            "text": "Our network generates two matrices, location offset y l \u2208 R np\u00d74 and class probability y c \u2208 R np\u00d7nc , where n p and n c refer to the number of anchors and the number of categories of the bounding boxes, respectively. The following data, default anchors y da \u2208 R np\u00d74 , the ground truth bounding boxes y l \u2208 R no\u00d74 and the true class label y c \u2208 R no\u00d71 are provided, where n o is the number of objects to be detected and is variable for different images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Training"
        },
        {
            "text": "To calculate the model's loss, we begin by selecting the top class and calculating the offset for each default anchor through matching the default anchors y da , the ground truth bounding boxes y l , and the true class label y c to obtain matched matrices p ml \u2208 R np\u00d74 and p mc \u2208 R np , where the rows of p ml and p mc denote the coordinates offsets and the labels with the highest probability for each default anchor, respectively. Then, we obtain the positive localization prediction and positive matched default anchors y + l \u2208 R p+\u00d74 and p + ml \u2208 R p+ by selecting the foreground boxes, where p + denotes the number of default anchors with non-zero top classification label. The L 1 -smooth loss L loc ( y + l , p + ml ) is used to perform box coordinates regression. Following that, the hard negative mining [29] is performed to obtain the sampled negative default anchors p \u2212 mc \u2208 R p\u2212 and predicted anchors y \u2212 c \u2208 R p\u2212 , where p \u2212 is the number of sampled negative anchors. Finally, we calculate the classification confidence loss by L conf ( y \u2212 c , p \u2212 mc ) + L conf ( y + c , p + mc ). In summary, the total loss is calculated as follows,",
            "cite_spans": [
                {
                    "start": 814,
                    "end": 818,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "D. Training"
        },
        {
            "text": "where n m is the number of matched default anchors, and \u03b1 is a weight for the localization loss.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D. Training"
        },
        {
            "text": "In the inference stage, the trained model generates the object's localization y l \u2208 R np\u00d74 and confidence y c \u2208 R np\u00d74 , where the second column of y c denoted as y n \u2208 R np is the probability of no mask wearing states; the third column of y c denoted as y cm \u2208 R np is the confidence of correct mask wearing states; the fourth column of y c denoted as y im \u2208 R np is the confidence of incorrect mask wearing states. We remove objects with confidences lower than t c and perform the NMS with IoUs larger than t nms to obtain the final predictions. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Inference"
        },
        {
            "text": "A. Dataset 1) AIZOO: The AIZOO Face Mask Dataset [30] has 7,959 images, where the faces are annotated either with a mask or without a mask. The dataset is a composite of the Wider Face [28] and MAFA datasets [21] , with approximately 50% of data from each. The predefined test set is used.",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 53,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 185,
                    "end": 189,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 208,
                    "end": 212,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "IV. EXPERIMENT AND DISCUSSION"
        },
        {
            "text": "2) MAFA-FMD: As described in section II, MAFA-FMD is a reannotated dataset, in which there are three classes, \"no mask wearing\", \"correct mask wearing\" and \"incorrect mask wearing\". The original test set split of MAFA is kept.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. EXPERIMENT AND DISCUSSION"
        },
        {
            "text": "The model was developed on PyTorch [31] deep learning framework. The model was trained for 250 epochs with a stochastic gradient descent (SGD) algorithm of learning rate 10 \u22123 and momentum 0.9. An NVIDIA GeForce RTX 2080 Ti GPU was employed. The input image resolution is 840 \u00d7 840 for RetinaFaceMask, and is 640 \u00d7 640 for RetinaFaceMask-Light.",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 39,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "B. Experiment Setup"
        },
        {
            "text": "We performed an ablation study to evaluate the effectiveness of CAM and TL using RetinaFaceMask on the AIZOO dataset. We used average precision (AP) for each class, and mean average precision (mAP) as the evaluation metrics [32] . AP N and AP M are APs for no mask wearing and mask wearing states, respectively. The experiment results were summarized in Table II and the best result was obtained by combing CAM and TL. The following sections discuss the effectiveness of each module.",
            "cite_spans": [
                {
                    "start": 224,
                    "end": 228,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [
                {
                    "start": 354,
                    "end": 362,
                    "text": "Table II",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "C. Ablation Study"
        },
        {
            "text": "1) Context Attention Module: By including CAM in the model, we observed an around 1% increase in mAP. In particular, AP for no mask wearing increased from 92.8% to 94.2%, and AP for mask wearing improved from 93.1% to 93.6%. These findings indicate that CAM can be used to focus on the desired face and mask features, which can alleviate the effect of the imbalanced problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Ablation Study"
        },
        {
            "text": "2) Transfer Learning: To evaluate the performance of TL using face detection knowledge, we added TL to the model. We noticed a considerable rise in mAP from 93.0% to 94.4% when compared to the baseline. The possible reason for this is because face detection and face mask detection are highly related, and so the features learned for the former become beneficial for the latter. Table III , we compared our model's performance with that of other widely used detectors for face mask detection. SSD is the baseline approach released by the AIZOO dataset's produce [30] . YOLOv3 has been used in numerous face mask detection investigations [19] , [20] . RetinaFace was also included in the comparison as an efficient face detector. We discovered that RetinaFaceMask can outperform YOLOv3 and RetinaFace by 1.7% and 1.8%, respectively, and obtain the state-of-theart result in terms of mAP. Additionally, for the APs with and without masks, RetinaFaceMask demonstrated the best outcome. Our lite version, RetinaFaceMask-Light, which utilizes a significantly smaller model, achieved an acceptable result of 92.0% in mAP. It should be noted that the number of parameters in RetinaFaceMask-Light is much less than other models.",
            "cite_spans": [
                {
                    "start": 562,
                    "end": 566,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 637,
                    "end": 641,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 644,
                    "end": 648,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 379,
                    "end": 388,
                    "text": "Table III",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "C. Ablation Study"
        },
        {
            "text": "Additionally, we showed some qualitative AIZOO dataset results in Fig 4(a) . As seen in the first and fourth images, the model is robust to confusing masking types. In the second and third images, faces with mask wearing were correctly spotted. We discovered that one of an infant's little faces was omitted from the last image. One probable explanation for this is that the training dataset lacks small faces, and hence the model does not learn a good representation for such faces.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 66,
                    "end": 74,
                    "text": "Fig 4(a)",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "C. Ablation Study"
        },
        {
            "text": "2) Comparison on MAFA-FMD: We also compared our method's performance on the MAFA-FMD dataset. Additional evaluation metrics: AP CM for the correct mask wearing, and AP IM for the incorrect mask wearing, are included. Since we only annotated masks that can protect humans in healthcare settings as valid masks, some masks which do not enclose the faces are denoted as no mask wearing. This may increase the hardness of learning, because they are hard to distinguish. In addition, the three-class task is likely to be harder than the two-class task. Although it is hard, our method still achieved the state-of-the-art performance on mAP and APs of different classes as shown in Table IV . Compared to the second best method RetinaFace, we had an around 2% improvement in mAP. However, our light-weighted version RetinaFaceMask-light only obtained a 59.8% mAP, which may be due to the reason that light and shallow models are hard to learn enough useful features. Fig. 4(b) illustrates some qualitative findings from the MAFA-FMD dataset. In comparison to the second AIZOO image in Fig. 4(a) , the model trained on our reannotated dataset is capable of correctly discriminating between correct and incorrect mask wearing cases, as demonstrated by the first three images. Additionally, the MAFA-FMD trained model is capable of capturing some small or blurred faces. However, rare failures may occur when the face is occluded by someone or something.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 676,
                    "end": 684,
                    "text": "Table IV",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 961,
                    "end": 970,
                    "text": "Fig. 4(b)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1079,
                    "end": 1088,
                    "text": "Fig. 4(a)",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "C. Ablation Study"
        },
        {
            "text": "In this paper, we proposed a novel single stage face mask detector, namely RetinaFaceMask. We made the following contributions. First, we created a new face mask detection dataset, MAFA-FMD, with a more realistic and informative classification of mask wearing states. Second, we proposed a new attention module, CAM, that would be dedicated to learning discriminated features associated with face mask wearing states. Third, we emulated humans' ability to transfer knowledge from the face detection task to improve face mask detection. The proposed method achieved state-of-theart results on the public face mask dataset as well as our new dataset. In particular, compared with the baseline method on the AIZOO dataset, we have improved the mAP by 4% than the baseline. Therefore, we believe our method can benefit both the emerging field of face mask detection and public healthcare to combat the spread of COVID-19. Further work may include tackling problems of occlusions or small faces in face mask detection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. CONCLUSIONS"
        },
        {
            "text": "ACKNOWLEDGMENT The authors thank Prof. H. Yan for valuable discussion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. CONCLUSIONS"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Coronavirus disease 2019 (COVID-19) weekly epidemiological update -29",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "H"
                    ],
                    "last": "Organization",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "A deep bayesian ensembling framework for COVID-19 detection using chest ct images",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Tabarisaadi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Khosravi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Nahavandi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics",
            "volume": "",
            "issn": "",
            "pages": "1584--1589",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "An uncertaintyaware transfer learning-based framework for COVID-19 diagnosis",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shamsi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Asgharnezhad",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Jokandan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Khosravi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "M"
                    ],
                    "last": "Kebria",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Nahavandi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Nahavandi",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Srinivasan",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "A comparative study of predictive machine learning algorithms for COVID-19 trends and analysis",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kunjir",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Joshi",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chadha",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Wadiwala",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Trikha",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics",
            "volume": "",
            "issn": "",
            "pages": "3407--3412",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Understanding global reaction to the recent outbreaks of COVID-19: Insights from instagram data analysis",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Rafi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Rana",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kaur",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [
                        "J"
                    ],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "M"
                    ],
                    "last": "Zadeh",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics",
            "volume": "",
            "issn": "",
            "pages": "3413--3420",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Face masks effectively limit the probability of SARS-CoV-2 transmission",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Witt",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Rapp",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Wild",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "O"
                    ],
                    "last": "Andreae",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "P\u00f6schl",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Science",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Rational use of face masks in the COVID-19 pandemic",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "J"
                    ],
                    "last": "Cowling",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "The Lancet Respiratory Medicine",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Transmission dynamics of the COVID-19 outbreak and effectiveness of government interventions: A data-driven analysis",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Penny",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Medical Virology",
            "volume": "92",
            "issn": "6",
            "pages": "645--659",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Face detection techniques: a review",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kaur",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Artificial Intelligence Review",
            "volume": "52",
            "issn": "2",
            "pages": "927--948",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Object detection with deep learning: A review",
            "authors": [
                {
                    "first": "Z.-Q",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "S.-T",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "volume": "30",
            "issn": "11",
            "pages": "3212--3232",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Rapid object detection using a boosted cascade of simple features",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Viola",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "1",
            "issn": "",
            "pages": "I--I",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "A discriminatively trained, multiscale, deformable part model",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Felzenszwalb",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Mcallester",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Deep learning for generic object detection: A survey",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ouyang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fieguth",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pietik\u00e4inen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International Journal of Computer Vision",
            "volume": "128",
            "issn": "2",
            "pages": "261--318",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "You only look once: Unified, real-time object detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Divvala",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "779--788",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "SSD: Single shot multibox detector",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Erhan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Reed",
                    "suffix": ""
                },
                {
                    "first": "C.-Y",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Berg",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "21--37",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Donahue",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "580--587",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Faster R-CNN: Towards realtime object detection with region proposal networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "91--99",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "RetinaFace: Single-shot multi-level face localisation in the wild",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ververas",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kotsia",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zafeiriou",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "5203--5212",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Robust deep learning method to detect face masks",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the International Conference on Artificial Intelligence and Advanced Manufacture",
            "volume": "",
            "issn": "",
            "pages": "74--77",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Mask wearing detection based on YOLOv3",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Physics: Conference Series",
            "volume": "1678",
            "issn": "",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Detecting masked faces in the wild with LLE-CNNs",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ge",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2682--2690",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Types of masks",
            "authors": [],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kalenichenko",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Weyand",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Andreetto",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1704.04861"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "CBAM: Convolutional block attention module",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Woo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "J.-Y.",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "S"
                    ],
                    "last": "Kweon",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Taskonomy: Disentangling task transfer learning",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "R"
                    ],
                    "last": "Zamir",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sax",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Guibas",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Savarese",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3712--3722",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Hybrid separable convolutional inception residual network for human facial expression recognition",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Qureshi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "R"
                    ],
                    "last": "Shahid",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "2020 International Conference on Machine Learning and Cybernetics",
            "volume": "",
            "issn": "",
            "pages": "21--26",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Wider Face: A face detection benchmark",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "C.-C",
                    "middle": [],
                    "last": "Loy",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "5525--5533",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Training region-based object detectors with online hard example mining",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shrivastava",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "761--769",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Detect faces and determine whether people are wearing mask",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chiang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Paszke",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gross",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Massa",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lerer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bradbury",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Chanan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Killeen",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Gimelshein",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Antiga",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "8024--8035",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "A survey on performance metrics for object-detection algorithms",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Padilla",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "L"
                    ],
                    "last": "Netto",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "A"
                    ],
                    "last": "Silva",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings in the International Conference on Systems, Signals and Image Processing",
            "volume": "",
            "issn": "",
            "pages": "237--242",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "YOLOv3: An incremental improvement",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.02767"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Architecture of RetinaFaceMask. FPN fuses high-level information with low-level information through upsampling and adding; CAMs can focus on learning discriminated face mask wearing states related features; knowledge learned from face detection is transferred into the backbone; B, N , CM and IM stand for background, no mask wearing, correct mask wearing, and incorrect mask wearing.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Illustration of CAM. It has a context enhancement block, a channel attention block, and a spatial attention block.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Qualitative Results on AIZOO and MAFA-FMD Datasets. Red boxes are no mask wearing on both datasets; green boxes are mask wearing on AIZOO, and correct mask wearing on MAFA-FMD; yellow boxes are incorrect mask wearing on MAFA-FMD. D. Comparison with other Methods 1) Comparison on AIZOO: In",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "BETWEEN MAFA AND MAFA-FMD",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "ABLATION STUDY OF RETINAFACEMASK.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "WITH OTHER METHODS ON AIZOO IN PERCENTAGE.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "WITH OTHER METHODS ON MAFA-FMD IN PERCENTAGE.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
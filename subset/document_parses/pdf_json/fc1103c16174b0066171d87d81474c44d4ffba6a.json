{
    "paper_id": "fc1103c16174b0066171d87d81474c44d4ffba6a",
    "metadata": {
        "title": "Metrical Approach to Measuring Uncertainty",
        "authors": [
            {
                "first": "Andrey",
                "middle": [
                    "G"
                ],
                "last": "Bronevich",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "National Research University Higher School of Economics",
                    "location": {
                        "addrLine": "Myasnitskaya 20",
                        "postCode": "101000",
                        "settlement": "Moscow",
                        "country": "Russia"
                    }
                },
                "email": ""
            },
            {
                "first": "(",
                "middle": [
                    "B"
                ],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Igor",
                "middle": [
                    "N"
                ],
                "last": "Rozenberg",
                "suffix": "",
                "affiliation": {
                    "laboratory": "JSC \"Research and Design Institute for Information Technology, Signalling and Telecommunications on Railway Transport\"",
                    "institution": "",
                    "location": {
                        "addrLine": "Orlikov per. 5, building 1",
                        "postCode": "107996",
                        "settlement": "Moscow",
                        "country": "Russia"
                    }
                },
                "email": "i.rozenberg@gismps.ru"
            }
        ]
    },
    "abstract": [
        {
            "text": "Many uncertainty measures can be generated by the corresponding divergences, like the Kullback-Leibler divergence generates the Shannon entropy. Divergences can evaluate the information gain obtained by knowing a posterior probability distribution w.r.t. a prior one, or the contradiction between them. Divergences can be also viewed as distances between probability distributions. In this paper, we consider divergences that satisfy a weak system of axioms. This system of axioms does not guaranty additivity of divergences and allows us to consider, for example, the L\u03b1-metric on probability measures as a divergence. We show what kind of uncertainty measures can be generated by such divergences, and how these uncertainty measures can be extended to credal sets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In our experience, we deal with various types of uncertainty. Probability theory allows us to describe conflict in information, other uncertainty theories can generalize it admitting imprecision or non-specificity into models like the theory of imprecise probabilities [1, 20] or the theory of belief functions [12, 17] . We also need to merge information from different sources, and during this process, it is important to analyze how these sources are contradictory or conflicting. Therefore, we need to measure conflict and non-specificity within each source of information and to measure contradiction among information sources. In probability theory, there are many functionals for evaluating conflict called entropies [9, 15, 16] and there are many statistical distances called also divergences that can be used for measuring contradiction between probability models [9, 16, 18, 19] . One can notice the visible interactions between various types of uncertainty like contradiction, conflict and non-specificity.",
            "cite_spans": [
                {
                    "start": 269,
                    "end": 272,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 273,
                    "end": 276,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 311,
                    "end": 315,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 316,
                    "end": 319,
                    "text": "17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 724,
                    "end": 727,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 728,
                    "end": 731,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 732,
                    "end": 735,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 873,
                    "end": 876,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 877,
                    "end": 880,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 881,
                    "end": 884,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 885,
                    "end": 888,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we argue that a measure of contradiction (or divergence) is the basic one, and show how other measures of uncertainty can be expressed through it. The paper has the following structure. In Sect. 2, we give some notations and definitions related to probability measures and credal sets. In Sect. 3, we formulate a system of axioms for conflict measures without the requirement of their additivity. In Sect. 4, we introduce a system of axioms for divergences and illustrate them by examples like the Kullback-Leibler divergence, the R\u00e9nyi divergence, and the g-divergence introduced in the paper. Section 5 is devoted to the question: how such uncertainty measures can be extended on credal sets. The paper finishes with the discussion of obtained results and conclusions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Let X be a finite non-empty set and let 2 X be the powerset of X. A set function P : 2 X \u2192 [0, 1] is called a probability measure on 2 X if 1) P (\u2205) = 0 and P (X) = 1; 2) P (A \u222a B) = P (A) + P (B) for all disjoint sets A, B \u2208 2 X . A function p(x) = P ({x}), x \u2208 X, is called the probability distribution. We see that P (A) = x\u2208A p(x) for every non-empty set A \u2208 2 X . We say that probabilities are uniformly distributed on X if p(x) = 1/ |X|. The probability measure that corresponds to the uniform probability distribution is denoted by P u . The set of all probability measures on 2 X is denoted by M pr (X), and we use the notation M pr if if the basic set X can be chosen arbitrary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some Notations and Definitions"
        },
        {
            "text": "We use the following operations on M pr : a) P = aP 1 + (1 \u2212 a)P 2 is the convex sum of P 1 , P 2 \u2208 M pr (X) with a \u2208 [0, 1] if P (A) = aP 1 (A) + (1 \u2212 a)P 2 (A) for all A \u2208 2 X ; b) let X and Y be non-empty finite sets, \u03d5 : X \u2192 Y be a mapping, and P \u2208 M pr (X), then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some Notations and Definitions"
        },
        {
            "text": "We see that M pr (X) is a convex set and its extreme points are Dirac measures, such measures are defined by a probability distribution p x , x \u2208 X, for which p x (y) = 1 if x = y, and p x (y) = 0 otherwise. Let P x \u2208 M pr be a Dirac measure with the probability distribution p x , then every P \u2208 M pr (X) with a probability distribution p is represented as the convex sum of Dirac measures:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some Notations and Definitions"
        },
        {
            "text": "Clearly, we can identify every P \u2208 M pr (X), where X = {x 1 , ..., x n }, with the point (p(x 1 ), ..., p(x n )) in R n . A non-empty subset P \u2286 M pr (X) is called a credal set [1] if this subset is closed and convex. Further Cr(X) denotes the family of all possible credal sets in M pr (X). We reserve the notation Cr if the basic set X can be chosen arbitrary. In the theory of imprecise probabilities, the model uncertainty based on credal sets is the general one. Some credal sets can be defined by monotone measures. A set function \u03bc : 2 X \u2192 [0, 1] is called a monotone measure [21] if 1) \u03bc(\u2205) = 0 and \u03bc(X) = 1; 2) \u03bc(A) \u03bc(B) for every A, B \u2208 2 X with A \u2286 B. A monotone measure \u03bc on 2 X is called a coherent upper probability [1, 20] if there is a credal set P \u2208 Cr(X) such that \u03bc(A) = sup{P (A)|P \u2208 P} for all A \u2208 2 X .",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 180,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 583,
                    "end": 587,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 730,
                    "end": 733,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 734,
                    "end": 737,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Some Notations and Definitions"
        },
        {
            "text": "In the next, for the sake of notation simplicity, we will write P (x) instead of P ({x}) omitting the figure brackets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some Notations and Definitions"
        },
        {
            "text": "In this paper, we will use axioms for a measure of conflict presented in [6] . Let us observe that in [6] such axioms have been formulated for belief functions and the authors show what it happens if the additivity axiom is dropped. We will formulate these axioms for probability measures.",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 76,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 102,
                    "end": 105,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Axioms for a Conflict Measure on M pr"
        },
        {
            "text": "A measure of conflict U C is the functional U C : M pr \u2192 [0, +\u221e) that satisfies the following axioms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Axioms for a Conflict Measure on M pr"
        },
        {
            "text": "Axiom 2. Let \u03d5 : X \u2192 Y be an injection, then U C (P \u03d5 ) = U C (P ) for every P \u2208 M pr (X).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Axioms for a Conflict Measure on M pr"
        },
        {
            "text": "Let \u03d5 : X \u2192 Y be an arbitrary mapping, then U C (P \u03d5 ) U C (P ) for every P \u2208 M pr (X).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Axiom 3."
        },
        {
            "text": "Let us discuss the above axioms. Axiom 1 postulates that the conflict is equal to zero only for the information without uncertainty. Axiom 2 accumulates two known axioms for the Shannon entropy [16] . If \u03d5 : X \u2192 Y is the bijection, then Axiom 2 says that U C (P ) does not depend on how elements of X are labeled. Let \u03d5 : X \u2192 Y be an injection such that X \u2286 Y and \u03d5(x) = x for all x \u2208 X, then P \u03d5 has the following probability distribution: P \u03d5 (x) = P (x) if x \u2208 X and P \u03d5 (x) = 0 otherwise. Thus, in this case Axiom 2 postulates that adding dummy elements to the set X has no influence on U C values. Axiom 3 says that the conflict is not increases after a mapping. Notice that such a mapping can produce a loss of information, when two separate elements can map to the same element in Y . Axiom 4 shows the behavior of U C after merging sources of information using the mixture rule of aggregation.",
            "cite_spans": [
                {
                    "start": 194,
                    "end": 198,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Axiom 3."
        },
        {
            "text": "In [6] , a reader can find the theorem that fully characterizes a system of functions that defines U C . In this paper, we only give some examples of U C discussed in [6] . ",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 6,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 167,
                    "end": 170,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Axiom 3."
        },
        {
            "text": "Some notable examples of this class of conflict measures are the Shannon entropy, when f (t) = \u2212tlog 2 t, and the Gini index, when f (t) = t(1 \u2212 t).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Axiom 3."
        },
        {
            "text": "We will establish later the connections between the conflict measure from Example 2 and the R\u00e9nyi entropy of order infinity and other functionals for measuring conflict within belief functions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Axiom 3."
        },
        {
            "text": "Although, the Shannon entropy and that the Kullback-Leibler divergence (also called Kullback-Leibler distance or relative entropy) are very popular in probability theory, one can find many other functionals [15, 16, 18, 19] that can be used to measure conflict within and between probability distributions. It is important to say that distances (or more exactly statistical distances) in probability theory are not distances as a reader would expect. They do not always obey the triangular inequality and can be non-symmetric. Such statistical distances measure the conflict (or contradiction) between a prior probability distribution of a random variable and its posterior distribution. We will also consider another possible interpretation of contradiction in Sect. 4. The aim of this section is to illustrate of how such distances or divergences can generate entropies or conflict measures on M pr .",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 211,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 212,
                    "end": 215,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 216,
                    "end": 219,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 220,
                    "end": 223,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Distances and Entropies in Probability Theory"
        },
        {
            "text": "We postulate that a statistical distance or divergence is the functional D: M pr \u00d7 M pr \u2192 [0, +\u221e] that satisfies the following axioms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Distances and Entropies in Probability Theory"
        },
        {
            "text": "the uniform probability distribution on X n . Then the functional",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Distances and Entropies in Probability Theory"
        },
        {
            "text": "is the conflict measure on M pr .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Distances and Entropies in Probability Theory"
        },
        {
            "text": "Let us discuss the introduced axioms. Axiom 5 reflects the behavior of D like a distance or divergence. Axiom 6 allows us to cover the case of the Shannon entropy and the mentioned above interpretation of divergence. Axiom 7 is the similar to Axiom 2 for a measure of conflict. Axiom 8 postulates that the greatest distance between P u (that symbolizes the highest uncertainty in M pr (X)) and a P \u2208 M pr (X) is achieved on P x (the case, when uncertainty is absent). Axiom 9 establishes the main definition of U C through D.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Distances and Entropies in Probability Theory"
        },
        {
            "text": "We will show several choices of divergences satisfying Axioms 5-9 described in the next subsections.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Distances and Entropies in Probability Theory"
        },
        {
            "text": "We remind that the Kullback-Leibler divergence (distance) [16, 19] is defined for probability measures P 1 , P 2 \u2208 M pr (X) by",
            "cite_spans": [
                {
                    "start": 58,
                    "end": 62,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 63,
                    "end": 66,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Kullback-Leibler Divergence"
        },
        {
            "text": "We see that H 1 is the Shannon entropy. We don't check Axioms 5-9 for D 1 , because Axioms 5-7 are well-known properties of the Kullback-Leibler divergence [16, 19] , Axiom 8 follows from the fact that D(P, P u ) is a convex function of P .",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 160,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 161,
                    "end": 164,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Kullback-Leibler Divergence"
        },
        {
            "text": "The R\u00e9nyi divergence [16, 19] is the parametrical generalization of the Kullback-Leibler divergence with the parameter \u03b1 \u2208 [0, +\u221e] defined as",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 25,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 26,
                    "end": 29,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "R\u00e9nyi Divergence"
        },
        {
            "text": "for \u03b1 = 0, 1, +\u221e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R\u00e9nyi Divergence"
        },
        {
            "text": "For special cases, when \u03b1 = 0, 1, +\u221e, D \u03b1 is defined by taking the limit on \u03b1. If \u03b1 \u2192 1, then we get the Kullback-Leibler divergence. Analogously,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R\u00e9nyi Divergence"
        },
        {
            "text": "The computation of U C for D = D \u03b1 with \u03b1 = 0, 1, +\u221e gives us the result",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R\u00e9nyi Divergence"
        },
        {
            "text": "H \u03b1 is called the R\u00e9nyi entropy of order \u03b1. Let us consider also the special cases \u03b1 = 0 and \u03b1 = \u221e. Substituting D by D 0 in the expression of U C , we get the result",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R\u00e9nyi Divergence"
        },
        {
            "text": "In this case, H 0 is called the Hartley entropy. Computing U C with D = D \u221e , we get result",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R\u00e9nyi Divergence"
        },
        {
            "text": "and H \u221e is called the min-entropy. Again, we do not check axioms, because they follow from the known properties of the R\u00e9nyi divergence and the R\u00e9nyi entropy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R\u00e9nyi Divergence"
        },
        {
            "text": "Let g : [0, 1] \u2192 [0, 1] be a convex, strictly increasing and continuous function on [0,1] such that g(0) = 0 and g(1) = 1. Then the g-divergence is the functional",
            "cite_spans": [],
            "ref_spans": [],
            "section": "g-divergence"
        },
        {
            "text": "where P 1 , P 2 \u2208 M pr (X). Let us compute the functional U C generated by D g . Assume that X n = {x 1 , ..., x n } and P ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "g-divergence"
        },
        {
            "text": "where P \u2208 M pr (X), and call it the g-entropy. Next result directly follows from Example 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "g-divergence"
        },
        {
            "text": "Proof. Obviously, the expression of H g can be rewritten using the function f as H g (P ) = x\u2208X f (P (x)). We see that f (0) = 0, f (1) = 1, f is a concave function on [0, 1], i.e. the all necessary conditions are fulfilled for H g to be a conflict measure. Proof. Let us check axioms. We see that the truth of Axioms 5-7 follows from the definition of D g . Let us check Axiom 8. It is easy to see that D g (P, P u ) is a convex function of P , i.e. D g (aP 1 +(1\u2212a)P 2 , P u ) aD g (P 1 )+(1\u2212a)D g (P 2 , P u ), for every a \u2208 [0, 1], and P 1 , P 2 \u2208 M pr (X). Thus, sup P \u2208Mpr(X) D g (P, P u ) is achieved on extreme points of M pr (X), i.e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposition 1. H g is a conflict measure on M pr if the function f (t) = t \u2212 g(t), t \u2208 [0, 1], is strictly decreasing at t = 1."
        },
        {
            "text": "sup P \u2208Mpr(X) D g (P, P u ) = max x\u2208X D g (P x , P u ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposition 1. H g is a conflict measure on M pr if the function f (t) = t \u2212 g(t), t \u2208 [0, 1], is strictly decreasing at t = 1."
        },
        {
            "text": "Axiom 9 follows from Proposition 1. The theorem is proved.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposition 1. H g is a conflict measure on M pr if the function f (t) = t \u2212 g(t), t \u2208 [0, 1], is strictly decreasing at t = 1."
        },
        {
            "text": "Let us analyze the range of D g . We see that D g (P 1 , P 2 ), where P 1 , P 2 \u2208 M pr (X), is convex on both arguments P 1 and P 2 , therefore, the maximum is achieved on extreme points of M pr (X) and sup P1,P2\u2208Mpr(X)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposition 1. H g is a conflict measure on M pr if the function f (t) = t \u2212 g(t), t \u2208 [0, 1], is strictly decreasing at t = 1."
        },
        {
            "text": "In some cases, it is convenient that D g should be normalized. Then we use the normalized D g defined by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposition 1. H g is a conflict measure on M pr if the function f (t) = t \u2212 g(t), t \u2208 [0, 1], is strictly decreasing at t = 1."
        },
        {
            "text": "The value D g (P 1 , P 2 ) = 1 manifests the highest contradiction between P 1 and P 2 . As a rule, in information theory, entropies are normalized. This means that H(P u ) = 1 for an entropy H : M pr \u2192 [0, +\u221e), where P u defines the uniform probability distribution on X and |X| = 2. Thus, we can introduce the normalized entropy H g (P ) as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposition 1. H g is a conflict measure on M pr if the function f (t) = t \u2212 g(t), t \u2208 [0, 1], is strictly decreasing at t = 1."
        },
        {
            "text": "Let us consider the case, when g(x) = x \u03b1 , where \u03b1 > 1, then we denote D g by D L\u03b1 and D L\u03b1 (P 1 , P 2 ) = 0.5 x\u2208X |P 1 (x) \u2212 P 2 (x)| \u03b1 , P 1 , P 2 \u2208 M pr (X).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Divergence and Entropy Based on L \u03b1 -metric"
        },
        {
            "text": "We use this notation, because d L\u03b1 (P 1 , P 2 ) = 2D L\u03b1 (P 1 , P 2 ) 1/\u03b1 is the L \u03b1 -metric on M pr . In this case, the corresponding H g for g(x) = x \u03b1 is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Divergence and Entropy Based on L \u03b1 -metric"
        },
        {
            "text": "Consider the case \u03b1 = 1. We see that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Divergence and Entropy Based on L \u03b1 -metric"
        },
        {
            "text": "and we can find the expression for H L1 taking the limit for \u03b1 \u2192 1. Using l'H\u00f4pital's rule, we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Divergence and Entropy Based on L \u03b1 -metric"
        },
        {
            "text": "Thus, H L1 (P ) is the Shannon entropy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Divergence and Entropy Based on L \u03b1 -metric"
        },
        {
            "text": "We see that any g-divergence has the properties that any metric has, and the entropies generated by D L\u03b1 , \u03b1 1, looks identical to the R\u00e9nyi entropy H \u03b1 of order \u03b1:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concluding Remarks"
        },
        {
            "text": "We see that H L1 = H 1 is the Shannon entropy. If we denote t = x\u2208X P 2 (x), t) . We see that both functions \u03d5 1 (t) = \u2212 1 1\u2212\u03b1 log 2 t and \u03d5 2 (t) =",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 77,
                    "end": 79,
                    "text": "t)",
                    "ref_id": null
                }
            ],
            "section": "Concluding Remarks"
        },
        {
            "text": "are decreasing on [0, 1], therefore, H \u03b1 and H L\u03b1 similarly discriminate uncertainty. Formally, we can take the L \u03b1 -metric",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concluding Remarks"
        },
        {
            "text": "as a divergence. The divergence d L\u03b1 generates the entropy h L\u03b1 (P ) = 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concluding Remarks"
        },
        {
            "text": "that is identical to the Shannon entropy if \u03b1 \u2192 1. We can also define d L\u03b1 (P 1 , P 2 ) and h L\u03b1 (P ) if \u03b1 \u2192 +\u221e. Then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concluding Remarks"
        },
        {
            "text": "The main idea is to use divergences or distances on probability measures and express through them other measures of uncertainty. As it was established by many papers (see [2] and the references there), imprecise probabilities describe two types of uncertainty: non-specificity and conflict. A pure conflict is described by the classical probability theory, non-specificity corresponds to the choice of a probability model among possible ones. It is also important to evaluate the degree of total uncertainty that aggregates conflict and non-specificity. In previous sections, we have described the choice of two functionals U C : M pr \u2192 [0, +\u221e) and D : M pr \u00d7 M pr \u2192 [0, +\u221e]. The functional U C evaluates the amount conflict if uncertainty in information described by a probability measure, and D describes the contradiction between two sources of information in a probabilistic setting. It is important to distinguish two possible interpretations of divergence. The first one related to the R\u00e9nyi and Kullback-Leibler divergences that evaluate the contradiction between a prior probability distribution and a posterior one. Thus, in this case D is not symmetrical, D(P 1 , P 2 ) = +\u221e iff P 1 is not absolutely continuous w.r.t. P 2 . If D is a g-divergence or the L \u03b1 -metric, then D can be used for evaluating difference between probability models, or for finding the degree of how information obtained from two sources is the same. Thus, values of U C can be understood differently. In the case of the R\u00e9nyi and Kullback-Leibler divergences, the part D(P \u03d5n , P u . Assume that P \u2208 Cr(X) and P \u2208 M pr (X). Let us introduce the following functionals: \u03a6 1 (P, P ) = inf{D(P , P )|P \u2208 P}, \u03a6 2 (P, P ) = sup{D(P , P )|P \u2208 P}.",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 174,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "We see that \u03a6 1 (P, P ) and \u03a6 2 (P, P ) give the smallest and highest information gains if the posterior probability distribution is described by the credal set P. According to the Laplace principle the prior information, described by P , has the highest uncertainty if P = P u , where P u is the uniform probability distribution on 2 X . Thus, \u03a6 1 (P, P u ) gives us the amount of information in P. Using the same logic as before, we define the measure U T of total uncertainty on Cr as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "where P \u2208 Cr(X) and we use notations and definitions from Axiom 9. If P = {P }, then the expression for U T (P) is simplified to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "i.e. U T is the conflict measure U C defined in Axiom 9. We see that U T on Cr can be seen as an extension of U T on M pr by the following formula:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "Observe that the functional U T is called the upper or maximal entropy [2, 14] if U T is the Shannon entropy on M pr . Let us analyze the functional",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 74,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 75,
                    "end": 78,
                    "text": "14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "where P \u2208 Cr(X) and P u is the uniform probability distribution on 2 X . We see that it characterizes the amount of uncertainty in choosing a true probability measure in P. Thus, we can choose as a measure of non-specificity the functional",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "where P \u2208 Cr(X) and we use notations and definitions from Axiom 9. Assume that the measure of total uncertainty aggregates non-specificity and conflict additively, i.e. for every P \u2208 Cr",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "where U C is a measure of conflict on Cr. This assumption implies that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "and we can express U C (P) through its values on M pr as U C (P) = inf{U C (P )|P \u2208 P}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "Note that U C is called the minimal or lower entropy [2, 14] on Cr if U C is the Shannon entropy on M pr .",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 56,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 57,
                    "end": 60,
                    "text": "14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Uncertainty Measures on Credal Sets"
        },
        {
            "text": "Let us notice that the axioms for the R\u00e9nyi divergence can be found in [16] . Although, in this paper, the author consider these axioms as evident ones, the interpretation of some of them seems to be problematic, because they are based on so called generalized probability distributions that are not necessarily normalized. In our approach, divergences and entropies are not necessarily additive, that allows, for example, to use L \u03b1 -metrics as such divergences. The results of the paper allow us to resolve some problems in the theory of belief functions. For example, in this theory [17] the conflict between two sources of information is evaluated using Dempster's rule of aggregation. If sources of information are described by probability measures P 1 , P 2 \u2208 M pr (X), then this evaluation is produced by the formula:",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 586,
                    "end": 590,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusion"
        },
        {
            "text": "We see that k(P, P ) = 1\u2212 x\u2208X P (x)P (x) = H L2 (P ) for P \u2208 M pr (X), i.e. k(P, P )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusion"
        },
        {
            "text": "is the Gini index that can be interpreted as entropy or conflict measure. Another representation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusion"
        },
        {
            "text": "implies that k(P 1 , P 2 ) = 0.5(H L2 (P 1 ) + H L2 (P 2 ) + D L2 (P 1 , P 2 )) consists of two parts: the part 0.5(H L2 (P 1 ) + H L2 (P 2 )) measures conflict within information sources and the part 0.5D L2 (P 1 , P 2 ) measures conflict (contradiction) between information sources. Let us notice that this problem of disaggregating of k on two parts for belief functions is investigated in [11] . We also pay attention on using h L\u221e for defining U C on credal sets. In this case,",
            "cite_spans": [
                {
                    "start": 393,
                    "end": 397,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusion"
        },
        {
            "text": "If a credal set P is described by a coherent upper probability \u03bc on 2 X , then the expression for U C can be rewritten as U C (\u03bc) = 2 1 \u2212 max x\u2208X \u03bc(x) . Such a functional (without the factor 2) is proposed for measuring inner conflict in the theory of belief functions [10] .",
            "cite_spans": [
                {
                    "start": 269,
                    "end": 273,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusion"
        },
        {
            "text": "Formally, in this paper we propose to analyze uncertainty by choosing a divergence D, and then to take the compatible with D measures of uncertainty. Let us discuss our approach in detail if we choose d L1 as divergence. We see that we can use d L1 with the Shannon entropy. Let us also look at the following functional:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusion"
        },
        {
            "text": "U C (P ) = min x\u2208X D(P x , P ), where P \u2208 M pr (X).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusion"
        },
        {
            "text": "We see that U C (P ) evaluates the distance between P and the exact information without uncertainty, thus, it can be considered as a candidate for the measure of conflict. If we take D = d L1 , then it is possible to show that Doing in the same way for D = D \u03b1 , we get that U C coincides in this case with the min-entropy. Let us consider the functional Con(P 1 , P 2 ) = inf {D(P 1 , P 2 )|P 1 \u2208 P 1 , P 2 \u2208 P 2 } , P 1 , P 2 \u2208 Cr(X).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusion"
        },
        {
            "text": "We see that Con(P 1 , P 2 ) = 0 iff P 1 \u2229 P 2 = \u2205, and Con can be used for measuring contradiction between information sources described by credal sets. This measure of contradiction for D = d L1 is well-known in the theory of belief functions [3, 8, 13] , its axiomatic can be found in [4, 7] and its extension to imprecise probabilities based on generalized credal sets is given in [5, 7] . Finalizing our paper, we can conclude that there is the variety of divergences and the corresponding uncertainty measures. The choice one of them can depend on the problem statement or on the complexity of realization, or on their additional desirable properties. We aware that in this paper we do not investigate in detail ways of evaluating conflict, non-specificity and contradiction in information presented by credal sets with the help of divergences. This can be the topic of our next research.",
            "cite_spans": [
                {
                    "start": 244,
                    "end": 247,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 248,
                    "end": 250,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 251,
                    "end": 254,
                    "text": "13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 287,
                    "end": 290,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 291,
                    "end": 293,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 384,
                    "end": 387,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 388,
                    "end": 390,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Introduction to Imprecise Probabilities. Wiley Series in Probability and Statistics",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Augustin",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "P A"
                    ],
                    "last": "Coolen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "De Cooman",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Troffaes",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Measures of uncertainty for imprecise probabilities: an axiomatic approach",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Bronevich",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "J"
                    ],
                    "last": "Klir",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Int. J. Approx. Reason",
            "volume": "51",
            "issn": "",
            "pages": "365--390",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "The choice of generalized Dempster-Shafer rules for aggregating belief functions",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Bronevich",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "N"
                    ],
                    "last": "Rozenberg",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. J. Approx. Reason",
            "volume": "56",
            "issn": "",
            "pages": "122--136",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Conjunctive rules in the theory of belief functions and their justification through decisions models",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Bronevich",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "N"
                    ],
                    "last": "Rozenberg",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "BELIEF 2016",
            "volume": "9861",
            "issn": "",
            "pages": "137--145",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-45559-4_14"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Modelling uncertainty with generalized credal sets: application to conjunction and decision",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Bronevich",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "N"
                    ],
                    "last": "Rozenberg",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int. J. Gen. Syst",
            "volume": "27",
            "issn": "1",
            "pages": "67--96",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Clustering a body of evidence based on conflict measures",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Bronevich",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Lepskiy",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 11th Conference of the European Society for Fuzzy Logic and Technology",
            "volume": "1",
            "issn": "",
            "pages": "328--333",
            "other_ids": {
                "DOI": [
                    "10.2991/eusflat-19.2019.47"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "The contradiction between belief functions: its description, measurement, and correction based on generalized credal sets",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Bronevich",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "N"
                    ],
                    "last": "Rozenberg",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. J. Approx. Reason",
            "volume": "112",
            "issn": "",
            "pages": "119--139",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Combining belief functions issued from dependent sources",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "E G V"
                    ],
                    "last": "Cattaneo",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "ISIPTA 2003: Proceedings in Informatics",
            "volume": "18",
            "issn": "",
            "pages": "133--147",
            "other_ids": {
                "DOI": [
                    "10.3929/ethz-a-004531249"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Axiomatic characterizations of information measures",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Csisz\u00e1r",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Entropy",
            "volume": "10",
            "issn": "",
            "pages": "261--273",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Conflicts within and between belief functions",
            "authors": [
                {
                    "first": "Milan",
                    "middle": [],
                    "last": "Daniel",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "6178",
            "issn": "",
            "pages": "696--705",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-14049-5_71"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Conflict between belief functions: a new measure based on their nonconflicting parts",
            "authors": [
                {
                    "first": "Milan",
                    "middle": [],
                    "last": "Daniel",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "8764",
            "issn": "",
            "pages": "321--330",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-11191-9_35"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Upper and lower probabilities induced by multivalued mapping",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Dempster",
                    "suffix": ""
                }
            ],
            "year": 1967,
            "venue": "Ann. Math. Stat",
            "volume": "38",
            "issn": "",
            "pages": "325--339",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Toward an axiomatic definition of conflict between belief functions",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Destercke",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Burger",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Cybern",
            "volume": "43",
            "issn": "2",
            "pages": "585--596",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Uncertainty and Information: Foundations of Generalized Information Theory",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "J"
                    ],
                    "last": "Klir",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Uncertainty of discrete stochastic systems: general theory and statistical theory",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Morales",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Pardo",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Vajda",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "IEEE Trans. Syst. Man Cybern",
            "volume": "26",
            "issn": "",
            "pages": "1--17",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "On measures of entropy and information",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "R\u00e9nyi",
                    "suffix": ""
                }
            ],
            "year": 1961,
            "venue": "Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability. Contributions to the Theory of Statistics",
            "volume": "1",
            "issn": "",
            "pages": "547--561",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A Mathematical Theory of Evidence",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Shafer",
                    "suffix": ""
                }
            ],
            "year": 1976,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Generalized symmetric divergence measures and the probability of error",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "J"
                    ],
                    "last": "Taneja",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Commun. Stat. -Theory Methods",
            "volume": "42",
            "issn": "9",
            "pages": "1654--1672",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "R\u00e9nyi divergence and Kullback-Leibler divergence",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Van Erven",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Harremos",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Inf. Theory",
            "volume": "60",
            "issn": "7",
            "pages": "3797--3820",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Statistical Reasoning with Imprecise Probabilities",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Walley",
                    "suffix": ""
                }
            ],
            "year": 1991,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Generalized Measure Theory",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "J"
                    ],
                    "last": "Klir",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1007/978-0-387-76852-6"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Let f : [0, 1] \u2192 [0, +\u221e) be a concave function with the following properties:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": ") = D(P x , P (n) u ) for every x \u2208 X n and, for our case, D g (P x , P u ) = g(1 \u2212 1/n) + (n \u2212 1)g(1/n). Without decreasing generality, consider P \u2208 M pr (X), where X = {x 1 , ..., x m }, such that P (x i ) > 0, i = 1, ..., m. Let us choose n such that P (x i ) > 1/n, i = 1, ..., m. Then D g (P \u03d5n , P (n) u ) = m i=1 g(P (x i )) + (n \u2212 m)g(1/n), and U C (P ) = lim n\u2192\u221e",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "D g satisfies Axioms 5-9 on M pr \u00d7 M pr if the conditions of Proposition 1 are fulfilled.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "(P ) gives us the amount of information gain after obtaining P \u03d5n w.r.t. the most uncertain information described by P ) \u2212 D(P \u03d5n , P(n) u ) evaluates the deficiency of information in P \u03d5n . If D is understood as a distance, then sup P \u2208Mpr(Xn) D(P , P (n) u ) is the distance between exact and the most uncertain information, and D(P \u03d5n , P (n) u ) characterizes how far P \u03d5n is located from P (n)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "L1 (P 1 , P 2 ) = 2 1 \u2212 x\u2208X min {P 1 (x), P 2 (x)} .This implies thatU C (P ) = 2 min x\u2208X (1 \u2212 P (x)) = 2 1 \u2212 max x\u2208X P (x) = h L\u221e (P ).",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}
{
    "paper_id": "c08277edb2d10d2aaa6992f691a93407084885a7",
    "metadata": {
        "title": "New Intent Discovery with Pre-training and Contrastive Learning",
        "authors": [
            {
                "first": "Yuwei",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {},
                "email": "zhangyuwei.work@gmail.com"
            },
            {
                "first": "Haode",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Li-Ming",
                "middle": [],
                "last": "Zhan",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Xiao-Ming",
                "middle": [],
                "last": "Wu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "\u2020",
                "middle": [],
                "last": "Albert",
                "suffix": "",
                "affiliation": {},
                "email": "albert@fano.ai"
            },
            {
                "first": "Y",
                "middle": [
                    "S"
                ],
                "last": "Lam",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "S",
                "middle": [
                    "A R"
                ],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "New intent discovery aims to uncover novel intent categories from user utterances to expand the set of supported intent classes. It is a critical task for the development and service expansion of a practical dialogue system. Despite its importance, this problem remains underexplored in the literature. Existing approaches typically rely on a large amount of labeled utterances and employ pseudo-labeling methods for representation learning and clustering, which are label-intensive, inefficient, and inaccurate. In this paper, we provide new solutions to two important research questions for new intent discovery: (1) how to learn semantic utterance representations and (2) how to better cluster utterances. Particularly, we first propose a multi-task pre-training strategy to leverage rich unlabeled data along with external labeled data for representation learning. Then, we design a new contrastive loss to exploit self-supervisory signals in unlabeled data for clustering. Extensive experiments on three intent recognition benchmarks demonstrate the high effectiveness of our proposed method, which outperforms state-of-the-art methods by a large margin in both unsupervised and semisupervised scenarios. The source code will be available at https://github.com/ zhang-yu-wei/MTP-CLNN.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Why Study New Intent Discovery (NID)? Recent years have witnessed the rapid growth of conversational AI applications. To design a natural language understanding system, a set of expected customer intentions are collected beforehand to train an intent recognition model. However, the predefined intents cannot fully meet customer needs. This implies the necessity of expanding the intent recognition model by repeatedly integrating new intents discovered from unlabeled user utterances * Work done while the author was with HK PolyU. \u2020 Corresponding author. ( Fig. 1) . To reduce the effort in manually identifying unknown intents from a mass of utterances, previous works commonly employ clustering algorithms to group utterances of similar intents (Cheung and Li, 2012; Hakkani-T\u00fcr et al., 2015; Padmasundari, 2018) . The cluster assignments thereafter can either be directly used as new intent labels or as heuristics for faster annotations.",
            "cite_spans": [
                {
                    "start": 749,
                    "end": 770,
                    "text": "(Cheung and Li, 2012;",
                    "ref_id": null
                },
                {
                    "start": 771,
                    "end": 796,
                    "text": "Hakkani-T\u00fcr et al., 2015;",
                    "ref_id": null
                },
                {
                    "start": 797,
                    "end": 816,
                    "text": "Padmasundari, 2018)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 559,
                    "end": 566,
                    "text": "Fig. 1)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "Research Questions (RQ) and Challenges. Current study of NID centers around two basic research questions: 1) How to learn semantic utterance representations to provide proper cues for clustering? 2) How to better cluster the utterances?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The study of the two questions are often interwoven in existing research. Utterances can be represented according to different aspects such as the style of language, the related topics, or even the length of sentences. It is important to learn semantic utterance representations to provide proper cues for clustering. Simply applying a vanilla pre-trained language model (PLM) to generate utterance representations is not a viable solution, which leads to poor performance on NID as shown by the experimental results in Section 4.2. Some recent works proposed to use labeled utterances of known intents for representation learning (Forman et al., 2015; Haponchyk et al., 2018; Lin et al., 2020; Zhang et al., 2021c; Haponchyk and Moschitti, 2021) , but they require a substantial amount of known intents and sufficient labeled utterances of each intent, which are not always available especially at the early development stage of a dialogue system. Further, pseudo-labeling approaches are often exploited to generate supervision signals for representation learning and clustering. For example, Lin et al. (2020) finetune a PLM with an utterance similarity prediction task on labeled utterances to guide the training of unlabeled data with pseudo-labels. Zhang et al. (2021c) adopt a deep clustering method (Caron et al., 2018) that uses k-means clustering to produce pseudo-labels. However, pseudo-labels are often noisy and can lead to error propagation.",
            "cite_spans": [
                {
                    "start": 631,
                    "end": 652,
                    "text": "(Forman et al., 2015;",
                    "ref_id": null
                },
                {
                    "start": 653,
                    "end": 676,
                    "text": "Haponchyk et al., 2018;",
                    "ref_id": null
                },
                {
                    "start": 677,
                    "end": 694,
                    "text": "Lin et al., 2020;",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 695,
                    "end": 715,
                    "text": "Zhang et al., 2021c;",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 716,
                    "end": 746,
                    "text": "Haponchyk and Moschitti, 2021)",
                    "ref_id": null
                },
                {
                    "start": 1254,
                    "end": 1274,
                    "text": "Zhang et al. (2021c)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1306,
                    "end": 1326,
                    "text": "(Caron et al., 2018)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our Solutions. In this work, we propose a simple yet effective solution for each research question. Solution to RQ 1: multi-task pre-training. We propose a multi-task pre-training strategy that takes advantage of both external data and internal data for representation learning. Specifically, we leverage publicly available, high-quality intent detection datasets, following Zhang et al. (2021d) , as well as the provided labeled and unlabeled utterances in the current domain, to fine-tune a pre-trained PLM to learn task-specific utterance representations for NID. The multi-task learning strategy enables knowledge transfer from general intent detection tasks and adaptation to a specific application domain. Solution to RQ 2: contrastive learning with nearest neighbors. We propose to use a contrastive loss to produce compact clusters, which is motivated by the recent success of contrastive learning in both computer vision (Bachman et al., 2019; He et al., 2019; Chen et al., 2020; Khosla et al., 2020) and natural language processing (Gunel et al., 2021; Gao et al., 2021; Yan et al., 2021) . Contrastive learning usually maximizes the agreement between different views of the same example and minimize that between different examples. However, the commonly used instance discrimination task may push away false negatives and hurts the clustering performance. Inspired by a recent work in computer vision (Van Gansbeke et al., 2020) , we introduce neighborhood relationship to customize the contrastive loss for clustering in both unsupervised (i.e., without any labeled utterances of known intents) and semi-supervised scenarios. Intuitively, in a semantic feature space, neighboring utterances should have a similar intent, and pulling together neighboring samples makes clusters more compact.",
            "cite_spans": [
                {
                    "start": 375,
                    "end": 395,
                    "text": "Zhang et al. (2021d)",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 930,
                    "end": 952,
                    "text": "(Bachman et al., 2019;",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 953,
                    "end": 969,
                    "text": "He et al., 2019;",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 970,
                    "end": 988,
                    "text": "Chen et al., 2020;",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 989,
                    "end": 1009,
                    "text": "Khosla et al., 2020)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1042,
                    "end": 1062,
                    "text": "(Gunel et al., 2021;",
                    "ref_id": null
                },
                {
                    "start": 1063,
                    "end": 1080,
                    "text": "Gao et al., 2021;",
                    "ref_id": null
                },
                {
                    "start": 1081,
                    "end": 1098,
                    "text": "Yan et al., 2021)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1413,
                    "end": 1440,
                    "text": "(Van Gansbeke et al., 2020)",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our main contributions are three-fold.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We show that our proposed multi-task pretraining method already leads to large performance gains over state-of-the-art models for both unsupervised and semi-supervised NID.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We propose a self-supervised clustering method for NID by incorporating neighborhood relationship into the contrastive learning objective, which further boosts performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We conduct extensive experiments and ablation studies on three benchmark datasets to verify the effectiveness of our methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "New Intent Discovery. The study of NID is still in an early stage. Pioneering works focus on unsupervised clustering methods. Shi et al. (2018) leveraged auto-encoder to extract features. Perkins and Yang (2019) considered the context of an utterance in a conversation. Chatterjee and Sengupta (2020) proposed to improve density-based models. Some recent works (Haponchyk et al., 2018; Haponchyk and Moschitti, 2021) studied supervised clustering algorithms for intent labeling, yet it can not handle new intents. Another line of works (Forman et al., 2015; Lin et al., 2020; Zhang et al., 2021c) investigated a more practical case where some known intents are provided to support the discovery of unknown intents, which is often referred to as semisupervised NID.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 143,
                    "text": "Shi et al. (2018)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 188,
                    "end": 211,
                    "text": "Perkins and Yang (2019)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 361,
                    "end": 385,
                    "text": "(Haponchyk et al., 2018;",
                    "ref_id": null
                },
                {
                    "start": 386,
                    "end": 416,
                    "text": "Haponchyk and Moschitti, 2021)",
                    "ref_id": null
                },
                {
                    "start": 536,
                    "end": 557,
                    "text": "(Forman et al., 2015;",
                    "ref_id": null
                },
                {
                    "start": 558,
                    "end": 575,
                    "text": "Lin et al., 2020;",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 576,
                    "end": 596,
                    "text": "Zhang et al., 2021c)",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "To tackle semi-supervised NID, Lin et al. (2020) proposed to first perform supervised training on known intents with a sentence similarity task and then use pseudo labeling on unlabeled utterances to learn a better embedding space. Zhang et al. (2021c) proposed to first pre-train on known intents and then perform k-means clustering to assign pseudo labels on unlabeled data for representation learning following Deep Clustering (Caron et al., 2018) . They also proposed to align clusters to accelerate the learning of top layers. Another approach is to first classify the utterances as known and unknown and then uncover new intents with the unknown utterances (Vedula et al., 2020; Zhang et al., 2021b) . Hence, it relies on accurate classification in the first stage.",
            "cite_spans": [
                {
                    "start": 232,
                    "end": 252,
                    "text": "Zhang et al. (2021c)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 430,
                    "end": 450,
                    "text": "(Caron et al., 2018)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 663,
                    "end": 684,
                    "text": "(Vedula et al., 2020;",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 685,
                    "end": 705,
                    "text": "Zhang et al., 2021b)",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "In this work, we address NID by proposing a multi-task pre-training method for representation learning and a contrastive learning method for clustering. In contrast to previous methods that rely on ample annotated data in the current domain for pre-training, our method can be used in an unsupervised setting and work well in data-scarce scenarios (Section 4.3).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "Pre-training for Intent Recognition. Despite the effectiveness of large-scale pre-trained language models (Radford and Narasimhan, 2018; Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020) , the inherent mismatch in linguistic behavior between the pre-training datasets and dialogues encourages the research of continual pre-training on dialogue corpus. Most previous works proposed to pre-train on open domain dialogues in a selfsupervised manner (Mehri et al., 2020; Henderson et al., 2020; Hosseini-Asl et al., 2020) . Recently, several works pointed out that pre-training with relavant tasks can be effective for intent recognition. For example, formulated intent recognition as a sentence similarity task and pre-trained on natural language inference (NLI) datasets. Vuli\u0107 et al. (2021) ; Zhang et al. (2021e) pre-trained with a contrastive loss on intent detection tasks. Our multi-task pre-training method is inspired from Zhang et al. (2021d) which leverages publicly available intent datasets and unlabeled data in the current domain for pre-training to improve the performance of few-shot intent detection. However, we argue that the method is more suitable to be applied for NID due to the natural existence of unlabeled utterances.",
            "cite_spans": [
                {
                    "start": 106,
                    "end": 136,
                    "text": "(Radford and Narasimhan, 2018;",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 137,
                    "end": 157,
                    "text": "Devlin et al., 2019;",
                    "ref_id": null
                },
                {
                    "start": 158,
                    "end": 175,
                    "text": "Liu et al., 2019;",
                    "ref_id": null
                },
                {
                    "start": 176,
                    "end": 195,
                    "text": "Brown et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 455,
                    "end": 475,
                    "text": "(Mehri et al., 2020;",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 476,
                    "end": 499,
                    "text": "Henderson et al., 2020;",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 500,
                    "end": 526,
                    "text": "Hosseini-Asl et al., 2020)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 779,
                    "end": 798,
                    "text": "Vuli\u0107 et al. (2021)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 801,
                    "end": 821,
                    "text": "Zhang et al. (2021e)",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 937,
                    "end": 957,
                    "text": "Zhang et al. (2021d)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "Contrastive Representation Learning. Contrastive learning has shown promising results in computer vision (Bachman et al., 2019; Chen et al., 2020; He et al., 2019; Khosla et al., 2020) and gained popularity in natural language processing. Some recent works used unsupervised contrastive learning to learn sentence embeddings (Gao et al., 2021; Yan et al., 2021; Kim et al., 2021; Giorgi et al., 2021) . Specifically, Gao et al. (2021) ; Yan et al. (2021) showed that contrastive loss can avoid an anisotropic embedding space. Kim et al. (2021) proposed a self-guided contrastive training to improve the quality of BERT representations. Giorgi et al. (2021) proposed to pre-train a universal sentence encoder by contrasting a randomly sampled text segment from nearby sentences. Zhang et al. (2021e) demonstrated that self-supervised contrastive pre-training and supervised contrastive fine-tuning can benefit few-shot intent recognition. Zhang et al. (2021a) showed that combining a contrastive loss with a clustering objective can improve short text clustering. Our proposed contrastive loss is tailored for clustering, which encourages utterances with similar semantics to group together and avoids pushing away false negatives as in the conventional contrastive loss.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 127,
                    "text": "(Bachman et al., 2019;",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 128,
                    "end": 146,
                    "text": "Chen et al., 2020;",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 147,
                    "end": 163,
                    "text": "He et al., 2019;",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 164,
                    "end": 184,
                    "text": "Khosla et al., 2020)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 325,
                    "end": 343,
                    "text": "(Gao et al., 2021;",
                    "ref_id": null
                },
                {
                    "start": 344,
                    "end": 361,
                    "text": "Yan et al., 2021;",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 362,
                    "end": 379,
                    "text": "Kim et al., 2021;",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 380,
                    "end": 400,
                    "text": "Giorgi et al., 2021)",
                    "ref_id": null
                },
                {
                    "start": 417,
                    "end": 434,
                    "text": "Gao et al. (2021)",
                    "ref_id": null
                },
                {
                    "start": 437,
                    "end": 454,
                    "text": "Yan et al. (2021)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 526,
                    "end": 543,
                    "text": "Kim et al. (2021)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 636,
                    "end": 656,
                    "text": "Giorgi et al. (2021)",
                    "ref_id": null
                },
                {
                    "start": 778,
                    "end": 798,
                    "text": "Zhang et al. (2021e)",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 938,
                    "end": 958,
                    "text": "Zhang et al. (2021a)",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "3 Method Problem Statement. To develop an intent recognition model, we usually prepare a set of expected intents C k along with a few annotated utterances D labeled known = {(x i , y i )|y i \u2208 C k } for each intent. After deployed, the system will encounter utterances D unlabeled = {x i |y i \u2208 {C k , C u }} from both predefined (known) intents C k and unknown intents C u . The aim of new intent discovery (NID) is to identify the emerging intents C u in D unlabeled . NID can be viewed as a direct extension of out-ofdistribution (OOD) detection, where we not only need to identify OOD examples but also discover the underlying clusters. NID is also different from zero-shot learning in that we do not presume access to any kind of class information during training. In this work, we consider both unsupervised and semi-supervised NID, which are distinguished by the existence of D labeled known , following Zhang et al. (2021c) .",
            "cite_spans": [
                {
                    "start": 911,
                    "end": 931,
                    "text": "Zhang et al. (2021c)",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "Overview of Our Approach. As shown in Fig. 2 , we propose a two-stage framework that addresses the research questions mentioned in Sec. 1. In the first stage, we perform multi-task pre-training (MTP) that jointly optimizes a crossentropy loss on external labeled data and a selfsupervised loss on target unlabeled data (Sec. 3.1). In the second stage, we first mine top-K nearest neighbors of each training instance in the embedding space and then perform contrastive learning with nearest neighbors (CLNN) (Sec. 3.2). After training, we employ a simple non-parametric clustering algorithm to obtain clustering results.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 38,
                    "end": 44,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Related Works"
        },
        {
            "text": "We propose a multi-task pre-training objective that combines a classification task on external data from publicly available intent detection datasets and a self-supervised learning task on internal data from the current domain. Different from previous works (Lin et al., 2020; Zhang et al., 2021c) , our pre-training method does not rely on annotated data (D labeled known ) from the current domain and hence can i=1 are plotted (hollow markers within large circles). Since x 2 falls within N 1 , x 2 along with its neighbors are taken as positive instance for x 1 (but not vice versa since x 1 is not in N 2 ). We also show an example of adjacency matrix A and augmented batch B . The pairwise relationships with the first instance in the batch are plotted with solid lines indicating positive pairs and dashed lines indicating negative pairs. be applied in an unsupervised setting.",
            "cite_spans": [
                {
                    "start": 258,
                    "end": 276,
                    "text": "(Lin et al., 2020;",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 277,
                    "end": 297,
                    "text": "Zhang et al., 2021c)",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Stage 1: Multi-task Pre-training (MTP)"
        },
        {
            "text": "Specifically, we first initialize the model with a pre-trained BERT encoder (Devlin et al., 2019). Then, we employ a joint pre-training loss as in Zhang et al. (2021d) . The loss consists of a crossentropy loss on external labeled data and a masked language modelling (MLM) loss on all available data from the current domain:",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 167,
                    "text": "Zhang et al. (2021d)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Stage 1: Multi-task Pre-training (MTP)"
        },
        {
            "text": "where \u03b8 are model parameters. For the supervised classification task, we leverage an external public intent dataset with diverse domains (e.g., CLINC150 (Larson et al., 2019) ), denoted as D labeled external , following Zhang et al. (2021d) . For the self-supervised MLM task, we use all available data (labeled or unlabeled) from the current domain, denoted as D all internal . Intuitively, the classification task aims to learn general knowledge of intent recognition with annotated utterances in external intent datasets, while the self-supervised task learns domain-specific semantics with utterances collected in the current domain. Together, they enable learning semantic utterance representations to provide proper cues for the subsequent clustering task. As will be shown in Sec. 4.3, both tasks are essential for NID.",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 174,
                    "text": "(Larson et al., 2019)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 220,
                    "end": 240,
                    "text": "Zhang et al. (2021d)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Stage 1: Multi-task Pre-training (MTP)"
        },
        {
            "text": "For semi-supervised NID, we can further utilize the annotated data in the current domain to conduct continual pre-training, by replacing D labeled external in Eq. 1 to D labeled known . This step is not included in unsupervised NID.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stage 1: Multi-task Pre-training (MTP)"
        },
        {
            "text": "In the second stage, we propose a contrastive learning objective that pulls together neighboring instances and pushes away distant ones in the embedding space to learn compact representations for clustering. Concretely, we first encode the utterances with the pre-trained model from stage 1. Then, for each utterance x i , we search for its top-K nearest neighbors in the embedding space using inner product as distance metric to form a neighborhood N i . The utterances in N i are supposed to share a similar intent as x i . During training, we sample a minibatch of utterances",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stage 2: Contrastive Learning with Nearest Neighbors (CLNN)"
        },
        {
            "text": "For each utterance x i \u2208 B, we uniformly sample one neighbor x i from its neighborhood N i . We then use data augmentation to generatex i andx i for x i and x i respectively. Here, we treatx i andx i as two views of x i , which form a positive pair. We then obtain an augmented batch",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stage 2: Contrastive Learning with Nearest Neighbors (CLNN)"
        },
        {
            "text": "with all the generated samples. To compute contrastive loss, we construct an adjacency matrix A for B , which is a 2M \u00d7 2M binary matrix where 1 indicates positive relation (either being neighbors or having the same intent label in semi-supervised NID) and 0 indicates negative relation. Hence, we can write the contrastive loss as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stage 2: Contrastive Learning with Nearest Neighbors (CLNN)"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stage 2: Contrastive Learning with Nearest Neighbors (CLNN)"
        },
        {
            "text": ".., 2M }} denotes the set of instances having positive relation with x i and |C i | is the cardinality.h i is the embedding for utterancex i . \u03c4 is the temperature parameter. sim(\u00b7, \u00b7) is a similarity function (e.g., dot product) on a pair of normalized feature vectors. During training, the neighborhood will be updated every few epochs. We implement the contrastive loss following Khosla et al. (2020) .",
            "cite_spans": [
                {
                    "start": 383,
                    "end": 403,
                    "text": "Khosla et al. (2020)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Stage 2: Contrastive Learning with Nearest Neighbors (CLNN)"
        },
        {
            "text": "Notice that the main difference between Eq. 2 and conventional contrastive loss is how we construct the set of positive instances C i . Conventional contrastive loss can be regarded as a special case of Eq. 2 with neighborhood size K = 0 and the same instance is augmented twice to form a positive pair (Chen et al., 2020). After contrastive learning, a non-parametric clustering algorithm such as kmeans can be applied to obtain cluster assignments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stage 2: Contrastive Learning with Nearest Neighbors (CLNN)"
        },
        {
            "text": "Data Augmentation. Strong data augmentation has been shown to be beneficial in contrastive learning (Chen et al., 2020). We find that it is inefficient to directly apply existing data augmentation methods such as EDA (Wei and Zou, 2019) , which are designed for general sentence embedding. We observe that the intent of an utterance can be expressed by only a small subset of words such as \"suggest restaurant\" or \"book a flight\". While it is hard to identify the keywords for an unlabeled utterance, randomly replacing a small amount of tokens in it with some random tokens from the library will not affect intent semantics much. This approach works well in our experiments (See Table 5 RTR).",
            "cite_spans": [
                {
                    "start": 217,
                    "end": 236,
                    "text": "(Wei and Zou, 2019)",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [
                {
                    "start": 680,
                    "end": 687,
                    "text": "Table 5",
                    "ref_id": null
                }
            ],
            "section": "Stage 2: Contrastive Learning with Nearest Neighbors (CLNN)"
        },
        {
            "text": "By introducing the notion of neighborhood relationship in contrastive learning, CLNN can 1) pull together similar instances and push away dissimilar ones to obtain more compact clusters; 2) utilize proximity in the embedding space rather than assigning noisy pseudo-labels (Van Gansbeke et al., 2020) ; 3) directly optimize in the feature space rather than clustering logits as in Van Gansbeke et al. (2020), which has been proven to be more effective by Rebuffi et al. (2020) ; and 4) naturally incorporate known intents with the adjacency matrix. Zhang et al. (2021b) . Details about dataset splitting are provided in the Appendix. Experimental Setup. We evaluate our proposed method on both unsupervised and semi-supervised NID. Notice that in unsupervised NID, no labeled utterances from the current domain are provided. For clarity, we define two variables. The proportion of known intents is defined as |C k |/(|C k |+|C u |) and referred to as \"known class ratio (KCR)\", and the proportion of labeled examples for each known intent is denoted as \"labeled ratio (LAR)\". The labeled data are randomly sampled from the original training set. Notice that, KCR = 0 means unsupervised NID, and KCR > 0 means semi-supervised NID. In the following sections, we provide experimental results for both unsupervised NID and semisupervised NID with KCR = {25%, 50%, 75%} and LAR = {10%, 50%}.",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 300,
                    "text": "(Van Gansbeke et al., 2020)",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 455,
                    "end": 476,
                    "text": "Rebuffi et al. (2020)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 549,
                    "end": 569,
                    "text": "Zhang et al. (2021b)",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Advantages of CLNN."
        },
        {
            "text": "Evaluation Metrics. We adopt three popular evaluation metrics for clustering: normalized mutual information (NMI), adjusted rand index (ARI), and accuracy (ACC).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Advantages of CLNN."
        },
        {
            "text": "Baselines and Model Variants. We summarize the baselines compared in our experiments for both unsupervised and semi-supervised NID. Our imple- (Yang et al., 2017) are unsupervised clustering methods based on stacked auto-encoder.",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 162,
                    "text": "(Yang et al., 2017)",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Advantages of CLNN."
        },
        {
            "text": "\u2022 Semi-supervised baselines.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Advantages of CLNN."
        },
        {
            "text": "(1) BERT-KCL (Hsu et al., 2018) and (2) (Zhang et al., 2021c) improves Deep Clustering (Caron et al., 2018) by aligning clusters between iterations.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 31,
                    "text": "(Hsu et al., 2018)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 40,
                    "end": 61,
                    "text": "(Zhang et al., 2021c)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 87,
                    "end": 107,
                    "text": "(Caron et al., 2018)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Advantages of CLNN."
        },
        {
            "text": "\u2022 Our model variants include MTP and MTP-CLNN, which correspond to applying kmeans on utterance representations learned in stage 1 and stage 2 respectively. Further, we continue to train a DAC model on top of MTP to form a stronger baseline MTP-DAC for semi-supervised NID.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Advantages of CLNN."
        },
        {
            "text": "Implementation. We take pre-trained bertbase-uncased model from Wolf et al. (2019) 2 as our base model and we use the [CLS] token as the BERT representation. For MTP, we first train until convergence on the external dataset, and then when training on D labeled known , we use a development set to validate early-stopping with a patience of 20 epochs following Zhang et al. (2021c) . For contrastive learning, we project a 768-d BERT embedding to an 128-d vector with a two-layer MLP and set the temperature as 0.07. For mining nearest neighbors, we use the inner product method 2 https://github.com/huggingface/ transformers provided by Johnson et al. (2017) 3 . We set neighborhood size K = 50 for BANKING and M-CID, and K = 500 for StackOverflow, since we empirically find that the optimal K should be roughly half of the average size of the training set for each class (see Section 4.4). The neighborhood is updated every 5 epochs. For data augmentation, the random token replacement probability is set to 0.25. For model optimization, we use the AdamW provided by Wolf et al. (2019) . In stage 1, the learning rate is set to 5e \u22125 . In stage 2, the learning rate is set to 1e \u22125 for BANKING and M-CID, and 1e \u22126 for StackOverflow. The batch sizes are chosen based on available GPU memory. All the experiments are conducted on a single RTX-3090 and averaged over 10 different seeds. More details are provided in the Appendix.",
            "cite_spans": [
                {
                    "start": 64,
                    "end": 82,
                    "text": "Wolf et al. (2019)",
                    "ref_id": null
                },
                {
                    "start": 360,
                    "end": 380,
                    "text": "Zhang et al. (2021c)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 637,
                    "end": 658,
                    "text": "Johnson et al. (2017)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 659,
                    "end": 660,
                    "text": "3",
                    "ref_id": null
                },
                {
                    "start": 1068,
                    "end": 1086,
                    "text": "Wolf et al. (2019)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Advantages of CLNN."
        },
        {
            "text": "Unsupervised NID. We show the results for unsupervised NID in Table 2 . First, comparing the performance of BERT-KM with GloVe-KM and SAE-KM, we observe that BERT embedding performs worse on NID even though it achieves better performance on NLP benchmarks such as GLUE, which manifests learning task-specific knowledge is important for NID. Second, our proposed pre- training method MTP improves upon baselines by a large margin. Take the NMI score of BANKING for example, MTP outperforms the strongest baseline SAE-DCN by 14.38%, which demonstrates the effectiveness of exploiting both external public datasets and unlabeled internal utterances. Furthermore, MTP-CLNN improves upon MTP by around 5% in NMI, 10% in ARI, and 10% in ACC across different datasets.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 62,
                    "end": 69,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Result Analysis"
        },
        {
            "text": "Semi-supervised NID. The results for semisupervised NID are shown in Table 3 . First, MTP significantly outperforms the strongest baseline DAC in all settings. For instance, on M-CID, MTP achieves 22.57% improvement over DAC in NMI. Moreover, MTP is less sensitive to the proportion of labeled classes. From KCR = 75% to KCR = 25% on M-CID, MTP only drops 8.55% in NMI, as opposed to about 21.58% for DAC. The less performance decrease indicates that our pretraining method is much more label-efficient. Furthermore, with our proposed contrastive learning, MTP-CLNN consistently outperforms MTP and the combined baseline MTP-DAC. Take BANK-ING with KCR = 25% for example, MTP-CLNN improves upon MTP by 4.11% in NMI while surpassing MTP-DAC by 2.63%. A similar trend can be observed when LAR = 50%, and we provide the results in the Appendix.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 69,
                    "end": 76,
                    "text": "Table 3",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Result Analysis"
        },
        {
            "text": "Visualization. In Fig. 3 , we show the t-SNE visualization of clusters with embeddings learned by two strongest baselines and our methods. It clearly shows the advantage of our methods, which can produce more compact clusters. Results on other datasets can be found in the Appendix.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 18,
                    "end": 24,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Result Analysis"
        },
        {
            "text": "To further illustrate the effectiveness of MTP, we conduct two ablation studies in this section. First, we compare MTP with the pre-training method employed in Zhang et al. (2021c) , where only internal labeled data are utilized for supervised pretraining (denoted as SUP). 4 In Fig. 4 , we show the results of both pre-training methods combined with CLNN with different proportions of known classes. Notice that when KCR = 0 there is no pretraining at all for SUP-CLNN. It can be seen that MTP-CLNN consistently outperforms SUP-CLNN. Furthermore, the performance gap increases while KCR decreases, and the largest gap is achieved when KCR = 0. This shows the high effectiveness of our method in data-scarce scenarios. Second, we decompose MTP into two parts: supervised pre-training on external public data (PUB) and self-supervised pre-training on internal unlabeled data (MLM). We report the results of the two pre-training methods combined with CLNN as well as MTP in Table 4 . We can easily conclude that either PUB or MLM is indispensable and multi-task pre-training is beneficial.",
            "cite_spans": [
                {
                    "start": 160,
                    "end": 180,
                    "text": "Zhang et al. (2021c)",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [
                {
                    "start": 279,
                    "end": 285,
                    "text": "Fig. 4",
                    "ref_id": null
                },
                {
                    "start": 972,
                    "end": 979,
                    "text": "Table 4",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Ablation Study of MTP"
        },
        {
            "text": "Number of Nearest Neighbors. We conduct an ablation study on neighborhood size K in Fig. 5 . We can make two main observations. First, although the performance of MTP-CLNN varies with different K, it still significantly outperforms MTP (dashed horizontal line) for a wide range of K. For example, MTP-CLNN is still better than MTP when K = 50 on StackOverflow or K = 200 on BANKING. Second, despite the difficulty to search for K with only unlabeled data, we empirically find an effective estimation method, i.e. to choose K as half of the average size of the training set for each class 5 . It can be seen that the estimated K \u2248 60 on BANKING and K \u2248 40 on M-CID (vertical dashed lines) lie in the optimal regions, which shows the effectiveness of our empirical estimation method.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 84,
                    "end": 90,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Analysis of CLNN"
        },
        {
            "text": "Exploration of Data Augmentation. We compare Random Token Replacement (RTR) used in our experiments with other methods. For instance, dropout is applied on embeddings to provide data augmentation in Gao et al. (2021) , randomly shuffling the order of input tokens is proven to be effective in Yan et al. (2021) , and EDA (Wei and Zou, 2019) is often applied in text classification. Furthermore, we compare with a Stop-words Replacement (SWR) variant that only replaces the stop-words with other random stop-words so it minimally af- 5 We presume prior knowledge of the number of clusters. There are some off-the-shelf methods that can be directly applied in the embedding space to determine the optimal number of clusters (Zhang et al., 2021c Table 5 : Ablation study on data augmentation for unsupervised NID. * is the method used in the main results.",
            "cite_spans": [
                {
                    "start": 199,
                    "end": 216,
                    "text": "Gao et al. (2021)",
                    "ref_id": null
                },
                {
                    "start": 293,
                    "end": 310,
                    "text": "Yan et al. (2021)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 321,
                    "end": 340,
                    "text": "(Wei and Zou, 2019)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 533,
                    "end": 534,
                    "text": "5",
                    "ref_id": null
                },
                {
                    "start": 722,
                    "end": 742,
                    "text": "(Zhang et al., 2021c",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [
                {
                    "start": 743,
                    "end": 750,
                    "text": "Table 5",
                    "ref_id": null
                }
            ],
            "section": "Analysis of CLNN"
        },
        {
            "text": "fects the intents of utterances. The results in Table 5 demonstrate that (1) RTR and SWR consistently outperform others, which verifies our hypothesis in Section 3.2.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 48,
                    "end": 55,
                    "text": "Table 5",
                    "ref_id": null
                }
            ],
            "section": "Analysis of CLNN"
        },
        {
            "text": "(2) Surprisingly, RTR and SWR perform on par with each other. For simplicity, we only report the results with RTR in the main experiments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis of CLNN"
        },
        {
            "text": "We have provided simple and effective solutions for two fundamental research questions for new intent discovery (NID): (1) how to learn better utterance representations to provide proper cues for clustering and (2) how to better cluster utterances in the representation space. In the first stage, we use a multi-task pre-training strategy to exploit both external and internal data for representation learning. In the second stage, we perform contrastive learning with mined nearest neighbors to exploit self-supervisory signals in the representation space. Extensive experiments on three intent recognition benchmarks show that our approach can significantly improve the performance of NID in both unsupervised and semi-supervised scenarios.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "There are two limitations of this work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "(1) We have only evaluated on balanced data. However, in real-world applications, most datasets are highly imbalanced.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "(2) The discovered clusters lack interpretability. Our clustering method can only assign a cluster label to each unlabeled utterance but cannot generate a valid intent name for each cluster.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "We would like to thank the anonymous reviewers for their valuable comments. This research was supported by the grants of HK ITF UIM/377 and PolyU DaSAIL project P0030935 funded by RGC. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        },
        {
            "text": "In this section, we provide more details about the datasets. The development sets are prepared to exclude no unknown intents.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Datasets"
        },
        {
            "text": "\u2022 BANKING (Casanueva et al., 2020) is a finegrained intent detection dataset in which 77 intents are collected for banking dialogue system. The dataset is splitted into 9,003, 1,000 and 3,080 for training, validation, and test sets respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Datasets"
        },
        {
            "text": "\u2022 StackOverflow (Xu et al., 2015) is a large scale dataset for online questioning which contains 20 intents with 1,000 examples in each class. We split the dataset into 18,000 for training, 1,000 for validation, and 1,000 for test.",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 33,
                    "text": "(Xu et al., 2015)",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "A.1 Datasets"
        },
        {
            "text": "\u2022 M-CID (Arora et al., 2020 ) is a small scale dataset for cross-lingual Covid-19 queries. We only use the English subset of this dataset which has 16 intents. We split the dataset into 1,220 for training, 176 for validation, and 349 for test.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 27,
                    "text": "(Arora et al., 2020",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A.1 Datasets"
        },
        {
            "text": "\u2022 CLINC150 (Larson et al., 2019) consists of 10 domains across multiple unique services. We use 8 domains 6 and remove the out-ofscope data. We only use this dataset during training stage 1.",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 32,
                    "text": "(Larson et al., 2019)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "A.1 Datasets"
        },
        {
            "text": "The batch size is set to 64 for stage 1 and 128 for stage 2 in all experiments to fully utilize the GPU memory. In stage 1, we first train until convergence on external data and then train with validation on internal data. In stage 2, we train until convergence without early-stopping.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2 Implementation"
        },
        {
            "text": "The results on semi-supervised NID when LAR = 50% are shown in Table 6 . It can be seen that our methods still achieve the best performance in this case. In Fig. 6 and Fig. 7 , we show the t-SNE visualization of clusters on BANKING and M-CID with embeddings learned by two strongest baselines and our methods. Again, it shows that our methods can produce more compact clusters. Table 6 : Performance on semi-supervised NID with different known class ratio. The LAR is set to 50%. For each dataset, the best results are marked in bold. Comb denotes the baseline method combined with our proposed MTP. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 63,
                    "end": 70,
                    "text": "Table 6",
                    "ref_id": null
                },
                {
                    "start": 157,
                    "end": 163,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 168,
                    "end": 174,
                    "text": "Fig. 7",
                    "ref_id": null
                },
                {
                    "start": 378,
                    "end": 385,
                    "text": "Table 6",
                    "ref_id": null
                }
            ],
            "section": "A.3 More Experimental Results"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Mrinal Mohit, Lorena Sainz-Maza Lecanda, and Ahmed Aly. 2020. Cross-lingual transfer learning for intent detection of covid-19 utterances",
            "authors": [
                {
                    "first": "Abhinav",
                    "middle": [],
                    "last": "Arora",
                    "suffix": ""
                },
                {
                    "first": "Akshat",
                    "middle": [],
                    "last": "Shrivastava",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Learning representations by maximizing mutual information across views",
            "authors": [
                {
                    "first": "Philip",
                    "middle": [],
                    "last": "Bachman",
                    "suffix": ""
                },
                {
                    "first": "Devon",
                    "middle": [],
                    "last": "Hjelm",
                    "suffix": ""
                },
                {
                    "first": "William",
                    "middle": [],
                    "last": "Buchwalter",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "32",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners",
            "authors": [
                {
                    "first": "Tom",
                    "middle": [],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "Benjamin",
                    "middle": [],
                    "last": "Mann",
                    "suffix": ""
                },
                {
                    "first": "Nick",
                    "middle": [],
                    "last": "Ryder",
                    "suffix": ""
                },
                {
                    "first": "Melanie",
                    "middle": [],
                    "last": "Subbiah",
                    "suffix": ""
                },
                {
                    "first": "Jared",
                    "middle": [
                        "D"
                    ],
                    "last": "Kaplan",
                    "suffix": ""
                },
                {
                    "first": "Prafulla",
                    "middle": [],
                    "last": "Dhariwal",
                    "suffix": ""
                },
                {
                    "first": "Arvind",
                    "middle": [],
                    "last": "Neelakantan",
                    "suffix": ""
                },
                {
                    "first": "Pranav",
                    "middle": [],
                    "last": "Shyam",
                    "suffix": ""
                },
                {
                    "first": "Girish",
                    "middle": [],
                    "last": "Sastry",
                    "suffix": ""
                },
                {
                    "first": "Amanda",
                    "middle": [],
                    "last": "Askell",
                    "suffix": ""
                },
                {
                    "first": "Sandhini",
                    "middle": [],
                    "last": "Agarwal",
                    "suffix": ""
                },
                {
                    "first": "Ariel",
                    "middle": [],
                    "last": "Herbert-Voss",
                    "suffix": ""
                },
                {
                    "first": "Gretchen",
                    "middle": [],
                    "last": "Krueger",
                    "suffix": ""
                },
                {
                    "first": "Tom",
                    "middle": [],
                    "last": "Henighan",
                    "suffix": ""
                },
                {
                    "first": "Rewon",
                    "middle": [],
                    "last": "Child",
                    "suffix": ""
                },
                {
                    "first": "Aditya",
                    "middle": [],
                    "last": "Ramesh",
                    "suffix": ""
                },
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Ziegler",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Clemens",
                    "middle": [],
                    "last": "Winter",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Hesse",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Eric",
                    "middle": [],
                    "last": "Sigler",
                    "suffix": ""
                },
                {
                    "first": "Mateusz",
                    "middle": [],
                    "last": "Litwin",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "33",
            "issn": "",
            "pages": "1877--1901",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Deep clustering for unsupervised learning of visual features",
            "authors": [
                {
                    "first": "Mathilde",
                    "middle": [],
                    "last": "Caron",
                    "suffix": ""
                },
                {
                    "first": "Piotr",
                    "middle": [],
                    "last": "Bojanowski",
                    "suffix": ""
                },
                {
                    "first": "Armand",
                    "middle": [],
                    "last": "Joulin",
                    "suffix": ""
                },
                {
                    "first": "Matthijs",
                    "middle": [],
                    "last": "Douze",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Momentum contrast for unsupervised visual representation learning",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Haoqi",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Yuxin",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Saining",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1911.05722"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "ConveRT: Efficient and accurate conversational representations from transformers",
            "authors": [
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Henderson",
                    "suffix": ""
                },
                {
                    "first": "I\u00f1igo",
                    "middle": [],
                    "last": "Casanueva",
                    "suffix": ""
                },
                {
                    "first": "Nikola",
                    "middle": [],
                    "last": "Mrk\u0161i\u0107",
                    "suffix": ""
                },
                {
                    "first": "Pei-Hao",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "Tsung-Hsien",
                    "middle": [],
                    "last": "Wen",
                    "suffix": ""
                },
                {
                    "first": "Ivan",
                    "middle": [],
                    "last": "Vuli\u0107",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
            "volume": "",
            "issn": "",
            "pages": "2161--2174",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/2020.findings-emnlp.196"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A simple language model for task-oriented dialogue",
            "authors": [
                {
                    "first": "Ehsan",
                    "middle": [],
                    "last": "Hosseini-Asl",
                    "suffix": ""
                },
                {
                    "first": "Bryan",
                    "middle": [],
                    "last": "Mccann",
                    "suffix": ""
                },
                {
                    "first": "Chien-Sheng",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Semih",
                    "middle": [],
                    "last": "Yavuz",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "volume": "2020",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Learning to cluster in order to transfer across domains and tasks",
            "authors": [
                {
                    "first": "Yen-Chang",
                    "middle": [],
                    "last": "Hsu",
                    "suffix": ""
                },
                {
                    "first": "Zhaoyang",
                    "middle": [],
                    "last": "Lv",
                    "suffix": ""
                },
                {
                    "first": "Zsolt",
                    "middle": [],
                    "last": "Kira",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Multi-class classification without multi-class labels",
            "authors": [
                {
                    "first": "Yen-Chang",
                    "middle": [],
                    "last": "Hsu",
                    "suffix": ""
                },
                {
                    "first": "Zhaoyang",
                    "middle": [],
                    "last": "Lv",
                    "suffix": ""
                },
                {
                    "first": "Joel",
                    "middle": [],
                    "last": "Schlosser",
                    "suffix": ""
                },
                {
                    "first": "Phillip",
                    "middle": [],
                    "last": "Odom",
                    "suffix": ""
                },
                {
                    "first": "Zsolt",
                    "middle": [],
                    "last": "Kira",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Billion-scale similarity search with gpus",
            "authors": [
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "Matthijs",
                    "middle": [],
                    "last": "Douze",
                    "suffix": ""
                },
                {
                    "first": "Herv\u00e9",
                    "middle": [],
                    "last": "J\u00e9gou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1702.08734"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Supervised contrastive learning",
            "authors": [
                {
                    "first": "Prannay",
                    "middle": [],
                    "last": "Khosla",
                    "suffix": ""
                },
                {
                    "first": "Piotr",
                    "middle": [],
                    "last": "Teterwak",
                    "suffix": ""
                },
                {
                    "first": "Chen",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Sarna",
                    "suffix": ""
                },
                {
                    "first": "Yonglong",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "Phillip",
                    "middle": [],
                    "last": "Isola",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Maschinot",
                    "suffix": ""
                },
                {
                    "first": "Ce",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Dilip",
                    "middle": [],
                    "last": "Krishnan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "33",
            "issn": "",
            "pages": "18661--18673",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Self-guided contrastive learning for BERT sentence representations",
            "authors": [
                {
                    "first": "Taeuk",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Sang-Goo",
                    "middle": [],
                    "last": "Kang Min Yoo",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
            "volume": "1",
            "issn": "",
            "pages": "2528--2540",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/2021.acl-long.197"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "An evaluation dataset for intent classification and out-of-scope prediction",
            "authors": [
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Larson",
                    "suffix": ""
                },
                {
                    "first": "Anish",
                    "middle": [],
                    "last": "Mahendran",
                    "suffix": ""
                },
                {
                    "first": "Joseph",
                    "middle": [
                        "J"
                    ],
                    "last": "Peper",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Clarke",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Parker",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "Jonathan",
                    "middle": [
                        "K"
                    ],
                    "last": "Kummerfeld",
                    "suffix": ""
                },
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Leach",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [
                        "A"
                    ],
                    "last": "Laurenzano",
                    "suffix": ""
                },
                {
                    "first": "Lingjia",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Jason",
                    "middle": [],
                    "last": "Mars",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
            "volume": "",
            "issn": "",
            "pages": "1311--1316",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/D19-1131"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Discovering new intents via constrained deep adaptive clustering with cluster refinement",
            "authors": [
                {
                    "first": "Hua",
                    "middle": [],
                    "last": "Ting-En Lin",
                    "suffix": ""
                },
                {
                    "first": "Hanlei",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Thirty-Fourth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Some methods for classification and analysis of multivariate observations",
            "authors": [
                {
                    "first": "James",
                    "middle": [],
                    "last": "Macqueen",
                    "suffix": ""
                }
            ],
            "year": 1967,
            "venue": "Proceedings of the fifth Berkeley symposium on mathematical statistics and probability",
            "volume": "1",
            "issn": "",
            "pages": "281--297",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Dialoglue: A natural language understanding benchmark for task-oriented dialogue",
            "authors": [
                {
                    "first": "Shikib",
                    "middle": [],
                    "last": "Mehri",
                    "suffix": ""
                },
                {
                    "first": "Mihail",
                    "middle": [],
                    "last": "Eric",
                    "suffix": ""
                },
                {
                    "first": "Dilek",
                    "middle": [],
                    "last": "Hakkani-Tur",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Intent discovery through unsupervised semantic text clustering",
            "authors": [
                {
                    "first": "Srinivas",
                    "middle": [],
                    "last": "Bangalore",
                    "suffix": ""
                },
                {
                    "first": "Padmasundari",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc",
            "volume": "",
            "issn": "",
            "pages": "606--610",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "GloVe: Global vectors for word representation",
            "authors": [
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {
                "DOI": [
                    "10.3115/v1/D14-1162"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Dialog intent induction with deep multi-view clustering",
            "authors": [
                {
                    "first": "Hugh",
                    "middle": [],
                    "last": "Perkins",
                    "suffix": ""
                },
                {
                    "first": "Yi",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
            "volume": "",
            "issn": "",
            "pages": "4016--4025",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/D19-1413"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Improving language understanding by generative pretraining",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Karthik",
                    "middle": [],
                    "last": "Narasimhan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Lsdc: Linearly separable deep clusters. arXiv",
            "authors": [
                {
                    "first": "Sebastien",
                    "middle": [],
                    "last": "Sylvestre-Alvise Rebuffi",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Ehrhardt",
                    "suffix": ""
                },
                {
                    "first": "Andrea",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Autodialabel: Labeling dialogue data with unsupervised learning",
            "authors": [
                {
                    "first": "Chen",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Qi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Lei",
                    "middle": [],
                    "last": "Sha",
                    "suffix": ""
                },
                {
                    "first": "Sujian",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Xu",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Houfeng",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Lintao",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "684--689",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/D18-1072"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Scan: Learning to classify images without labels",
            "authors": [
                {
                    "first": "Simon",
                    "middle": [],
                    "last": "Wouter Van Gansbeke",
                    "suffix": ""
                },
                {
                    "first": "Stamatios",
                    "middle": [],
                    "last": "Vandenhende",
                    "suffix": ""
                },
                {
                    "first": "Marc",
                    "middle": [],
                    "last": "Georgoulis",
                    "suffix": ""
                },
                {
                    "first": "Luc",
                    "middle": [],
                    "last": "Proesmans",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the European Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Automatic discovery of novel intents & domains from text utterances",
            "authors": [
                {
                    "first": "Nikhita",
                    "middle": [],
                    "last": "Vedula",
                    "suffix": ""
                },
                {
                    "first": "Rahul",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "Aman",
                    "middle": [],
                    "last": "Alok",
                    "suffix": ""
                },
                {
                    "first": "Mukund",
                    "middle": [],
                    "last": "Sridhar",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "ConvFiT: Conversational fine-tuning of pretrained language models",
            "authors": [
                {
                    "first": "Ivan",
                    "middle": [],
                    "last": "Vuli\u0107",
                    "suffix": ""
                },
                {
                    "first": "Pei-Hao",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "Samuel",
                    "middle": [],
                    "last": "Coope",
                    "suffix": ""
                },
                {
                    "first": "Daniela",
                    "middle": [],
                    "last": "Gerz",
                    "suffix": ""
                },
                {
                    "first": "Pawe\u0142",
                    "middle": [],
                    "last": "Budzianowski",
                    "suffix": ""
                },
                {
                    "first": "I\u00f1igo",
                    "middle": [],
                    "last": "Casanueva",
                    "suffix": ""
                },
                {
                    "first": "Nikola",
                    "middle": [],
                    "last": "Mrk\u0161i\u0107",
                    "suffix": ""
                },
                {
                    "first": "Tsung-Hsien",
                    "middle": [],
                    "last": "Wen",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "1151--1168",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
            "authors": [
                {
                    "first": "Jason",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Zou",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
            "volume": "",
            "issn": "",
            "pages": "6382--6388",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/D19-1670"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue",
            "authors": [
                {
                    "first": "Chien-Sheng",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "H"
                    ],
                    "last": "Steven",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Hoi",
                    "suffix": ""
                },
                {
                    "first": "Caiming",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "917--929",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/2020.emnlp-main.66"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Unsupervised deep embedding for clustering analysis",
            "authors": [
                {
                    "first": "Junyuan",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of The 33rd International Conference on Machine Learning",
            "volume": "48",
            "issn": "",
            "pages": "478--487",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Short text clustering via convolutional neural networks",
            "authors": [
                {
                    "first": "Jiaming",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Peng",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Guanhua",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "Bo",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Fangyuan",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Hongwei",
                    "middle": [],
                    "last": "Hao",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "62--69",
            "other_ids": {
                "DOI": [
                    "10.3115/v1/W15-1509"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "ConSERT: A contrastive framework for self-supervised sentence representation transfer",
            "authors": [
                {
                    "first": "Yuanmeng",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "Rumei",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Sirui",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Fuzheng",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Weiran",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
            "volume": "1",
            "issn": "",
            "pages": "5065--5075",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/2021.acl-long.393"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Towards k-means-friendly spaces: Simultaneous deep learning and clustering",
            "authors": [
                {
                    "first": "Bo",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Xiao",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "Nicholas",
                    "middle": [
                        "D"
                    ],
                    "last": "Sidiropoulos",
                    "suffix": ""
                },
                {
                    "first": "Mingyi",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 34th International Conference on Machine Learning",
            "volume": "70",
            "issn": "",
            "pages": "3861--3870",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Supporting clustering with contrastive learning",
            "authors": [
                {
                    "first": "Dejiao",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Feng",
                    "middle": [],
                    "last": "Nan",
                    "suffix": ""
                },
                {
                    "first": "Xiaokai",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Shang-Wen",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Henghui",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Kathleen",
                    "middle": [],
                    "last": "Mckeown",
                    "suffix": ""
                },
                {
                    "first": "Ramesh",
                    "middle": [],
                    "last": "Nallapati",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [
                        "O"
                    ],
                    "last": "Arnold",
                    "suffix": ""
                },
                {
                    "first": "Bing",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "",
            "issn": "",
            "pages": "5419--5430",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/2021.naacl-main.427"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "TEXTOIR: An integrated and visualized platform for text open intent recognition",
            "authors": [
                {
                    "first": "Hanlei",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Xiaoteng",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Hua",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Panpan",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Kang",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
            "volume": "",
            "issn": "",
            "pages": "167--174",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/2021.acl-demo.20"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Discovering new intents with deep aligned clustering",
            "authors": [
                {
                    "first": "Hanlei",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Hua",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Rui",
                    "middle": [],
                    "last": "Ting-En Lin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lyu",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "35",
            "issn": "",
            "pages": "14365--14373",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Effectiveness of pre-training for few-shot intent classification",
            "authors": [
                {
                    "first": "Haode",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Yuwei",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Li-Ming",
                    "middle": [],
                    "last": "Zhan",
                    "suffix": ""
                },
                {
                    "first": "Jiaxin",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Guangyuan",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Xiao-Ming",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Albert",
                    "middle": [
                        "Y S"
                    ],
                    "last": "Lam",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Few-shot intent detection via contrastive pre-training and finetuning",
            "authors": [
                {
                    "first": "Jianguo",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Trung",
                    "middle": [],
                    "last": "Bui",
                    "suffix": ""
                },
                {
                    "first": "Seunghyun",
                    "middle": [],
                    "last": "Yoon",
                    "suffix": ""
                },
                {
                    "first": "Xiang",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Zhiwei",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Congying",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "Hung",
                    "middle": [],
                    "last": "Quan",
                    "suffix": ""
                },
                {
                    "first": "Walter",
                    "middle": [],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "Philip",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "1906--1912",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Discriminative nearest neighbor few-shot intent detection by transferring natural language inference",
            "authors": [
                {
                    "first": "Jianguo",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Kazuma",
                    "middle": [],
                    "last": "Hashimoto",
                    "suffix": ""
                },
                {
                    "first": "Wenhao",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Chien-Sheng",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Yao",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                },
                {
                    "first": "Philip",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Caiming",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Online. Association for Computational Linguistics. BANKING StackOverflow M-CID KCR Methods NMI ARI ACC NMI ARI ACC NMI ARI ACC",
            "volume": "",
            "issn": "",
            "pages": "5064--5082",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/2020.emnlp-main.411"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "New Intent Discovery.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The left part shows the overall workflow of our method where the training order is indicated by the red arrow. The datasets and corresponding loss functions used in each training stage are indicated by the black arrows. The right part illustrates a simple example of CLNN. A batch of four training instances {x i } 4 i=1 (solid markers) and their respective neighborhoods {N i } 4",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "mentation is based onZhang et al. (2021b).Unsupervised baselines. (1) GloVe-KM and (2) GloVe-AG are based on GloVe (Pennington et al., 2014) embeddings and then evaluated with k-means (MacQueen et al., 1967) or agglomerative clustering (Gowda, 1984) respectively. (3) BERT-KM applies kmeans on BERT embeddings. (4) SAE-KM 1 For fair comparison, the baselines are re-run with TEX-TOIR: https://github.com/thuiar/TEXTOIR, and hence some results are different from those reported in Lin et al. (2020); Zhang et al. (2021c). adopts k-means on embeddings of stacked auto-encoder. (5) Deep Embedding Clustering (SAE-DEC) (Xie et al., 2016) and (6) Deep Clustering Network (SAE-DCN)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "BERT-MCL (Hsu et al., 2019) employs pairwise similarity task for semi-supervised clustering. (3) BERT-DTC (Han et al., 2019) extends DEC into semi-supervised scenario. (4) CDAC+ (Lin Visulization of embeddings on StackOverflow. KCR = 25%, LAR = 10%. Best viewed in color. Ablation study on the effectiveness of MTP. The LAR is set to 10%. SUP stands for supervised pretraining on internal labeled data only. The three columns correspond to results in the three metrics respectively. et al., 2020) employs a pseudo-labeling process. (5) Deep Aligned Clustering (DAC)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Analysis on the number of nearest neighbors in CLNN for unsupervised NID. Vertical dashed lines correspond to our empirical estimations of optima. Horizontal dashed lines represent the results of only training with MTP. When the number of nearest neighbors is 0, we simply augment the same instance twice as in conventional contrastive learning(Chen et al., 2020). The three columns correspond to results in the three metrics respectively.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": ": t-SNE visulization of embeddings on BANKING. KCR = 25%, LAR = 10%. Best viewed in color. (a) CDAC+ (b) DAC (c) MTP (Ours) (d) MTP-CLNN (Ours) Figure 7: t-SNE visulization of embeddings on M-CID. KCR = 25%, LAR = 10%. Best viewed in color.",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "Dataset statistics.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Performance on unsupervised NID. For each dataset, the best results are marked in bold. DAC (Comb) 85.78 65.28 75.43 80.89 71.17 84.20 80.94 68.27 80.89 MTP-CLNN (Ours) 87.52 70.00 79.74 82.56 75.66 87.63 83.75 73.22 84.36",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Performance on semi-supervised NID with different known class ratio. The LAR is set to 10%. For each dataset, the best results are marked in bold. Comb denotes the baseline method combined with our proposed MTP.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Ablation study of MTP for unsupervised NID.",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": ").",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "I\u00f1igo Casanueva, Tadas Tem\u010dinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli\u0107. 2020. Efficient intent detection with dual sentence encoders. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 38-45, Online. Association for Computational Linguistics. Ajay Chatterjee and Shubhashis Sengupta. 2020. Intent mining from past conversations for conversational agent. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4140-4152, Barcelona, Spain (Online). International Committee on Computational Linguistics. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. Jackie Chi Kit Cheung and Xiao Li. 2012. Sequence clustering and labeling for unsupervised query intent discovery. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 383-392. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics. George Forman, Hila Nachlieli, and Renato Keshet. 2015. Clustering by intent: A semi-supervised method to discover relevant clusters incrementally. In Machine Learning and Knowledge Discovery in Databases, pages 20-36, Cham. Springer International Publishing. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep contrastive learning for unsupervised textual representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 879-895, Online. Association for Computational Linguistics. K Chidananda Gowda. 1984. A feature reduction and unsupervised classification algorithm for multispectral data. Pattern recognition, 17(6):667-676. Beliz Gunel, Jingfei Du, Alexis Conneau, and Veselin Stoyanov. 2021. Supervised contrastive learning for pre-trained language model fine-tuning. In International Conference on Learning Representations. Hakkani-T\u00fcr, Yun-Cheng Ju, Geoffrey Zweig, and Gokhan Tur. 2015. Clustering novel intents in a conversational interaction system with semantic parsing. In Sixteenth Annual Conference of the International Speech Communication Association. Kai Han, Andrea Vedaldi, and Andrew Zisserman. 2019. Learning to discover novel visual categories via deep transfer clustering. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). Haponchyk and Alessandro Moschitti. 2021. Supervised neural clustering via latent structured output learning: Application to question intents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3364-3374, Online. Association for Computational Linguistics. Iryna Haponchyk, Antonio Uva, Seunghak Yu, Olga Uryupina, and Alessandro Moschitti. 2018. Supervised clustering of questions into intents for dialog system applications. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2310-2321, Brussels, Belgium. Association for Computational Linguistics.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}
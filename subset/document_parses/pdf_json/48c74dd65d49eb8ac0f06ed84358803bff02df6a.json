{
    "paper_id": "48c74dd65d49eb8ac0f06ed84358803bff02df6a",
    "metadata": {
        "title": "Tracking social media during the COVID-19 pandemic: The case study of lockdown in New York State",
        "authors": [
            {
                "first": "Lin",
                "middle": [],
                "last": "Miao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Ben-Gurion University of the Negev",
                    "location": {
                        "postBox": "P.O.B. 653",
                        "postCode": "8410501",
                        "settlement": "Be'er Sheva",
                        "country": "Israel"
                    }
                },
                "email": ""
            },
            {
                "first": "Mark",
                "middle": [],
                "last": "Last",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Ben-Gurion University of the Negev",
                    "location": {
                        "postBox": "P.O.B. 653",
                        "postCode": "8410501",
                        "settlement": "Be'er Sheva",
                        "country": "Israel"
                    }
                },
                "email": ""
            },
            {
                "first": "Marina",
                "middle": [],
                "last": "Litvak",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Shamoon College of Engineering",
                    "location": {
                        "addrLine": "56 Bialik St",
                        "postCode": "8410802",
                        "settlement": "Be'er Sheva",
                        "country": "Israel"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Facing the COVID-19 pandemic, governments have implemented a wide range of policies to contain the spread of the virus. During the pandemic, large amounts of COVID-19-related tweets emerge every day. Real-time processing of daily tweets may offer insights for monitoring public opinion about intervention measures implemented. In this work, lockdown policy in New York State has been set as a target of public opinion research. This task includes two stages, stance detection and opinion monitoring. For the stance detection stage, we explored several combinations of different text representations and classification algorithms, finding that the combination of Long Short-Term Memory (LSTM) with Global Vectors for Word Representation (GloVe) outperforms others. Due to the shortage of labeled data, we adopted the data distillation method for the training data augmentation. The augmentation of the training data allows to improve the performance of the model with a very small amount of manually-labeled data. After applying the distillation method, the accuracy of the model has been significantly improved. Utilizing the enhanced model, automatically classified tweets are analyzed over time to monitor the public opinion. By exploring the tweets in New York from January 22nd until September 30th, 2020, we show the correlation of public opinion with COVID-19 cases and mortality data, and the effect of government responses on the opinion shift. These results demonstrate the capability of the presented method to effectively and efficiently monitor public opinion during a pandemic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "E-mail addresses: miaol@post.bgu.ac.il (L. Miao), mlast@bgu.ac.il (M. Last), marinal@sce.ac.il (M. Litvak). 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "As many countries are facing unprecedented challenges from COVID-19, the strain on the governments to handle this pandemic and contain the spread of the disease is extreme. Governments have implemented a range of different measures to limit the spread of the COVID-19 virus, safeguard people's lives and health, and secure adequate health care capacity. COVID-19 control measures require cooperation between government and citizens, as the worldwide impact of the pandemic continues to grow. Hence, it is critical for decision-makers to monitor public opinion about the intervention measures over time, so they can adjust their policy accordingly. During the COVID-19 pandemic, social media has seen a significant increase in use; therefore, the Twitter platform may be an important source of large amounts of data for realtime opinion monitoring. This paper aims at using social media for realtime monitoring of public opinion about intervention measures during the COVID-19 pandemic. To monitor public opinion on social media, stance detection is conducted to identify agreement or disagreement on a certain control measure, such as the lockdown. This task can be successfully handled by supervised learning algorithms, which usually rely on large amounts of annotated data. As such, the limited amount of manually-labeled tweets related to COVID-19 presents a major challenge for real-time public opinion monitoring.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Facing the shortage of annotated stance detection data, we apply a data distillation method for data augmentation. In our previously published work (Miao et al., 2020) , we explored several data augmentation methods, showing that with a small manually-labeled dataset, data distillation outperforms other methods on this task. Data distillation is a simple omni-supervised learning method that uses labeled and unlabeled data, together with self-training, to enhance the performance of the model (Radosavovic et al., 2018; Furlanello et al., 2018; Zhang and Sabuncu, 2020) . The previous studies have shown that when updated with unseen data in several iterations, data distillation models can reach a consistent improvement.",
            "cite_spans": [
                {
                    "start": 148,
                    "end": 167,
                    "text": "(Miao et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 496,
                    "end": 522,
                    "text": "(Radosavovic et al., 2018;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 523,
                    "end": 547,
                    "text": "Furlanello et al., 2018;",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 548,
                    "end": 572,
                    "text": "Zhang and Sabuncu, 2020)",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "After applying stance detection to COVID-19-related tweets, we monitor the changes in public attitude (against or in favor of intervention measures), as expressed in these tweets over time. To more deeply understand these changes, we sought the answers for the following questions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 How does public opinion about intervention measures change over time during the COVID-19 pandemic? \u2022 How are COVID-19 statistics (e.g., confirmed cases, death cases) related to public opinion? \u2022 How do government intervention measures affect the public opinion? \u2022 What triggers the changes in distribution of negative and positive stance tweets?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Based on these questions, we explore the correlation between public opinion, government policies, and COVID-19 statistics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The main contributions of our work are summarized as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We explored several text representations and classification approaches to monitor public attitude towards intervention measures during COVID-19 pandemic. \u2022 We conducted a comprehensive analysis of public opinion about COVID-19 in NY State from January 22nd to September 30th, 2020, to understand its relations with COVID-19 statistics and the intervention measures taken by the government. \u2022 We evaluated the performance of the data distillation method for training data augmentation using only a small set of manuallylabeled samples. \u2022 We created and released a new dataset of COVID-19-related tweets, which can be helpful for other researchers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Real-time information from online social networks has been widely used for various emergency incidents and events. During the H1N1 Influenza pandemic in United States, Signorini et al. (2011) used Twitter data to build a regression model to measure public concerns about that pandemic and to track public sentiment about H1N1, in order to estimate the disease activity level in real time. During the 2016 Zika virus epidemic, Masri et al. (2019) demonstrated the feasibility of utilizing Twitter data for disease surveillance on a national and state (Florida) level. To explore the possibility of detecting influenza outbreaks by analyzing Twitter data, Culotta (2010) correlated the influenza-related tweets with the statistics from the Centers for Disease Control and Prevention (CDC).",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 191,
                    "text": "Signorini et al. (2011)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 426,
                    "end": 445,
                    "text": "Masri et al. (2019)",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 654,
                    "end": 668,
                    "text": "Culotta (2010)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "During the COVID-19 pandemic, extensive research has been conducted focusing on Twitter data, to analyze public sentiment and responses. To analyze epidemic-related content in social networks, Li et al. (2020) trained a traditional Support Vector Machine (SVM) classifier on manually-labeled data and applied the induced SVM model to label the rest of the data automatically. To understand the temporal sentiment, Wang et al. (2020) used a exicon-based model to output the sentiment score of each tweet. They also used a case study format to explore public attitude towards specific measures.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 209,
                    "text": "Li et al. (2020)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 414,
                    "end": 432,
                    "text": "Wang et al. (2020)",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "Most of the sentiment analysis studies deal with general public sentiment on some events or during some specific time period. In our work, we are interested in exploring public opinion about intervention measures taken by the NY State government during the COVID-19 outbreak, which is more than just the sentiment. Public opinion is defined as stance (support or against) about a target topic, regardless of whether positive or negative language is used in the text.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "In terms of stance analysis on Twitter, general approach is to build supervised classifiers using labeled data. Some works employed the Multinomial Naive Bayes or the SVM model, with Bag-of-Words (BOW) text representation, while others used n-grams as tokens (D'Andrea et al., 2019; Kunneman et al., 2020; Skeppstedt et al., 2017) . Compared with D' Andrea et al. (2019) , which manually labeled a small training dataset for supervised classification, Kunneman et al. (2020) ; Skeppstedt et al. (2017) conducted an expensive annotation process to get sufficient labeled data for machine learning classifiers. In contrast, Lukasik et al. (2016) explored the use of Hawkes Processes, taking into account temporal information in addition to text, for stance classification of Twitter rumors. To reduce the manual labeling effort, Rajadesingan and Liu (2014) presented a semi-supervised retweet-based label propagation method, to detect opinion about a certain topic on Twitter. Unlike most of the works, they utilized the retweeting activity of users to obtain the annotations, requiring only a small amount of manually-labeled data. Recently, deep learning methods have become widespread in stance analysis. In the SemEval-2016 stance detection challenge (Mohammad et al., 2016) , several deep-learning-based methods were top performers, such as transfer learning based on Recurrent Neural Networks (RNN) in Zarrella and Marsh (2016) , Convolutional neural network (CNN) was used in Wei et al. (2016) , and so on. More recently, an attention based neural ensemble method was developed by Siddiqua et al. (2019) for tweet stance detection. The authors used multi-kernel convolution to extract higher-level features, then combined densely connected Bidirectional Long Short-Term Memory (Bi-LSTM) and nested LSTM models to predict stance. Augenstein et al. (2016) implemented a bidirectional conditional LSTM encoding model for stance classification. However, most of the works on stance detection rely on large amounts of manually-labeled data. In our work, we explore a data distillation method for augmenting the training data by leveraging the unlabeled data. In addition, we also utilize deep learning approaches and state-ofthe-art text representation models for the stance classification of tweets.",
            "cite_spans": [
                {
                    "start": 259,
                    "end": 282,
                    "text": "(D'Andrea et al., 2019;",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 283,
                    "end": 305,
                    "text": "Kunneman et al., 2020;",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 306,
                    "end": 330,
                    "text": "Skeppstedt et al., 2017)",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 350,
                    "end": 370,
                    "text": "Andrea et al. (2019)",
                    "ref_id": null
                },
                {
                    "start": 452,
                    "end": 474,
                    "text": "Kunneman et al. (2020)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 477,
                    "end": 501,
                    "text": "Skeppstedt et al. (2017)",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 622,
                    "end": 643,
                    "text": "Lukasik et al. (2016)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 827,
                    "end": 854,
                    "text": "Rajadesingan and Liu (2014)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1253,
                    "end": 1276,
                    "text": "(Mohammad et al., 2016)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1406,
                    "end": 1431,
                    "text": "Zarrella and Marsh (2016)",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 1481,
                    "end": 1498,
                    "text": "Wei et al. (2016)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1586,
                    "end": 1608,
                    "text": "Siddiqua et al. (2019)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1834,
                    "end": 1858,
                    "text": "Augenstein et al. (2016)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Stance analysis"
        },
        {
            "text": "Data augmentation is critical for supervised learning to tackle the shortage of labeled training data (Dao et al., 2019) . Han et al. (2019) leveraged a large domain-specific corpus to fine-tune a language model, which enabled researchers to augment training data by weak supervision for rumor detection. Different from this work, we leverage a small labeled dataset using a distillation method to augment training data. Liu et al. (2020) proposed a text augmentation method using reinforcement learning to guide conditional text generation. For classification of tweets, Sharifirad et al. (2018) conducted data augmentation by using a combination of knowledge graphs to add the related concepts to the original tweets. To boost the performance of text classification, Wei and Zou (2019) proposed to conduct data augmentation using four operations: synonym replacement, random insertion, random swap, and random deletion. This data augmentation method requires to extend the datasets with complex transformations of the original data. In contrast to these works, we aim to explore automatic data augmentation under weak supervision leveraging of unlabeled data, without requiring data transformation or data generation. Hinton et al. (2015) proposed to use knowledge distillation to transfer knowledge from an ensemble of models into a single model that can be trained easily and rapidly. Cho and Hariharan (2019) conducted a comprehensive experimental analysis to explore the possible factors that may influence knowledge distillation. Li et al. (2017) used data a distillation method for dialogue generation task. Differently, they aimed to increase the specificity of the trained models to generate dialogue, so their data distillation is to remove the training examples from the dataset, which will not contribute to the specificity of the model in each iteration. The idea of data distillation has been adopted in various semi-supervised learning tasks on weakly-labeled data. Radosavovic et al. (2018) trained a model on large amounts of labeled data to generate the annotations on unlabeled data; they then retrained the model using generated annotations. Based on data distillation framework, Liu et al. (2019) distilled predictions from a teacher model to guide the learning of student model.",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 120,
                    "text": "(Dao et al., 2019)",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 123,
                    "end": 140,
                    "text": "Han et al. (2019)",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 421,
                    "end": 438,
                    "text": "Liu et al. (2020)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 572,
                    "end": 596,
                    "text": "Sharifirad et al. (2018)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 769,
                    "end": 787,
                    "text": "Wei and Zou (2019)",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 1220,
                    "end": 1240,
                    "text": "Hinton et al. (2015)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1537,
                    "end": 1553,
                    "text": "Li et al. (2017)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1982,
                    "end": 2007,
                    "text": "Radosavovic et al. (2018)",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 2201,
                    "end": 2218,
                    "text": "Liu et al. (2019)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Data augmentation on text"
        },
        {
            "text": "Inspired by the idea of data distillation, we apply it for training data augmentation, using one small dataset of manually-labeled tweets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Distillation methods"
        },
        {
            "text": "The methodology pipeline is shown in Fig. 1 . First, manually-labeled training data is used to train a classification model for stance detection. Then, after the distillation process, the trained model is applied in realtime on incoming Twitter data for downstream opinion monitoring.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 37,
                    "end": 43,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Methodology"
        },
        {
            "text": "Text classification depends on the quality of the text representation model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tweet representation"
        },
        {
            "text": "In this work, we experiment with two kinds of text representations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tweet representation"
        },
        {
            "text": "A vector represents the frequency of each word in a pre-defined dictionary of words.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bag-of-Words (BOW)."
        },
        {
            "text": "Pre-trained language model representations. The intuition behind pre-trained language models is to train a model based on large corpora and use the resulting representations on other specific tasks in that language. Recently, pre-trained language models have achieved remarkable success, presenting state-of-the-art results in different tasks of natural language processing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bag-of-Words (BOW)."
        },
        {
            "text": "After representing manually-labeled data as numerical vectors, we use them to train a supervised classification model. In this work, we consider SVM, LSTM, and fine-tuned Bidirectional Encoder Representations from Transformers (BERT) for the stance classification task. We evaluate and compare between different combinations of text representations and classification models. The best model is used for monitoring the public opinion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tweet classification"
        },
        {
            "text": "To overcome the shortage of labeled data, we use a data distillation method for training data augmentation, which exploits the large-scale unlabeled data. In our previous work (Miao et al., 2020) , we explored, for data augmentation purposes, the distillation method that is adopted from Liu et al. (2019) and Xie et al. (2020) .",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 195,
                    "text": "(Miao et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 288,
                    "end": 305,
                    "text": "Liu et al. (2019)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 310,
                    "end": 327,
                    "text": "Xie et al. (2020)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Tweet classification"
        },
        {
            "text": "First, we use a manually-labeled dataset to train a basic teacher model. We then apply the trained model to unlabeled data to get predicted labels. Following that, we train a student model that is initialized with identical architecture and parameters as the teacher model, and apply it to the union of manually and automatically-labeled data. Next, we use the trained student model as a new teacher model. We repeat the process a number of times. During each iteration, only the student model is tested on the validation set. Finally, we adopt the student model, which provides no further performance improvement on validation set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tweet classification"
        },
        {
            "text": "The intuition behind this approach is that a high-quality teacher model brings up a good student model; the improvement of the student model will reinforce and strengthen the teacher model, reversely. The better the teacher model, the more accurate will be the labels predicted for unlabeled data, leading to better learning for the student model. On the other hand, during each training iteration, the student model evaluates the reliability of the labels predicted by the teacher model, thereby enhancing the teacher model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tweet classification"
        },
        {
            "text": "The pseudocode of our data augmentation method is shown in Algorithm 1. Note that we applied data distillation only on deep learning models, because multiple configurable hyper-parameters should be optimized during the data distillation process. The pipeline of the distillation method is shown in Fig. 2 . ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 298,
                    "end": 304,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Tweet classification"
        },
        {
            "text": "The collected tweets are labeled with our most accurate classification model. To analyze the tweets over time and monitor the public opinion, we focus on the following aspects.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Monitoring online public opinion"
        },
        {
            "text": "1. Daily tweet numbers. We analyze the changes of daily numbers to observe the public concerns about the target. 2. Daily stance polarity ratios. We track the daily stance polarity numbers and ratios to reveal daily public opinion changes. 3. Government response timeline. To explore the correspondence between the public opinion and government policies, the announcement dates of COVID-19 intervention measures are aligned with the timeline of daily opinion tracing. 4. Daily COVID-19 statistics. We align the daily COVID-19 statistics with the daily analysis of tweets. From global trends and local spikes, we analyze the changes and the trigger factors. We also measure the correlation between COVID-19 statistics and changes in public opinion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Monitoring online public opinion"
        },
        {
            "text": "In this work, we built a tweet dataset LockdownTweets related to the lockdown policy in NY State during COVID-19 pandemic. To build LockdownTweets, we selected tweets satisfying our search criteria. These were selected from a large dataset related to COVID-19 and provided by Chen et al. (2020) . First, we selected the tweets written in English. Then, we filtered the corpus to obtain only tweets related to NY State and lockdown, using a list of keywords 2 .",
            "cite_spans": [
                {
                    "start": 276,
                    "end": 294,
                    "text": "Chen et al. (2020)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Dataset and data annotation"
        },
        {
            "text": "Afterwards, the noisy tweets were removed for various reasons, such as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset and data annotation"
        },
        {
            "text": "1. The very short tweet does not contain enough text for opinion identification. 2. The tweet contains \"New York,\" however, it is not related to \"the lockdown of New York.\" For example, \"New York\" can be part of \"The New York Times\" or \"New York Post.\" 3. Duplicates.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset and data annotation"
        },
        {
            "text": "The retweets represent a big proportion in LockdownTweets, which is around 0.73. We decided to keep retweets in our dataset because they could also reflect the opinion of the users.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset and data annotation"
        },
        {
            "text": "To manually label training data for supervised learning, we built annotation guidelines (details are shown in Appendix A.1). Based on the guidelines, three annotators labeled 1098 tweets with 0.843 interannotator Cohen's kappa coefficient agreement. We used majority vote strategy to determine the final labels for nine tweets where our annotators disagreed. Fig. 3 shows some examples of the manuallylabeled tweets. Finally, the labeled dataset is split into Labeled-train (254 Against, 216 Support and 263 None), and Labeled-test (126 Against, 108 Support and 131 None). The labeled dataset will be used to train models for stance detection phase. Besides, there are 38,169 Unlabeled tweets dated from January 22nd until September 30th, 2020. These Unlabeled tweets will be labeled by the stance detection model we finally choose for further opinion monitoring.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 359,
                    "end": 365,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Dataset and data annotation"
        },
        {
            "text": "Our evaluations address both tasks performed for tracking public opinion: stance detection and opinion monitoring. For stance detection stage, models were trained using manually-labeled training data. In this work, distillation method was applied to optimize the parameters of the trained model for improving. Then, the best stance detection model was applied to unlabeled data for further analysis and monitoring.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "This part of the experiments aim at measuring the quality of the proposed stance classification models and choosing the best model for further monitoring of public opinion regarding lockdown.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stance detection"
        },
        {
            "text": "Below, we describe data preprocessing, baselines, text representations, data description, evaluated classification models, and quality metrics applied in our experiments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "Tweets Preprocessing. For preprocessing, duplicates were filtered out after removal of URLs (some tweets have the same texts surrounding URLs). All punctuation, hashtags, and mentions were retained.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "Baselines. We evaluated nine models (given three representation models and three classification algorithms) and compared them to the following baselines:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "\u2022 Random is a classifier that randomly assigns a label to each given instance. \u2022 Majority is a classifier that assigns the label to the instance with the majority class.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "Text Representations. We used n-grams, where n \u2208 (1,3), as tokens for BOW representation. And, we used GloVe, word vectors pre-trained on Twitter, (Pennington et al., 2014) for tweet representations. We also compared between different dimensions of GloVe vectors, attempting to choose the optimal one for this task. Additionally, we utilized BERT, pretrained on data extracted from the BooksCorpus and English Wikipedia Data Description. We consider three related datasets, containing two external sources, for our supervised training experiments. The details of the training datasets are shown in Table 1 .",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 172,
                    "text": "(Pennington et al., 2014)",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [
                {
                    "start": 598,
                    "end": 605,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Design of experiments"
        },
        {
            "text": "\u2022 StanceData is the dataset from SemEval-2016 Task #6 3 for stance detection in tweets, containing several topics different from the target in our task. \u2022 Sentiment140 is the training data 4 for sentiment analysis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "\u2022 Labeled-train is a small dataset we manully labeled from Lock-downTweets for this task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "In our previous work (Miao et al., 2020) , we tried to build transfer learning models using only external datasets, getting promising results. Instead of using only external datasets, in this work, we incorporated the external datasets and the lockdown-related dataset, under the assumption that the combined datasets may improve the performance. Due to the imbalance of three classes in the datasets we used, we applied Synthetic Minority Oversampling Technique (SMOTE) 5 for oversampling. As an initial attempt, we conducted the experiments on SVM_BOW to see the results with now-combined dataset and SMOTE. The experiments were implemented using sklearn 6 . The results show that SMOTE does not improve the model's accuracy. Therefore, we decided not to employ this technique with other models. Moreover, the results of using integrated datasets from different sources are not promising. So, in other classification models, we only used Labeledtrain for supervised learning.",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 40,
                    "text": "(Miao et al., 2020)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 657,
                    "end": 658,
                    "text": "6",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "Evaluated Classification Models. For the stance classification, we applied LSTM, BERT, and SVM models with different text representations. Then, we applied the distillation method on the best model of those, aiming to improve the model with unlabeled data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "The architecture of our applied LSTM model is shown in Fig. 4 . The experiments were implemented using Keras 7 . After examining the text lengths in datasets, we set the maximum sequence length to 55. We used Keras to pad digital sequences to a maximum length. A spatial dropout was applied on the embedding layer to reduce overfitting. We used bidirectional Gated Recurrent Unit (GRU), a faster variant of the LSTM architecture. We relied on the Sigmoid activation function, and learned the weights using the Adam optimizer and Cross Entropy loss. We used a typical batch size of 32.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 55,
                    "end": 61,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Design of experiments"
        },
        {
            "text": "For fine-tuning BERT on our data and task, we used a pre-trained English bert-base-cased model (Devlin et al., 2019) , which has 12 transformer layers, 12 self-attention heads, and a hidden size of 768. We froze all layers of BERT model, attached a dense layer, a softmax layer, and trained the new model. The Adam optimizer and Cross Entropy loss were used for training. Empirically, the initial learning rate was set to 0.00001, and the batch size was set to 32.",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 116,
                    "text": "(Devlin et al., 2019)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "In this stage, we employed LSTM_GloVe and fine-tuned BERT with the distillation method. In each iteration, 500 new unlabeled tweets from Unlabeled joined the distillation process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "Quality Metrics. We evaluated the performance of induced models on Labeled-test, using macro-averaged precision, recall, f1-score, and accuracy. The formulas are shown as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of experiments"
        },
        {
            "text": "The results in Tables 2 and 3 did not support the hypothesis that incorporating other datasets can provide better results. From the results, we can see that using only lockdown-related tweets enables us to build a better model in spite of the small dataset. Besides, compared with Sentiment140, StanceData is the dataset from the same domain, showing better performance, but not good enough as Labeled-train. Also, models using SMOTE did not show a clear improvement. As a result, we used only Labeled-train to train the models without applying SMOTE.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 15,
                    "end": 29,
                    "text": "Tables 2 and 3",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Results and discussion"
        },
        {
            "text": "We compared between the results produced by LSTM using different dimensions of GloVe. The results of these comparisons can be seen in Table 4 . It shows that using 50 dimensions provides the best results. Therefore, we chose to apply 50-dimensional GloVe word vectors for text representation in all our models. Table 5 shows the results of different algorithms with different text representations. As for SVM, the model using BOW outperforms others; with regard to LSTM, using GloVe provides the best performance. Notably, models based on BERT did not show better results when compared with others. It is important to note that both are pre-trained language representations, with GloVe trained on Twitter data and BERT on Wikipedia and BookCorpus. We assume that because GloVe was pre-trained on Twitter, it performs better on our data. As can be seen from Table 5 , LSTM_GloVe significantly outperforms other supervised models.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 134,
                    "end": 141,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 311,
                    "end": 318,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 857,
                    "end": 864,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Results and discussion"
        },
        {
            "text": "Based on our previous attempt of the distillation method for training data augmentation, we employed it on our lockdown data, in order to get an accurate model without extensive manual annotation. We employed LSTM_GloVe with the distillation method, which converged after three iterations. For the comparison, the distillation method was also applied to fine-tuned BERT, which achieved convergence at the second iteration. Table 6 compares the results of distillation methods with several benchmarks. From the results, we can see that SVM_BOW slightly outperforms the Majority model. It is noteworthy that the distillation method provides significant improvement of the initial teacher models. However, regardless of the improvement from fine-tuned BERT to BERT_Distillation, LSTM_GloVe_Distillation outperforms BERT_Distillation. Table 7 shows the detailed performance reports of LSTM_GloVe_-Distillation's, with relatively high precision on three classes. Precision was calculated as a ratio of true positive instances to all instances automatically retrieved (as positive) by a model. We are interested in precision more than in recall be-cause we care less about \"missing\" some positive samples in monitoring than about identifying actual negative samples as false positives.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 423,
                    "end": 430,
                    "text": "Table 6",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 831,
                    "end": 838,
                    "text": "Table 7",
                    "ref_id": null
                }
            ],
            "section": "Results and discussion"
        },
        {
            "text": "We chose LSTM_GloVe_Distillation as the most accurate model for classifying the Unlabeled tweets for opinion analysis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and discussion"
        },
        {
            "text": "To better understand our model, we analyzed several error cases, and summarized four reasons for misclassification.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "1. Ironic expressions lead to inverse polarity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "The two examples are classified as \"Favor,\" but the true labels are \"Against.\" The tweets were meant to be ironic, which is not fully understood by the model. Example1: I personally believe this lockdown is to keep us from having mass shootings while the banking transition occurs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "Example2: @DavidFDodge1 @daktheDUD @NYR Fan Jay @CNN Cuomo forced Nursing homes to take infected patients. NY has MORE cases and MORE deaths than ANY OTHER COUNTRY ON EARTH. NY has MORE deaths in Nursing homes than ANY other state has TOTAL deaths except NJ.. Yeah the lockdown worked great 2. Some tweets need more context to infer the stance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "The example is classified as \"Against,\" but the annotators labeled it as \"Favor,\" assuming that something \"bad\" happens \"without a lockdown.\"",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "Example: @ogthottie69 @themarkbanker @NYGovCuomo http s://t.co/hfJLM2tVeZ this is what happens without a lockdown. 3. Irrelevant to the target.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "The example is classified as \"Favor,\" but the true label is \"None.\" This tweet has \"lockdown\" and \"NY\" included, but does not express the stance to \"lockdown in New York State.\" The model is not able to recognize that the tweet does not express the stance on the target.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "Example: If people don't want another lockdown in #NYC and keep the curve flat. Wear a mask; it's that simple. It's a virus, not a bacteria. #COVID19 #WearAMask 4. Keywords mislead the model Example1 is classified as \"Against,\" but the true label is \"Favor.\" The phrase \"call for an end to the lockdown\" leads the model to predict this tweet as \"Against,\" because the model observed many \"Against\" instances with \"end lockdown.\" Example2 is classified as \"Favor,\" while the true label is \"Against.\" The phrase \"should be on lockdown\" plays an important role.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "Example 1: Easy for rich people who can work and self isolate in their gated mansions to call for an end to the lockdown. NYC shows us those who die first are the poor, the old, the people living in crowded homes, the essential workers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "Example2: @seanhannity The only people that should be on lockdown are Cuomo and Dblasio",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Error analysis"
        },
        {
            "text": "In this stage, the best stance detection model was applied to all unlabelled tweets for monitoring the public opinion regarding lockdown.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Opinion monitoring"
        },
        {
            "text": "After preprocessing and classifying all unlabeled tweets, we counted the daily number of tweets for each class. To monitor the public opinion, the tweets labeled as \"Favor\" and \"Against\" were the most important ones. The numbers of \"Favor,\" \"Against,\" and the sum of three classes are shown in Fig. 5 . Fig. 5 shows that from February to the middle of March 2020, few tweets expressed concern about lockdown in NY State. Before March 22nd (when Governor Cuomo announced the statewide stay-at-home order), most of the tweets were in favor of lockdown. As the epidemic spread to NY State, however, a significant increase in the number of ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 294,
                    "end": 300,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 303,
                    "end": 309,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Daily opinion analysis"
        },
        {
            "text": "To smooth the plots of daily numbers, we applied the 7-day moving average, referring to the statistical methods from the Worldometers website 8 . The daily statistics of COVID-19 in NY State were extracted from the COVID-19 in USA dataset 9 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Opinion shift analysis"
        },
        {
            "text": "To investigate how actions taken by the government influence public opinion, we built the Timeline for State Government Response (details are shown in Appendix A.2). We extracted several important events and aligned them with the 7-day moving average of daily positive cases, daily deaths, number of Against tweets, number of Favor tweets, total number of tweets, and the Against to Favor tweets ratio. The results are shown in Fig. 6 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 428,
                    "end": 434,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Opinion shift analysis"
        },
        {
            "text": "From Fig. 6 , we can get a number of interesting findings:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 5,
                    "end": 11,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Opinion shift analysis"
        },
        {
            "text": "\u2022 Public opinion changes along with the COVID-19 statistics. It can be seen that before positive cases were found in NY State, people were indifferent about the lockdown measures. The number of tweets about lockdown went up along with the daily cases and deaths. However, when the number of daily cases and deaths started to decrease after May, the number of lockdown-related tweets was still seeing a big rise. We could understand from our observations that after a long-time lockdown and seeing the drop of deaths and positive cases, people were more willing to stop the lockdown. \u2022 Government measures affect the public opinion. It appears that measures taken by government trigger local peaks of the daily number of tweets in both Favor and Against opinion. \u2022 Both COVID-19 statistics and government measures can trigger the changes in distribution of Against and Favor tweets. We can see that in the early stage before March, the ratios of both Against and Favor tweets stay at a relatively low level. Basically, at that time more Twitter users were indifferent to the lockdown and did not express any polar opinions in their tweets. After that period, however, the ratio of Favor tweets showed a sharp increase when the infected cases were found in NY State. With the announcement of a statewide lockdown order, the ratio of Favor tweets saw a drop, while the ratio of Against tweets began to increase immediately. As far as the daily number of COVID-19 positive diagnoses and death cases climbing is concerned, the ratio of Favor tweets stays relatively high, while after the daily number of positive and death case decreases, the ratio of Favor tweets goes down as well, and tends to remain stable. It can be seen that, when the \"Phase 1 of reopening\" was implemented, the ratio of Against tweets reached a peak. It demonstrates that some specific government measures have obvious influence on the public opinion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Opinion shift analysis"
        },
        {
            "text": "To analyze the correlation between public opinion and COVID-19 statistics, we calculated Pearson Correlation Coefficients. We reported the p-value to indicate whether the correlation is significantly different from zero. We calculated the ratio of Favor and Against tweets; the correlation was measured after smoothing data with the 7-day moving average. The results are shown in Fig. 7 . The scatterplots are used to visualize the correlation coefficient.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 380,
                    "end": 386,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Correlation analysis"
        },
        {
            "text": "The figure shows that the proportion of Against tweets has a strong positive association with total cases, total deaths, and total recovered cases, while the proportion of Favor tweets has a moderate negative association with the same variables. Because the total cases, total deaths, and total recovered cases are all cumulative numbers, these associations can also be explained by the correlation of the Against to Favor tweets ratio with time. Notably, the corresponding value of statistical significance for the correlation coefficient is zero, representing statistically significant results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Correlation analysis"
        },
        {
            "text": "In this work, we aim at using social media to monitor public opinion about intervention measures during the COVID-19 pandemic. Firstly, stance detection is conducted to identify agreement or disagreement on a certain control measure. Secondly, public opinions are monitored relying on the outputs from stance detection. According to our comparative evaluation of several different approaches, the model built by LSTM with GloVe word vectors and data distillation showed the most promising results for stance detection. In contrast to most of the works on stance detection demanding large amount of manually-labeled data, L. Miao et al. our data augmentation method is more efficient as it extends the datasets using small amounts of manually-labeled data.",
            "cite_spans": [
                {
                    "start": 624,
                    "end": 635,
                    "text": "Miao et al.",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "After automatic classification of unlabeled data with the most accurate stance detection model (LSTM_ GloVe_Distillation), we analyzed the NY State lockdown-related tweets from January 22nd until September 30th, 2020, to monitor the public opinion. Aligning the results with COVID-19 data and the timeline of important government response measures shows that changing of COVID-19 statistics could cause the opinion shift, and that intervention measures are able to trigger local peaks in the number of tweets. Specifically, we analyzed the correlation coefficient of COVID-19 data and the daily moving averages of two opinion polarities. It can be seen that the ratio of Against tweets has a strong positive correlation with cumulative COVID-19 statistics, while the ratio of Favor tweets has a moderate negative correlation with it.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Social media has been widely used for monitoring public opinion about government policies. In this work, we developed and evaluated a pipeline aiming at monitoring public opinion about intervention policy during the COVID-19 pandemic. The proposed pipeline enhances stance detection and the resulting opinion monitoring by efficiently utilizing data distillation to expand the amount of labelled data. The presented case study demonstrates the analysis of public opinion about intervention measures in NY State from January 22nd to September 30th, 2020. We showed that the analysis performed with the introduced pipeline can gain important insights for the decision-makers. It is recommended to use distillation methods for data augmentation when annotated social media data is limited for supervised opinion mining tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "Limitations of this work include the bias towards populations/ countries with wide usage of Twitter. A data sample collected from a social media platform does not necessarily represent the entire population. Because of the Twitter API constraints, only small random samples of tweets are accessible. Furthermore, the insufficient amount of annotated data limits the accuracy of this work, even though we obtained statistically significant results using data augmentation. Besides, only English tweets were considered, while discarding the relevant tweets in other languages, which might have some effect on our results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Limitations and future work"
        },
        {
            "text": "In the future work, one could explore more advanced methods to improve data augmentation and provide more accurate tweet classification results. Moreover, multilingual capabilities may be implemented for more comprehensive public opinion monitoring. Additionally, time series forecasting models can be utilized for predicting the public opinion trends. The effect of public opinion on the intervention measures effectiveness may be explored as well. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Limitations and future work"
        },
        {
            "text": "Timeline for Policy Announcement.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Table A8"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Stance detection with bidirectional conditional encoding",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Augenstein",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Rock\u1e97aschel",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vlachos",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Bontcheva",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "876--885",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Tracking social media discourse about the covid-19 pandemic: Development of a public coronavirus twitter data set",
            "authors": [
                {
                    "first": "Emily",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Lerman",
                    "suffix": ""
                },
                {
                    "first": "Emilio",
                    "middle": [],
                    "last": "Ferrara",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "JMIR Public Health and Surveillance",
            "volume": "6",
            "issn": "2",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.2196/19273"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "On the efficacy of knowledge distillation",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Hariharan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "4794--4802",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Towards detecting influenza epidemics by analyzing twit-ter messages",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Culotta",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the first workshop on social media ana-lytics",
            "volume": "",
            "issn": "",
            "pages": "115--122",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Monitoring the public opinion about the vaccination topic from tweets analysis",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "D&apos;andrea",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Eleonora",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ducange",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pietro",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bechini",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Alessio",
                    "suffix": ""
                },
                {
                    "first": "Alessandro",
                    "middle": [],
                    "last": "Renda",
                    "suffix": ""
                },
                {
                    "first": "Francesco",
                    "middle": [],
                    "last": "Marcelloni",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Expert Systems with Applications",
            "volume": "116",
            "issn": "",
            "pages": "209--226",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "A kernel theory of modern data augmentation",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dao",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Ratner",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Smith",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "De Sa",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "R\u00e9",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of machine learning research 97",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "NAACL--HLT",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Born again neural networks, International Conference on. Machine Learning",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Furlanello",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lipton",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tschannen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Itti",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Anandkumar",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1607--1616",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Data augmentation for rumor detection using context-sensitive neural language model with large-scale credibility corpus",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ciravegna",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Distilling the knowledge in a neural network",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "stat",
            "volume": "1050",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Monitoring stance towards vaccination in twitter messages",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Kunneman",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lambooij",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Van Den",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bosch",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mollema",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "BMC Medical Informatics and Decision Making",
            "volume": "20",
            "issn": "",
            "pages": "1--14",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Data distillation for controlling specificity in dialogue generation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Monroe",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Jurafsky",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1702.06703"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "State began producing its own brand of hand sanitizer. 12-Mar-2020 All gatherings of less than 500 people ordered to cut capacity by 50%. All gatherings of more than 500 people ordered to cancel",
            "authors": [],
            "year": 2020,
            "venue": "Date Policy 7-Mar-2020 State of emergency declared",
            "volume": "9",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "All NY City schools ordered to close until",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "State-wide stay-at-home order declared. All non-essential businesses ordered to close. All non-essential gatherings canceled/postponed",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "25",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Advisory issued ordering nursing homes to admit patients who test pos-itive for the coronavirus and to not allow testing of prospective nursing home patients. This order was revoked on May 10th. 27-Mar-2020 All schools statewide ordered to remain closed until",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "List of businesses deemed essential expanded. 15-Apr-2020 All state residents ordered to wear face masks/coverings in public places where social distancing is not possible. 16-Apr-2020 Statewide stay-at-home order and school closures extended to May 15. 1-May-2020 All schools and universities ordered to remain closed for the remainder of the academic year. 7-May-2020 Statewide four-phase reopening plan is first announced",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "14",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Drive-in theaters, landscaping/gardening businesses allowed to reopen state-wide (regardless of Phase 1 qualifications). 23-May-2020 Gatherings of up to 10 people allowed as long as social distancing is practiced. 8-Jun-2020 NY City meets conditions for Phase 1, allowing the reopening of construction, manufacturing, agriculture, forestry, fishing, and select retail businesses that can offer curbside pickup",
            "authors": [],
            "year": 2020,
            "venue": "Phase 1 of reopening allowed for counties that met qualifications. Five counties met qualifications and began reopening on this date",
            "volume": "15",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "NY City meets conditions for Phase 2, allowing the reopening of outdoor dining at restaurants, hair salons and barber shops, offices, real estate firms, in-store retail, vehicle sales, retail rental, repair services, cleaning services, and commercial building management businesses. 10-Jul-2020 Malls allowed to open at 25\\% capacity for regions in Phase 4, with all patrons required to wear masks",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "4",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Characterizing the propagation of situational information in social media during covid-19 epidemic: A case study on weibo",
            "authors": [
                {
                    "first": "Lifang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Qingpeng",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Jun",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Tian-Lu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Fei-Yue",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Transactions on Computational Social Systems",
            "volume": "7",
            "issn": "2",
            "pages": "556--562",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Ddflow: Learning optical flow with unlabeled data distillation",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "King",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Lyu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "8770--8777",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Data boost: Text data augmentation through reinforcement learning guided conditional generation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vosoughi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "9031--9041",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Hawkes processes for continuous time sequence classification: An applica-tion to rumour stance classification in twitter",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lukasik",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Srijith",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Vu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Bontcheva",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zubiaga",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Cohn",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
            "volume": "2",
            "issn": "",
            "pages": "393--398",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Use of twitter data to improve zika virus surveillance in the united states during the 2016 epidemic",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Masri",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "C"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "BMC Public Health",
            "volume": "19",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Twitter data augmentation for mon-itoring public opinion on covid-19 intervention measures",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Miao",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Last",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Litvak",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 1st Workshop on NLP for COVID-19",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Semeval-2016 task 6: Detecting stance in tweets",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mohammad",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kiritchenko",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sobhani",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cherry",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)",
            "volume": "",
            "issn": "",
            "pages": "31--41",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Glove: Global vectors for word representation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Data dis-tillation: Towards omni-supervised learning",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Radosavovic",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Gkioxari",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "4119--4128",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Identifying users with opposing opin-ions in twitter debates",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rajadesingan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International conference on social computing, behavioral-cultural modeling, and prediction",
            "volume": "",
            "issn": "",
            "pages": "153--160",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Boosting text classification performance on sexist tweets by text augmentation and text generation using a combination of knowledge graphs",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sharifirad",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Jafarpour",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Matwin",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2nd work-shop on abusive language online (ALW2)",
            "volume": "",
            "issn": "",
            "pages": "107--114",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Tweet stance detection using an attention based neural ensemble model",
            "authors": [
                {
                    "first": "U",
                    "middle": [
                        "A"
                    ],
                    "last": "Siddiqua",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Chy",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aono",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Compu-tational Linguistics: Human Language Technologies",
            "volume": "1",
            "issn": "",
            "pages": "1868--1873",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "The use of twitter to track levels of disease activity and public concern in the us during the influenza a h1n1 pandemic",
            "authors": [
                {
                    "first": "Alessio",
                    "middle": [],
                    "last": "Signorini",
                    "suffix": ""
                },
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Segre",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Maria",
                    "suffix": ""
                },
                {
                    "first": "Philip",
                    "middle": [
                        "M"
                    ],
                    "last": "Polgreen",
                    "suffix": ""
                },
                {
                    "first": "Alison",
                    "middle": [
                        "P"
                    ],
                    "last": "Galvani",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "PloS one",
            "volume": "6",
            "issn": "5",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Automatic detection of stance towards vaccination in online discussion forums",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Skeppstedt",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kerren",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Stede",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the International Workshop on Digital Disease Detection using Social Media",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Dutch general public reaction on governmental covid-19 measures and announcements in twitter data",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schraagen",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "T K"
                    ],
                    "last": "Sang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dastani",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.07283"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Eda: Easy data augmentation techniques for boost-ing performance on text classification tasks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zou",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "6383--6389",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "pkudblab at semeval-2016 task 6: A specific convolutional neural network system for effective stance detection",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 10th international workshop on semantic evaluation",
            "volume": "",
            "issn": "",
            "pages": "384--388",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Self-training with noisy stu-dent improves imagenet classification",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "T"
                    ],
                    "last": "Luong",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Hovy",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [
                        "V"
                    ],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "10687--10698",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Mitre at semeval-2016 task 6: Transfer learning for stance detection",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zarrella",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Marsh",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)",
            "volume": "",
            "issn": "",
            "pages": "458--463",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Self-distillation as instance-specific label smoothing",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Sabuncu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.05065"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Input: Labeled Data L, Unlabeled Data U ; Output: Trained Model; 1: Train Teacher Model (TM) using L; 2: while U != 0 do 3:Select a subset S from U; 4:Label all samples of S using TM; 5:L = L \u222a S; 6:U = U -S; 7:Initialize Student Model (SM), SM < -TM; 8:Train Student Model (SM) using L; 9:if SM is better than TM then 10:TM = SM; 11:else 12:Stop 13:end if 14: end while 15: return Trained Teacher Model (TM)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Methodology pipeline.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Pipeline of data distillation method.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Examples of manually labeled lockdown tweets. 2 NYS-related keywords: New York, NYS, NYC, Governor Cuomo. Lockdownrelated keywords: lockdown, stay-at-home, Pause, NY on Pause, shelter-at-home, NYPause, stay home.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Devlin et al., 2019), to produce dense vector representations for sentences.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Daily numbers of tweets. 8 https://www.worldometers.info/coronavirus/ 9 https://www.kaggle.com/sudalairajkumar/covid19-in-usa",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Daily statistics of 7-day moving average aligned with government response timeline.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Correlation coefficient plots.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Training datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Results obtained by SVM.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Results obtained by SVM with SMOTE.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Different dimensions of GloVe using LSTM.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Comparison of different classifiers with different text representations.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Comparison of distillation methods with several benchmarks.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Declaration of Competing Interest"
        },
        {
            "text": "Twitter users express their opinions in different ways, including supporting or opposing the target explicitly, supporting or opposing some entities related to the target, or by re-tweeting someone else's supporting or opposing tweet.Target: Lockdown in New York State Opinion Labels: Support, Against, None Support/Against It can be inferred from a tweet that the user supports/against the target, if:\u2022 The tweet supports/opposes the target explicitly.\u2022 The tweet supports/opposes someone or something related to the target, from which we could infer the support of/opposition to the target. \u2022 The tweet does not support or oppose anything, but it contains some clues can infer support or opposition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A.1. Guideline for annotation"
        },
        {
            "text": "None of the two cases above.\u2022 The tweet has neutral opinion on the target.\u2022 Cannot conclude the opinion of the target from the tweet Appendix A.2. Detailed Government Responses Timeline Table A8 shows the detailed government responses timeline built according to Wikipedia 10 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 186,
                    "end": 194,
                    "text": "Table A8",
                    "ref_id": null
                }
            ],
            "section": "None"
        }
    ]
}
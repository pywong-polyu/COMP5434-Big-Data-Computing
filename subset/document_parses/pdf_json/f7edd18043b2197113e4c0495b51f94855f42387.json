{
    "paper_id": "f7edd18043b2197113e4c0495b51f94855f42387",
    "metadata": {
        "title": "FaiR-IoT: Fairness-aware Human-in-the-Loop Reinforcement Learning for Harnessing Human Variability in Personalized IoT",
        "authors": [
            {
                "first": "Salma",
                "middle": [],
                "last": "Elmalaki",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of California",
                    "location": {
                        "settlement": "Irvine"
                    }
                },
                "email": "salma.elmalaki@uci.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Thanks to the rapid growth in wearable technologies, monitoring complex human context becomes feasible, paving the way to develop human-in-the-loop IoT systems that naturally evolve to adapt to the human and environment state autonomously. Nevertheless, a central challenge in designing such personalized IoT applications arises from human variability. Such variability stems from the fact that different humans exhibit different behaviors when interacting with IoT applications (intra-human variability), the same human may change the behavior over time when interacting with the same IoT application (inter-human variability), and human behavior may be affected by the behaviors of other people in the same environment (multi-human variability). To that end, we propose FaiR-IoT, a general reinforcement learning-based framework for adaptive and fairness-aware human-in-the-loop IoT applications. In FaiR-IoT, three levels of reinforcement learning agents interact to continuously learn human preferences and maximize the system's performance and fairness while taking into account the intra-, inter-, and multi-human variability. We validate the proposed framework on two applications, namely (i) Human-in-the-Loop Automotive Advanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House. Results obtained on these two applications validate the generality of FaiR-IoT and its ability to provide a personalized experience while enhancing the system's performance by 40%\u221260% compared to non-personalized systems and enhancing the fairness of the multi-human systems by 1.5 orders of magnitude.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Ubiquitous computing-that interacts and adapts to humans-is inevitable. In these pervasive systems, human reactions and behavior are observed and coupled into the loop of computation [15] . By allowing autonomy into the essence of IoT systems, these evolving systems provide services that are adaptable to the human context and intervene and take actions that are tailored to the human reaction and behavior. Despite the IoT system's ability to collect and analyze a significant amount of sensory data, traditional IoT typically depends on fixed policies and schedules to enhance user experience. However, fixed policies that do not account for variations in human mood, reactions, and expectations, fail to achieve the promised user experience. Moreover, with the continuous and inevitable interaction of the human with these systems, it becomes a pressing need to adapt to the physical environment changes and adapt to human preferences and behavior. This opens the question of how to use the monitored human state to design human-in-theloop IoT systems that provide a personalized experience. A personalized IoT system needs to \"infer\" or \"learn\" the human preference and continuously \"adapts\" and \"takes actions\" whether autonomously or in the form of recommendation. Hence, we need a human-in-the-loop framework that moves from a one-sizefits-all approach to a personalized process in which learning and adaptation agents in IoT systems are tailored towards humans' individual needs. This feedback property opens the door to design a \"Reinforcement Learning (RL)\" based agent. Unfortunately, applying standard RL algorithms, such as Q-learning, faces several challenges in the context of human-in-the-loop IoT applications. In particular, adapting to the human reaction and behavior poses a new set of challenges due to the intra-human and inter-human variability-for example, the same human behavior and reaction change over time. Even for a small period of time, the same human may produce different reactions based on unmodeled external effects. Similarly, different humans have different reactions under similar conditions. In addition to the variability introduced by humans, the IoT system used to infer human states suffers itself from variability (e.g., power constraints, connectivity status, classification/signal processing errors) that introduce another level of complexity. Moreover, as IoT applications are becoming more ubiquitous, multiple humans may interact within the same application space affecting each other's reaction and the way the application adapts. This multi-human interaction poses a new set of challenges to the IoT application relating to the \"fairness\" of adaptation.",
            "cite_spans": [
                {
                    "start": 183,
                    "end": 187,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we propose a framework that can be used in conjunction with IoT applications to provide a personalized experience while addressing the aforementioned challenges. We purpose Fairness-aware Human-in-the-Loop Reinforcement Learning framework, or FaiR-IoT for short, that monitors the change in the human reaction and behavior while interacting with the IoT system and addresses the intra-human and inter-human variability, as well as, the multi-human interactions, to provide personalized adaptation that enhances the human's experience while ensuring fairness across all human sharing the same IoT application. The proposed framework is then used to build two human-in-theloop IoT applications, (1) personalized advanced driver assistance system and (2) personalized smart home.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Human-in-the-Loop IoT: Besides the fact that IoT solutions nowadays have billions of connected devices [32] , humans themselves arXiv:2103.16033v1 [cs. LG] 30 Mar 2021 are becoming a walking sensor network equipped with wearables that are rich in sensors and network capabilities [12, 39] . Human sensing has a central role in many IoT applications. In the area of managing the energy consumption of buildings, detecting the number of occupants through human sensing has been a primary target of many energy-saving techniques, such as correlating electrical load usage with occupancy sensors [3, 43] . In automotive applications, the human driving behavior has been integrated into the loop of computation of many Advanced Driver Assistance Systems (ADAS) [11, 36] , such as activate the automatic cruise control [33] or adjust the threshold of the forward collision warning [14] . In smart cities, human tracking is used for dynamic resource allocation of community services [50] . Moreover, as IoT applications are becoming more human-centric applications, efficient human modeling, human state estimation, and human adaptation are vital components for IoT applications that interact with humans [28, 53] . In this paper, we propose a general framework that can be integrated into many of these applications to provide a personalized experience while addressing the human variability that arises while interacting with these applications.",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 107,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 147,
                    "end": 151,
                    "text": "[cs.",
                    "ref_id": null
                },
                {
                    "start": 156,
                    "end": 158,
                    "text": "30",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 280,
                    "end": 284,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 285,
                    "end": 288,
                    "text": "39]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 592,
                    "end": 595,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 596,
                    "end": 599,
                    "text": "43]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 756,
                    "end": 760,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 761,
                    "end": 764,
                    "text": "36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 813,
                    "end": 817,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 875,
                    "end": 879,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 976,
                    "end": 980,
                    "text": "[50]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "text": "[28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1203,
                    "end": 1206,
                    "text": "53]",
                    "ref_id": "BIBREF54"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Reinforcement learning for human adaptation: This tight coupling between human behavior and computing promises a radical change in human life [41] . In the area of cognitive learning and human-in-the-loop IoT applications, reinforcement learning (RL) has proven to be adequate to monitor human intentions and responses [20, 44, 45] . Multisample RL [14] can adapt to humans and changes in their response times under various autonomous actions. Amazon has used personalized reinforcement learning to adapt to students' preferences [5] . Moreover, RL models have been used to decide residential load scheduling [55] . In this paper, we build upon work in RL literature to monitor changes in human behavior to achieve personalized IoT applications while adapting the RL models at runtime to address the different variability that arises in human-in-the-loop IoT systems.",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 146,
                    "text": "[41]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 319,
                    "end": 323,
                    "text": "[20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 324,
                    "end": 327,
                    "text": "44,",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 328,
                    "end": 331,
                    "text": "45]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 349,
                    "end": 353,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 530,
                    "end": 533,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 609,
                    "end": 613,
                    "text": "[55]",
                    "ref_id": "BIBREF56"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Hierarchical reinforcement learning: Multi-layer RL has been used in the domain of parameter search [25] . In particular, RL has been used to learn policies to converge to good hyper-parameters that achieve the best performance [54] . In addition to parameter tuning, hierarchical RL was used to train multiple levels of policies or to decompose complex learning tasks into sub-goals [35] . However, these methods learn the hyper-parameters once and fit them to the model or learns the different layers of policies and fix them. In the domain of human-in-the-loop IoT systems, this is not applicable. Humans change their behavior, and there exists intra-human and inter-human variability that needs to be addressed.",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 104,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 228,
                    "end": 232,
                    "text": "[54]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 384,
                    "end": 388,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Fairness in reinforcement learning: The question of fairness in RL where the agent prefers one action over another becomes more significant in multi-agent systems [22, 26] . As each agent learns its optimal policy, the notion of efficiency (system performance) and fairness may be conflicting. One approach is to train each agent independently but design the reward to be fairefficient [24] . Another approach is the notion of envy and guilt in the study of inequity aversion to shape the fair reward function [21] . However, in the multi-human application space, a policy that is considered fair at one time may become a discriminatory policy after some time as human preferences change. In this paper, we build upon the definition of fairness proposed in literature Figure 1 : FaiR-IoT framework is divided into three RL agents. Multisample RL and Governor RL interact through action , state , and reward to handle the intra-human and inter-human variability at the edge. While a third RL agent, the Mediator RL, interacts with the environment through action , state , and reward in the cloud while providing a feedback action to both the Multisample RL and the Governor RL to handle the multi-human variability.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 167,
                    "text": "[22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 168,
                    "end": 171,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 386,
                    "end": 390,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 510,
                    "end": 514,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 768,
                    "end": 776,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Related Work"
        },
        {
            "text": "while proposing a fairness measure that can be embedded in human adaptation models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "This paper aims to put the intrinsic variation in human behavior and reaction into the loop of computation while addressing the fairness in multi-human IoT applications. In particular, we make the following contributions: (1) Designing FaiR-IoT, an adaptation framework named \"Fairnessaware Human-in-the-Loop Reinforcement Learning\" that addresses the intra-human and inter-human variability in the context of IoT applications. The framework continuously monitors the human state through the IoT sensors and the changes in the environment with which the human interacts to adapt to the environment accordingly and enhance the human's experience. (2) Extending the framework to address the multi-human interaction within the same application space. The framework continuously monitors how each human behavior can affect other humans' behaviors, which changes the environment and the way it is adapting. (3) Proposing fairness-aware methodology for adapting to multiple humans sharing the same IoT application space. (4) We use the proposed framework to develop two Human-inthe-Loop IoT applications in the context of advanced driver assistance systems (ADAS) and automated smart home.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Paper Contribution"
        },
        {
            "text": "We consider one of the IoT applications in the automated smart home as a motivating example for designing the proposed framework. In particular, we focus on the application of designing smart thermostats. Current state-of-art smart thermostats can adapt the home temperature based on room occupation [3] using fixed schedules and policies [31] . In particular, preset configurations are provided by homeowners, and smart thermostat abides by these configurations. However, human needs and behavior vary across time, such as a change in the body's temperature is affected by multiple aspects, such as excitement, anxiety, physical activity, and health issues. In principle, IoT systems can play a role in detecting the human mood [1] and activity [18] to \"adapt\" the home temperature accordingly. This adaptation can further be automated and \"tuned\" by monitoring the human thermal satisfaction, which can be inferred as direct feedback from the human [42] or as an indirect measurement from another edge device, such as, black globe thermometer [6] or skin temperature monitor [48] . The end goal is to achieve a personalized experience by learning the best home temperature that provides the best thermal sensation for the current human state. This \"learn\", \"adapt\", and \"tune\" feedback loop fits well with the RL paradigm. The environment in the RL setting encapsulates the interaction between the human and the IoT applications. The RL agent makes a recommendation to the human or takes action (such as changing the set-point of the thermostat) to adapt the IoT application. This action or recommendation is continuously being adapted to match the changes in human behavior (such as mood and physical activity) and reaction (such as a change in the thermal sensation). This model keeps training online or offline until it converges to the best policy, which is the best thermostat set-point for a particular human mood and physical activity. However, we argue that this setting will not achieve the overarching goal of personalized experience in IoT applications. Using the same motivating example, we list the cases in which the setting mentioned above will fall short.",
            "cite_spans": [
                {
                    "start": 300,
                    "end": 303,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 339,
                    "end": 343,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 729,
                    "end": 732,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 746,
                    "end": 750,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 951,
                    "end": 955,
                    "text": "[42]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1045,
                    "end": 1048,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1077,
                    "end": 1081,
                    "text": "[48]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "Fairness-aware Human-in-the-Loop Reinforcement Learning Framework"
        },
        {
            "text": "(1) Intra-human variability: Even under the same mood and physical activity, the same human may change the personal preference for the best room temperature. Hence, an RL model that learns a fixed policy will not be adequate. (2) Inter-human variability: Different humans may have different thermal sensations even with the same home temperature under the same mood and physical activity. Moreover, the human body response to a change in ambient temperature can differ across humans [10] . Hence, the same RL model design (same parameters and same reward function) may adapt correctly for some humans while performing poorly for others. (3) Multi-human variability: More than one human can be in the same house, each with a different mood (relaxed or stressed) and different physical activity (sleeping or doing domestic work).",
            "cite_spans": [
                {
                    "start": 483,
                    "end": 487,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Fairness-aware Human-in-the-Loop Reinforcement Learning Framework"
        },
        {
            "text": "One set-point that achieves the most comfortable thermal sensation for one human may not be the best one for another. Hence, an RL model that does not take the effect of the interaction between multiple humans within the same IoT application space will not satisfy the individual needs and will not account for the fairness of the adaptation policy across the humans.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fairness-aware Human-in-the-Loop Reinforcement Learning Framework"
        },
        {
            "text": "Accordingly, we propose \"Human-in-the-Loop Fairness-aware Reinforcement Learning\" framework to address this variability. Our framework is divided into three components name Multisample RL to handle the intra-human variability, Governor RL to handle the inter-human variability, and Mediator RL to handle the multi-human variability while ensuring fairness. A conceptual figure for the proposed framework is shown in Figure 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 416,
                    "end": 424,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Fairness-aware Human-in-the-Loop Reinforcement Learning Framework"
        },
        {
            "text": "In the domain of human-in-the-loop IoT application, the changes in the human response time and the preferences across time makes the reward for the RL agent a time-varying function. The problem of time-varying reward function was addressed by Multisample Qlearning algorithm [14] that divides the time horizon into three overlapping different time scales, named (1) state observing rate ( ) where the state of the environment is observed every time new sensor data is available. This depends on the sampling rate of the sensors at the edge devices, (2) actuation rate ( ), which is the time by which the agent should take action, and (3) evaluation rate ( ) where the reward-corresponding to a particular taken action-should be evaluated at a relatively slow rate to take into account the delay in the environment change.",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 279,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Intra-Human Variability"
        },
        {
            "text": "Multisample RL can handle the intra-human variability, where humans can change their behavior and reaction, and the continuous reward calculation tracks this change and adapts the RL action accordingly. However, it assumes that the response time of all humans is the same. By fixing the values of and , Multisample RL assumes that all humans have the same response time. However, and should be adaptive based on the human interaction with the IoT system. By adapting and , we can take the inter-human variability as well into the loop of computation. This leads to designing Governor RL as will be explained in the next section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Intra-Human Variability"
        },
        {
            "text": "To account for the inter-human variability, the RL-agent needs to adapt and assumed by the Multisample RL algorithm to personalize the human experience. Such adaptation can be modeled as another RL-agent that observes the change in the human's experience as the parameters and change. To that end, and as shown in Figure 2 , we add a Governor RL layer that personalizes the values of and for Multisample RL. The Multisample RL underneath runs the Q-learning algorithm with the three overlapped time scales ( , , and ) and adapts the IoT system that interacts with the human through an action or a recommendation to the human. A personalized Q-learning policy is learned based on the reward propagated from the environment. This reward is a function of the human experience and the IoT system performance. After the policy is converged (the reward value is not increasing), the Governor RL observes the cumulative reward for this particular action that modifies the values of and and adapts them accordingly. Eventually, Governor RL will converge to the values of and that achieve the best cumulative reward for a particular human. The algorithm and the design of the Governor RL will be discussed in Section 3.1.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 314,
                    "end": 322,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Inter-Human Variability"
        },
        {
            "text": "When multiple humans interact in the same IoT application space, their preferences differ based on the number of people they interact with and the type of this interaction. Moreover, when the application adapts to one human, the environment changes accordingly to the adaptation action, and thus it has a direct effect on the other humans in the same environment. In addition to that, multiple humans may require different adaptation actions based on their different preferences. Hence, we need a mediator to intervene and manage these multiple adaptation actions from multiple humans to achieve an aggregate better state for all the humans collectively. Accordingly, we propose the third layer in our framework, the Mediator RL. As shown in Figure 3 , the Mediator RL takes the adaptation actions ( 1 , 2 , ..., ) from all the Multisample RL agents.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 742,
                    "end": 750,
                    "text": "Figure 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Multi-Human Variability"
        },
        {
            "text": "As mentioned in Section 2.1, is the evaluation rate of the RL agent, while is the actuation rate that governs the time to apply the action on the environment. Since captures the environment's response time, including the human response time, it should not change even while mediating multiple humans. However, the actuation rate should be mediated because it controls how often we apply an action to the environment. Hence, the Mediator RL has to mediate two values, (1) : which is the action taken by each Governor RL to adapt , and (2) : which is the action taken by each Multisample RL for each human.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-Human Variability"
        },
        {
            "text": "Accordingly, the Mediator RL is responsible for mediating by choosing the smallest value of across all . This will ensure the fastest actuation rate across all humans. However, by changing the actuation rate , Governor RL has to be notified accordingly because now the reward that the Governor RL observes is accounted for a different value of . Hence, to ensure that the reward is associated with the right action, the Mediator RL echos back the mediated , which has the same but a different .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-Human Variability"
        },
        {
            "text": "Next, the Mediator RL should choose the right action to apply to the environment. To that end, and not to compromise the effectiveness of the action on the human experience, we can not just choose one action from the list of actions. Hence, we use a weighted average of all the actions. In principle, the weighted average takes into account the varying degrees of importance of the numbers. However, since we do not know apriori, which action is more important than others to achieve a better cumulative experience for all humans, we can not fix these weights. Moreover, as humans' preferences, moods, and reactions are changing, one action may have more weight at a particular time while having less weight at another time. Hence, we need to learn these weights and continuously adapt them based on all humans interacting in the same environment. Accordingly, the Mediator RL agent adjusts the actions' weights and then applies the weighted average action to the environment. The Mediator RL agent then observes the effect of these weights by collecting the reward , a cumulative reward from all the humans' experiences in the environment. However, since the actual action taken on the environment is different from the individual actions by the different Multisample RL, has to be echoed back to every Multisample RL to associate the reward it is observing with the correct action. Moreover, to ensure that the Mediator RL does not keep on favoring one action over the rest. We extend the design of the Mediator RL to include a fairness measure combined with the humans' experiences to calculate the reward . The design of the Mediator RL will be discussed in Section 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-Human Variability"
        },
        {
            "text": "This section describes how the Governor RL and the Mediator RL agents are designed and how they interact with each other.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm Design"
        },
        {
            "text": "We model the problem of adapting the values of and as executing an optimal policy over a Markov Decision Process (MDP) whose states are defined over and and whose reward function captures the IoT system performance. By executing such an optimal policy, the RL agent is guaranteed to search over the space of and values to maximize the system's performance. The MDP main components namely state space, action space, and reward function are designed as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of the Governor RL Agent"
        },
        {
            "text": "Governor MDP State space X G : The MDP states are the different values that and can take. That is, each state \u2208 X G is determined by a tuple ( , ). Since RL agents' performance depends heavily on the cardinality of the state space, we discretize the continuous values of and into a finite number of values where such discretization depends on the context of the application as it will be discussed in Sections 5 and ??.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of the Governor RL Agent"
        },
        {
            "text": "Governor MDP Action space A G : The action space A G in this MDP contains all the possible combinations of changing the and values. Particularly, the action can be either decrement with some value \u2193, increment with some value \u2191, or remain the same value \u27f2. Such actions are defined for both and under the constraint that \u2265 . We designed the actions as an increment or a decrement ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of the Governor RL Agent"
        },
        {
            "text": "Hyper parameters: Learning parameters: , , Require:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Governor RL"
        },
        {
            "text": "Calculate the performance of Calculate ( ) according to G and exploration strategy:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Governor RL"
        },
        {
            "text": "with probability : ( ) \u2190 choose an action a at random, with probability 1 \u2212 :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Governor RL"
        },
        {
            "text": "rather than choosing a specific tuple ( , ) from all the possible combinations to make sure that there is no big sudden change in the actuator rate ( ) which may compromise the human's experience.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Governor RL"
        },
        {
            "text": "Governor MDP Reward Function G : Each state \u2208 S G is associated with a performance . Such performance is a measure of the human's experience and the performance of the IoT system, which depends on the context of the application, as it will be discussed in Section 4. The performance is calculated after running the Multisample -learning [14] using the values of and associated with state . The associated performance plays a role in determining the reward value that accrued due to taking the action at the state . In particular, the reward value = G ( , ) (positively or negatively) depends on a weighted difference between the performance of the system at the state (before modifying the values of and ) and the performance of the system \u2032 at the new state \u2032 (after modifying the values of and ).",
            "cite_spans": [
                {
                    "start": 337,
                    "end": 341,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Algorithm 1 Governor RL"
        },
        {
            "text": "In this MDP setting, the transition probabilities are known apriori since increasing/decreasing/remain the same or leads to a known state. However, the reward is unknown apriori because it depends on the human's experience. Moreover, it can change over time. Hence, to solve the MDP when the reward values are unknown, we use -learning, which is summarized in Algorithm 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Governor RL"
        },
        {
            "text": "We model the problem of learning the best weights ( 1 , 2 , ... ) for the adaptation actions collected for individual personalized performance in a multi-human setting ( 1 , 2 , ..., ) as executing an optimal policy over a Markov Decision Process (MDP).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of the Mediator RL Agent"
        },
        {
            "text": "Mediator MDP State space X M : The MDP states are the different values of that can be summed up to 1. We discretize their continuous values into finite number of state space X M , where their values are chosen from a predefined set of values {0, 0.2, 0.4, 0.6, 0.8, 1}. The state space's size depends on the number of actions that we need to take their weighted average, reflecting the number of humans in the environment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of the Mediator RL Agent"
        },
        {
            "text": "Mediator MDP Action space A M : The action space A M of the Mediator MDP contains all the possible jumps \u2197 to all the states with the same probability. Such a design choice allows the Mediator RL to rapidly switch between weights and converge faster to the optimal assignment of weights.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of the Mediator RL Agent"
        },
        {
            "text": "Mediator MDP Reward function M : Each state has a performance that is associated with it. The performance is a measure of all the humans' cumulative experience and the performance of the IoT system, which depends on the context of the application, as will be discussed in Section ??. The performance is calculated after applying the weighted average action ( = =1 =1 ) at a rate of the minimum across all individual humans (as chosen by their individual Governor RL). Each human will have a different experience with the applied action . Hence, the cumulative performance associated with this state will be a function of all the human experiences, which depends on the context of the application. To calculate the reward, we use the same idea of calculating the reward in Governor RL. In particular, the associated reward value is computed by the reward function M ( , ) which is a function of the relative performance between two states (which corresponds to the current weights) and the next state \u2032 (which corresponds to the new weights). In this MDP setting, the reward is unknown apriori as it depends on the cumulative humans' experiences. Moreover, it can change over time. Hence, to solve the MDP when the reward values are unknown, we use the -learning summarized in Algorithm 2. In particular, the algorithm is divided into two procedures, , which runs the Q-learning algorithm in which the reward value depends on the performance of the current state and next state \u2032 . The performance of each state is calculated in another procedure that takes a state as input and gets the preferred actions per human (by running ) and the different actuation rates (by running ) to calculate the cumulative performance .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of the Mediator RL Agent"
        },
        {
            "text": "The Mediator RL tries to maximize the total reward by choosing different weights for the different adaptation actions learned from multiple humans preferences. In one extreme case, as we will show in the results (Section 7), the Mediator RL may end up favoring one adaptation action over the others (by keeping assigning high weight to it) to increase its total reward. This means that the Mediator RL favors one human preference over the others because it looks at the cumulative performance. To model this inequality, we borrow concepts from sociology which named this phenomenon the \"Matthew effect\", which is summarized as the rich get richer and the poor get poorer [7] . Accordingly, to ensure fairness across all humans, we need to avoid the \"Matthew effect\" by modifying the reward function of the Mediator RL. In particular, we model the Mediator RL as a resource distributed among multiple humans, and we want to ensure the fair-share of this resource. This leads us to use the notion of utility. We define the utility of each human adaptation \u210e at timestep as:",
            "cite_spans": [
                {
                    "start": 671,
                    "end": 674,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Design of Fairness-aware Mediator RL"
        },
        {
            "text": "In particular, \u210e is the average weight assigned by the Mediator RL for a particular human \u210e over a time horizon [0 : ], where the factor is used to give more value to the recent weights learnt by the Mediator RL over the ones in the past. We then measure the fairness of the Mediator RL using the coefficient of variation ( ) of the human utilities [23] :",
            "cite_spans": [
                {
                    "start": 349,
                    "end": 353,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Design of Fairness-aware Mediator RL"
        },
        {
            "text": "where\u00afis the average utility of all humans. The Mediator RL is said to be more fair if and only if the is smaller. Accordingly, we modify the reward function M to reflect the changes in . In particular, in Algorithm 2, we add a new fairness measure F which denotes the difference between the values at and \u2032 . Hence, we change to become:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of Fairness-aware Mediator RL"
        },
        {
            "text": "where is used as a fairness scale from 0 to 1. As we will show in the results (Section 7), the objective to increase the performance measure W and the objective to increase the fairness measure F can be two opposing objectives.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Design of Fairness-aware Mediator RL"
        },
        {
            "text": "We applied the proposed framework to two applications, the first one is in the domain of Advanced Driver Assistance System (ADAS), and in particular, we focus on the Forward Collision Warning (FCW) application. The second application is in the smart home automation system domain, and in particular, we focus on heating, ventilation, and air conditioning systems (HVAC). To ensure the safety of the real human subjects, we decided to use a simulation environment for both applications 1 . In each application, we will explain the used simulator, the various human behaviors, how the performance is calculated, and how the proposed FaiR-IoT framework can track the intra-human, inter-human, and multi-human variability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation and Evaluation Plan"
        },
        {
            "text": "Standard FCW system measures the time-to-crash based on the distance and the relative velocity of the front object and, if the time-to-crash is below a certain threshold (signaling a possible risk of collision), it alerts the driver to apply the brakes. A human-inthe-loop FCW should take the human state and preferences while calculating this threshold. For example, if the driver is distracted, the alarm should be displayed earlier. Moreover, some drivers take more time to react to the alarm and press the brakes (response time).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application 1: Human-in-the-Loop ADAS -Forward Collision Warning (FCW)"
        },
        {
            "text": "Hence, the alarm threshold should count for this response time as well. Personalized FCW has been addressed in the literature using offline learning of the threshold based on statistical modeling to reduce the required data [34] , drivers' expected response deceleration (ERDs) [52] , parameter identification of driver behavior is proposed in [51] , and adapting the time of the FCW based on the response time and the mental state of the driver [14] .",
            "cite_spans": [
                {
                    "start": 224,
                    "end": 228,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 278,
                    "end": 282,
                    "text": "[52]",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 344,
                    "end": 348,
                    "text": "[51]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 446,
                    "end": 450,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Application 1: Human-in-the-Loop ADAS -Forward Collision Warning (FCW)"
        },
        {
            "text": "Simulator: We used the Skoda Octavia model [49] for vehicle dynamics to simulate the physics of both the driving car and the leading car. We interfaced the vehicle model with the Matlab virtual reality toolbox (VR) to virtualize the vehicle dynamics. Each vehicle has parameters that specify its mass, the wheels' mechanical friction on the ground, its acceleration, and braking forces. Simulated human subjects: Following the observations in [14] to model driving behavior, we simulated three humans with three different driving behavior as follows:",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 47,
                    "text": "[49]",
                    "ref_id": null
                },
                {
                    "start": 443,
                    "end": 447,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Application 1: Human-in-the-Loop ADAS -Forward Collision Warning (FCW)"
        },
        {
            "text": "\u2022 Moderate driver behavior 1 : This driver has a moderate behavior with average braking intensity, acceleration intensity, average relative distance to the leading car, and average response time. \u2022 Aggressive driver behavior 2 : This driver tends to apply high intensity on both the brakes and the acceleration pedal. This is accompanied by a small relative distance to the leading car and a short response time. \u2022 Slow driver behavior 3 : This driver tends to be more conservative. The driver tends to apply low intensity of both the brakes and the acceleration pedal. This is accompanied by a large relative distance to the leading car and a larger response time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application 1: Human-in-the-Loop ADAS -Forward Collision Warning (FCW)"
        },
        {
            "text": "Adapting to the intra-human and inter-human variability using Governor RL-agent: As described in Algorithm 1, the state of MDP is a tuple of ( , ). The response time of the driver and the behavior of the driver are unknown apriori. Different values of and may result in better performance and better driving experience for different humans (inter-human variability). We quantize the state space into 32 states. The sampling time is fixed at 0.25 seconds while and are multiples of the sampling time. In particular, we choose \u2208 [80, 90, 100, 110] and \u2208 [8, 9, 10, 11, 12, 13, 14, 15] multiples of . A state is one combination of and , such as (80,8) . The different states capture the inter-human variability where different humans can have different response times which entail different learning rate ( ) and different actuation rate ( ).",
            "cite_spans": [
                {
                    "start": 552,
                    "end": 555,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 556,
                    "end": 558,
                    "text": "9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 559,
                    "end": 562,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 563,
                    "end": 566,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 567,
                    "end": 570,
                    "text": "12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 571,
                    "end": 574,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 575,
                    "end": 578,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 579,
                    "end": 582,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 642,
                    "end": 648,
                    "text": "(80,8)",
                    "ref_id": null
                }
            ],
            "section": "Application 1: Human-in-the-Loop ADAS -Forward Collision Warning (FCW)"
        },
        {
            "text": "The performance is measured in terms of the following parameters:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance function:"
        },
        {
            "text": "\u2022 False Negatives (FN): The relative distance between the leading car and the driving car is below the safety distance of 7m, which is calculated based on the 2 seconds rule [37] . \u2022 False Positives (FP): The alarm with signaled unnecessarily.",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 178,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Performance function:"
        },
        {
            "text": "\u2022 True Positives (TP): The alarm is signaled, and the driver pressed the brakes. \u2022 True Negatives (TN): The alarm is not signaled, and the relative distance between the leading car and the driving car is above the safety distance of 7m. Based on these parameters, the accuracy (Acc) and Matthews correlation coefficient (MCC) are then calculated for a specific state . We combine both metrics (the Acc and the MCC) to measure the performance of state . MCC has shown to be a good metric for binary classification evaluation [9] , while accuracy gives the proportion of correct predictions. Hence, the performance as indicated in Algorithm 1 is calculated as = + . This value is normalized from 0 to 1.",
            "cite_spans": [
                {
                    "start": 524,
                    "end": 527,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Performance function:"
        },
        {
            "text": "The performance for each state indicated by ( , ) for humans 1 , 2 , and 3 are calculated ahead of time to obtain the ground truth that allows us to validate the ability of Algorithm 1 to obtain the values of ( , ) that maximizes the performance of the system. However, when the algorithm runs, it does not know apriori the performance value of a state until Multisample -learning runs for some time and returns the MCC and the Acc for this specific state. As shown in Figure 4 (top), the best state for 1 is (110,12) then (80,13) with a very small difference. Similarly, the best state for 2 is (110,13) , which was the worst for 1 , followed by (90,15). The best state for human 3 is (80,13), then (80,8). These different states highlight the inter-human variability that needs to be accounted for to provide a personalized experience.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 469,
                    "end": 477,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 596,
                    "end": 604,
                    "text": "(110,13)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Experiment 1: Inter-Human Variability"
        },
        {
            "text": "We run Algorithm 1 for 9000 iterations. As shown in Figure 4 (center), the algorithm converges to states (80,13), (110,13), and (80,13) for humans 1 , 2 and 3 , respectively. These states are the second best state for 1 , and are the states with best performance for humans 2 and 3 . This is also reflected in how the performance values (which is used to calculate the reward changes with the number of iterations for 1 , as shown in Figure 4 (bottom) where the performance value converges to value 0.95 which corresponds to state (80,13). As for 2 , the performance value converges to 1, which corresponds to state (110,13), and for 3 the performance value converges to 1 which corresponds to state (80,13), as shown in Figure 4 (bottom). These results show the proposed Governor RL's ability to track the inter-human variability and converge to the best (for 2 and 3 ) or near best (for 1 ) performance for each human with different driving behavior.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 52,
                    "end": 60,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 434,
                    "end": 442,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 721,
                    "end": 729,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Experiment 1: Inter-Human Variability"
        },
        {
            "text": "We now study the proposed Governor RL's ability to adapt to the changes in human preference across time. To that end, to show how the algorithm can trace the change in behavior to adapt to the intra-human variability. We switch between humans on the same simulator after 10000 iterations and observe how the algorithm will adapt to the new behavior. First, we switch from 1 to 2 , then vice versa in Figure 5 (top) and the respective performance progress across the number of iterations are shown in Figure 5 (bottom). We repeat the same experiment but between 1 and 3 as seen in Figure 5 and the respective performance progress across the number of iterations in Figure 5 . These figures show the ability of Governor RL to adapt to the changes in human behavior.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 400,
                    "end": 408,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 500,
                    "end": 508,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 580,
                    "end": 588,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 664,
                    "end": 672,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Experiment 2: Intra-Human Variability"
        },
        {
            "text": "Recent work in literature targets human-in-the-loop HVAC [27] while trying to assist human satisfaction and addressing fairness across all occupants [47] . Reinforcement learning has been proposed to adapt the HVAC set-point based on human activity [13] . A human-in-the-loop thermal system should take the human state and preferences into the computation loop while calculating the heater set-point. For example, the human body temperature decreases when the human goes to sleep [4] , while the body temperature increases when human exercises [30] and with stress and anxiety [40] . Monitoring the human state [29] , sleep cycle [38] , physical activity [46] are all possible using IoT edge devices.",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 61,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 149,
                    "end": 153,
                    "text": "[47]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 249,
                    "end": 253,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 480,
                    "end": 483,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 544,
                    "end": 548,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 577,
                    "end": 581,
                    "text": "[40]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 611,
                    "end": 615,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 630,
                    "end": 634,
                    "text": "[38]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 655,
                    "end": 659,
                    "text": "[46]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "Application 2: Human-in-the-Loop Smart House -A Thermal System"
        },
        {
            "text": "Simulator: We simulated a mathematical model for a thermal house. In particular, we utilize a thermodynamic model of the house that considers the geometry of the house, the number of windows, the roof pitch angle, and the type of insulation used. The house is being heated by a heater with an airflow with a temperature of 50 \u2022 . A thermostat is used to allow fluctuation of 2.5 \u2022 above and below the desired set-point, which specifies the temperature that must be maintained indoors. The set-point is controlled by an external controller that runs the proposed Governor RL algorithm. The heat flow into the house is calculated by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application 2: Human-in-the-Loop Smart House -A Thermal System"
        },
        {
            "text": "where is the heat flow from the heater into the house, is the heat capacity of the air at constant pressure, and is the air mass flow rate through the heater (set at 1 / ). \u210e and are the temperature of hot air from the heater and the current room air temperature, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application 2: Human-in-the-Loop Smart House -A Thermal System"
        },
        {
            "text": "The thermal system in the house considers the heat flow from the heater and the heat loss to the environment. Heat losses to the environment depend on the thermal resistance of the house calculated by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application 2: Human-in-the-Loop Smart House -A Thermal System"
        },
        {
            "text": "where is the external temperature and is the thermal equivalence of the house taken into consideration the thermal resistance of the wall and the windows. We assume that it is the winter season, so the is kept at 50 \u2022 with a random range of daily temperature variation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application 2: Human-in-the-Loop Smart House -A Thermal System"
        },
        {
            "text": "We model the humans as a heat source with heat flow that depends on the average exhale breath temperature (EBT) and the respiratory minute volume (RMV) ( \u210e \u221d \u00b7 ) [19] . The respiratory minute volume is the product of the breathing frequency ( ) and the volume of gas exchanged during the breathing cycle, which are highly dependent on human activity. For example, \u2248 6 / when the human is resting while \u2248 12 / represents a human performing moderate exercise [8] . We assume that the age/sex/time-of-day have no significance in the model.",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 166,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 457,
                    "end": 460,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Application 2: Human-in-the-Loop Smart House -A Thermal System"
        },
        {
            "text": "Switching from 2 to 1 Switching from 1 to 3 Hr  0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23 Table 1 : Human activity across the 24 hours of the day. Sleeping:\u2605, seated relaxed:\u2022, standing at rest:\u229b, standing light activity:\u0394, light domestic work: , standing medium activity:\u2663, washing dishes standing: , running:\u2660, and not home: . When multiple activities located in the same time slot, one of them is chosen randomly.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 44,
                    "end": 132,
                    "text": "Hr  0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23",
                    "ref_id": null
                },
                {
                    "start": 133,
                    "end": 140,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Switching from 1 to 2"
        },
        {
            "text": "Finally, the indoor temperature-time derivative is directly proportional to the difference between the heat flow rate from the heater and the heat losses to the environment while taking into consideration the heat flow from the occupants. We calculate it as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Switching from 1 to 2"
        },
        {
            "text": "where is the number of occupants in the house. We simulated five days of a working human using this simulation environment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Switching from 1 to 2"
        },
        {
            "text": "We simulated the behavior of three humans based on their activity and their stress level. The activity is simulated by the value of the RMV [8] and the metabolic rate [16] . Human stress level is simulated by an increase in the metabolic rate [40] . We simulated nine activities arranged in the ascending order of RMV: sleeping, seated relaxed, standing at rest, standing light activity, light domestic work, standing medium activity, washing dishes standing, and running on a treadmill. We randomize the behavior by having different choices of activities in the same time slot. For example, as seen in Table 1 , 1 can be doing one of three activities (sleeping, standing at rest, running) between 6 am to 7 am. In total, we simulated five working days for each human.",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 143,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 167,
                    "end": 171,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 243,
                    "end": 247,
                    "text": "[40]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [
                {
                    "start": 603,
                    "end": 610,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Simulated human subjects:"
        },
        {
            "text": "\u2022 Occupant 1 ( 1 ) is an active human. 1 wakes up early and exercises for one hour before leaving for work. When 1 returns home, the activity changes according to the hour of the day. \u2022 Occupant 2 ( 2 ) is a less active human. 2 wakes up later than 1 and does not perform any physical exercise before leaving for work. \u2022 Occupant 3 ( 3 ) is a less active human with more time at home.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulated human subjects:"
        },
        {
            "text": "Adapting to the intra-human and inter-human variability using Governor RL agent: As described in Algorithm 1, the state of the MDP is a tuple of ( , ). This state is passed to Multisample -learning as a parameter. The thermal sensation, the behavior, and the state of the human are unknown apriori to the algorithm. Different values of ( ) and ( ) may give better thermal comfort for different humans (inter-human variability). We quantize the state space into 25 states. The sampling time is = 6 min. and are multiples of the sampling time. In particular, \u2208 [10, 15, 20, 30] and \u2208 [2, 4, 6, 8, 10, 12, 14, 16] multiples of such that, \u2265 . A state is one combination of and , such as (15, 10) . After choosing the MDP state for the Governor RL, Multisample -learning runs for simulated five days. We design the Multisample RL agent by modeling the human as an MDP with states corresponding to different activity ( ) and indoor temperature. A state is defined by a tuple of and . We quantize the indoor temperature ( ) between 60 \u2022 F to 80 \u2022 F with 1 \u2022 F granularity. When applying an action on a state , it can transition to any other state with unknown transition probability due to the change in human behavior. The action space contains the different set-points between 70 \u2022 F to 80 \u2022 F with 2 \u2022 F granularity.",
            "cite_spans": [
                {
                    "start": 559,
                    "end": 563,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 564,
                    "end": 567,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 568,
                    "end": 571,
                    "text": "20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 572,
                    "end": 575,
                    "text": "30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 582,
                    "end": 585,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 586,
                    "end": 588,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 589,
                    "end": 591,
                    "text": "6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 592,
                    "end": 594,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 595,
                    "end": 598,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 599,
                    "end": 602,
                    "text": "12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 603,
                    "end": 606,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 607,
                    "end": 610,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 683,
                    "end": 687,
                    "text": "(15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 688,
                    "end": 691,
                    "text": "10)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Simulated human subjects:"
        },
        {
            "text": "The reward function ( , ) depends on the thermal comfort of the human, which can be estimated using Prediction Mean Vote (PMV) [17] . PMV score indicates the thermal sensation of a human. It depends mainly on human activity, metabolic rate, clothing, and other environmental variables (airspeed, air temperature, mean radiant temperature, and vapor pressure). The scale of PMV ranges from \u22123 (very cold) to 3 (very hot). According to ISO standard ASHRAE 55 [2] , a PMV in the range of \u22120.5 and +0.5 for the interior space is recommended to achieve thermal comfort. Estimation of the PMV score is calculated based on the knowledge of clothing insulation, the metabolic rate, the air vapor pressure, the air temperature, and the mean radiant temperature [17] . The human thermal sensation can change for the same state due to unmodelled external factors. Hence, the reward value can change over time. Moreover, each human can have a different response time (the time the human takes to feel a difference in thermal sensation). In addition to the variability introduced by the human, the response time of the thermal system (the time taken by the HVAC to actually reach the set-point) can be different and change with time due to system aging or unmodelled effects. Hence, and cannot be fixed and should differ from one human to another.",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 131,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 457,
                    "end": 460,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 752,
                    "end": 756,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Simulated human subjects:"
        },
        {
            "text": "Adapting to the multi-human variability using Mediator RL agent: Since we modeled the human as a heat source, human activity affects the individual thermal sensation and affects indoor temperature. Moreover, when multiple humans exist in the same space and their activities are different, their thermal comfort will be at a different indoor temperature. To that end, to adapt to multiple humans in the same house, we need to measure their individual thermal comfort and then take their aggregate thermal comfort to find the HVAC set-point that can achieve a compromised thermal comfort for all the occupants. Accordingly, the HVAC set-point is controlled by running Algorithm 2 to mediate the diverse preferences of set-points from multiple humans. As mentioned in Algorithm 2, a Governor RL with Multisample RL gets the individual preferred set-point per human ( ), as well as the individual actuation rate . States of the Mediator RL models the different weights for three set-points with a total of 21 states. At each state , Mediator RL takes the weighted average of the three set-points with the respective weights and sends it to the HVAC. For example, if the preferred set-points from the 3 humans are 72, 75, 78, respectively, and the state is (0.2, 0.6, 0.4), then = 75.5, and hence the set-point that is sent to the HVAC is 76 which is the nearest non-decimal number.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulated human subjects:"
        },
        {
            "text": "Performance function: The performance is calculated based on two parameters, (1) the moving average of the absolute value of the PMV score over five days and (2) the moving standard deviation of the PMV score. This is used to account for high fluctuation in thermal sensation. The performance value depends on a weighted sum of these parameters. In particular, the performance for a particular state is calculated as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulated human subjects:"
        },
        {
            "text": ", where 1 > 2 and is the number of samples. Indeed, the lower the value of the performance, the worst the thermal sensation that the human experiences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulated human subjects:"
        },
        {
            "text": "Reward functions: As described in Algorithm 1, the reward function is calculated based on the performance of the current state (denoted as ) and the next state \u2032 (denoted as \u2032 ) after applying an action . If \u2032 gives better performance than , then it is a positive reward, otherwise, it is a negative reward. We use weighted performance difference between and \u2032 to calculate the reward value. The reward value is calculated according the following reward function G ( , ):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulated human subjects:"
        },
        {
            "text": "where W is an increasing step function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulated human subjects:"
        },
        {
            "text": "We start by studying the proposed Governor RL agent's ability to adapt to individual human behaviors to select the optimal ( , ) for each human. To that end, we apply the the same steps performed in application 1 in Section 5 for 1 and 2 . Multisample -learning returns MA and MA for a specific state . By brute-forcing all combinations to discover the state with the maximum performance, we conclude that the best state for 1 is (30,12) followed by (10, 10) while the best states for 2 are (15, 8) and (30, 10) , as shown in Figure 6 (left). It is worth noting that the best state for 2 results in poor performance for 1 , highlighting again the inter-human variability that motivates the use of the proposed Governor RL. We run Algorithm 1 report the states chosen by the Governor RL across time in Figure 6 (center) 2 . We run the algorithm for a total of 10000 iterations with adaptive that decreases when the reward value does not change for multiple consecutive iterations. The algorithm converges to the states (10,10) and (15, 8) for humans 1 and 2 , respectively, which are the second-best and best states for the simulated humans. These results validate the designed Governor RL's ability to generalize to multiple applications and address the inter-human variability challenge.",
            "cite_spans": [
                {
                    "start": 450,
                    "end": 454,
                    "text": "(10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 455,
                    "end": 458,
                    "text": "10)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 491,
                    "end": 495,
                    "text": "(15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 496,
                    "end": 498,
                    "text": "8)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 503,
                    "end": 507,
                    "text": "(30,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 508,
                    "end": 511,
                    "text": "10)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1030,
                    "end": 1034,
                    "text": "(15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1035,
                    "end": 1037,
                    "text": "8)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 526,
                    "end": 534,
                    "text": "Figure 6",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 801,
                    "end": 809,
                    "text": "Figure 6",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Experiment 3: Inter-Human Variability"
        },
        {
            "text": "Now, we study the ability of the proposed system (with both Governor RL and Mediator RL agents interacting together) to provide personalized performance when multiple occupants are present in the system. Similar to the previous experiments, we start by brute-forcing all the states of the Governor and the Mediator RL agents to obtain the ground truth for the performance of the system under different values for , for each human along with the different values for the mediator weights 1 , 2 , 3 . While the optimal values for , for each human (individually) was discussed in Experiment 3, we report in Figure 7a the system performance for different values of 1 , 2 , 3 . In particular, state (0, 0, 1) shows the 2 Due to space limitation, we are showing the results for 1 and 2 only. best performance while states (0.6, 0, 0.4) and (0.8, 0.2, 0) show the worst performances. Next, we run the whole system and compares its convergence to the state with the maximum total reward. First we run the whole system without fairness in Mediator RL reward by assigning to value as discussed in Section 3.3. The progress of the states is shown in Figure 7b , where the Mediator RL converges to (0, 0, 1) at iteration 3000, which is the state of maximum performance. This is also reflected in how the performance value changes in Figure 7d . This is a counter-intuitive result since it entails that the system's optimal performance is achieved when the HVAC set-point ultimately favors human 3 . However, one explanation for this result is that human 3 stays in the house for more time than both 1 and 2 and hence the moving average reward is maximized when human 3 is given the highest priority. This motivates using the fairness-aware Mediator RL as explained in Section 3.3 for the next experiment.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 604,
                    "end": 613,
                    "text": "Figure 7a",
                    "ref_id": null
                },
                {
                    "start": 1139,
                    "end": 1148,
                    "text": "Figure 7b",
                    "ref_id": null
                },
                {
                    "start": 1321,
                    "end": 1330,
                    "text": "Figure 7d",
                    "ref_id": null
                }
            ],
            "section": "Experiment 4: Multi-Human Variability"
        },
        {
            "text": "We run the same experiment discussed in experiment 4 but with the fairness-aware Mediator RL by assigning a value of 0.5 to balance the reward between the performance measure W and the fairness measure F . As seen in Figure 7f , the states that the Mediator RL chooses were a combination of different states across time. In particular, states (0, 0, 1), (0, 1, 0), (0.2, 0.6, 0.2), (0.6, 0.4, 0), (0.8, 0, 0.2), and (0.8, 0.2, 0) were selected across time. This shows that the Mediator RL changes the priority of adaptation between 1 , 2 , and 3 across time to ensure fairness. As a result, the performance does not converge to the maximum value state, but it changes depending on the current weights assigned by the Mediator RL. However, even though the performance does not converge to the maximum value, the fairness increases. This is reflected by the difference in the coefficient of variation ( ) values between experiment 4 ( Figure 7d ) and experiment 5 (Figure 7h ). The lower value of indicates higher fairness. In particular, the has been improved by 1.5 orders of magnitude through integrating the fairness measure F in the reward function of the Mediator RL.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 217,
                    "end": 226,
                    "text": "Figure 7f",
                    "ref_id": null
                },
                {
                    "start": 933,
                    "end": 942,
                    "text": "Figure 7d",
                    "ref_id": null
                },
                {
                    "start": 962,
                    "end": 972,
                    "text": "(Figure 7h",
                    "ref_id": null
                }
            ],
            "section": "Experiment 5: Fairness with Multi-Human Variability"
        },
        {
            "text": "Finally, we assess the value of personalizing the experience of the smart HVAC system compared to non-personalized HVAC that uses fixed, manually chosen set-point. Figure 8 (left) shows the PMV of all three simulated humans for fixed set-points of 70 and 76 , respectively. These figures show that the PMV of the three residents exceeds the acceptable range of comfortable thermal, which is between -1 and 1 [2] . On the other hand, and as shown in Figure 8 (right), the PMV of the three residents exhibits a more favorable behavior due to the set-point adaptations provided by the proposed framework. It is worth noting that the personalized framework does utilize these two set-points 70 and 76 most of the time (as shown in Figure 8 (right)). Nevertheless, thanks to its ability to switch between these two values depending on the resident comfort, the system's performance is improved by 41.7% and 58.96% compared to the manually fixed set-point scenarios. ",
            "cite_spans": [
                {
                    "start": 408,
                    "end": 411,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [
                {
                    "start": 164,
                    "end": 172,
                    "text": "Figure 8",
                    "ref_id": null
                },
                {
                    "start": 449,
                    "end": 457,
                    "text": "Figure 8",
                    "ref_id": null
                },
                {
                    "start": 727,
                    "end": 735,
                    "text": "Figure 8",
                    "ref_id": null
                }
            ],
            "section": "Experiment 6: Personalized vs Static Smart HVAC Systems"
        },
        {
            "text": "Human modeling and human preference prediction hold the promise of disrupting the status-quo by designing complex IoT systems that weave advances in human sensing into the fabric of large-scale and societal IoT systems. However, with multiple humans interacting within the same IoT application space, adaptation fairness is a crucial concern. In this paper, we proposed FaiR-IoT, a Fairnessaware Human-in-the-Loop Reinforcement Learning framework that addresses the challenge of human variability modeling with the fairness of adaptation. FaiR-IoT uses three hierarchical RL agents; a Governor RL with a Multisample RL to address the intra-human and inter-human variability, and a fairness-aware Mediator RL to address the multi-human variability. We showed the ability of FaiR-IoT to personalize two different IoT applications in the domain of ADAS and smart homes. By adapting to the human's variability, FaiR-IoT was able to improve the human experience by 40% to 60% compared to the non-personalized systems and enhancing the system's fairness by 1.5 orders of magnitude. Given that this framework focuses on establishing the intersection between human modeling and fairness in IoT applications, it opens up new research and application development directions for more fair and human-centric IoT.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Smart homes that monitor breathing and heart rate",
            "authors": [
                {
                    "first": "Hongzi",
                    "middle": [],
                    "last": "Fadel Adib",
                    "suffix": ""
                },
                {
                    "first": "Zachary",
                    "middle": [],
                    "last": "Mao",
                    "suffix": ""
                },
                {
                    "first": "Dina",
                    "middle": [],
                    "last": "Kabelac",
                    "suffix": ""
                },
                {
                    "first": "Robert C",
                    "middle": [],
                    "last": "Katabi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 33rd annual ACM conference on human factors in computing systems",
            "volume": "",
            "issn": "",
            "pages": "837--846",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Standard 55-2010 American Society of Heating, Refrigerating, and Air-Conditioning Engineers",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Ashrae/Ansi",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Sentinel: occupancy based HVAC actuation using existing WiFi infrastructure within commercial buildings",
            "authors": [
                {
                    "first": "Bharathan",
                    "middle": [],
                    "last": "Balaji",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Anthony",
                    "middle": [],
                    "last": "Nwokafor",
                    "suffix": ""
                },
                {
                    "first": "Rajesh",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "Yuvraj",
                    "middle": [],
                    "last": "Agarwal",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems",
            "volume": "",
            "issn": "",
            "pages": "1--14",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "The sleep-evoked decrease of body temperature",
            "authors": [
                {
                    "first": "Judith",
                    "middle": [],
                    "last": "Barrett",
                    "suffix": ""
                },
                {
                    "first": "Leon",
                    "middle": [],
                    "last": "Lack",
                    "suffix": ""
                },
                {
                    "first": "Mary",
                    "middle": [],
                    "last": "Morris",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Sleep",
            "volume": "16",
            "issn": "",
            "pages": "93--99",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Reinforcement Learning for the Adaptive Scheduling of Educational Activities",
            "authors": [
                {
                    "first": "Jonathan",
                    "middle": [],
                    "last": "Bassen",
                    "suffix": ""
                },
                {
                    "first": "Bharathan",
                    "middle": [],
                    "last": "Balaji",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Schaarschmidt",
                    "suffix": ""
                },
                {
                    "first": "Candace",
                    "middle": [],
                    "last": "Thille",
                    "suffix": ""
                },
                {
                    "first": "Jay",
                    "middle": [],
                    "last": "Painter",
                    "suffix": ""
                },
                {
                    "first": "Dawn",
                    "middle": [],
                    "last": "Zimmaro",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Games",
                    "suffix": ""
                },
                {
                    "first": "Ethan",
                    "middle": [],
                    "last": "Fast",
                    "suffix": ""
                },
                {
                    "first": "John C",
                    "middle": [],
                    "last": "Mitchell",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
            "volume": "",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "The globe thermometer in studies of heating and ventilation",
            "authors": [
                {
                    "first": "Th",
                    "middle": [],
                    "last": "Bedford",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Warner",
                    "suffix": ""
                }
            ],
            "year": 1934,
            "venue": "Epidemiology & Infection",
            "volume": "34",
            "issn": "",
            "pages": "458--473",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "The Matthew effect in science funding",
            "authors": [
                {
                    "first": "Thijs",
                    "middle": [],
                    "last": "Bol",
                    "suffix": ""
                },
                {
                    "first": "Arnout",
                    "middle": [],
                    "last": "Mathijs De Vaan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Van De Rijt",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the National Academy of Sciences",
            "volume": "115",
            "issn": "",
            "pages": "4887--4890",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Pulmonary System",
            "authors": [
                {
                    "first": "Robert",
                    "middle": [
                        "G"
                    ],
                    "last": "Carroll",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Elsevier's Integrated Physiology",
            "volume": "10",
            "issn": "",
            "pages": "99--115",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation",
            "authors": [
                {
                    "first": "Davide",
                    "middle": [],
                    "last": "Chicco",
                    "suffix": ""
                },
                {
                    "first": "Giuseppe",
                    "middle": [],
                    "last": "Jurman",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "BMC genomics",
            "volume": "21",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "The response of human thermal sensation and its prediction to temperature step-change (cool-neutral-cool)",
            "authors": [
                {
                    "first": "Xiuyuan",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "Baizhan",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Hong",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Dong",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Jianke",
                    "middle": [],
                    "last": "Liao",
                    "suffix": ""
                },
                {
                    "first": "Zhichao",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Kechao",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "PloS one",
            "volume": "9",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "AutoRate: How attentive is the driver?",
            "authors": [
                {
                    "first": "Isha",
                    "middle": [],
                    "last": "Dua",
                    "suffix": ""
                },
                {
                    "first": "Akshay",
                    "middle": [],
                    "last": "Uttama Nambi",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "V"
                    ],
                    "last": "Jawahar",
                    "suffix": ""
                },
                {
                    "first": "Venkat",
                    "middle": [],
                    "last": "Padmanabhan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Towards Internet-of-Things for Wearable Neurotechnology",
            "authors": [
                {
                    "first": "Salma",
                    "middle": [],
                    "last": "Elmalaki",
                    "suffix": ""
                },
                {
                    "first": "Mojtaba",
                    "middle": [],
                    "last": "Berken Utku Demirel",
                    "suffix": ""
                },
                {
                    "first": "Sara",
                    "middle": [],
                    "last": "Taherisadr",
                    "suffix": ""
                },
                {
                    "first": "Jack",
                    "middle": [
                        "J"
                    ],
                    "last": "Stern-Nezer",
                    "suffix": ""
                },
                {
                    "first": "Mohammad Abdullah Al",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Faruque",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Proceedings of the The 22nd International Symposium on Quality Electronic Design (ISQED'21)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Internet of Personalized and Autonomous Things (IoPAT) Smart Homes Case Study",
            "authors": [
                {
                    "first": "Salma",
                    "middle": [],
                    "last": "Elmalaki",
                    "suffix": ""
                },
                {
                    "first": "Yasser",
                    "middle": [],
                    "last": "Shoukry",
                    "suffix": ""
                },
                {
                    "first": "Mani",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 1st ACM International Workshop on Smart Cities and Fog Computing",
            "volume": "",
            "issn": "",
            "pages": "35--40",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Sentio: Driver-in-the-Loop Forward Collision Warning Using Multisample Reinforcement Learning",
            "authors": [
                {
                    "first": "Salma",
                    "middle": [],
                    "last": "Elmalaki",
                    "suffix": ""
                },
                {
                    "first": "Huey-Ru",
                    "middle": [],
                    "last": "Tsai",
                    "suffix": ""
                },
                {
                    "first": "Mani",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems",
            "volume": "",
            "issn": "",
            "pages": "28--40",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Caredroid: Adaptation framework for android context-aware applications",
            "authors": [
                {
                    "first": "Salma",
                    "middle": [],
                    "last": "Elmalaki",
                    "suffix": ""
                },
                {
                    "first": "Lucas",
                    "middle": [],
                    "last": "Wanner",
                    "suffix": ""
                },
                {
                    "first": "Mani",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 21st Annual International Conference on Mobile Computing and Networking",
            "volume": "",
            "issn": "",
            "pages": "386--399",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Engineering ToolBox",
            "authors": [],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Thermal comfort. Analysis and applications in environmental engineering. Thermal comfort. Analysis and applications in environmental engineering",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Poul",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Fanger",
                    "suffix": ""
                }
            ],
            "year": 1970,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "WiFi-enabled smart human dynamics monitoring",
            "authors": [
                {
                    "first": "Xiaonan",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Bo",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Cong",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Hongbo",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Yingying",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Mooi Choo",
                    "middle": [],
                    "last": "Chuah",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 15th ACM Conference on Embedded Network Sensor Systems",
            "volume": "",
            "issn": "",
            "pages": "1--13",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Characterizing exhaled airflow from breathing and talking",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Jitendra",
                    "suffix": ""
                },
                {
                    "first": "Chao-Hsin",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "Qingyan",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Indoor air",
            "volume": "20",
            "issn": "",
            "pages": "31--39",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Cooperative inverse reinforcement learning",
            "authors": [
                {
                    "first": "Dylan",
                    "middle": [],
                    "last": "Hadfield-Menell",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Stuart",
                    "suffix": ""
                },
                {
                    "first": "Pieter",
                    "middle": [],
                    "last": "Russell",
                    "suffix": ""
                },
                {
                    "first": "Anca",
                    "middle": [],
                    "last": "Abbeel",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dragan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "3909--3917",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Inequity aversion improves cooperation in intertemporal social dilemmas",
            "authors": [
                {
                    "first": "Edward",
                    "middle": [],
                    "last": "Hughes",
                    "suffix": ""
                },
                {
                    "first": "Joel",
                    "middle": [
                        "Z"
                    ],
                    "last": "Leibo",
                    "suffix": ""
                },
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Phillips",
                    "suffix": ""
                },
                {
                    "first": "Karl",
                    "middle": [],
                    "last": "Tuyls",
                    "suffix": ""
                },
                {
                    "first": "Edgar",
                    "middle": [],
                    "last": "Due\u00f1ez-Guzman",
                    "suffix": ""
                },
                {
                    "first": "Antonio",
                    "middle": [],
                    "last": "Garc\u00eda Casta\u00f1eda",
                    "suffix": ""
                },
                {
                    "first": "Iain",
                    "middle": [],
                    "last": "Dunning",
                    "suffix": ""
                },
                {
                    "first": "Tina",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Mckee",
                    "suffix": ""
                },
                {
                    "first": "Raphael",
                    "middle": [],
                    "last": "Koster",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "3326--3336",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Fairness in reinforcement learning",
            "authors": [
                {
                    "first": "Shahin",
                    "middle": [],
                    "last": "Jabbari",
                    "suffix": ""
                },
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Joseph",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Kearns",
                    "suffix": ""
                },
                {
                    "first": "Jamie",
                    "middle": [],
                    "last": "Morgenstern",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1617--1626",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "A quantitative measure of fairness and discrimination",
            "authors": [
                {
                    "first": "Dah-Ming W",
                    "middle": [],
                    "last": "Rajendra K Jain",
                    "suffix": ""
                },
                {
                    "first": "William",
                    "middle": [
                        "R"
                    ],
                    "last": "Chiu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hawe",
                    "suffix": ""
                }
            ],
            "year": 1984,
            "venue": "Digital Equipment Corporation",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Learning fairness in multi-agent systems",
            "authors": [
                {
                    "first": "Jiechuan",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Zongqing",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "13854--13865",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Hyp-rl: Hyperparameter optimization by reinforcement learning",
            "authors": [
                {
                    "first": "Josif",
                    "middle": [],
                    "last": "Hadi S Jomaa",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [],
                    "last": "Grabocka",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Schmidt-Thieme",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.11527"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Fairness in learning: Classic and contextual bandits",
            "authors": [
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Joseph",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Kearns",
                    "suffix": ""
                },
                {
                    "first": "Jamie",
                    "middle": [
                        "H"
                    ],
                    "last": "Morgenstern",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "325--333",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Towards integration of doppler radar sensors into personalized thermoregulation-based control of HVAC",
            "authors": [
                {
                    "first": "Wooyoung",
                    "middle": [],
                    "last": "Jung",
                    "suffix": ""
                },
                {
                    "first": "Farrokh",
                    "middle": [],
                    "last": "Jazizadeh",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "DeepAlerts: Deep Learning Based Multi-Horizon Alerts for Clinical Deterioration on Oncology Hospital Wards",
            "authors": [
                {
                    "first": "Dingwen",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Patrick",
                    "suffix": ""
                },
                {
                    "first": "Chenyang",
                    "middle": [],
                    "last": "Lyons",
                    "suffix": ""
                },
                {
                    "first": "Marin",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kollef",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "34",
            "issn": "",
            "pages": "743--750",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Moodscope: Building a mood sensor from smartphone usage patterns",
            "authors": [
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Likamwa",
                    "suffix": ""
                },
                {
                    "first": "Yunxin",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Nicholas",
                    "suffix": ""
                },
                {
                    "first": "Lin",
                    "middle": [],
                    "last": "Lane",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Zhong",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceeding of the 11th annual international conference on Mobile systems, applications, and services",
            "volume": "",
            "issn": "",
            "pages": "389--402",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Human thermoregulation and measurement of body temperature in exercise and clinical settings",
            "authors": [
                {
                    "first": "Chin",
                    "middle": [],
                    "last": "Leong Lim",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Byrne",
                    "suffix": ""
                },
                {
                    "first": "Jason Kw",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Annals Academy of Medicine Singapore",
            "volume": "37",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "The smart thermostat: using occupancy sensors to save energy in homes",
            "authors": [
                {
                    "first": "Jiakang",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Tamim",
                    "middle": [],
                    "last": "Sookoor",
                    "suffix": ""
                },
                {
                    "first": "Vijay",
                    "middle": [],
                    "last": "Srinivasan",
                    "suffix": ""
                },
                {
                    "first": "Ge",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Brian",
                    "middle": [],
                    "last": "Holben",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Stankovic",
                    "suffix": ""
                },
                {
                    "first": "Eric",
                    "middle": [],
                    "last": "Field",
                    "suffix": ""
                },
                {
                    "first": "Kamin",
                    "middle": [],
                    "last": "Whitehouse",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 8th ACM Conference on Embedded Networked Sensor Systems",
            "volume": "",
            "issn": "",
            "pages": "211--224",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Forecast: The internet of things, worldwide",
            "authors": [
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Middleton",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Kjeldsen",
                    "suffix": ""
                },
                {
                    "first": "Jim",
                    "middle": [],
                    "last": "Tully",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Human driving data-based design of a vehicle adaptive cruise control algorithm",
            "authors": [
                {
                    "first": "Seungwuk",
                    "middle": [],
                    "last": "Moon",
                    "suffix": ""
                },
                {
                    "first": "Kyongsu",
                    "middle": [],
                    "last": "Yi",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Vehicle System Dynamics",
            "volume": "46",
            "issn": "",
            "pages": "661--690",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Statistical Behavior Modeling for Driver-Adaptive Precrash Systems",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Muehlfeld",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Doric",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ertlmeier",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brandmeier",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Transactions on Intelligent Transportation Systems",
            "volume": "14",
            "issn": "4",
            "pages": "1764--1772",
            "other_ids": {
                "DOI": [
                    "10.1109/TITS.2013.2267799"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Dataefficient hierarchical reinforcement learning",
            "authors": [
                {
                    "first": "Ofir",
                    "middle": [],
                    "last": "Nachum",
                    "suffix": ""
                },
                {
                    "first": "Shane",
                    "middle": [],
                    "last": "Shixiang",
                    "suffix": ""
                },
                {
                    "first": "Honglak",
                    "middle": [],
                    "last": "Gu",
                    "suffix": ""
                },
                {
                    "first": "Sergey",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Levine",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "3303--3313",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Hams: Driver and driving monitoring using a smartphone",
            "authors": [
                {
                    "first": "Shruthi",
                    "middle": [],
                    "last": "Akshay Uttama Nambi",
                    "suffix": ""
                },
                {
                    "first": "Ishit",
                    "middle": [],
                    "last": "Bannur",
                    "suffix": ""
                },
                {
                    "first": "Harshvardhan",
                    "middle": [],
                    "last": "Mehta",
                    "suffix": ""
                },
                {
                    "first": "Aditya",
                    "middle": [],
                    "last": "Kalra",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Virmani",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Venkata",
                    "suffix": ""
                },
                {
                    "first": "Ravi",
                    "middle": [],
                    "last": "Padmanabhan",
                    "suffix": ""
                },
                {
                    "first": "Bhaskaran",
                    "middle": [],
                    "last": "Bhandari",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Raman",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 24th Annual International Conference on Mobile Computing and Networking",
            "volume": "",
            "issn": "",
            "pages": "840--842",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "New York State Department of Motor Vehicles",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Defensive Driving",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "A lightweight and inexpensive in-ear sensing system for automatic whole-night sleep stage monitoring",
            "authors": [
                {
                    "first": "Anh",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "Raghda",
                    "middle": [],
                    "last": "Alqurashi",
                    "suffix": ""
                },
                {
                    "first": "Zohreh",
                    "middle": [],
                    "last": "Raghebi",
                    "suffix": ""
                },
                {
                    "first": "Farnoush",
                    "middle": [],
                    "last": "Banaei-Kashani",
                    "suffix": ""
                },
                {
                    "first": "Ann",
                    "middle": [
                        "C"
                    ],
                    "last": "Halbower",
                    "suffix": ""
                },
                {
                    "first": "Tam",
                    "middle": [],
                    "last": "Vu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 14th ACM Conference on Embedded Network Sensor Systems CD-ROM",
            "volume": "",
            "issn": "",
            "pages": "230--244",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "A survey on human-inthe-loop applications towards an internet of all",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Sousa Nunes",
                    "suffix": ""
                },
                {
                    "first": "Pei",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Jorge S\u00e1",
                    "middle": [],
                    "last": "Silva",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Communications Surveys & Tutorials",
            "volume": "17",
            "issn": "",
            "pages": "944--965",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Stress-induced hyperthermia and anxiety: pharmacological validation",
            "authors": [
                {
                    "first": "Berend",
                    "middle": [],
                    "last": "Olivier",
                    "suffix": ""
                },
                {
                    "first": "Theo",
                    "middle": [],
                    "last": "Zethof",
                    "suffix": ""
                },
                {
                    "first": "Tommy",
                    "middle": [],
                    "last": "Pattij",
                    "suffix": ""
                },
                {
                    "first": "Meg",
                    "middle": [],
                    "last": "Van Boogaert",
                    "suffix": ""
                },
                {
                    "first": "Ruud",
                    "middle": [],
                    "last": "Van Oorschot",
                    "suffix": ""
                },
                {
                    "first": "Christina",
                    "middle": [],
                    "last": "Leahy",
                    "suffix": ""
                },
                {
                    "first": "Ronald",
                    "middle": [],
                    "last": "Oosting",
                    "suffix": ""
                },
                {
                    "first": "Arjan",
                    "middle": [],
                    "last": "Bouwknecht",
                    "suffix": ""
                },
                {
                    "first": "Jan",
                    "middle": [],
                    "last": "Veening",
                    "suffix": ""
                },
                {
                    "first": "Jan",
                    "middle": [],
                    "last": "Van Der Gugten",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "European journal of pharmacology",
            "volume": "463",
            "issn": "",
            "pages": "117--132",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Affective computing",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Rosalind",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Picard",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Occupant thermal feedback for improved efficiency in university buildings",
            "authors": [
                {
                    "first": "Marco",
                    "middle": [],
                    "last": "Pritoni",
                    "suffix": ""
                },
                {
                    "first": "Kiernan",
                    "middle": [],
                    "last": "Salmon",
                    "suffix": ""
                },
                {
                    "first": "Angela",
                    "middle": [],
                    "last": "Sanguinetti",
                    "suffix": ""
                },
                {
                    "first": "Joshua",
                    "middle": [],
                    "last": "Morejohn",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [],
                    "last": "Modera",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Energy and Buildings",
            "volume": "144",
            "issn": "",
            "pages": "241--250",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Sensor Andrew: Large-scale campus-wide sensing and actuation",
            "authors": [
                {
                    "first": "Anthony",
                    "middle": [],
                    "last": "Rowe",
                    "suffix": ""
                },
                {
                    "first": "Mario",
                    "middle": [
                        "E"
                    ],
                    "last": "Berges",
                    "suffix": ""
                },
                {
                    "first": "Gaurav",
                    "middle": [],
                    "last": "Bhatia",
                    "suffix": ""
                },
                {
                    "first": "Ethan",
                    "middle": [],
                    "last": "Goldman",
                    "suffix": ""
                },
                {
                    "first": "Ragunathan",
                    "middle": [],
                    "last": "Rajkumar",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "James",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Garrett",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "F"
                    ],
                    "last": "Jos\u00e9",
                    "suffix": ""
                },
                {
                    "first": "Lucio",
                    "middle": [],
                    "last": "Moura",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Soibelman",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IBM Journal of Research and Development",
            "volume": "55",
            "issn": "2",
            "pages": "6--7",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Active Preference-Based Learning of Reward Functions",
            "authors": [
                {
                    "first": "Dorsa",
                    "middle": [],
                    "last": "Sadigh",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anca",
                    "suffix": ""
                },
                {
                    "first": "Shankar",
                    "middle": [],
                    "last": "Dragan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sastry",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sanjit",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Seshia",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Robotics: Science and Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Information gathering actions over human internal state",
            "authors": [
                {
                    "first": "Dorsa",
                    "middle": [],
                    "last": "Sadigh",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Shankar Sastry",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sanjit",
                    "suffix": ""
                },
                {
                    "first": "Anca",
                    "middle": [],
                    "last": "Seshia",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dragan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "volume": "",
            "issn": "",
            "pages": "66--73",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Sensors-based wearable systems for monitoring of human movement and falls",
            "authors": [
                {
                    "first": "Tal",
                    "middle": [],
                    "last": "Shany",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Stephen",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Redmond",
                    "suffix": ""
                },
                {
                    "first": "Nigel",
                    "middle": [
                        "H"
                    ],
                    "last": "Michael R Narayanan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lovell",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Sensors Journal",
            "volume": "12",
            "issn": "",
            "pages": "658--670",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Exploring Fairness in Participatory Thermal Comfort Control in Smart Buildings",
            "authors": [
                {
                    "first": "Eun-Jeong",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "Roberto",
                    "middle": [],
                    "last": "Yus",
                    "suffix": ""
                },
                {
                    "first": "Sharad",
                    "middle": [],
                    "last": "Mehrotra",
                    "suffix": ""
                },
                {
                    "first": "Nalini",
                    "middle": [],
                    "last": "Venkatasubramanian",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments (BuildSys '17)",
            "volume": "19",
            "issn": "",
            "pages": "1--19",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Wearable sweat rate sensors for human thermal comfort monitoring",
            "authors": [
                {
                    "first": "Jai Kyoung",
                    "middle": [],
                    "last": "Sim",
                    "suffix": ""
                },
                {
                    "first": "Sunghyun",
                    "middle": [],
                    "last": "Yoon",
                    "suffix": ""
                },
                {
                    "first": "Young-Ho",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Scientific reports",
            "volume": "8",
            "issn": "",
            "pages": "1--11",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Skoda Octavia model",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Towards a socially responsible smart city: dynamic resource allocation for smarter community service",
            "authors": [
                {
                    "first": "Huey-Ru Debbie",
                    "middle": [],
                    "last": "Tsai",
                    "suffix": ""
                },
                {
                    "first": "Yasser",
                    "middle": [],
                    "last": "Shoukry",
                    "suffix": ""
                },
                {
                    "first": "Min",
                    "middle": [
                        "Kyung"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Vasumathi",
                    "middle": [],
                    "last": "Raman",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "A forward collision warning algorithm with adaptation to driver behaviors",
            "authors": [
                {
                    "first": "Jianqiang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Chenfei",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Shengbo",
                    "middle": [
                        "Eben"
                    ],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Likun",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Transactions on Intelligent Transportation Systems",
            "volume": "17",
            "issn": "",
            "pages": "1157--1167",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Development of a Kinematic-Based Forward Collision Warning Algorithm Using an Advanced Driving Simulator",
            "authors": [
                {
                    "first": "Xuesong",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Ming",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Meixin",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Tremont",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Transactions on Intelligent Transportation Systems",
            "volume": "17",
            "issn": "",
            "pages": "2583--2591",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Markov-optimal sensing policy for user state estimation in mobile devices",
            "authors": [
                {
                    "first": "Yi",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Bhaskar",
                    "middle": [],
                    "last": "Krishnamachari",
                    "suffix": ""
                },
                {
                    "first": "Qing",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Murali",
                    "middle": [],
                    "last": "Annavaram",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks",
            "volume": "",
            "issn": "",
            "pages": "268--278",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Reinforcement learning for learning rate control",
            "authors": [
                {
                    "first": "Chang",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Tao",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "Gang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Tie-Yan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1705.11159"
                ]
            }
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "A cooperative multi-agent deep reinforcement learning framework for real-time residential load scheduling",
            "authors": [
                {
                    "first": "Chi",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Sanmukh",
                    "suffix": ""
                },
                {
                    "first": "Chuanxiu",
                    "middle": [],
                    "last": "Kuppannagari",
                    "suffix": ""
                },
                {
                    "first": "Rajgopal",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kannan",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Viktor",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Prasanna",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the International Conference on Internet of Things Design and Implementation",
            "volume": "",
            "issn": "",
            "pages": "59--69",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Governor RL interacts with Multisample RL to adapt to the interhuman variability. Governor RL adapts the values of and",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Mediator RL adapts to multi-human variability by adapting the weights of the adaptation actions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Humans H M = {\u210e 1 , \u210e 2 , . . . , \u210e } Sates X M = { ( 1 , 2 , . . . , ) 1 , . . . , ( 1 , 2 , . . . , ) } Actions A M = { (\u2197 1, \u2197 2, . . . , \u2197 ) } Reward function M : X M \u00d7 A M \u2192 R Transition function M : X M \u00d7 A M \u2192 X M Multisample -learning algorithm per human \u210e: ( \u210e ), . . \u210e \u2190 current action by \u210e \u2200\u210e \u2208 H M , .insert( ), . . \u2190 current by \u210e \u2190 WAVG(Act, ) \u2190 min{ } \u2200\u210e \u2208 H M , Update \u210e with \u2200\u210e \u2208 H M , Update with \u2200\u210e \u2208 H M , .insert( ), . . \u2190 CalculatePerformance (\u210e, ) \u2190CalculateCumulativePerformance( ) return procedure (X M , A M , M , M , , ) Initialize M : X M \u00d7 A M \u2192 R with 0 while true do Start in state Calculate the performance of Calculate according to M and exploration strategy: with probability : ( ) \u2190 choose an action a at random, with probability 1 \u2212 : ( ) \u2190 argmax M ( , Calculate the performance of \u2032 \u2190 M ( , ) = W ( \u2032 , ) \u22b2 Receive the reward M ( \u2032 , ) \u2190 (1 \u2212 ) \u00b7 M ( , ) + \u00b7 ( + \u00b7 max \u2032 M ( \u2032 , \u2032 )) \u2190 \u2032 return M",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Numerical results of Experiment 1. (Top) the performance of the system-obtained through brute force search-under different assignments for and for humans 1 on the left, human 2 in the middle, and human 3 on the right. (Center) the progress of the states of the Governor RL agent across different iterations of the system showing the convergence to the states with maximum system performance. (Bottom) the performance of the system due to the adaptations of the Governor RL agent showing an increase in the performance over time.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Numerical results of Experiment 2. (Top) the progress of the states of the Governor RL across different iterations of the system when the simulated human behavior changes across time, showing the ability of the Governor RL agent to track states with maximum system performance. (Bottom) the performance of the system when the simulated human behavior changes across time, showing the ability of the Governor RL to maximize the system performance over time.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Numerical results of Experiment 3. (Left) The system's performance-obtained through brute force search-under different assignments for and for both human 1 on the top and human 2 on the bottom. (Center) the progress of the states of the Governor RL agent across different iterations of the system showing the convergence to the states with maximum system performance. (Right) The system's performance due to the adaptations of the Governor RL agent showing an increase in the performance over time.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Numerical results of Experiment 4 and 5 by comparing the performance of the Mediator RL without/with the fairness measure. (a) and (e) show the performance of the system-obtained through brute force search-under different assignments for ( 1 , 2 , 3 ). The states that the Mediator RL chooses are circled. (b) shows the progress of the states of the Mediator RL agent without the fairness measure F across different iterations of the system showing the convergence to the state with maximum system performance, while (f) shows the states that Mediator RL chooses with fairness measure which are multiple states that correspond to the circled states in (e). While (c) shows the performance of the system due to the adaptations of both the Mediator RL and the Governor RL agents showing an increase in the performance over time, while (g) shows that with fairness measure, the performance of the system depends on the states that the Mediator RL chooses to balance the fairness across the humans. (d) and (h) show the difference in the coefficient of variation. The low values indicate higher fairness. Numerical results of Experiment 6 comparing the PMV of the three residents when: Moving Average of PMV for the three humans at fixed HVAC set-point of 70 and 76 and Moving average of PMV for the three humans when the proposed FaiR-IoT framework is used to adapt the HVAC set-point along with the computed set points. Thanks to the proposed FaiR-IoT framework, the PMV of the three residents is maintained within the acceptable thermal comfort of [-1,1].",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}
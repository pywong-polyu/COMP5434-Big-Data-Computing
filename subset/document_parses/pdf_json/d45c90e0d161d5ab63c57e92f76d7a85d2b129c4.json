{
    "paper_id": "d45c90e0d161d5ab63c57e92f76d7a85d2b129c4",
    "metadata": {
        "title": "Fighting COVID-19 in the Dark: Methodology for Improved Inference Using Homomorphically Encrypted DNN",
        "authors": [
            {
                "first": "Moran",
                "middle": [],
                "last": "Baruch",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Lev",
                "middle": [],
                "last": "Greenberg",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Guy",
                "middle": [],
                "last": "Moshkowich",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Research",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Privacy-preserving deep neural network (DNN) inference is a necessity in different regulated industries such as healthcare, finance, and retail. Recently, homomorphic encryption (HE) has been used as a method to enable analytics while addressing privacy concerns. HE enables secure predictions over encrypted data. However, there are several challenges related to the use of HE, including DNN size limitations and the lack of support for some operation types. Most notably, the commonly used ReLU activation is not supported under some HE schemes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "We propose a structured methodology to replace ReLU with a quadratic polynomial activation. To address the accuracy degradation issue, we use a pre-trained model that trains another HE-friendly model, using techniques such as 'trainable activation' functions and knowledge distillation. We demonstrate our methodology on the AlexNet architecture, using the chest X-Ray and CT datasets for COVID-19 detection. Our experiments show that by using our approach, the gap between the F 1 score and accuracy of the models trained with ReLU and the HEfriendly model is narrowed down to within a mere 1.1 -5.3 percent degradation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "One of the recent promising approaches for maintaining the confidentiality of private data in untrusted environments is homomorphic encryption (HE), which allows computation over encrypted data. At its core, an HE scheme provides three capabilities: encryption (Enc), evaluation (Eval), and decryption (Dec). The data owner, say the hospital in our example, can encrypt a message m by invoking c = Enc(m) and then upload the ciphertext c to the cloud together with some function f that it wishes to evaluate on m. Subsequently, the cloud evaluates c \u2032 = Eval(f, c) without learning anything about m or the value that c \u2032 encrypts. It returns encrypted results to the data-owner, who can decrypt it using m \u2032 = f (m) = Dec(c \u2032 ) and get the desired results. For further information about HE see [Gen10].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Preprint. Under review.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The ability to run DNN inference on untrusted cloud environments is becoming critical for many industries such as healthcare, finance, and retail. Doing so while adhering to privacy regulations such as HIPAA [Cen96] and GDPR [EU 16] is not trivial. For example, consider a hospital that wishes to analyze and classify medical images (e.g., [WLW20, GWW20] ) on the cloud. Regulations may force the hospital to encrypt these images before uploading them to the cloud, which can prevent the analytical evaluation of the data in the cloud without first performing decryption. HE for DNN inference is an active research topic [NRPH19, GBDL + 16, HTG17] , with the goal of using a trained DNN model to classify encrypted data. However, this task is not trivial due to limitations that an HE scheme may possess. We describe two principal challenges of HE inference. Multiplication depth. Some HE schemes only allow for a certain number of consecutive multiplication operations. To tackle this challenge, such schemes use a bootstrapping operation [Gen09] that allows further computation. Multiplication depth is defined as the longest chain of sequential multiplication operations of the HE evaluated function. Because bootstrapping is an expensive operation in terms of run-time, reducing the multiplication depth allows us to to reduce or even avoid bootstrapping altogether, while speeding up the entire computation.",
            "cite_spans": [
                {
                    "start": 208,
                    "end": 215,
                    "text": "[Cen96]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 340,
                    "end": 347,
                    "text": "[WLW20,",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 348,
                    "end": 354,
                    "text": "GWW20]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 635,
                    "end": 640,
                    "text": "+ 16,",
                    "ref_id": null
                },
                {
                    "start": 641,
                    "end": 647,
                    "text": "HTG17]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1040,
                    "end": 1047,
                    "text": "[Gen09]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Non-polynomial operations. Some modern HE schemes support only basic arithmetic operations of addition and multiplication e.g., CKKS [CKKS17] and BGV [BGV14] schemes. This means that a DNN component that cannot be represented as a composition of these arithmetic operations, cannot be computed directly in HE.",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 141,
                    "text": "[CKKS17]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 150,
                    "end": 157,
                    "text": "[BGV14]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "One way to overcome this limitation is by using a polynomial approximation to approximate the operation. For example, the ReLU activation function defined as ReLU(x) = max(0, x) is approximated by a polynomial in [LKL + 21, TPD + 19, MZ17, HTG17] . A second option is to replace the operation with a similar but different HE-friendly operation. For example, this may involve replacing a max-pooling operation with the HE-friendly operation of average-pooling, which in many use cases does not affect the DNN performance [GBDL + 16] . A third way, is to use a client-aided design [LTJS + 21] , where the hard-to-compute operation is sent to the data-owner who decrypts the data, computes the operation, encrypts the result, and sends it back to the cloud to continue its HE computation. We prefer to avoid this method because, in addition to the communication complexity, a recent theoretical attack [AV21] raised concerns about the security of client-aided designs.",
            "cite_spans": [
                {
                    "start": 234,
                    "end": 239,
                    "text": "MZ17,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 240,
                    "end": 246,
                    "text": "HTG17]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 520,
                    "end": 531,
                    "text": "[GBDL + 16]",
                    "ref_id": null
                },
                {
                    "start": 579,
                    "end": 590,
                    "text": "[LTJS + 21]",
                    "ref_id": null
                },
                {
                    "start": 899,
                    "end": 905,
                    "text": "[AV21]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our Contributions. We propose a new methodology that combines several techniques for adapting and training HE-friendly DNNs for encrypted inference. In these DNNs, we replace the ReLU activation functions by customized quadratic polynomial approximations. Our methodology also enables the entire inference process to occur without interaction with the data-owner, e.g., in the cloud environment. We show empirically that the resulting inference accuracy is comparable with the inference accuracy of our baseline, the original DNN with ReLU activation, and max-pooling.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The applied techniques include:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 Low-degree polynomial activation functions with trainable coefficients \u2022 Method for gradual replacement of the original activation functions during the training \u2022 Adaptation of the knowledge distillation (KD) technique to train an HE-friendly model from a pre-trained baseline model in its vanilla settings",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We evaluated the efficiency of our method on the AlexNet [KSH12] architecture for COVID-19 detection over CT and chest X-Ray (CXR) large images of size 224\u00d7224\u00d73. The results demonstrated a minimal degradation of up to 5.3% in the F 1 score, compared to the original AlexNet model.",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 64,
                    "text": "[KSH12]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Organization. The document is organized as follows. Section 2 surveys the relevant literature. Section 3 presents our methodology and the techniques that we use. We present our experiments in Section 4 and conclude in Section 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The ReLU function uses the non-polynomial max operation, which is not supported by some HE schemes such as CKKS. For these schemes, addressing this limitation is possible using methods such as lookup tables and polynomial approximations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Using lookup tables to approximate ReLU were introduced in [PUZ93, KM10] , and were used for training DNN homomorphically in [LFFJ19, NRPH19] . One disadvantage of this approach is the low resolution of the lookup table, which is limited by the number of lookup table entries. This number is significantly lower than the number of values possible in a single or double floating-point number. In addition, this technique is not available for all HE schemes, e.g., for CKKS.",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 72,
                    "text": "KM10]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 125,
                    "end": 133,
                    "text": "[LFFJ19,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 134,
                    "end": 141,
                    "text": "NRPH19]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The second approach involves techniques for replacing the ReLU activation function with a polynomial approximation function. This can be done by using either an analytical method to approximate the polynomial or machine learning to train the polynomial coefficients. The work of [CKK + 19] describes a method for approximating the generic max function. However, using this method often leads to a high degree polynomial approximation, which increases the multiplication depth and the accumulated noise. In addition, this approximation is applicable only when the input operands are limited to a specific range. Unlike the above methods that can approximate generic functions, we are interested in ReLU. ReLU is a special case of the max function, where one of the max input operands is fixed to zero. Hence, it is possible to use other approximation methods that yield polynomials with even lower degrees and better performance, while improving the overall efficiency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The square function (square(x) = x 2 ) [GBDL + 16] is a well-known low-degree polynomial replacement for the ReLU function. However, when the number of layers in the model grows, the accuracy of the model degrades significantly. To mitigate this degradation, [LKL + 21, TPD + 19, MZ17, HTG17] suggested using a higher-degree polynomial, which again leads to high computational depth. Another mitigation was suggested in [TPD + 19] , which approximated ReLU using a quadratic polynomial instead of a simple square function. To evaluate the performance of these methods, the authors use a lighter variant of AlexNet [KSH12] with images of size 32 \u00d7 32 \u00d7 3. We tested their approach on the original AlexNet architecture with larger images of size 224 \u00d7 224 \u00d7 3.",
            "cite_spans": [
                {
                    "start": 420,
                    "end": 430,
                    "text": "[TPD + 19]",
                    "ref_id": null
                },
                {
                    "start": 614,
                    "end": 621,
                    "text": "[KSH12]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "As reported in Section 4, this approximation suffers from a significant accuracy degradation of up to 35%.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The studies above searched for a ReLU replacement that would serve as a good polynomial approximation. They then replace all the ReLU occurrences in a model with this approximation. In contrast, we consider a fine-tuning approach, in which we use a different activation per layer, without necessarily approximating the ReLU activation function. To this end, we used a DNN to train the coefficients of the different polynomials. We call this technique 'trainable activation' 1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "A similar approach was suggested by [PUZ93, ZXF02, SSCU19] . Here, the authors trained a polynomial per neuron in small networks of several dozens of neurons. Obviously, this approach is not feasible in modern networks, where the number of neurons is in the order of millions and the number of parameters requiring optimization is huge.",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 43,
                    "text": "[PUZ93,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 44,
                    "end": 50,
                    "text": "ZXF02,",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 51,
                    "end": 58,
                    "text": "SSCU19]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Instead of training a polynomial per neuron, [WLW + 18] suggested training a polynomial per layer, for all channels together. Subsequently, they evaluated their activation functions on 3 models, where the largest model has 4 convolutional layers, 2 average-pooling layers, 2 fully-connected and 3 polynomial activations, accompanied by a batch normalization layer. The experiments were applied on the MNIST dataset, which consists of images of size 28 \u00d7 28 \u00d7 1. We scaled up this approach by using a larger model (see Appendix 5) , over larger images, and extended their methodology by combining additional techniques to help the model converge better. Our evaluation shows that we were able to narrow down the accuracy gap between the HE-friendly AlexNet, trained using the approach of [WLW + 18], and our baseline. Our experiments show that, compared to their approach, we improved the performance of the model by up to 12.5%.",
            "cite_spans": [
                {
                    "start": 518,
                    "end": 529,
                    "text": "Appendix 5)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Our goal was to replace the ReLU function with a polynomial activation function. This section describes the training methodology we used to achieve comparable performance with the baseline model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Our approach was to design a trainable polynomial that would replace the ReLU activation, without approximating it. Recent papers suggested approximating ReLU using high-degree polynomials to achieve a good approximation. However, for a polynomial of degree n, this requires an order of log 2 (n) multiplications [Sch75] , which increases the computation depth significantly as the number of multiplications grows. Therefore, we suggest using a trainable polynomial activation of a 2 nd degree polynomial without the constant term. We use the form ax 2 + bx, where a and b are trainable coefficients, which we train per layer individually. A similar approach was presented by [WLW + 18], where each such activation layer only increments the multiplication depth by 1.",
            "cite_spans": [
                {
                    "start": 313,
                    "end": 320,
                    "text": "[Sch75]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Trainable Polynomial Activation"
        },
        {
            "text": "Applying such a significant architecture change at once to a complex model, without first adapting the model weights, can lead to a steep drop in accuracy. Hence, we designed a new approach that we call Smooth-Transition. We start by training a model that includes ReLU activation layers for e 0 epochs. On the next d epochs, we smoothly transition from the ReLU functions to the polynomial activation functions poly_act(). Then, we continue to train the model on the transitioned HE-friendly architecture. To model this, we use the ratio parameter \u03bb e per epoch e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Smooth-Transition"
        },
        {
            "text": "and set the weighted activation function at epoch e as weighted_act \u03bbe (x) := (1 \u2212 \u03bb e ) \u00b7 ReLU (x) + \u03bb e \u00b7 poly_act(x). To help the network converge, we initiate the quadratic function poly_act(x) = ax 2 + bx as a linear function that is somewhat similar to ReLU, by setting a = 0 and b = 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Smooth-Transition"
        },
        {
            "text": "Using polynomial activations instead of ReLU activations is less suitable for the classification task. To strengthen the model, we adopted the well known KD [HVD15] technique. KD enables a \"knowledge\" transfer from a stronger pre-trained 'teacher' model to a weaker 'student' model. In practice, the student model is usually smaller than the teacher [MFL + 20]. In our case, replacing ReLU by a polynomial activation weakens the HE-friendly model; therefore, the original model is used as a 'teacher' model to assist in training the 'student' HE-friendly model.",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 164,
                    "text": "[HVD15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Knowledge Distillation"
        },
        {
            "text": "We use the response-based KD approach, one of the simplest KD methods [GYMT21] . Here, an additional term is added to the loss function to measure discrepancies between the predictions of the teacher and student models. Moreover, in the commonly used soft target technique [HVD15] , soft targets are used instead of the original predictions of the teacher and student models:",
            "cite_spans": [
                {
                    "start": 70,
                    "end": 78,
                    "text": "[GYMT21]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 273,
                    "end": 280,
                    "text": "[HVD15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Knowledge Distillation"
        },
        {
            "text": "is the soft target version of the prediction for the class i, z i are the original prediction logits, and \u03c4 is the temperature [HVD15] . With \u03c4 = 1 the above formula becomes the standard \"softmax\" output: using a higher temperature value (\u03c4 > 1) produces a more uniform distribution of the probabilities over the classes. The resulting loss function becomes [Li18, Li21] :",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 134,
                    "text": "[HVD15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 358,
                    "end": 364,
                    "text": "[Li18,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 365,
                    "end": 370,
                    "text": "Li21]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Knowledge Distillation"
        },
        {
            "text": "s and Q \u03c4 t are vectors of the soft target predictions of student and teacher models with the same temperature \u03c4 > 1, Q 1 s is the \"softmax\" student prediction, y true is the original labels, CE is cross-entropy loss function and \u03b1 is the hyperparameter controlling the relative weight of the additional KD loss term.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Knowledge Distillation"
        },
        {
            "text": "For our experiments we consider two datasets: COVIDx and COVIDx CT-2A.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "COVIDx [WLW20] . This is a dataset of CXR images labeled as: Normal, Pneumonia, or COVID-19. It is an open access benchmark dataset comprising \u223c20, 000 CXR images, with the largest number of publicly available COVID-19 positive cases. This dataset collects its data from six chest X-Ray datasets [CMD + 20, A20, TSL + 21, CRK + 20, CVS + 13, RKQ + 21], and combines them into a big dataset that is updated over time with more COVID-19 positive CXR images. The number of images we used per class is depicted in Table 1 . When creating this dataset, we verified that there are no patients overlapping between the train, test, and validation subsets. We applied an augmentation process to the data, similar to [WLW20] . COVIDx CT-2A [GWW20] . This dataset consists of 194,922 chest CT slices from 3,745 patients, with the same classes as in the previous dataset. Due to time limitations, we used a random subset of the original dataset, as depicted in Table 1 .",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 14,
                    "text": "[WLW20]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 707,
                    "end": 714,
                    "text": "[WLW20]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 730,
                    "end": 737,
                    "text": "[GWW20]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [
                {
                    "start": 510,
                    "end": 517,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 949,
                    "end": 956,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "Each image was augmented as follows: resize into 224 \u00d7 224 \u00d7 3, random rotation, horizontal flip, vertical flip, color jitter, and normalize.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "The authors of the COVIDx chest x-ray and CT datasets [WLW20, GWW20] also developed tailored models named COVID-Net. Due to the constraint of computational depth, we instead used a pretrained AlexNet [KSH12] model that yields similar results. Since the original AlexNet is not HEfriendly, we present the steps for transforming the original model into an HE-friendly one.",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 207,
                    "text": "[KSH12]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "We implemented an AlexNet model based on PyTorch 2 , and added a batch normalization layer. To avoid additional computational depth, after the training process ends, we absorb the coefficients of the batch normalization into the weights of the next layer, as suggested by [I\u00d618] . See Appendix A for a detailed description of neural network architectures.",
            "cite_spans": [
                {
                    "start": 272,
                    "end": 278,
                    "text": "[I\u00d618]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Table 2 summarizes our experimental results using different methods on the COVIDx and COVIDx-CT-2A datasets. In every experiment, we measured the accuracy and macro-average of the F 1 scores on all of the three classes. We repeated every experiment five times with different seeds and report the average results and the standard deviation in Table 2 . For more details regarding the training setup, see Appendix B.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 342,
                    "end": 349,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "As can be seen from ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "We introduced a new methodology for training an HE-friendly model that replaces the ReLU activation functions with a trainable quadratic approximation. Our approach uses techniques such as polynomial activation functions with trainable coefficients, gradual replacement of activation layers during training, and KD. We tested our methodology on chest CT and CXR image datasets using the AlexNet architecture, and showed that the performance of our trained model is only 5% less accurate than the reference model, making it at least 15% better than all previously suggested HE-friendly models. In future work, we plan to further evaluate our methods on more complicated models and domains, and evaluate the new HE-model's performance in the encrypted domain.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Actualmed COVID-19 chest x-ray data initiative",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chung",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "On the privacy of protocols based on cpa-secure homomorphic encryption",
            "authors": [
                {
                    "first": "Adi",
                    "middle": [],
                    "last": "Akavia",
                    "suffix": ""
                },
                {
                    "first": "Margarita",
                    "middle": [],
                    "last": "Vald",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Leveled) fully homomorphic encryption without bootstrapping",
            "authors": [
                {
                    "first": "Zvika",
                    "middle": [],
                    "last": "Brakerski",
                    "suffix": ""
                },
                {
                    "first": "Craig",
                    "middle": [],
                    "last": "Gentry",
                    "suffix": ""
                },
                {
                    "first": "Vinod",
                    "middle": [],
                    "last": "Vaikuntanathan",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ACM Transactions on Computation Theory (TOCT)",
            "volume": "6",
            "issn": "3",
            "pages": "1--36",
            "other_ids": {
                "DOI": [
                    "10.1145/2633600"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "The Health Insurance Portability and Accountability Act of 1996 (HIPAA)",
            "authors": [
                {
                    "first": "Medicare &amp; Medicaid",
                    "middle": [],
                    "last": "Centers For",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Services",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Numerical method for comparison on homomorphically encrypted numbers",
            "authors": [
                {
                    "first": "Dongwoo",
                    "middle": [],
                    "last": "Jung Hee Cheon",
                    "suffix": ""
                },
                {
                    "first": "Duhyeong",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Hee",
                    "middle": [],
                    "last": "Hun",
                    "suffix": ""
                },
                {
                    "first": "Keewoo",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on the Theory and Application of Cryptology and Information Security",
            "volume": "",
            "issn": "",
            "pages": "415--445",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-34621-8_15"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Homomorphic encryption for arithmetic of approximate numbers",
            "authors": [
                {
                    "first": "Andrey",
                    "middle": [],
                    "last": "Jung Hee Cheon",
                    "suffix": ""
                },
                {
                    "first": "Miran",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Yongsoo",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on the Theory and Application of Cryptology and Information Security",
            "volume": "",
            "issn": "",
            "pages": "409--437",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-70694-8_15"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Nasser Al Emadi, Mamun Bin Ibne Reaz, and Mohammad Tariqul Islam. Can AI Help in Screening Viral and COVID-19 Pneumonia?",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "H"
                    ],
                    "last": "Muhammad",
                    "suffix": ""
                },
                {
                    "first": "Tawsifur",
                    "middle": [],
                    "last": "Chowdhury",
                    "suffix": ""
                },
                {
                    "first": "Amith",
                    "middle": [],
                    "last": "Rahman",
                    "suffix": ""
                },
                {
                    "first": "Rashid",
                    "middle": [],
                    "last": "Khandakar",
                    "suffix": ""
                },
                {
                    "first": "Muhammad",
                    "middle": [
                        "Abdul"
                    ],
                    "last": "Mazhar",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kadir",
                    "suffix": ""
                },
                {
                    "first": "Khandakar Reajul",
                    "middle": [],
                    "last": "Zaid Bin Mahbub",
                    "suffix": ""
                },
                {
                    "first": "Muhammad",
                    "middle": [
                        "Salman"
                    ],
                    "last": "Islam",
                    "suffix": ""
                },
                {
                    "first": "Atif",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Iqbal",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository",
            "volume": "8",
            "issn": "",
            "pages": "1045--1057",
            "other_ids": {
                "DOI": [
                    "10.1007/s10278-013-9622-7"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)",
            "authors": [],
            "year": 2016,
            "venue": "Official Journal of the European Union",
            "volume": "119",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy",
            "authors": [
                {
                    "first": "Nathan",
                    "middle": [],
                    "last": "+ 16] Ran Gilad-Bachrach",
                    "suffix": ""
                },
                {
                    "first": "Kim",
                    "middle": [],
                    "last": "Dowlin",
                    "suffix": ""
                },
                {
                    "first": "Kristin",
                    "middle": [],
                    "last": "Laine",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Lauter",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Naehrig",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wernsing",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "201--210",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Fully homomorphic encryption using ideal lattices",
            "authors": [
                {
                    "first": "Craig",
                    "middle": [],
                    "last": "Gentry",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the forty-first annual ACM symposium on Theory of computing",
            "volume": "",
            "issn": "",
            "pages": "169--178",
            "other_ids": {
                "DOI": [
                    "10.1145/1536414.1536440"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Computing arbitrary functions of encrypted data",
            "authors": [
                {
                    "first": "Craig",
                    "middle": [],
                    "last": "Gentry",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Communications of the ACM",
            "volume": "53",
            "issn": "3",
            "pages": "97--105",
            "other_ids": {
                "DOI": [
                    "10.1145/1666420.1666444"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "COVIDNet-CT: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases From Chest CT Images",
            "authors": [
                {
                    "first": "Hayden",
                    "middle": [],
                    "last": "Gunraj",
                    "suffix": ""
                },
                {
                    "first": "Linda",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Alexander",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Frontiers in medicine",
            "volume": "7",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3389/fmed.2020.608525"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Knowledge distillation: A survey",
            "authors": [
                {
                    "first": "Jianping",
                    "middle": [],
                    "last": "Gou",
                    "suffix": ""
                },
                {
                    "first": "Baosheng",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Stephen",
                    "suffix": ""
                },
                {
                    "first": "Dacheng",
                    "middle": [],
                    "last": "Maybank",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "International Journal of Computer Vision",
            "volume": "129",
            "issn": "6",
            "pages": "1789--1819",
            "other_ids": {
                "DOI": [
                    "10.1007/s11263-021-01453-z"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Deep neural networks over encrypted data",
            "authors": [
                {
                    "first": "Ehsan",
                    "middle": [],
                    "last": "Hesamifard",
                    "suffix": ""
                },
                {
                    "first": "Hassan",
                    "middle": [],
                    "last": "Takabi",
                    "suffix": ""
                },
                {
                    "first": "Mehdi",
                    "middle": [],
                    "last": "Ghasemi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Cryptodl",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Distilling the knowledge in a neural network",
            "authors": [
                {
                    "first": "Geoffrey",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "Oriol",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "NIPS Deep Learning and Representation Learning Workshop",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "FHE-Compatible Batch Normalization for Privacy Preserving Deep Learning",
            "authors": [
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Ibarrondo",
                    "suffix": ""
                },
                {
                    "first": "Melek",
                    "middle": [],
                    "last": "\u00d6nen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Data Privacy Management, Cryptocurrencies and Blockchain Technology",
            "volume": "",
            "issn": "",
            "pages": "389--404",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-00305-0_27"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "An optimized lookup-table for the evaluation of sigmoid function for artificial neural networks",
            "authors": [
                {
                    "first": "Pramod",
                    "middle": [],
                    "last": "Kumar Meher",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "18th IEEE/IFIP International Conference on VLSI and System-on-Chip",
            "volume": "",
            "issn": "",
            "pages": "91--95",
            "other_ids": {
                "DOI": [
                    "10.1109/VLSISOC.2010.5642617"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Imagenet classification with deep convolutional neural networks",
            "authors": [
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Advances in neural information processing systems",
            "volume": "25",
            "issn": "",
            "pages": "1097--1105",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Fast and accurately training deep neural networks on encrypted data",
            "authors": [
                {
                    "first": "Qian",
                    "middle": [],
                    "last": "Lou",
                    "suffix": ""
                },
                {
                    "first": "Bo",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Geoffrey",
                    "suffix": ""
                },
                {
                    "first": "Lei",
                    "middle": [],
                    "last": "Fox",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Glyph",
                    "suffix": ""
                }
            ],
            "year": 1911,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Exploring knowledge distillation of dnns for efficient hardware solutions",
            "authors": [
                {
                    "first": "Haitong",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Exploring knowledge distillation of dnns for efficient hardware solutions",
            "authors": [
                {
                    "first": "Haitong",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "GitHub repository",
            "volume": "",
            "issn": "",
            "pages": "6124--67",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Privacy-preserving machine learning with fully homomorphic encryption for deep neural network",
            "authors": [
                {
                    "first": "Joon-Woo",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Hyungchul",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "Yongwoo",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Woosuk",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "Jieun",
                    "middle": [],
                    "last": "Eom",
                    "suffix": ""
                },
                {
                    "first": "Maxim",
                    "middle": [],
                    "last": "Deryabin",
                    "suffix": ""
                },
                {
                    "first": "Eunsang",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Junghyun",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Donghoon",
                    "middle": [],
                    "last": "Yoo",
                    "suffix": ""
                },
                {
                    "first": "Young-Sik",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "21",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Enabling homomorphically encrypted inference for large dnn models",
            "authors": [
                {
                    "first": "Marc",
                    "middle": [],
                    "last": "+ 21] Guillermo Lloret-Talavera",
                    "suffix": ""
                },
                {
                    "first": "Harald",
                    "middle": [],
                    "last": "Jorda",
                    "suffix": ""
                },
                {
                    "first": "Fabian",
                    "middle": [],
                    "last": "Servat",
                    "suffix": ""
                },
                {
                    "first": "Chetan",
                    "middle": [],
                    "last": "Boemer",
                    "suffix": ""
                },
                {
                    "first": "Shigeki",
                    "middle": [],
                    "last": "Chauhan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Tomishima",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Nilesh",
                    "suffix": ""
                },
                {
                    "first": "Antonio J",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pena",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "IEEE Transactions on Computers",
            "volume": "",
            "issn": "",
            "pages": "1--1",
            "other_ids": {
                "DOI": [
                    "10.1109/TC.2021.3076123"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Improved knowledge distillation via teacher assistant",
            "authors": [
                {
                    "first": "Mehrdad",
                    "middle": [],
                    "last": "Seyed Iman Mirzadeh",
                    "suffix": ""
                },
                {
                    "first": "Ang",
                    "middle": [],
                    "last": "Farajtabar",
                    "suffix": ""
                },
                {
                    "first": "Nir",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Akihiro",
                    "middle": [],
                    "last": "Levine",
                    "suffix": ""
                },
                {
                    "first": "Hassan",
                    "middle": [],
                    "last": "Matsukawa",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ghasemzadeh",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "34",
            "issn": "",
            "pages": "5191--5198",
            "other_ids": {
                "DOI": [
                    "10.1609/aaai.v34i04.5963"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "SecureML: A system for scalable privacypreserving machine learning",
            "authors": [
                {
                    "first": "Payman",
                    "middle": [],
                    "last": "Mohassel",
                    "suffix": ""
                },
                {
                    "first": "Yupeng",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE Symposium on Security and Privacy (SP)",
            "volume": "",
            "issn": "",
            "pages": "19--38",
            "other_ids": {
                "DOI": [
                    "10.1109/SP.2017.12"
                ]
            }
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Towards deep neural network training on encrypted data",
            "authors": [
                {
                    "first": "Karthik",
                    "middle": [],
                    "last": "Nandakumar",
                    "suffix": ""
                },
                {
                    "first": "Nalini",
                    "middle": [],
                    "last": "Ratha",
                    "suffix": ""
                },
                {
                    "first": "Sharath",
                    "middle": [],
                    "last": "Pankanti",
                    "suffix": ""
                },
                {
                    "first": "Shai",
                    "middle": [],
                    "last": "Halevi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops",
            "volume": "",
            "issn": "",
            "pages": "0--0",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Neural networks with digital lut activation functions",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Piazza",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Uncini",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zenobi",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93",
            "volume": "2",
            "issn": "",
            "pages": "1401--1404",
            "other_ids": {
                "DOI": [
                    "10.1109/IJCNN.1993.716806"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Exploring the effect of image enhancement techniques on covid-19 detection using chest x-ray images",
            "authors": [
                {
                    "first": "Tawsifur",
                    "middle": [],
                    "last": "Rahman",
                    "suffix": ""
                },
                {
                    "first": "Amith",
                    "middle": [],
                    "last": "Khandakar",
                    "suffix": ""
                },
                {
                    "first": "Yazan",
                    "middle": [],
                    "last": "Qiblawey",
                    "suffix": ""
                },
                {
                    "first": "Anas",
                    "middle": [],
                    "last": "Tahir",
                    "suffix": ""
                },
                {
                    "first": "Serkan",
                    "middle": [],
                    "last": "Kiranyaz",
                    "suffix": ""
                },
                {
                    "first": ";",
                    "middle": [],
                    "last": "Susu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zughaier",
                    "suffix": ""
                },
                {
                    "first": "Muhammad",
                    "middle": [
                        "Salman"
                    ],
                    "last": "Khan",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Computers in biology and medicine",
            "volume": "132",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1016/j.compbiomed.2021.104319"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "A lower bound for the length of addition chains",
            "authors": [
                {
                    "first": "Arnold",
                    "middle": [],
                    "last": "Sch\u00f6nhage",
                    "suffix": ""
                }
            ],
            "year": 1975,
            "venue": "Theoretical Computer Science",
            "volume": "1",
            "issn": "1",
            "pages": "1--12",
            "other_ids": {
                "DOI": [
                    "10.1016/0304-3975(75)90008-0"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Learning activation functions from data using cubic spline interpolation",
            "authors": [
                {
                    "first": "Simone",
                    "middle": [],
                    "last": "Scardapane",
                    "suffix": ""
                },
                {
                    "first": "Michele",
                    "middle": [],
                    "last": "Scarpiniti",
                    "suffix": ""
                },
                {
                    "first": "Danilo",
                    "middle": [],
                    "last": "Comminiello",
                    "suffix": ""
                },
                {
                    "first": "Aurelio",
                    "middle": [],
                    "last": "Uncini",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Neural Advances in Processing Nonlinear Dynamic Signals",
            "volume": "",
            "issn": "",
            "pages": "73--83",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-95098-3_7"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Privacy preserving neural network inference on encrypted data with gpus",
            "authors": [
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Takabi",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Podschwadt",
                    "suffix": ""
                },
                {
                    "first": "Curt",
                    "middle": [],
                    "last": "Druce",
                    "suffix": ""
                },
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Procopio",
                    "suffix": ""
                }
            ],
            "year": 1911,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "The RSNA international COVID-19 open annotated radiology database (RICORD)",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Emily",
                    "suffix": ""
                },
                {
                    "first": "Scott",
                    "middle": [],
                    "last": "Tsai",
                    "suffix": ""
                },
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Simpson",
                    "suffix": ""
                },
                {
                    "first": "Michelle",
                    "middle": [],
                    "last": "Lungren",
                    "suffix": ""
                },
                {
                    "first": "Leonid",
                    "middle": [],
                    "last": "Hershman",
                    "suffix": ""
                },
                {
                    "first": "Errol",
                    "middle": [],
                    "last": "Roshkovan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Colak",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bradley",
                    "suffix": ""
                },
                {
                    "first": "George",
                    "middle": [],
                    "last": "Erickson",
                    "suffix": ""
                },
                {
                    "first": "Anouk",
                    "middle": [],
                    "last": "Shih",
                    "suffix": ""
                },
                {
                    "first": "Jaysheree",
                    "middle": [],
                    "last": "Stein",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kalpathy-Cramer",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1148/radiol.2021203957"
                ]
            }
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "PPolyNets: Achieving high prediction accuracy and efficiency with parametric polynomial activations",
            "authors": [
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Huimei",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Fengyi",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Ming",
                    "middle": [],
                    "last": "Xian",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Access",
            "volume": "6",
            "issn": "",
            "pages": "72814--72823",
            "other_ids": {
                "DOI": [
                    "10.1109/ACCESS.2018.2882407"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images",
            "authors": [
                {
                    "first": "Linda",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Alexander",
                    "middle": [],
                    "last": "Zhong Qiu Lin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Scientific Reports",
            "volume": "10",
            "issn": "1",
            "pages": "1--12",
            "other_ids": {
                "DOI": [
                    "10.1038/s41598-020-76550-z0"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Neuron-adaptive higher order neuralnetwork models for automated financial data modeling",
            "authors": [
                {
                    "first": "Ming",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shuxiang",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fulcher",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "IEEE Transactions on Neural Networks",
            "volume": "13",
            "issn": "1",
            "pages": "188--204",
            "other_ids": {
                "DOI": [
                    "10.1109/72.977302"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "MaxPool2d(kernel=3 \u00d7 3, stride=2)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "MaxPool2d(kernel=3 \u00d7 3, stride=2)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Dropout(p=0.2)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "FC(in=9216, out=4096, ReLU)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Dropout(p=0.2)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "FC(in=4096, out=4096, ReLU)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "FC(in=4096, out=3)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "AvgPool2d(kernel=3 \u00d7 3, stride=2)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "AvgPool2d(kernel=3 \u00d7 3, stride=2)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Dropout(p=0.2)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "FC(in=4096, out=3) Total number of layers",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "TABREF0": {
            "text": "COVIDx and COVIDx CT-2A data sizes per class as used in this paper",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "the trainable activation improved the results when compared to the quadratic approximated ReLU by 14-20%. The smooth transition (ST) approach, which gradually changes the activation function over 10 epochs starting from the 3 rd epoch, further improved the results by 3.4-8.6%, when compared to replacing all activations at once. Finally, combining both approaches with KD, where the original AlexNet with ReLU was used as a teacher to the new adapted architecture, performed even better with an improvement of 1.3-3.6%. This almost closed the gap to the original reference model, with only 1.1-5.3% degradation.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "A comparison of our suggested methods and their contributions to previous works. Results are reported on the test data of COVIDx and COVIDx CT-2A images. The baseline network is the original network with max pooling and ReLU. For all columns, higher values are better. We write TP for Trainable Polynomials, and ST for smooth transition. Our method TP+ST+KD 0.848 \u00b1 0.04 0.847 \u00b1 0.09 0.913 \u00b1 0.10 0.907 \u00b1 0.16",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "To the original AlexNet network, we added a batch normalization layer as our baseline. Here, all convolution layers use padding='same'. Note that the original model implemented by PyTorch has a Global Average Pooling layer with an output size that is 6 \u00d7 6. For input images of size 224 \u00d7 224, it is equivalent to the identity function so we ignored it in our experiments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A -AlexNet Network Architecture"
        },
        {
            "text": "We evaluated each experiment with 5 different seeds: 111, 222, 333, 444, 555. During the training process, we loaded images in mini-batches of size 32, and optimized the loss function using Adam optimizer. The number of epochs differs for each task, as does the learning rate, which was usually 3e-5 or 3e-4.The activation replacement started at epoch 3, and in case of smooth transition it was gradually replaced for 10 epochs. We replaced all ReLU activations in parallel. We found that it is better to initialize the coefficients with values similar to the form of ReLU. Therefore, the coefficients of each trainable activation were initialized as 0.0X 2 + 1.1x or 0.0X 2 + 1.1x.For the distillation process described in Section 3.3, we set the temperature value to 10, and the \u03b1 parameter was set to 0.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix B -Model Hyperparameters"
        }
    ]
}
{
    "paper_id": "0e7b43ad9bcc13d2cf7c16a41f1d252e044c6fa5",
    "metadata": {
        "title": "Estimating the Efficiency Gain of Covariate-Adjusted Analyses in Future Clinical Trials Using External Data",
        "authors": [
            {
                "first": "Xiudi",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Washington",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Sijia",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Washington",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Alex",
                "middle": [],
                "last": "Luedtke",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Washington",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "We present a general framework for using existing data to estimate the efficiency gain from using a covariate-adjusted estimator of a marginal treatment effect in a future randomized trial. We describe conditions under which it is possible to define a mapping from the distribution that generated the existing external data to the relative efficiency of a covariate-adjusted estimator compared to an unadjusted estimator. Under conditions, these relative efficiencies approximate the ratio of sample size needed to achieve a desired power. We consider two situations where the outcome is either fully or partially observed and several treatment effect estimands that are of particular interest in most trials. For each such estimand, we develop a semiparametrically efficient estimator of the relative efficiency that allows for the application of flexible statistical learning tools to estimate the nuisance functions and an analytic form of a corresponding Wald-type confidence interval. We also propose a double bootstrap scheme for constructing confidence intervals. We demonstrate the performance of the proposed methods through simulation studies and apply these methods to data to estimate the relative efficiency of using covariate adjustment in Covid-19 therapeutic trials.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The aim of most clinical trials is to estimate a marginal treatment effect that contrasts outcomes in a treatment group to those in a control group. In addition to the treatment assignment and outcome, data on prognostic baseline covariates are often available. In the case of continuous outcomes, the U.S. Food and Drug Administration [10] recommends adjusting for these baseline covariates through ANCOVA or linear regression models. However, such covariate-adjusted analyses are often underutilized in practice, especially with ordinal or time-to-event data [2] .",
            "cite_spans": [
                {
                    "start": 336,
                    "end": 340,
                    "text": "[10]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 561,
                    "end": 564,
                    "text": "[2]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Compared with unadjusted analyses, analyses that adjust for baseline covariates have several benefits. First, adjusted analyses can lead to consistent estimators of the treatment effect under weaker assumptions. One such example arises when right-censoring is present in a trial with a time-to-event outcome. Adjusted analyses often give consistent estimates provided that the censoring and survival times are independent given treatment and covariates [22] . This condition is more plausible in many trial settings than is the requirement made in unadjusted analyses that the censoring and survival times are independent given treatment alone. Second, adjusting for covariates that are predictive of the outcome can improve precision, and thus a smaller sample size can be required to achieve a desired power.",
            "cite_spans": [
                {
                    "start": 453,
                    "end": 457,
                    "text": "[22]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Such precision gain is generally expected when the outcome is fully observed, and also applies in certain cases where the outcome is only partially observed -some exceptions occur, for example, when the covariates are highly predictive of the censoring time but are only weakly predictive of the survival time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Despite these potential benefits, covariate adjustment is underutilized in analyzing clinical trial data. This is partly because, at the trial planning stage, there is typically little prior knowledge about the amount of precision gain that should be expected to result from using covariate adjustment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To address this problem, many previous works have aimed to estimate this precision gain using external datasets. In particular, some works have demonstrated the potential precision gain of covariate adjustment in clinical trial settings by comparing the standard errors of adjusted and unadjusted estimators on existing clinical trial datasets [e.g., 26, 34] . When the data-generating mechanism that gave rise to one of these existing datasets is reflective of the corresponding mechanism that is anticipated in an upcoming trial, these point estimates may yield a reasonable estimate of the precision gain anticipated in these future trials. It is worth noting, however, that the sampling variability in the existing trial dataset induces uncertainty in this point estimate. Other works have used an existing trial dataset to design a simulation study that can be used to estimate the precision gain [e.g., 16, 7, 26] . However, even when these simulation studies involve many repetitions, so that the Monte Carlo error is negligible, there is still uncertainty associated with these precision gain estimates that arises due to sampling variability in the existing trial dataset. In many cases, there may not be data available from a clinical trial that is reflective of the upcoming trial. An alternative approach, which does not require access to such data but can leverage it when it is available, is to conduct a simulation based on an external dataset that may be reflective of the covariate and outcome distributions that will be seen in the control arm of the upcoming trial [3] . This data may, for example, be derived from a pilot study or an observational study.",
            "cite_spans": [
                {
                    "start": 351,
                    "end": 354,
                    "text": "26,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 355,
                    "end": 358,
                    "text": "34]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 909,
                    "end": 912,
                    "text": "16,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 913,
                    "end": 915,
                    "text": "7,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 916,
                    "end": 919,
                    "text": "26]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "text": "[3]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Treatment arm data can then be simulated under a sharp null of no effect or as a user-defined shift of the conditional distribution of the outcome given covariates in the pilot study. As for the earlier simulation approach, a point estimate of the precision gain can be easily obtained, but there is still uncertainty in this point estimate that arises from the sampling variability in the dataset upon which the simulation is based.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "It can be challenging to be confident that a favorable estimated precision gain is not due to random noise, especially when the external dataset is small. Consequently, some clinical trialists may be cautious when making decisions about using covariate adjustment in future clinical trials based on a point estimate alone, even if the external dataset upon which it is based is known to be reflective of the data that will be seen in the trial. In other statistical estimation problems, the lack of interpretability of point estimates is often addressed by reporting a confidence interval alongside each point estimate. However, to the best of our knowledge, the problem of making statistical inferences about the precision gains of covariate-adjusted estimators has not been formally investigated. In this work, we aim to fill this critical knowledge gap.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "When doing so, we focus on the most general case described above, namely that data from an external study that is reflective of the covariate and control arm outcome distributions are available. Special cases of this setting include the case where data are available from a previous trial and the control arm data are used for the external study, and also the case where covariate and outcome data are available from an observational study.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We primarily consider treatment effect estimands that can be written as contrasts of the distributions of the outcomes within each treatment arm. Most commonly, investigators perform an unadjusted analysis that uses the empirical distribution to estimate these two arm-specific distributions. One approach to covariate adjustment involves instead estimating these distributions with possibly-misspecified working models. Specifically, this involves fitting a working parametric model within each arm that conditions on covariates, and then marginalizing over the arm-pooled empirical distribution of the covariates [21, 3] . In many cases, this approach can result in consistent and asymptotically normal estimators of the marginal effect of interest, even if the working model is misspecified. Nevertheless, these approaches are typically inefficient when the model is not correct, in the sense that they fail to achieve the asymptotic efficiency bound within a model that only imposes that treatment is randomized [6] . In contrast, many covariate-adjusted estimators have been proposed recently that achieve the efficiency bound under appropriate regularity conditions (see, for example, [33, 9] for ordinal outcomes; and [22, 27, 8] for survival outcomes). These approaches usually involve estimating nuisance parameters such as the treatment mechanism and the outcome regression functions. While being more efficient, these estimators are often more difficult for practitioners to understand because they cannot typically be framed as corresponding to a commonly used estimator within a parametric working model. In this paper, we consider both of the above-described strategies for covariate adjustment, which we refer to as working-model-based approaches and fully adjusted approaches, respectively.",
            "cite_spans": [
                {
                    "start": 615,
                    "end": 619,
                    "text": "[21,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 620,
                    "end": 622,
                    "text": "3]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1016,
                    "end": 1019,
                    "text": "[6]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1191,
                    "end": 1195,
                    "text": "[33,",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1196,
                    "end": 1198,
                    "text": "9]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "text": "[22,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "text": "27,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1234,
                    "end": 1236,
                    "text": "8]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our main contributions are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "1. we provide a framework for using external data to identify the efficiency gain in terms of percentage reduction in sample size needed to achieve a desired power from using covariate-adjusted rather than unadjusted estimation methods on future clinical trial data; 2. we introduce efficient estimators of this quantity that allow for the incorporation of flexible statistical learning tools to estimate the needed nuisance functions;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "3. we present statistical inference procedures to accompany the proposed estimators, namely a Waldtype procedure that requires knowledge of their influence functions but is widely applicable and a bootstrap procedure that applies only to working-model-based estimators but is easy to implement; and 4. we evaluate the performance of the proposed methods in a simulation study and an application to a dataset of Covid-19 patients hospitalized at the University of Washington Medical Center. This paper is organized as follows. In Section 2, we provide background on efficient estimation in semiparametric models and describe the relevance of the relative efficiency and local alternatives to clinical trial settings. In Section 3, we introduce the framework to identify and estimate the efficiency gain when the outcome is fully observed. We also propose efficient estimators and develop analytical and bootstrap inference procedures for estimands that are of frequent interest in the cases of continuous and ordinal outcomes. In Section 4, we study the case where the outcome is partially observed and consider time-to-event outcomes with right-censoring as an example. In Section 5, we demonstrate the performance of the proposed methods through simulation experiments and an analysis of a real dataset. Section 6 concludes with a discussion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "2 Review of efficiency theory and its relevance to clinical trial settings 2.1 Pathwise differentiability and regular and asymptotically linear estimators The theory of efficient estimation in nonparametric and semiparametric models was described in Pfanzagl [23] and Bickel et al. [6] . Here we give a brief review of the relevant concepts. Let X denote a generic data unit with distribution P and let is referred to as the tangent space. A parameter \u03c8 : M \u2192 R is called pathwise differentiable at P in M if there exists a function D \u2208 L 2 0 (P ) such that, for all submodels {P \u01eb : \u01eb \u2208 R} \u2208 M(P ), it holds that \u2202 \u2202\u01eb \u03c8(P \u01eb )| \u01eb=0 = E P [D(X)s(X)], where s is the score function of {P \u01eb : \u01eb \u2208 R} at \u01eb = 0. Any such function D is called a gradient of \u03c8 with respect to M at P . The canonical gradient D * is the gradient that lies in the tangent space T M (P ) -it can be shown that this gradient is unique. We note that the X \u2192 R functions D and D * both depend on P .",
            "cite_spans": [
                {
                    "start": 259,
                    "end": 263,
                    "text": "[23]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 282,
                    "end": 285,
                    "text": "[6]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We refer to an estimator\u03c8 of \u03c8 as a random variable that is a function of an independent and identically distributed (iid) sample X := {X 1 , . . . , X n } drawn from some distribution. An estimator \u03c8 is called regular if there exists a real-valued probability distribution L such that, for all submodels {P \u01eb : \u01eb \u2208 R} in M(P ) and all c \u2208 R, \u221a n \u03c8 \u2212 \u03c8 (P cn \u22121/2 )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Importantly, note that, if an estimator is regular, then the distribution L above does not depend on the choice of submodel in M(P ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "An estimator\u03c8 of \u03c8(P ) is called asymptotically linear if there exists some function \u03be P \u2208 L 2 0 (P ) such that\u03c8 \u2212 \u03c8(P ) = 1 n n i=1 \u03be P (X i ) + o P (n \u22121/2 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The function \u03be P is referred to as the influence function of\u03c8. Asymptotically linear estimators are consistent and asymptotically normal, in the sense that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "where \u03c3 2 P is the variance of \u03be P (X) when X \u223c P . If\u03c8 is asymptotically linear and \u03c8 is pathwise differentiable, then\u03c8 is regular if, and only if, \u03be P is a gradient of \u03c8 with respect to M at P . Among the collection of gradients, the canonical gradient D * has the smallest variance, and is also called the efficient influence function (EIF). This variance characterizes the efficiency bound of estimating \u03c8 given the model M with a regular and asymptotically linear (RAL) estimator. An estimator is called efficient if it is RAL and its influence function is the same as the EIF.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Suppose that we have available an initial estimatorP of the distribution. A plug-in estimator is defined as \u03c8(P ). However, such estimators may not be \u221a n-consistent due to the potential bias in the initial estimators. One way to construct a RAL estimator with influence function D is through onestep estimation [15, 5, 24] , which corrects for this bias by using\u03c8 = \u03c8(P ) + P n D(P ) where P n (\u00b7) is the empirical mean. Estimating equations [29, 28] and targeted minimum loss-based estimation [30] are alternative approaches. These techniques are often used to construct efficient covariate-adjusted estimators of a treatment effect. Later, we will also use them to estimate the relative efficiency of two estimators based on external data.",
            "cite_spans": [
                {
                    "start": 312,
                    "end": 316,
                    "text": "[15,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 317,
                    "end": 319,
                    "text": "5,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 320,
                    "end": 323,
                    "text": "24]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 443,
                    "end": 447,
                    "text": "[29,",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 448,
                    "end": 451,
                    "text": "28]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 495,
                    "end": 499,
                    "text": "[30]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the context that we consider in this paper, the treatment effect measure that will be estimated with the future clinical trial data will often correspond to an evaluation \u03c8(P ) of a pathwise differentiable parameter \u03c8. In many cases, a primary objective of the forthcoming trial will be to test the null hypothesis that this quantity is equal to zero against a one-sided alternative, for example, that this quantity is positive.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": "Suppose that a level \u03b1 Wald test is performed, which corresponds to evaluating whether zero is smaller than\u03c8 \u2212 n \u22121/2 z 1\u2212\u03b1\u03c3P based on a RAL estimator\u03c8, where z 1\u2212\u03b1 is the (1 \u2212 \u03b1)-quantile of a standard normal distribution and\u03c3 P is a consistent estimator of \u03c3 P , as defined in (2) . Fix an arbitrary c = 0.",
            "cite_spans": [
                {
                    "start": 279,
                    "end": 282,
                    "text": "(2)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": "As \u03c8 is pathwise differentiable, for any {P \u01eb : \u01eb \u2208 R} in M(P ) with score function s at \u01eb = 0, it holds that \u03c8(P cn \u22121/2 ) = \u03c8(P ) + cn \u22121/2 \u00b5 P,s + o(n \u22121/2 ), where \u00b5 P,s := E P [D * (X)s(X)]. If P is such that the null that \u03c8(P ) = 0 holds, then this shows that n 1/2 \u03c8(P cn \u22121/2 ) \u2192 c\u00b5 P,s as n \u2192 \u221e. If s is such that \u00b5 P,s > 0, then we call {P cn \u22121/2 } \u221e n=1 a sequence of local alternatives -this name is natural given that \u03c8(P cn \u22121/2 ) > 0 for all n large enough (and so the alternative holds for all n large enough), while also \u03c8(P cn \u22121/2 ) \u2192 0 as n \u2192 \u221e (and so these alternatives are local to the null hypothesis). Because\u03c8 is RAL, combining (1) and (2) with Slutsky's theorem implies that \u221a n\u03c8",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": "where \u03c3 2 P is as defined below (2) . Let \u03b2 denote the power for rejecting the null that \u03c8(P ) = 0 under sampling from P cn \u22121/2 -that is, let",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 35,
                    "text": "(2)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": "Letting \u03a6(\u00b7) denote the cumulative distribution function (CDF) of the standard normal distribution, (3) implies that \u03b2 = 1 \u2212 \u03a6(z 1\u2212\u03b1 \u2212 c\u00b5 P,s /\u03c3 P ) which lies in (\u03b1, 1) when the shift in the mean of the limit normal distribution c\u00b5 P,s is positive. This gives us a way to quantify the power of the test in a range of settings where the effect size is small. Hence, effect sizes scaling as n \u22121/2 are interesting in general testing problems, given that it is exactly at these effect sizes that the problem is neither asymptotically trivial (power converging to 1) or impossible (power converging to \u03b1). Nevertheless, in many statistical problems, there may be no a priori reason to believe that the effect size will be of the order n \u22121/2 .",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 103,
                    "text": "(3)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": "The setup is quite different in randomized trials. Indeed, these local alternatives are natural to think about in these settings because, under sampling from such a sequence of alternatives, the asymptotic power takes some intermediate value between \u03b1 and 1, which reflects the fact that the sample size in most trials is specified so that a test of the null will have a chosen power \u03b2 * \u2208 (\u03b1, 1). To be more concrete, suppose that {\u03c8 (k) } \u221e k=1 is a decreasing sequence of effect sizes that satisfy the alternative hypothesis, that is, that are such that \u03c8 (k) \u2193 0 as k \u2192 \u221e. We suppose that these effect sizes arise from some sequence of distributions {P (k) } \u221e k=1 that belong to some submodel M 1 := {P \u01eb : \u01eb \u2208 R} \u2208 M(P ), so that \u03c8(P (k) ) = \u03c8 (k) for each k. Our objective is to establish an expression for the sequence of sample sizes {n (k) } \u221e k=1 so that, as k \u2192 \u221e, the power for rejecting the null hypothesis converges to \u03b2 * when n (k) iid observations are drawn from P (k) . Let s denote the score of \u01eb at 0 in the submodel M 1 , and suppose that \u00b5 P,s = 0. To derive the sequence {n (k) } \u221e k=1 , it will be helpful to first find a c such that, when n iid observations are drawn from P cn \u22121/2 , the power converges to the desired \u03b2 * as n \u2192 \u221e. Because",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": ", it will then be reasonable to expect that, when a sequence of tests is conducted based on n (k) iid observations sampled from each P (k) , the power for rejecting the null hypothesis will converge to \u03b2 * as k \u2192 \u221e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": "We now find the expressions for c and n (k) that were described in the last paragraph. Recalling (4) and the alternative expression for the power given below that display, we see that, when c = \u03c3 P (z 1\u2212\u03b1 \u2212",
            "cite_spans": [
                {
                    "start": 97,
                    "end": 100,
                    "text": "(4)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": "To find the expression for n (k) , we note that, as \u03c8 is pathwise differentiable, \u03c8(P cn \u22121/2 ) = cn \u22121/2 \u00b5 P,s + o(n \u22121/2 ) when n is large -here, the little-oh term describes behavior as n \u2192 \u221e. Hence, P (k) \u2248 P c/ \u221a n (k) , where n (k) = \u2308(c\u00b5 P,s /\u03c8 (k) ) 2 \u2309 = \u2308\u03c3 2 P (z 1\u2212\u03b1 \u2212z 1\u2212\u03b2 * ) 2 /(\u03c8 (k) ) 2 \u2309. Thus, to achieve asymptotic power \u03b2 * when sampling n (k) iid observations from P (k) , n (k) must scale proportionally with \u03c3 2 P . These calculations also provide a means to compare the sample sizes needed to achieve the same power based on two different RAL estimators. In particular, suppose that a second RAL estimator is available and its asymptotic variance is equal to\u03c3 2 P \u2264 \u03c3 2 P . In this case, the proportional reduction in sample size needed to achieve power \u03b2 * when using this estimator rather than the estimator with variance \u03c3 2 P is approximately equal to 1 \u2212\u03c3 2 P /\u03c3 2 P when k is large. In fact,\u03c3 2 P /\u03c3 2 P is often referred to as the relative efficiency of the RAL estimator with variance\u03c3 2 P versus the RAL estimator with variance \u03c3 2 P , and so this proportional reduction is exactly equal to one minus the relative efficiency of these two estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": "3 When the outcome is fully observed 3 ",
            "cite_spans": [
                {
                    "start": 37,
                    "end": 38,
                    "text": "3",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Local alternatives, relative efficiency, and their relevance to clinical trial settings"
        },
        {
            "text": "We now propose a general framework to identify the relative efficiency of covariate-adjusted estimators. We start by defining notation that we will use to describe the data that will arise in the future clinical trial. Let A denote the binary treatment, W denote the d-dimensional covariate vector and Y denote the outcome. We will use superscript t to denote random variables in a future clinical trial. Let P 1 be the conditional distribution of Y t |A = 1 and P 0 be the conditional distribution of Y t |A = 0. The treatment effect is often a functional f of these distributions, i.e., \u03c8 = f (P 1 , P 0 ) for some f . An example is the average treatment effect for continuous outcome, where f (P 1 ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "Randomization implies that A and W t are independent, and thus that the distribution of X t , denoted by \u03bd, is determined by the marginal distribution of A, the conditional distribution of Y t given (A = 0, W t = w), the conditional distribution of Y t given (A = 1, W t = w), and the marginal distribution of W t -we denote these distributions by \u03a0, P t 0 , P t 1 , and P W , respectively. Let M X consist of all distributions for which A and W t are independent. In the randomized trial settings that we consider, \u03bd \u2208 M X . The adjusted analysis uses X t , while the unadjusted analysis ignores the covariate W t . Let\u03c8 u ,\u03c8 a and\u03c8 m be a specified unadjusted estimator, fully adjusted estimator and working-model-based adjusted estimator of \u03c8, respectively. Further suppose that these estimators are regular and asymptotically linear with influence functions D u , D a and D m , respectively. We note that these influence functions all depend on the underlying distribution \u03bd, but we will omit this dependence when it is clear from the context.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "We assume the following regularity condition holds throughout, which guarantees that the treatment effects of interest can be estimated using strategies typically employed in randomized trial settings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "Although some of the conditions can in principle be relaxed, they cover most realistic clinical trial settings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "Condition A1. Treatment is independent of covariates (A \u22a5 W t ), the treatment probability \u03a0(A = 1) falls in (0, 1), and Y and W t both have bounded support under sampling from \u03bd.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "Let W \u2286 R d and Y \u2286 R be bounded and convex sets that contain the support of W t and Y , respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "Our objective is to quantify the relative precision of the specified adjusted and unadjusted estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "To do this, we will consider the relative efficiency of these two estimators, defined as the ratio between the asymptotic variances of the adjusted estimator and the unadjusted estimator under a sharp null distribution. Though we will focus on the sharp null when introducing these relative efficiencies, these quantities also correspond to the relative efficiencies under local alternatives (see Section 2.2). Consequently, our apparent restriction to the sharp null setting will in fact not be restrictive at all. Indeed, from this sharp null setting, we can typically approximate the reduction in sample size needed to achieve a desired power at all local alternatives that are consistent with the design alternative used to size the trial (ibid.).",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "We now define these relative efficiencies. Consider a trial where the sharp null holds, that is, the treatment has no effect and P t 1 = P t 0 . In this case, let P denote the joint distribution of (Y t , W t ), determined by the pair (P t 1 , P W ). Under Condition A1, \u03bd is equal to the product measure P \u03a0 in this sharp null setting. The relative efficiency of the fully adjusted estimator compared to that of the unadjusted estimator is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "whereas the analogous quantity for the working-model-based estimator is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "Although in general \u03bd depends on both \u03a0 and P , we define relative efficiencies as functions of P only.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "As we will show, in many cases that are of interest in practice, the relative efficiency does not depend on \u03a0. Even in cases where it does depend on \u03a0, the investigator in a trial would have control over the treatment distribution \u03a0 in the trial setting, and the only unknown component would still be P .",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "The local alternatives we consider allow for a variety of perturbations to the underlying distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "The direction of these perturbations is described by their scores, which belong to the tangent space",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": ". This tangent space decomposes into three subspaces, corresponding to the marginal distribution of W t , the distribution of treatment A, and the conditional distribution of Y t |A, W t . The score in a smooth submodel can lie in one or more subspaces, which means that the local alternative can perturb one or more of the four components of \u03bd, namely \u03a0, P t 0 , P t 1 , and P W . For an example of how these perturbations may impact \u03bd, consider the special case of the average treatment effect",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "that was introduced at the beginning of this section. We note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "is the conditional average treatment effect function. Here, \u03c8 will be zero when this function is zero for all values of the covariates. Now, for any We now describe conditions that we use to identify the relative efficiency \u03c6 a (P ) and \u03c6 m (P ) using the external data that are available at the trial planning stage. When doing this, we assume that P \u2208 M,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "where M is a locally nonparametric model of all distributions of (Y t , W t ), that is, a model where the tangent space at P is L 2 0 (P ). Let X = (Y, W ) be the data unit in the external dataset, which we assume consists of n iid draws from some distribution. The identifiability condition that we consider imposes that the external data should accurately reflect the distribution of covariate and outcomes in future trials where treatment has no effect.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "Condition A2. A random variate X = (Y, W ) from the external dataset has distribution P .",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "Under this condition, the relative efficiencies in (5) and (6) can be estimated based on the external data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Framework to identify the relative efficiency"
        },
        {
            "text": "We now consider estimating relative efficiency for certain treatment effect estimands that are of particular interest in many clinical trials. We focus primarily on continuous and ordinal outcomes in this section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the relative efficiency"
        },
        {
            "text": "For the examples we consider, the asymptotic variances of the adjusted and unadjusted estimators factorize into a product of two terms, one that depends on \u03a0 only and another that depends on P only.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the relative efficiency"
        },
        {
            "text": "Moreover, the term that depends only on \u03a0 is the same for both the adjusted and unadjusted estimators, and the relative efficiency is a function of P only. In particular, (5) and (6) now take the following forms, \u03c6 a (P ) = \u03c3 2 a (P )/\u03c3 2 u (P ), \u03c6 m (P ) = \u03c3 2 m (P )/\u03c3 2 u (P ).",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 174,
                    "text": "(5)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Estimating the relative efficiency"
        },
        {
            "text": "The exact forms of \u03c3 a , \u03c3 m and \u03c3 u depend on the treatment effect estimand and the specified estimators but do not depend on \u03a0. Some specific examples are presented in the remainder of this section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the relative efficiency"
        },
        {
            "text": "To illustrate the idea, we start with a simple example where the outcome is continuous and we are interested in the average treatment effect, defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": ". Let n t denote the sample size of the future trial dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "The unadjusted estimator that we consider corresponds to the difference between the arm-specific",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "For the fully adjusted estimator, we consider the augmented inverse probability weighted (AIPW) estimator, namel\u0177",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "wherer a (w) is an estimator of the conditional mean function r a (w) := E[Y t |A = a, W t = w] and\u03c0 is an estimator of the treatment mechanism \u03c0(w) := P (A = 1|W t = w). In randomized trials, the treatment mechanism can be estimated with\u03c0, the empirical marginal of A, which is \u221a n-consistent, and the AIPW estimator is efficient provided thatr a is consistent and satisfies appropriate conditions. The estimator will be consistent and asymptotically normal even ifr a is inconsistent but has an appropriately defined limit.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "For the working-model-based adjusted estimator, we consider linear models, which are commonly used by practitioners for continuous outcomes [10] . Specifically, we fit an arm-specific linear model for the outcome regression, which assumes that",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 144,
                    "text": "[10]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "and denote by\u03b1 a ,\u03b2 a the fitted coefficients. To estimate the average treatment effect, we marginalize the fitted values over all covariates and take the difference between treatment arms,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "We note again that the consistency and asymptotic normality of this estimator does not rely on the arm-specific linear models being correct.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "The following lemma gives the forms of the relevant variances in the definition of relative efficiency in this setting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "Suppose that the appropriate regularity conditions hold such that the AIPW estimator\u03c6 a is efficient. Then, for the above\u03c8 u ,\u03c8 a and\u03c8 m , we have that \u03c3 2 u (P ) = var P (Y ), \u03c3 2 a (P ) =",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "We now estimate these variances using external data {(Y i , W i ), i = 1, . . . , n}. Specifically, we use the sample variance for the unadjusted variance,\u03c3 2 u = n i=1 (Y i \u2212\u0232 ) 2 /n, where\u0232 is the overall mean of the outcome. Letr(w) be an estimator of r(w) := E P [Y |W = w], then we estimate the adjusted variance b\u0177",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "where (\u03b1,\u03b2) are the coefficients in the linear regression of Y on W . We estimate the relative efficiencies by\u03c6 a =\u03c3 2 a /\u03c3 2 u and\u03c6 m =\u03c3 2 m /\u03c3 2 u .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "For a generic W \u2192 R function f , define its (squared) L 2 (P W ) norm as f 2 L 2 (PW ) := f (w) 2 dP W (w). The following theorem establishes the asymptotic linearity of\u03c6 a and\u03c6 m under appropriate conditions. Theorem 1. Suppose that Conditions A1 and A2 hold. Suppose, in addition, that the random function r : W \u2192 Y is such that r \u2212 r L 2 (PW ) = o P (n \u22121/4 ) and belongs to some fixed P -Donsker class F of functions with probability tending to one. Then,\u03c6 a is an efficient estimator of \u03c6 a and\u03c6 m is an efficient estimator of \u03c6 m .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Continuous outcomes and average treatment effect"
        },
        {
            "text": "Now suppose that the outcome is ordinal and takes value in {1, 2, . . . , K}. Dichotomous outcomes correspond to the special case where K = 2. Let F a (k) := P (Y t \u2264 k|A = a) denote the treatment-specific CDF. The treatment effect estimands we consider can all be written as \u03c8 = g {F 0 (k), F 1 (k)} K\u22121 k=1 for some real-valued function g.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ordinal outcomes"
        },
        {
            "text": "The proportional odds model [19] is a commonly used parametric model for ordinal outcomes. Here we use a treatment-specific proportional odds model as our working parametric model. For k \u2208 {1, . . . , K \u2212 1}, the model assumes that P (Y t \u2264 k|A = a, W t = w) = \u03b8 \u03b1a,\u03b2a (k, w), where logit \u03b8 \u03b1a,\u03b2a (k, w) = \u03b1 a (k) + \u03b2 \u22a4 a w.",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 32,
                    "text": "[19]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Ordinal outcomes"
        },
        {
            "text": "The above reduces to a logistic regression when the outcome is dichotomous. Let (\u03b1 a ,\u03b2 a ) be the coefficients, fitted by minimizing the following empirical risk function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ordinal outcomes"
        },
        {
            "text": "where I{\u00b7} is the indicator function. In the special case that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ordinal outcomes"
        },
        {
            "text": "For such cases we use the conventions that logit \u22121 (\u2212\u221e) = 0, logit \u22121 (\u221e) = 1, and 0 log(0) = 0. The treatment-arm-specific CDFs are estimated byF a (k) = n t i=1 \u03b8\u03b1 a,\u03b2a (k, W t i )/n t . In addition, we define (\u03b1 * a , \u03b2 * a ) as the minimizer of E[L n,a (\u03b1, \u03b2)] over R K\u22121 \u00d7 R d .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ordinal outcomes"
        },
        {
            "text": "We first establish the RAL property ofF a (k), which holds even when the proportional odds model is misspecified. Let \u03b8 a (k, w) := P (Y t \u2264 k|A = a, W t = w) be the true outcome regression, and let \u03b8 * a (k, w) = \u03b8 \u03b1 * a ,\u03b2 * a (k, w) be the best model approximation to the true outcome regression according to the population analogue of the risk in (9) . Note that \u03b8 * a (k, w) can be different from \u03b8 a (k, w) in the presence of misspecification.",
            "cite_spans": [
                {
                    "start": 351,
                    "end": 354,
                    "text": "(9)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Ordinal outcomes"
        },
        {
            "text": "Lemma 2. Suppose that Condition A1 holds and that (\u03b1 a ,\u03b2 a ) is estimated by minimizing (9), then F a (k) is an asymptotically linear estimator of F a (k), for k \u2208 {1, . . . , K \u2212 1}. Its influence function is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ordinal outcomes"
        },
        {
            "text": "Following [3] , we focus on three treatment effect estimands that are often of interest.",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 13,
                    "text": "[3]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Ordinal outcomes"
        },
        {
            "text": "for a pre-specified monotone transformation u(\u00b7). This reduces to the average treatment effect when u(\u00b7) is the identity function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "The unadjusted estimator is the difference between the arm-specific sample means,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Instead of using sample means, the adjusted estimator based on proportional odds model computes means with respect to the estimated CDFsF a .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Finally, we define an AIPW estimator similarly to (8) , but with Y t replaced by u(Y t ). We denote this estimator as\u03c8 a . As in the previous section, this estimator achieves the semiparametric efficiency bound when the treatment mechanism is estimated with the marginal proportion of treatment and the outcome regression is consistently estimated. For the above three estimators, the variances in (7) are given in the following lemma. ",
            "cite_spans": [
                {
                    "start": 50,
                    "end": 53,
                    "text": "(8)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "where \u03b8 * (k, w) = \u03b8 \u03b1 * ,\u03b2 * (k, w) and (\u03b1 * , \u03b2 * ) maximizes the following objective:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "We now propose estimators of these quantities for settings where external data are available. Letr(w)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "be an estimator of r(w) := E P [u(Y )|W = w], the conditional mean of u(Y ), and let\u016b n be the sample mean of u(Y ). We estimate the unconditional and conditional variances b\u0177",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "To estimate \u03c3 2 m , we fit the proportional odds model by maximizing the empirical counterpart of (10), and let (\u03b1,\u03b2) denote the fitted coefficients. We then construct a plug-in estimator",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Finally, we estimate the relative efficiency by\u03c6 a =\u03c3 2 a /\u03c3 2 u and\u03c6 m =\u03c3 2 m /\u03c3 2 u .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "In the upcoming theorem, we let u(Y) denote the convex hull of {u(y) : y \u2208 Y}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Theorem 2. Suppose that Conditions A1 and A2 hold. Suppose, in addition, that the random function r : W \u2192 u(Y) is such that r \u2212 r L 2 (PW ) = o P (n \u22121/4 ) and belongs to some fixed P -Donsker class F of functions with probability tending to one. Then\u03c6 a is an efficient estimator of \u03c6 a . Moreover,\u03c6 m is an efficient estimator of \u03c6 m .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Exact forms of the influence functions of\u03c6 a and\u03c6 m are given in Appendix B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "The Mann-Whitney estimand (MW) is defined as \u03c8 = P (Y t 1 >\u1ef8 t 0 ) + P (Y t 1 =\u1ef8 t 0 )/2, for two independent variables Y t 1 \u223c P 1 and\u1ef8 t 0 \u223c P 0 . It is the probability that a randomly chosen individual's outcome under treatment is larger than another randomly chosen individual's outcome under control plus one half times the probability that the two outcomes are equal. Define h(x, y) = I{x > y} + I{x = y}/2. Then the Mann-Whitney parameter can be alternatively written as \u03c8 = h(x, y)dP 1 (x)dP 0 (y). This alternative definition suggests the following unadjusted estimator",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": ", and the following working-model-based estimator",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "whereP a is the distribution with CDFF a (k). In addition, let\u03c8 a be the covariate-adjusted estimator in Vermeulen et al. [33] for the MW parameter, which is efficient under appropriate regularity conditions. Lemma 4. Let (Y, W ) \u223c P , and define \u03b7 P (k) = P (Y < k) + P (Y = k)/2 and p k = P (Y = k).",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 126,
                    "text": "[33]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Suppose that the appropriate regularity conditions hold such that\u03c6 a is an efficient estimator of the MW parameter. Then, for the above\u03c8 u ,\u03c8 a and\u03c8 m , we have that \u03c3 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "We now propose estimators for these quantities. Unlike in the case of the DIM estimand, \u03b7 P (\u00b7) depends on the unknown marginal distribution of Y and needs to be estimated from the external data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "A simple estimator is based on the empirical distribution, ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Finally, a natural plug-in estimator for\u03c3 2 m is given b\u0177",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "where (\u03b1,\u03b2) is again the fitted coefficients from the proportional odds model, by maximizing the sample counterpart of (10). The relative efficiency can be estimated as\u03c6 a =\u03c3 2 a /\u03c3 2 u and\u03c6 m =\u03c3 2 m /\u03c3 2 u .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "The next theorem establishes the asymptotic properties of these estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Theorem 3. Suppose that Conditions A1 and A2 hold. Suppose, in addition, that the random function r : W \u2192 R is such that r \u2212 r L 2 (PW ) = o P (n \u22121/4 ) and belongs to some fixed P -Donsker class F of functions with probability tending to one. Then,\u03c6 a is an efficient estimator of \u03c6 a and\u03c6 m is an efficient",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "The influence functions of\u03c8 a and\u03c8 m are given in Appendix B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "The log odds ratio (LOR) is defined as \u03c8 =",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": ", which is an average of the cumulative log odds ratios [9] . In general, one can also consider a weighted average. Employing this definition for the LOR ensures that the LOR is well-defined even in settings where a proportional odds assumption fails.",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 59,
                    "text": "[9]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "The unadjusted estimator is given b\u0177",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "The working-model-based adjusted estimator\u03c8 m replacesF a (k) with the proportional odds model-based estimatorF a (k) that was defined earlier. Finally, let\u03c8 a be the covariate-adjusted estimator proposed in D\u00edaz et al. [9] , which achieves the semiparametric efficiency bound under regularity conditions. The following lemma gives the forms of the relevant variances.",
            "cite_spans": [
                {
                    "start": 220,
                    "end": 223,
                    "text": "[9]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Suppose that the appropriate regularity conditions hold such that\u03c6 a is an efficient estimator of the LOR.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Then, for the above\u03c8 u ,\u03c8 a and\u03c8 m , we have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Let\u03b8(k, w) be an estimator of the true conditional distribution function \u03b8(k, w) := P (Y \u2264 k|W = w) in the setting where external data are available. We estimate the relative efficiencies by\u03c6 a =\u03c3 2 a /\u03c3 2 u and\u03c6 m =\u03c3 2 m /\u03c3 2 u , with the variance estimators all taking the following form with certain choice of the estimator\u03b8 c :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "is replaced with\u03b8(k, w); for\u03c3 2 u , we us\u1ebd F (k); and, for\u03c3 2 m , we take\u03b8 c (k, w) = \u03b8\u03b1 ,\u03b2 (k, w).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Theorem 4. Suppose that Conditions A1 and A2 hold and that there exists a constant \u03b4 > 0 such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Suppose, in addition, that, for all k \u2208 {1, . . . , K \u2212 1}, the random function\u03b8(k, \u00b7) : W \u2192 R is such that \u03b8 (k, \u00b7) \u2212 \u03b8(k, \u00b7) L 2 (PW ) = o P (n \u22121/4 ) and belongs to some fixed P -Donsker class F of functions with probability tending to one. Then,\u03c6 a is an efficient estimator of \u03c6 a . Moreover,\u03c6 m is an efficient estimator of \u03c6 m .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "For all of the aforementioned treatment effect estimands, \u03c6 a \u2208 (0, 1], since the fully adjusted estimator achieves the semiparametric efficiency bound. However, \u03c6 m might be larger than 1 if the proportional odds model is far from the truth.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Wald-type intervals are a standard approach for constructing confidence intervals when an asymptotically linear estimator\u03c6 of \u03c6 is available. Specifically, a (1 \u2212 \u03b1)-CI is given by\u03c6 \u00b1 n \u22121/2 z 1\u2212\u03b1/2 P n\u03c4 2 ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "where z 1\u2212\u03b1/2 is the (1 \u2212 \u03b1/2)-quantile of a standard normal distribution and\u03c4 is the influence function of\u03c6 except that we replace unknown quantities with consistent estimates. However, there are certain cases where, even though such consistent estimates are used, the Wald-type confidence interval will not provide asymptotically valid coverage. A key time when this challenge arises occurs when the limiting distribution of a RAL estimator is degenerate because the influence function is almost everywhere zero, which will often occur in our setting when the relative efficiency is one. One such example arises in estimating the relative efficiency of the fully adjusted estimator to the unadjusted estimator for the ATE or DIM estimands. In this case, the influence function of\u03c6 a is almost everywhere zero when",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "for almost all w. While a Wald-type interval will typically achieve asymptotically valid coverage outside of these degenerate cases, there is no way to know in advance whether or not P is such that degeneracy will occur. To overcome this challenge, we propose an alternative approach that yields a confidence set that achieves the desired coverage regardless of whether degeneracy occurs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "Most importantly, the resulting confidence sets are valid regardless of whether or not the true relative efficiency is, in fact, one.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "The proposed confidence set is constructed as follows. Suppose that we have available a valid level \u03b1 test of the null hypothesis H 0 : \u03c6 a = 1 (\u03c6 m = 1) -one such test based on sample splitting is given in Appendix E. Denote the Wald confidence interval by I wald . We first test this null hypothesis. If we do not reject it, we take the 1 \u2212 \u03b1 confidence set to be I wald \u222a {1}. If, instead, the null hypothesis is rejected, we take the confidence set to be I wald . At first glance, it seems that the proposed approach may fail to achieve valid coverage given that it uses the same data twice -once to test the null hypothesis and a second time to form the Wald-type interval. Nevertheless, as we show in Appendix E, the confidence set resulting from this procedure in fact has at least 1 \u2212 \u03b1 asymptotic probability of covering the truth. It may happen that I wald and {1} are disjoint. In this case, a disconnected confidence set I wald \u222a {1} can be reported or, if this is considered undesirable, the convex hull of this confidence set can be taken to form a confidence interval.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Difference in mean (DIM) is defined as"
        },
        {
            "text": "The inferential procedures described in the preceding subsection are based on closed-form expressions for the relative efficiency parameter in several problems and knowledge of the corresponding efficient influence functions. On the one hand, now that these expressions have been calculated, the estimators that we have presented can be used in any problems in which the relative efficiency takes this form. On the other hand, if a new effect estimand or working parametric model is of interest in a future setting, then new analytical calculations will need to be conducted to derive the closed-form expression for the relative efficiency parameter and develop corresponding estimators and confidence intervals. Here, we",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "propose an automated double bootstrap procedure that avoids the need to perform these potentiallytedious analytic calculations. When doing so, we focus on the case where the goal is to infer about the relative efficiency of a new working-model-based adjusted estimator, that is, we focus on \u03c6 m . The reason for this choice is discussed at the end of this subsection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "Before describing this procedure, we first investigate the applicability of a more traditional, one-layer bootstrap procedure. Suppose that the relative efficiency parameter \u03a6 m : M \u2192 R + is sufficiently smooth so that a plug-in estimator of \u03a6 m (P ) based on the empirical distribution is asymptotically linear [see, e.g., Theorem 20.8 in 31] . In this case, we can construct a plug-in estimator based on the empirical distribution P n of a sample of iid observations. We denote this plug-in estimator by \u03a6 m (P n ) and note that all the estimators proposed so far for \u03c6 m correspond to plug-in estimators of this form. In a traditional setting where the bootstrap would be applied, a closed-form expression for the functional \u03a6 m would be available and so would be the plug-in estimator \u03a6 m (P n ), and the goal would be to derive a corresponding confidence interval. In particular, let the n entries of X = (X k ) n k=1 correspond to an iid sample of external data, where X k = (Y k , W k ). We sample from X with replacement B 1 times, to form the bootstrap resamples X * i of size n for i = 1, . . . , B 1 . Letting P * n,i denote the empirical distribution of the observations in X * i , we could then use the empirical standard deviation of \u03a6 m (P * n,i ), i = 1, . . . , B 1 , as the standard error estimate used to construct a confidence interval centered around \u03a6 m (P n ). Though this traditional bootstrap approach is useful in that it avoids explicitly computing the influence function of \u03a6 m (P n ), it does not fully avoid the aforementioned analytic calculations. Indeed, in many cases, deriving the plug-in estimator will itself require deriving the explicit form of the relative efficiency parameter, which in turn relies on computing inefficient and efficient gradients of the treatment effect estimand in the model M X . Computing these gradients requires specialized calculations that are unfamiliar to many practitioners.",
            "cite_spans": [
                {
                    "start": 324,
                    "end": 343,
                    "text": "Theorem 20.8 in 31]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "To avoid this challenge, we approximate the plug-in estimator with an alternative estimator\u03c6 that can be obtained in a fully automated fashion. Specifically, we propose to use an additional layer of resampling to approximate \u03c6 m . Evaluating the resulting estimation strategy only requires having access to the external data and the treatment effect estimator that will be used to analyze data from the future clinical trial.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "be the first layer resamples, and in addition define X * 0 := X. For each i \u2265 0, we then letX ij , j = 1, . . . , B 2 , denote an iid sample of size N from the product measure P * n,i \u03a0, where \u03a0 is the known distribution of treatment. To simulateX ij , we first draw an iid sample of size N from the empirical distribution P * n,i and then append a random draw of the treatment vector, which is a tuple consisting of N iid draws from a Bernoulli(\u03c0) distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "For each X * i , we will construct an estimator\u03c6(X * i ) using the collection of second-layer resamples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "Specifically, for each (i, j), we compute the adjusted and unadjusted estimators based on the sampl\u1ebd",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "A stochastic approximation of the parameter evaluation \u03a6 m (P * n,i ) is then given b\u1ef9",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "We note that\u03c6 n (X * i ) depends on {X ij } B2 j=1 , and therefore also on B 2 and N -we omit these dependencies in the notation. A Wald-type bootstrap confidence interval can be constructed by using the empirical standard deviation of\u03c6 n (X * i ) over i = 1, . . . , B 1 as the standard error and\u03c6 n (X) :=\u03c6 n (X * 0 )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "as the center. This double bootstrap procedure is summarized in Algorithm 1 in Appendix F.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "We now provide some intuition behind why the above-described double bootstrap procedure is ex-pected to work. We then provide a theorem that formalizes these arguments. First, we observe that the double bootstrap procedure is analogous to a traditional single-layer bootstrap, except that we replace the plug-in estimator with an estimator\u03c6 n , which itself is defined through an additional layer of bootstrap. Intuitively, if\u03c6 n (X * i ) is close enough to the plug-in estimator \u03a6 m (P * n,i ) on all X * i , we would expect that using the stochastic approximation instead of the plug-in makes little difference and the procedure works similarly as the traditional bootstrap works. We now give heuristic arguments showing that\u03c6 n (X * i ) and \u03a6 m (P * n,i ) should indeed be close, in the sense that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "To see this, for an arbitrary j \u2208 {1, . . . , B 2 }, consider a general asymptotically linear estimator\u03c8 of the treatment effect that satisfie\u015d",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "whereX l ij is the l-th observation inX ij . In our upcoming theorem, we will assume that Rem is negligible in an appropriate sense. Suppose that we take sufficiently many samples from P * n,i \u03a0 -that is, that B 2 is sufficiently large -so that the Monte-Carlo error from the second bootstrap layer is negligible. We can then accurately approximate the sampling distribution of",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "under P * n,i by the empirical distribution of {\u03c8(X ij )} B2 j=1 . Applying these arguments at\u03c8 u and\u03c8 m suggests that\u03c6 n (X * i ) accurately approximates\u03c3 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "are the variances of the sampling distributions whereX ij is an iid sample from P * n,i \u03a0. In addition, provided that N \u226b n so that the remainders in the above linear expansion (12) are sufficiently small when\u03c8 is equal to\u03c8 u and\u03c8 m , the ratio between these variances\u03c3 2",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 181,
                    "text": "(12)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "As a result, we expect\u03c6 n (X * i ) to be reasonably close to the plug-in estimator \u03a6 m (P * n,i ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "The upcoming theorem formalizes the heuristic argument given in the previous paragraph. Before giving this result, we define a key differentiability concept that is useful for establishing theoretical We will assume that the following conditions hold:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "Condition B1. Both \u03c3 2 u (\u00b7) and \u03c3 2 m (\u00b7) are Hadamard differentiable;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "Condition B2. There exists a \u03b3 \u2208 (1/2, \u221e) such that the remainder Rem 1 in Eq. 12 is such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "where the expectation is over the draw of the bootstrap sample P * n,1 and X 1 , X 2 , . . .;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "Condition B3. B 2 grows with n in such a way that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "We are now ready to state the theorem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "of the functional \u03a6 m and G is a mean-zero Gaussian process with covariance cov(Gf 1 ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "The proof is a modification of the proof of Theorem 23.9 in Van der Vaart [31] , and is given in",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 78,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "To approximate the limiting distribution \u03a6 \u2032 m (G) given in the above theorem, Algorithm 1 uses the empirical distribution of \u221a n{\u03c6 n (X * i )\u2212\u03c6 n (X)} across the B 1 bootstrap replicates X * 1 , . . . , X * B1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "We now discuss the conditions of Theorem 5. Condition B1 ensures the Hadamard differentiability of the relative efficiency parameter \u03a6 m (\u00b7), which is used in most standard sets of sufficient conditions for the validity of bootstrap methods. We can establish these Hadamard differentiability conditions by noting that the variance is essentially the mean of a function indexed by nuisance parameters, which themselves are transformations of some population means. We use the Mann-Whitney estimand in the ordinal outcome case as an example. The variance of the adjusted estimator takes the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "Here b, \u03b1, \u03b2 are nuisance parameters, which are defined, either explicitly or implicitly, with a set of population means. The mean functional is Hadamard differentiable, for example, when the support is bounded. One can then apply the chain rule of Hadamard differentiability as in [14] .",
            "cite_spans": [
                {
                    "start": 282,
                    "end": 286,
                    "text": "[14]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "Condition B2 ensures that the remainder term in the asymptotic linear expansion is sufficiently small.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "For the examples we have considered, it is possible to show that we can take \u03b3 = 1 under mild conditions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "Conditions B3 and B4 require that the user selects sufficiently large values for B 2 and N . Condition B3 places a restriction on the Monte Carlo approximation\u03c6 n (X * 1 ) of\u03c3 2 m,1 /\u03c3 2 u,1 . In most cases, this condition will hold provided the number of second-layer bootstrap samples goes to infinity faster than does n, that is, so that n/B 2 \u2192 0. Condition B4 places a restriction on the sample size N of each second-layer bootstrap sample. When \u03b3 = 1, this condition requires that these samples be of a larger order than the original sample size n. Taken together, Conditions B3 and B4 impose that sufficient computing power must be available to compute the estimator\u03c8 approximately B 1 B 2 times on samples of size N -in contrast, the analytic method in the previous section only required fitting the estimator\u03c6 m (and estimating its standard error) once on a sample of size n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "We conclude by noting that we can define a double bootstrap procedure analogous to Algorithm 1 for the estimation of the relative efficiency of a fully adjusted estimator \u03c6 a . However, our arguments cannot generally be used to establish the validity of double bootstrap confidence intervals for \u03c6 a . The problem arises because the asymptotic variance of the fully adjusted estimator often involves a regression function of the outcome against the covariate. Because the statistical model is nonparametric up to knowledge of the treatment probability, this dependence will often make it so that the parameter \u03c3 2 a (\u00b7) is not Hadamard differentiable, and so the theoretical guarantee presented above for our double bootstrap procedure may not apply. It is therefore an open question as to whether the double bootstrap will yield valid confidence intervals for the relative efficiency of fully adjusted estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bootstrap procedure for working-model-based estimators"
        },
        {
            "text": "We now consider settings where the outcome in the trial is only partially observed. For this purpose, we use the notion of coarsening-at-random [12, 13] . Let Z t = (T t , A, W t ) be the full data unit in the trial, C t be a coarsening variable, and X t = G a (Z t , C t ) be the observation unit in the trial where G a (\u00b7, \u00b7) is some many-to-one function. We further assume that, under G a , the covariate W t is fully observed. The adjusted analysis estimates the treatment effect \u03c8 based on X t . We can write X t as (X t , W t ), wher\u1ebd X t represents the components in X t that are not covariates and W t is the covariate vector. Define a function c(\u00b7) such that c(X t ) :=X t , and write G u to denote the composition c \u2022 G a . The unadjusted analysis ignores the covariate information, which is equivalent to working with the observation unit",
            "cite_spans": [
                {
                    "start": 144,
                    "end": 148,
                    "text": "[12,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 149,
                    "end": 152,
                    "text": "13]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "The relative efficiency, defined in terms of the variances of the unadjusted and adjusted estimators, is interesting only when both analyses give consistent estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "Thus, we will assume that both conditional distributions G a (Z t , C t )|Z t and G u (Z t , C t )|Z t satisfy the coarsening-at-random assumption, so that both the unadjusted and adjusted analyses are asymptotically unbiased for the treatment effect.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "Let \u03bd denote the distribution of X t . We again define the relative efficiencies by focusing on trials under the sharp null, that is, the conditional distributions T t |A = 0, W t and T t |A = 1, W t are the same. We let G(\u00b7|a, w) denote the conditional distribution of C t given that (A, W t ) = (a, w). Under the sharp null, \u03bd is fully characterized by the treatment distribution \u03a0, the conditional distribution of C t characterized by G, and the joint distribution of (T t , W t ) denoted by P -when we wish to emphasize this dependence, we write \u03bd \u03a0,G,P . We define the relative efficiencies as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "where D u , D a and D m are the influence functions of the unadjusted, fully adjusted and working-modelbased adjusted estimators, respectively. We will often suppress the dependence of these relative efficiencies on \u03a0 and G in the notation by writing \u03a6 a (P ) and \u03a6 m (P ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "We aim to identify and estimate these relative efficiencies from external data available at the trial planning stage. Like the future trial data, the external data can be subject to coarsening. Let C be the coarsening variable, and \u0393(\u00b7, \u00b7) be a many-to-one function. The full data unit in the external dataset is Z = (T, W ), and the observed data unit is X = \u0393(Z, C). Let Q be the distribution of X, induced by the joint distribution of (Z, C) and the many-to-one function \u0393. To identify the relative efficiencies from the observed external data X, we assume the following condition holds throughout this section. This condition is similar to Condition A2 and assumes in addition that coarsening-at-random holds in the external data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "Condition A3. A full data unit in the external data Z = (T, W ) has distribution P , and the conditional distribution \u0393(Z, C) | Z satisfies the coarsening-at-random assumption.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "Under this condition, it is possible to identify the relative efficiencies in (13) as parameters of the distribution of the observed external data, and also to show that, under reasonable conditions, these parameters will be smooth enough so that it should be possible to develop regular and asymptotically linear estimators based on the external data [Theorem 1.3 in 29].",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 82,
                    "text": "(13)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "The external data might be obtained from various settings including observational studies, some of which are distinct from randomized clinical trials. Consequently, the reasons for coarsening can be much different from those in the future trial. For example, for time-to-event data, administrative censoring may account for a large proportion of right censoring in clinical trials, but a lesser proportion for observational data. Thus, it is often not plausible to assume that we can identify G from the external data. To overcome this issue, we define the relative efficiencies for a particular G, and the user can choose a coarsening mechanism that is expected to reflect the setting of a future trial.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "In Appendix A, we use the identifiability result stemming from Condition A3 to develop estimators and confidence intervals for the relative efficiency in settings where there are time-to-event outcomes with right censoring. In this case, T t is the time to some event of interest and C t is the censoring",
            "cite_spans": [],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "The mapping that gives rise to this observation unit is given by G a (z t , c t ) = (min{t t , c t }, I{t t \u2264 c t }, a, w t ). The validity of the unadjusted analysis relies on the condition that T t \u22a5 C t |A, while the validity of the adjusted analysis relies on the condition ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "When the outcome is partially observed"
        },
        {
            "text": "For the ordinal outcome case, we generate data based on a CDC report describing the age distribution and probabilities of various outcomes within age groups for hospitalized Covid-19 patients [4] , which are also presented in Table 1 . The ordinal outcome is assigned the value 1, 2, or 3 for \"death\", \"ICU and survived\", or \"no ICU and survived\", respectively. Age category is the only covariate we adjust for.",
            "cite_spans": [
                {
                    "start": 192,
                    "end": 195,
                    "text": "[4]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 226,
                    "end": 233,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Simulations"
        },
        {
            "text": "We consider both the fully adjusted and working-model-based estimators. The relative efficiency of fully adjusted estimators is estimated with the analytical approach, while for the working-model-based estimators, we use both the analytical and the bootstrap approaches. We consider three estimands of the treatment effect: difference in mean, Mann-Whitney, and average log odds ratio.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "As the covariate is ordinal as well, the nuisance conditional mean functions are estimated by sample averages within each age group. In the analytical approach, we build Wald-type confidence intervals on the logit scale first and transform them. For the bootstrap, we take the number of bootstrap resamples in the two layers to be 100 and 500. Though these resample sizes are small compared to those used in typical applications of the bootstrap, we use them to reduce the computational cost in this Monte Carlo simulation. We do 1,000 replications for the analytical approach and 200 for the bootstrap.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "The simulation results for sample size 1,000 are presented in Table 2 . We observe that despite the small number of resamples, the bootstrap procedure gives approximately 95% coverage, but that this estimator has larger variance than does the analytical estimator across all settings considered. We expect the performance to improve as the number of resamples increases. The coverage of the analytical approach is close to the nominal level. Additional results for sample sizes 200 and 500 are given in Appendix D. We note that, as the true relative efficiency is strictly less than 1, the confidence sets constructed using the two-step approach detailed in Section 3.2 have the same coverage.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 62,
                    "end": 69,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Simulations"
        },
        {
            "text": "For survival outcomes, we only consider the relative efficiency of the fully adjusted estimators. We generate a univariate covariate W \u223c Uniform(0, 1), and the survival time follows an exponential distribution Y |W \u223c Exp{(1 + 9W )/10}. The censoring time in the external data C is generated from an",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "The user-specified censoring mechanism in the trial is taken to be the same, that is, Exp(0.1). We again consider three estimands: risk difference (RD), relative risk With continuous time, the nuisance functions are estimated using a sequence of Cox proportional hazard models with polynomials of the covariate. We select the best model based on BIC. For discrete time, we use a proportional odds model instead, which slightly outperforms the Cox model in the simulations. Results for sample size 1,000 are presented in Table 3 . The coverage of the confidence intervals is close to the nominal level across all settings. The uncertainty in the estimates becomes larger as time (t) increases, due to the reduced size of the risk set.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 520,
                    "end": 527,
                    "text": "Table 3",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Simulations"
        },
        {
            "text": "The R scripts for all the simulation experiments are available as supplementary files. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "We apply the proposed methods to assess the efficiency gain of covariate-adjustment using Covid-19 data. Ordinal Outcome. We use the following mutually exclusive ordinal outcome based on the severity of a patient's Covid-19 status: (1) censor or discharge, (2) intubation or ventilation, and (3) death.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application to Covid-19 data"
        },
        {
            "text": "Among 40 patients who had been admitted twice, only 14 patients had different outcomes between the two visits (9 patients were classified as 2 during first admission and as 1 in the second admission while 5 patients transited from 1 to 2). Among 3 patients who had been admitted three times, only 1 patient had different ordinal outcomes between 3 visits that he was classified a 1, 2, and 1 respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application to Covid-19 data"
        },
        {
            "text": "For all patients who had been admitted more than once, there was no death. To deal with duplicated observations for these patients, we only include the observations with a more severe outcome. As a result of the above classification, there are 207 (60%) censor/discharges, 59 (17%) intubation or ventilation, and 79 (23%) deaths. We consider three estimands of treatment effects: difference in mean (DIM),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application to Covid-19 data"
        },
        {
            "text": "Mann-Whitney (MW), and average log odds ratio (LOR). To estimate the nuisance functions for fully adjusted estimators, we fit a series of polynomial regressions from order 1 to 5 and then select the optimal model based on BIC score for DIM and MW. For LOR, these nuisance functions are estimated by fitting proportional odds models with polynomials of order 1 to 5 and selecting the best model based on BIC.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application to Covid-19 data"
        },
        {
            "text": "We present the relative efficiency of covariate-adjusted estimators that adjust for all the covariates in Table 4 . The estimated efficiency gain is about 7% for the fully adjusted estimator, whereas for the working-model-based estimators we do not see evidence of a significant efficiency gain. In contrast, adjusting for a single baseline covariate gives an estimated efficiency gain ranging from 1% to 5%, and the difference between using fully adjusted and working-model-based estimators is negligible when only adjusting for one covariate. We leave the details to Appendix D.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 106,
                    "end": 113,
                    "text": "Table 4",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "Application to Covid-19 data"
        },
        {
            "text": "Survival Outcome. We choose the time point of interest to be t = 350 hours, where the overall survival is around 70%, and assess the relative efficiency for survival outcomes. We consider three estimands of treatment effects: risk difference (RD), relative risk (RR), and restricted mean survival time (RMST). We use elastic net [11] for variable selection, where the tuning penalty parameter is selected via 5-fold cross validation. In particular, we select those variables with nonzero coefficients. To estimate the nuisance functions, we then fit a sequence of Cox proportional hazards models with polynomials of orders 1 to 7 of the selected variables and select the model with the smallest BIC score. The results are shown in Table 5 . Adjusting for a single baseline covariate gives an efficiency gain ranges from 1% to 9% in estimating RD or RR, with age being the most prognostic factor. A similar trend is observed for RMST. Using elastic net, we select the following 4 baseline factors: age, CVD, chronic kidney disease, and cholesterol medications. Adjusting for these four factors gives an 11% efficiency gain in estimating RD or RR and RMST.",
            "cite_spans": [
                {
                    "start": 329,
                    "end": 333,
                    "text": "[11]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 731,
                    "end": 738,
                    "text": "Table 5",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Application to Covid-19 data"
        },
        {
            "text": "In this paper, we presented a framework to use external data to infer about the relative efficiency of covariate-adjusted analyses in a future clinical trial. We also exhibited the applicability of our framework for a variety of treatment effect estimands of particular interest. For each of these estimands, we introduced a consistent and asymptotically normal estimator of the relative efficiency and provided an analytic means to develop Wald-type confidence intervals. We also introduced a double bootstrap scheme that enables confidence interval construction in certain problems even when an analytic form for the standard error is not available.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "When the outcome is only partially observed, standard unadjusted and adjusted analyses typically The relative efficiency we considered is based on a sharp null setting where the treatment has no effect. As a consequence, we do not need to specify the full distribution of Y t |A, W t expected in the trial. Moreover, if the treatment effect estimator is regular, which is the case for all those that we considered, then the relative efficiency at this sharp null also serves as an accurate approximation to the relative efficiency under a variety of local alternatives. Though accurate in such settings, we expect that this approximation may be poor when the treatment is extremely beneficial in some subgroups while being quite harmful in some others. While a subgroup analysis might be able to detect this after the trial is completed, it is not generally possible to know a priori whether this kind of subgroup effect exists. An alternative approach would involve specifying a particular alternative distribution that the investigator is interested in. In this case, the relative efficiency under that alternative can be derived and estimated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Our framework for estimating relative efficiencies based on external data can be easily modified for this setting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Observational settings and clinical trials can be quite different in terms of coarsening, and thus we define relative efficiency for a user-specified coarsening mechanism that approximates that of the future trial. This also extends to the case where the covariate distribution is different between the external data and the future clinical trial due to, for example, trial eligibility criteria. In such cases, a particular covariate distribution for the future trial can be imposed when defining the relative efficiency, and the external data can then be used to estimate the distribution of the outcome conditional on covariates.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "This appendix is organized as follows. In Appendix A, we develop estimators and confidence intervals for the relative efficiency in settings where there are time-to-event outcomes with right censoring. In Appendix B, we prove lemmas and theorems in Section 3 on continuous and ordinal outcomes. In",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "Appendix C, we prove lemmas and theorems in Appendix A on time-to-event outcomes with right censoring. In Appendix D, we show some additional experiment results. In Appendix E, we develop a two-step procedure with sample splitting to construct confidence intervals, and show that it achieves nominal coverage. In Appendix F, we give the pseudocode for the double bootstrap scheme presented in Section 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "A Estimation of relative efficiencies for time-to-event outcome with right censoring",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "We consider three estimands of treatment effect, which are all functionals of the treatment-arm-specific survival function S a (t) := P (T t > t|A = a). For the unadjusted analysis, we consider plug-in estimators based on the treatment-arm-specific Kaplan-Meier estimator [17] , which we denote asS a (t). Such plug-in estimators are consistent and asymptotically linear provided that T t \u22a5 C t |A [see, for example, 8].",
            "cite_spans": [
                {
                    "start": 272,
                    "end": 276,
                    "text": "[17]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "In contrast, the consistency of covariate-adjusted estimators often relies on the assumption that A) . In fact, many recently proposed adjusted estimators are based on the efficient influence function of the treatment effect estimand in a model where the only assumption is that",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 98,
                    "end": 100,
                    "text": "A)",
                    "ref_id": null
                }
            ],
            "section": "Appendix"
        },
        {
            "text": "[e.g., 22, 27, 8] . Under regularity conditions, these estimators achieve the semiparametric efficiency bound in this model. Constructing these estimators often requires estimation of nuisance functions such as the conditional hazard function h a (t, w) = P (T t = t|T t \u2265 t, A = a, W t = w), the conditional survival function S a (t, w) = P (T t > t|A = a, W t = w), the conditional distribution of censoring time G a (t, w) := P (C t \u2265 t|A = a, W t = w) or the treatment mechanism \u03c0(w) = P (A = 1|W t = w). We call these estimators \"fully adjusted\".",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 10,
                    "text": "22,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 11,
                    "end": 14,
                    "text": "27,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 15,
                    "end": 17,
                    "text": "8]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "As discussed in Section 4, the efficiency of an adjusted estimator relative to that of an unadjusted estimator is relevant only when both estimators are consistent -as noted earlier, a sufficient condition for this to hold is that the observed data arises from a distribution in the intersection model consisting of all distributions of (Z t , C t ) for which T t \u22a5 C t |(W t , A) and T t \u22a5 C t |A. Notably, there is not generally any guarantee that a fully adjusted estimator will be efficient relative to the observed data model consisting of the distributions of G a (Z t , C t ) generated by sampling (Z t , C t ) from a distribution in this intersection model. Stated more plainly, if it is known in advance that both the adjusted and unadjusted survival function estimators are consistent, then, in certain cases, there may exist a more efficient estimator of this survival function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "Unlike the cases of continuous or ordinal outcomes that we considered in Section 3, we are not aware of a parametric working model for the conditional distribution of T t |A, W t that yields a RAL estimator of S 0 and S 1 when marginalized over the distribution of the covariate W t . Nevertheless, it is possible to define adjusted estimators based on working models in this setting. To see this, note that many of the aforementioned fully adjusted estimators do have the doubly robust property: they are consistent if either (S 0 , S 1 ) or (G, \u03c0) is correctly specified, and are efficient if both are correctly specified. This allows us to use potentially misspecified parametric working models to estimate (S 0 , S 1 ) as long as we estimate the distribution of censoring time using a correctly specified semiparametric or nonparametric model -this is the case, for example, if we estimate the censoring distribution via a correctly specified arm-specific Kaplan-Meier estimator. Such estimators are rarely used in practice. We, therefore, focus on computing the relative efficiency of fully adjusted estimators, which see more use, as compared to that of unadjusted estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "As in previous works [22, 27, 8] , we assume that survival and censoring time are discrete, and take values in {t 1 , t 2 , . . . , t K }. We let t 0 = 0 be the baseline time. We expect similar derivations can be done for continuous time, and in the simulation studies we empirically validate the performance of our proposed methods when time is measured on a continuous scale.",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 25,
                    "text": "[22,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 26,
                    "end": 29,
                    "text": "27,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 30,
                    "end": 32,
                    "text": "8]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "The first two estimands we consider focus on survival functions at a specific time point. The risk difference (RD) is defined as S 0 (t k ) \u2212 S 1 (t k ) for a time t k of interest. The relative risk (RR) is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "for RD and {1 \u2212S 1 (t k )}/{1 \u2212S 0 (t k )} for RR, whereS a is the Kaplan-Meier estimator within each treatment group. Let\u015c a denote the efficient adjusted estimator proposed in Moore and van der Laan [20] . For each of the two estimands under consideration, we refer to the estimator that replacesS a in the unadjusted estimator with\u015c a as the fully adjusted estimator.",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 205,
                    "text": "[20]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Recall that\u015c a is a consistent estimator of S a when T t \u22a5 C t |(A, W t ). Under additional regularity conditions given in Theorem 1 in Moore and van der Laan [22] , for each a \u2208 {0, 1} and k \u2208 {1, . . . , K}, S a (t k ) is an asymptotically linear estimator of S a (t k ) with influence function",
            "cite_spans": [
                {
                    "start": 159,
                    "end": 163,
                    "text": "[22]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Moreoever, for each a \u2208 {0, 1} and k \u2208 {1, . . . , K},S a (t k ) is a RAL estimator of S a (t k ) when T t \u22a5 C t |A with influence function [see, e.g., 8]",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Here, h a (t) is the hazard corresponding to S a at time t and G a (t) := P (C t \u2265 t|A = a). The influence functions of the fully adjusted and unadjusted estimators of the treatment effect estimand, which we denote as D a and D u , respectively, can then be derived via the delta method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "As in Section 4, we define the relative efficiency as the ratio between the variances of D a and D u under the sharp null. In such cases, the distribution of the observed data in the trial is characterized by the marginal distribution of A, denoted by \u03a0, the joint distribution of (T t , W t ), denoted by P , and the conditional distribution of C t given (A, W t ). In particular, this implies that S 1 (t, w) = S 0 (t, w) = S(t, w)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "for all (t, w), where S(t, w) := P (T t > t|W t = w) is the conditional survival function under P , and also that S 1 (t) = S 0 (t) = S(t) for all t, where S(t) := P (T t > t) is the marginal survival function under P . To simplify the presentation, we suppose additionally that C t \u22a5 A|W t = w, and write G(t, w) := P (C t \u2265 t|W t = w) and G(t) := P (C t \u2265 t). For given G and \u03a0, the relative efficiency parameter is a functional of P .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Before presenting the form of the relative efficiency, we introduce some additional needed notation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "For (T, W ) \u223c P , let h(t, w) := P (T = t|T \u2265 t, W = w) and h(t) := P (T = t|T \u2265 t) be the conditional and marginal hazard functions under P , respectively. We define the following quantities, which will be useful throughout this section:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Interestingly, in the null case that we consider, the relative efficiencies are the same for the RD and RR estimands.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Lemma 6. Suppose that Conditions A1 and A3 hold and, in addition, that S(t k ) < 1. Suppose that S a andS a are asymptotically linear with influence functions given in (14) and (15), respectively. For both the RR and RD estimand, the relative efficiency of the fully adjusted estimator as compared to the unadjusted estimator is given by \u03a6 a (P ) = \u03c3 2 a (P )/\u03c3 2 u (P ), where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "In what follows, we will often write \u03c6 a for \u03a6 a (P ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "To estimate \u03c6 a from the external data (Y, \u2206, W ), we estimate \u03c3 2 u and \u03c3 2 a separately. We observe that \u03c3 2 u is a transformation of S(t), and hence we construct a plug-in estimator\u015d kl j of s kl j using covariateadjusted estimator of S(t) given in Moore and van der Laan [20] and estimate \u03c3 2 u by\u03c3 2 u = k j=1\u015d kk j /G(t j ).",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 279,
                    "text": "[20]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "We estimate \u03c3 2 a using one-step estimation based on its EIF. Recall that C is the censoring time in the external data. We define H(t, w) := P (C \u2265 t|W = w). For notational convenience, we define the following function, which appears multiple times in the EIF of \u03c3 2 a :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "The efficient influence function of \u03c3 2 a relative to the observed data model is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "The derivation of this expression is deferred to Appendix C. Let\u015c(t, w) and\u0125(t, w) be estimators of the conditional survival and hazard functions, respectively. Let\u0124(t, w) be an estimator of the conditional censoring distribution. Define\u011d kk j ,f kk j with these estimates. We estimate \u03c3 2 a wit\u0125",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "We then estimate \u03c6 a by\u03c6 =\u03c3 2 a /\u03c3 2 u . The properties of\u03c6 are given in the following theorem. The influence function of\u03c6 is given in Appendix C.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "The final treatment effect estimand we consider is the restricted mean survival time (RMST), defined as \u03c8 = k j=1 {S 1 (t j ) \u2212 S 0 (t j )}. We again consider two plug-in estimators: the unadjusted KM-based",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Lemma 7. Suppose that Conditions A1 and A3 hold. Then, the relative efficiency \u03c6 a takes the following form \u03c6 a = \u03c3 2 a (P )/\u03c3 2 u (P ), where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "We construct\u03c3 2 u in a similar way as in the case of the risk difference, namely by plugging in an efficient adjusted estimator of S(\u00b7). As for \u03c3 2 a , its EIF can be derived in a similar fashion as in the case of RD, and we defer the details to Appendix C. We propose the following estimator",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "The relative efficiency is estimated by\u03c6 =\u03c3 2 a /\u03c3 2 u .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Based on (20) , it appears that computing the quadruple sum used to define\u03c3 2 a will take order nk 3 time. As it turns out, these sums can be computed much more efficiently. In Appendix C, we show that for a given i and given estimates of \u03c4 and S, the inner three sums can be computed in O(k) time, resulting in an O(nk) complexity for computing the above quadruple sum.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 13,
                    "text": "(20)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Theorem 7. Under the same conditions as in Theorem 6,\u03c6 is an efficient estimator of \u03c6 a .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "Again, the specific form of its influence function is given in Appendix C.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.1 Estimation of relative efficiency"
        },
        {
            "text": "As discussed at the end of Section 3.2, the influence function of\u03c6 is identically 0 in certain special cases. One such case arises when T \u22a5 W under P and the mapping G does not depend on W .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "In these cases, a two-step procedure can be considered for inference -see the end of Section 3.2 for a description of such an approach in a similar setting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "As noted earlier, for it to be interesting to compare the efficiency of the adjusted and unadjusted estimators, it must be the case that both are asymptotically linear. In such settings, we now characterize cases in which the adjusted estimator will be more efficient than will the unadjusted estimator. Moreover, unlike in the uncoarsened data setting, there are also settings where the adjusted estimator may be less efficient than the unadjusted estimator. We also characterize these cases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Theorem 8. For all three estimands considered:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "1. If the conditions in Lemma 6 hold and P is such that T \u22a5 W , then \u03c3 2 u (P ) \u2264 \u03c3 2 a (P ), that is, the unadjusted estimator is at least as efficient as the adjusted estimator. Moreover, if var P [G(t j , W )] > 0 for some j \u2208 {1, . . . , k}, then the inequality is strict: \u03c3 2 u (P ) < \u03c3 2 a (P ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "2. If the conditions in Lemma 6 hold and G(\u00b7, \u00b7) is such that G(t, w 1 ) = G(t, w 2 ) for all t \u2264 t k and w 1 , w 2 \u2208 W, then \u03c3 2 a (P ) \u2264 \u03c3 2 u (P ), that is, the adjusted estimator is at least as efficient as the unadjusted estimator.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "B Proofs of results in the case where the outcome is fully observed B.1 Supporting lemmas for proofs in Section 3",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Lemma 8 (EIF of mean conditional variance). For a given function u P (\u00b7) that can be written as u P (y) = h(x, y)dP (x) for some function h, the canonical gradient of \u03c3 2 a = E P [var(u P (Y )|W )] is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "where f (w) = E P [u P (Y )|W = w].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Proof. We prove this lemma by directly applying the definition of a gradient. We consider the one- ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "The second term on the right has the following derivative with respect to \u01eb at \u01eb = 0",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "and so will contribute {u P (Y ) \u2212 f P (W )} 2 \u2212 \u03c3 2 a to the gradient. We now focus on the first term, which re-writes as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "where c(\u01eb) = h(x, y){\u01ebs 1 (x|w) + \u01ebs 2 (w) + \u01eb 2 s 1 s 2 }dP (x, w). The first term in the above display has derivative 0 with respect to \u01eb at \u01eb = 0, as f P is the true conditional mean. The second term also has derivative 0 at \u01eb = 0, as it is quadratic in \u01eb. At \u01eb = 0, the third term has derivative 2 h(x, y){s 1 (x|w) + s 2 (w)}dP (x, w) h(x, y)dP (x, w) \u2212 f P (w) dP (y, w)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "The inner integral has mean",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Therefore the following is a gradient:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Since we are working within a locally nonparametric model, the above is also the canonical gradient.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Lemma 9 (EIF of mean conditional covariance). Consider a locally nonparametric model of distributions of (Y, W ). For given functions u(\u00b7) and v(\u00b7), the canonical gradient of",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Proof. As in the proof of Lemma 8, we consider a one-dimensional submodel {P \u01eb : \u01eb} with density",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Also, because",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "This shows that D cov is a gradient, and, because the model is locally nonparametric, D cov must therefore be the canonical gradient.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Let X be a generic random variate with distribution P \u2208 M with support in a bounded set X \u2282 R d .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Let U \u03b1 : X \u2212 \u2192 R m be a function indexed by \u03b1 \u2208 R m . Suppose that P U \u03b1 = 0 has a unique solution in \u03b1, and we denote this solution by \u03c8 1 = \u03c8 1 (P ). In general, we can regard \u03c8 1 (P ) as a parameter defined implicitly through the estimating equation. The following lemma establishes the pathwise differentiability of this and a related parameter, under appropriate conditions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Lemma 10 (Pathwise differentiability of parameters defined via estimating equations). Let \u03c8 1 : M \u2212 \u2192 R m be such that \u03c8 1 (P ) is the unique solution in \u03b1 to the estimating equation P U \u03b1 = 0. Suppose that, for each x \u2208 X , U \u03b1 (x) is continuously differentiable in \u03b1, with derivativeU \u03b1 (x) := \u2202 \u2202\u03b1 U\u03b1(x)|\u03b1 =\u03b1 . Suppose in addition that U \u03c81 ,U \u03c81 \u2208 L 2 (P ) and that PU \u03c81 is invertible. Then, \u03c8 1 is pathwise differentiable and its gradient relative to any locally nonparametric model is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Moreover, for each \u03b1 \u2208 R m , let g \u03b1 : X \u2212 \u2192 R be a function, and suppose that \u03b1 \u2192 g \u03b1 (x) is differentiable for all x \u2208 X . For each P \u2208 M, define \u03c8 2 := P g \u03c81 . Then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Proof. For h \u2208 L 2 0 (P ) whose range is contained in [\u22121, 1], consider the one-dimensional submodel {P \u01eb : \u01eb},",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "where each P \u01eb has density p \u01eb (x) = {1 + \u01ebh(x)}p(x). This submodel has score h at \u01eb = 0. By definition,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "\u01ebh(x)}p(x)dx, which is linear in \u01eb. Thus, the continuous differentiability of U as a function of \u03b1 implies that f is also continuously differentiable. Then the implicit function theorem implies that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "We note that D 1 \u2208 L 2 0 (P ). Hence by definition \u03c8 1 is pathwise differentiable with gradient D 1 , which is also the only gradient in any locally nonparametric model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Also, noting that \u03c8 2 (P \u01eb ) = g \u03c81(P\u01eb) (x){1 + \u01ebh(x)}dP (x), we see that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Thus, \u03c8 2 is pathwise differentiable and D 2 is the gradient in any locally nonparametric model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Remark 1."
        },
        {
            "text": "Proof of Lemma 1. For notational convenience, we define \u00b5 a = E[Y t |A = a], for a \u2208 {0, 1}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "We first establish properties of the working-model-based estimator of \u00b5 a , given by\u03bc a =\u03b1 a +\u03b2 \u22a4 aW t .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "To do this, we note that the fitted coefficients from arm-specific linear regression (\u03b1 a ,\u03b2 \u22a4 a ) satisfy the following first-order conditions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Let (\u03b1 * a , \u03b2 * a ) be the large-sample limit of (\u03b1 a ,\u03b2 a ), defined implicitly as the solution to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "The efficient influence function of \u00b5 a when the treatment is randomized is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "where \u03c0 a = \u03a0(A = a). We claim that D a defined below is also a gradient of \u00b5 a in this model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "To show this, we will show that D * a \u2212 D a lies in the orthogonal complement of the tangent space. First, define L 2 0,A (\u03bd) = {s \u2208 L 2 0 (v) : s(y, a, w) = s(y \u2032 , a, w \u2032 ) \u2200(y, w), (y \u2032 , w \u2032 )};",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "The tangent space decomposes as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "We note that each individual factor in the above display has mean 0. This, together with the independence between A and W t , implies that s, D a \u2212 D * a \u03bd = 0 for any s in L 2 0,A (\u03bd), L 2 0,W t (\u03bd) or L 2 0,Y t |A,W t (\u03bd). This implies that the difference is orthogonal to each component of the tangent space, and hence the tangent space itself.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Next, we note that\u03bc a re-writes as a one-step estimator based on the gradient D a . Let\u03bd be a",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "the sample proportion of A = a. The remainder is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "which is o P (n \u22121/2 ) as the first term is O P (n \u22121/2 ) and the second term is o P (1). We note that Y and W both have bounded support and \u03c0 a \u2208 (0, 1). Thus, D a (\u03bd) \u2212 D a L 2 (\u03bd) = o P (1), which follows from the convergence of\u03b1 a ,\u03b2 a and\u03c0 a to their population counterparts. Let \u03b3 * := (\u03c0 a , \u03b1 * a , \u03b2 * a , \u00b5 a ) be the true parameter value and \u03b3 = (\u03c0, \u03b1, \u03b2, \u00b5) be a generic parameter value, and define a class of functions",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "small neighborhood of \u03b3 * . We note that D a (\u03bd) \u2208 F with probability tending to 1. The boundedness of Y t , W t and \u03c0 a implies that this class of functions is Lipschitz in \u03b3, and Example 19.7 in Van der Vaart [31] implies that F is a Donsker class.",
            "cite_spans": [
                {
                    "start": 211,
                    "end": 215,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Therefore,\u03bc a is asymptotically linear with influence function D a , and so is\u03c8 m with influence function D m = D 1 \u2212 D 0 . Now focusing on \u03bd where the sharp null holds, D m simplifies to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Due to the independence of A and (Y t , W t ) under the sharp null, it has variance \u03c3 2 m /(\u03c0 1 \u03c0 0 ) under the sharp null. Now, we derive the asymptotic variances of the unadjusted estimator and the fully adjusted estimator.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "In doing so, we consider a more general parameter",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "for a given function u. The average treatment effect corresponds to the special case of u being the identity function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Recall that the unadjusted estimator is given by the difference between arm-specific mean\u015d",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": ".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Applying the delta method, we see that\u03c8 u is asymptotically linear with influence function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Under the sharp null, it simplifies to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "with variance under the sharp null var P (u(Y ))/(\u03c0 1 \u03c0 0 ), due to the independence between A and (Y t , W t ) under the sharp null.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "The fully adjusted estimator we consider is the AIPW estimator given b\u0177",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "The influence function of this estimator\u03c8 a is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Under the sharp null, it simplifies to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "with variance under the sharp null E P [var P (u(Y )|W )]/(\u03c0 1 \u03c0 0 ), where we again use the independence between A and (Y t , W t ) under the sharp null.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Proof of Theorem 1. First, we consider the estimation of \u03c3 2 m . Define a parameter P \u2192 (\u03b1 * (P ), \u03b2 * (P )),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": ". Define a set of estimating",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "). The first-order condition implies that (\u03b1 * (P ), \u03b2 * (P ))",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "is the unique solution to the estimating equation P U (\u03b1, \u03b2) = 0. This solution can be written as a",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Hence, by the chain rule, (\u03b1 * (P ), \u03b2 * (P )) is pathwise differentiable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "In the remainder of this proof, we will often write (\u03b1 * (P ), \u03b2 * (P )) as (\u03b1 * , \u03b2 * ). The fitted coefficients (\u03b1,\u03b2) solve the empirical estimating equation P n U (\u03b1, \u03b2) = 0, and are asymptotically linear estimators of (\u03b1 * , \u03b2 * ) with influence function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Lemma 10 implies that this influence function is also the canonical gradient of (\u03b1 * , \u03b2 * ) in any locally nonparametric model, and therefore (\u03b1,\u03b2) is also regular [Proposition 2.3.i, 23]. Define the estimating function U AT E (\u03b1, \u03b2, \u03c3 2 ) := (y \u2212 \u03b1 \u2212 \u03b2 \u22a4 w) 2 \u2212 \u03c3 2 . We note that \u03c3 2 m is the solution in \u03c3 2 to the estimating equation P U AT E (\u03b1 * , \u03b2 * , \u03c3 2 ) = 0 and\u03c3 2 m solves its empirical counterpart P n U AT E (\u03b1,\u03b2, \u03c3 2 ) = 0. Hence,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "The consistency of (\u03b1,\u03b2) implies that\u03c3 2 m is also consistent. This together with the bounded support",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": ". Furthermore, we can show that U AT E is a Lipschitz function of (\u03b1, \u03b2, \u03c3 2 ) in a neighborhood of (\u03b1 * , \u03b2 * , \u03c3 2 m ), and thus U AT E (\u03b1,\u03b2,\u03c3 2 m ) belongs to a Donsker class with probability tending to 1 [Example 19.7, 31] . Lemma 19.24 ",
            "cite_spans": [
                {
                    "start": 208,
                    "end": 222,
                    "text": "[Example 19.7,",
                    "ref_id": null
                },
                {
                    "start": 223,
                    "end": 226,
                    "text": "31]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 229,
                    "end": 240,
                    "text": "Lemma 19.24",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "B.2 Results in Section 3"
        },
        {
            "text": "Vaart [31] implies that the last term in the above display is o P (n \u22121/2 ). Applying a Taylor expansion to the second term, we hav\u00ea",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 10,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "In particular, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "both of which have mean 0 at (\u03b1 * , \u03b2 * ) by the first-order condition of (\u03b1 * , \u03b2 * ). This implies that\u03c3 2 m is asymptotically linear with influence function IF m (y, w) = (y \u2212 \u03b1 * \u2212 w \u22a4 \u03b2 * ) 2 \u2212 \u03c3 2 m . Lemma 10 implies that this is also the canonical gradient of \u03c3 2 m , and hence\u03c3 2 m is also regular.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Next we estimate \u03c3 2 a . The proposed estimator is a one-step estimator based on the canonical gradient.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Applying Lemma 8 with h(x, y) = y, we obtain the canonical gradient IF a (y,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "LetP be a distribution of (Y, W ) such that the conditional mean of Y given W isr(w) and the marginal distribution of W is the empirical distribution of W . Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Asr(w) is uniformly bounded and belongs to a Donsker class F with probability tending to 1 and Y has bounded support, Theorem 2.10.6 in Van Der Vaart and Wellner [32] implies that IF a (P ) = {y \u2212r(w)} 2 \u2212\u03c3 2 a belongs to a Donsker classF that is also bounded. The difference between IF a (P ) and IF a (P ) is given by",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 166,
                    "text": "[32]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "for some M as the support of Y andr are bounded. The first term in the last line is o P (1) by the assumption that r \u2212 r L 2 (P ) = o P (n \u22121/4 ). Thus, IF a (P ) \u2212 IF a (P ) L 2 (P ) = o P (1) provided that\u03c3 2 a is a consistent estimator of \u03c3 2 a . Suppose for now that this is indeed the case, then Lemma 19.24 in Van der",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Vaart [31] implies that (P n \u2212 P ){IF a (P ) \u2212 IF a (P )} is o P (n \u22121/2 ). This shows that\u03c3 2 a is regular and asymptotically linear with influence function IF a .",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 10,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "We now show that\u03c3 2 a is indeed a consistent estimator of \u03c3 2 a . Note that Finally, we estimate \u03c3 2 u with the sample variance of Y , which is regular and asymptotically linear",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Theorem 2 then follows by applying the delta method. Specifically, the influence function of\u03c6 a is given by (IF a \u2212 \u03c6 a IF u )/\u03c3 2 u , and similarly the influence function of\u03c6 m is (IF m \u2212 \u03c6 m IF u )/\u03c3 2 u . Both estimators are efficient as we work in a locally nonparametric model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Proof of Lemma 2. First recall that \u03b8 * a (k, w) is the best approximation to the true outcome regression \u03b8 a (k, w) within the proportional odds model. The coefficients \u03b1 * a and \u03b2 * a satisfy the following first-order",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "We claim that when the treatment is randomized, the following is a gradient of F a (k),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "To show this, we will show that the difference between IF Fa(k) and the canonical gradient of F a (k) lies in the orthogonal complement of the tangent space. The canonical gradient is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Hence,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "As each individual factor has mean 0 and A \u22a5 W t , the difference is orthogonal to each component of the tangent space and hence the tangent space itself.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Finally,F a re-writes as a one-step estimator based on the gradient IF Fa(k) . In particular, let\u03bd be a distribution with outcome regression \u03b8\u03b1 a ,\u03b2a and treatment mechanism\u03c0 1 , then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "First we note that R(\u03bd, \u03bd) is o P (n \u22121/2 ) as the first term in the last line is O P (n \u22121/2 ) and the second term is o P (1). Also given the bounded support of W t and the fact that \u03c0 1 and \u03c0 0 are bounded away from 0, we can show that the convergence of\u03c0 a ,\u03b1 a and\u03b2 a implies that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Example 19.7 in Van der Vaart [31] shows that IF Fa(k) (v) lies in a Donsker class with probability tending to 1, as IF Fa(k) is Lipschitz in its indexing parameters in a neighborhood of the true parameter value, again due to the boundedness of W t and \u03c0 a . Lemma 19.24 in Van der Vaart [31] implies that",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 34,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 288,
                    "end": 292,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "ThusF a (k) is asymptotically linear with influence function IF Fa(k) . ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Under the sharp null, the above display simplifies to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Under the sharp null, its variance is E P",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "/(\u03c0 1 \u03c0 0 ) due to the inde-pendence between A and (Y t , W t ) under the sharp null.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Proof of Theorem 2. We first consider estimating \u03c3 2 m . To start, we show that (\u03b1,\u03b2), the maximizer of the empirical version of (10), is a RAL estimator of (\u03b1 * , \u03b2 * ). The first-order conditions of this maximization imply that (\u03b1,\u03b2) solves a set of estimating equations, as the parameter space is unconstrained.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Specifically, define U (\u03b1, \u03b2) = (U 1 (\u03b1, \u03b2) , . . . , U K\u22121 (\u03b1, \u03b2), U K (\u03b1, \u03b2)) with",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 32,
                    "end": 43,
                    "text": "(U 1 (\u03b1, \u03b2)",
                    "ref_id": null
                }
            ],
            "section": "in Van der"
        },
        {
            "text": ", k = 1, . . . , K \u2212 1,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": ".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Then we have that P n U k (\u03b1,\u03b2) = 0 for all k = 1, . . . , K. We can show, through the usual arguments used to study estimating equations [ Chapter 5, 31] , that the influence function of (\u03b1,\u03b2) is",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 150,
                    "text": "Chapter 5,",
                    "ref_id": null
                },
                {
                    "start": 151,
                    "end": 154,
                    "text": "31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "In particular, the derivative matrixU can be partitioned into",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Lemma 10 implies that IF ab is the canonical gradient of (\u03b1 * , \u03b2 * ) in a locally nonparametric model, and thus (\u03b1,\u03b2) is also regular [Proposition 2.3.i, 23].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Next, we define the following estimating equation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "By definition, \u03c3 2 m is the unique solution in \u03c3 2 to the equation P U DIM (\u03b1 * , \u03b2 * , \u03c3 2 ) = 0, and\u03c3 2 m solves its empirical counterpart P n U DIM (\u03b1,\u03b2, \u03c3 2 ) = 0. Hence, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "The consistency of (\u03b1,\u03b2) implies that\u03c3 2 m is also consistent. Combining this with the bounded support of W , we see that U DIM (\u03b1,\u03b2,\u03c3 2 m ) \u2212 U DIM (\u03b1 * , \u03b2 * , \u03c3 2 m ) L 2 (P ) = o P (1). Furthermore, it can be shown that U DIM is a Lipschitz transformation of (\u03b1, \u03b2, \u03c3 2 ) in a neighborhood of (\u03b1 * , \u03b2 * , \u03c3 2 m ), and thus that [31] implies that the last term in the above display is o P (n \u22121/2 ). Thus,",
            "cite_spans": [
                {
                    "start": 333,
                    "end": 337,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "The partial derivatives are given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "Combining all the results above, we see that\u03c3 2 m is an asymptotically linear estimator of \u03c3 2 m with influence function IF m , where We now consider the estimation of \u03c3 2 a . The proposed estimator is a one-step estimator based on the canonical gradient, and the proof is very similar to that of Theorem 1. Applying Lemma 8 with h(x, y) = u(y), we obtain the canonical gradient IF a (y, w) = {u(y)\u2212 r(w)} 2 \u2212 \u03c3 2 a . LetP be a distribution of (Y, W ) such that the conditional mean of u(Y ) given W isr(w), and that the marginal distribution of W is the empirical distribution of W . We have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "in Van der"
        },
        {
            "text": "where the latter equality holds by assumption. Asr(w) is uniformly bounded and belongs to a Donsker class F with probability tending to 1 and Y has bounded support, Theorem 2.10.6 in Van Der Vaart and Wellner [32] implies that IF a (P ) = {u(y) \u2212r(w)} 2 \u2212\u03c3 2 a belongs to a transformed Donsker classF that is also bounded. The difference between IF a (P ) and IF a (P ) is given by",
            "cite_spans": [
                {
                    "start": 209,
                    "end": 213,
                    "text": "[32]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "for some M as the support of Y andr are bounded. The first term in the last line is o P (1) by the assumption that r \u2212 r L 2 (P ) = o P (n \u22121/4 ). Thus, IF a (P ) \u2212 IF a (P ) L 2 (P ) = o P (1) provided that\u03c3 2 a is a consistent estimator of \u03c3 2 a . Suppose for now that this is indeed the case, then Lemma 19.24 in Van der Vaart [31] implies that (P n \u2212 P ){IF a (P ) \u2212 IF a (P )} is o P (n \u22121/2 ). Hence, if we show that\u03c3 2 a is a consistent estimator of \u03c3 2 a , then we will have shown that\u03c3 2 a is regular and asymptotically linear with influence function IF a .",
            "cite_spans": [
                {
                    "start": 330,
                    "end": 334,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We now show that\u03c3 2 a is indeed a consistent estimator of \u03c3 2 a . Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "for some constant M 1 , where we used the fact that both Y and W have bounded support andr is uniformly bounded. The first term in the last line is o P (1) by the weak law of large numbers. As for the second term, we see that it is equal to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Lemma 19.24 in Van der Vaart [31] implies that (P n \u2212 P )|r \u2212 r| = o P (1) as |r \u2212 r| lies in a Donsker class with probability tending to 1 and r \u2212 r L 2 (P ) = o P (1). In addition, P |r \u2212 r| \u2264 r \u2212 r L 2 (P ) = o P (1).",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "This establishes the consistency of\u03c3 2 a .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Finally, we estimate \u03c3 2 u with the sample variance of u(Y ), which is regular and asymptotically linear",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Theorem 2 then follows by applying the delta method. Specifically, the influence function of\u03c6 a is given by (IF a \u2212 \u03c6 a IF u )/\u03c3 2 u , and, similarly, the influence function of\u03c6 m is (IF m \u2212 \u03c6 m IF u )/\u03c3 2 u .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Proof of Lemma 4. Recall that the unadjusted estimator i\u015d",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "First we introduce some notation. Let \u03bd n be the empirical distribution of X t in the future trial data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Define the functions \u03b7 a (\u00b7), a \u2208 {0, 1}, analogously to \u03b7(\u00b7) but within each treatment arm as \u03b7 a (k) := P a (Y < k) + P a (Y = k)/2 for a \u2208 {0, 1}, and define\u03c8 u1 :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We note that\u03c8 u1 is a V-statistic with symmetric kernelh(X t 1 , X t 2 ) :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": ". With this notation,\u03c8 u1 = \u03bd 2 nh . Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "To establish the asymptotic linearity of\u03c8 u1 , we first show that (\u03bd n \u2212 \u03bd) 2h is o P (n \u22121/2 ). To start, define a class of functionsH := {x \u2192h(x, x 2 ) : x 2 \u2208 X } where X is the support of X t . Each function inH is a weighted sum of 4 binary terms, with weights being either 1 or 1/2. Each term is indexed by a 2 and y t 2 , and can be computed with 3 arithmetic operations and 1 comparison. Theorem 8.4 in Anthony and Bartlett [1] implies that each binary term belongs to a VC-class with VC dimension at most 64, and Lemma 19.15 in Van der Vaart [31] in turn implies that this class is Donsker. Theorem 2.10.6 in Van Der Vaart and Wellner [32] then implies thatH is a Donsker class (hence also Glivenko-Cantelli). Next we note that \u03bdh(x t , \u00b7) :",
            "cite_spans": [
                {
                    "start": 432,
                    "end": 435,
                    "text": "[1]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 551,
                    "end": 555,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 644,
                    "end": 648,
                    "text": "[32]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Combining this with the previous results, we have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Applying the delta method, we see that\u03c8 u is asymptotically linear with influence function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "which, under the sharp null, simplifies to ( a t \u03c01 \u2212 1\u2212a t \u03c00 ){\u03b7(y t )\u2212 1 2 }. The variance of \u03b7(Y t ) can be calculated exactly, and equals to (1 \u2212 K k=1 p 3 k )/12. Next we look at the fully adjusted estimator. The efficient influence function of \u03c8 was given in, for example, Mao [18] ,",
            "cite_spans": [
                {
                    "start": 284,
                    "end": 288,
                    "text": "[18]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "which, under the sharp null, simplifies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "where we use the independence between A and (Y t , W t ) under the sharp null.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Finally we consider the estimator based on proportional odds model. LetP a be a distribution with CDFF a (k) for a = 0, 1. Recall that\u03c8 m = h(x, y)dP 1 (x)dP 0 (y). SinceF 's are asymptotically linear, we hav\u00ea",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "The first term can be alternatively written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": ", similarly for the second term. Then Lemma 2 implies that\u03c8 m is asymptotically linear with influence function D m (y t , a t , w t ) =",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "which, under the sharp null, simplifies to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "The variance of D m under the sharp null is \u03c3 2 m /(\u03c0 1 \u03c0 0 ), where we used the independence between A and (Y t , W t ) under the sharp null. Lemma 4 follows by the definition of relative efficiency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Proof of Theorem 3. We first study\u03c3 2 m based on estimating equations. The proof is similar to that of Theorem 2 except that we need to estimate the marginal distribution of Y in addition. We use the sample proportionp k , which is asymptotically linear with influence function IF p k (y, w) = I{y = k} \u2212 p k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Consider the following estimating equation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "By definition, \u03c3 2 m is the unique solution to the equation P U MW (\u03b1 * , \u03b2 * , p, \u03c3 2 ) = 0, and\u03c3 2 m solves its empirical counterpart. Hence, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Consistency of (\u03b1,\u03b2) andp implies that\u03c3 2 m is also consistent. This together with the bounded sup-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "show that U MW is a Lipschitz function of (\u03b1, \u03b2,p, \u03c3 2 ) in a neighborhood of (\u03b1 * , \u03b2 * , p, \u03c3 2 m ), and thus [31] implies that the last term in the above display is o P (n \u22121/2 ).",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 116,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Applying a Taylor expansion, we have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We note that using similar arguments as Lemma 10, we can show that IF m is the canonical gradient of We estimate the unadjusted variance by plugging inp k . By the delta method,\u03c3 2 u is asymptotically linear with influence function IF u = \u2212 K k=1 p 2 k IF p k /4. Next we establish the asymptotic linearity of\u03c3 2 a . Applying Lemma 8 with h(\u1ef9, y) = I{\u1ef9 < y}+ 1 2 {\u1ef9 = y}, we obtain the EIF of \u03c3 2 a as follows",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We show through direct linearization that\u03c3 2 a is indeed asymptotically linear with influence function IF a .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "To start, we note that\u03c3 2 a = P n (\u03b7 \u2212r) 2 and \u03c3 2 a = P (\u03b7 \u2212 r) 2 . Thus,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We analyze term 2 first.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "The first term on the right-hand side is already linear and contributes the first term to the influence function. By the assumption that r \u2212 r L 2 (PW ) = o P (n \u22121/4 ), the third term is negligible because",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We now turn to the second term on the right-hand side of (23). By Theorem 2.10.6 in Van Der Vaart and Wellner [32] , the fact thatr belongs to a bounded P -Donsker class with probability tending to 1 implies that (\u03b7 \u2212r) 2 \u2212 (\u03b7 \u2212 r) 2 also belongs to a P -Donsker class with probability tending to 1, as \u03b7 and r are both fixed and bounded functions. Furthermore,",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 114,
                    "text": "[32]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "for some constant M . Lemma 19.24 in Van der Vaart [31] implies that the second term is also o P (n \u22121/2 ).",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 33,
                    "text": "Lemma 19.24",
                    "ref_id": null
                },
                {
                    "start": 51,
                    "end": 55,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We now analyze term 1. Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We note that\u03b7 belongs to a bounded P -Donsker class, as the empirical distribution function lies in the closure of the convex hull of a P -Donsker class. Theorem 2.10.6 in Van Der Vaart and Wellner [32] again implies that (\u03b7 \u2212r) 2 \u2212 (\u03b7 \u2212r) 2 belongs to a P -Donsker class with probability tending to 1. Moreover,",
            "cite_spans": [
                {
                    "start": 198,
                    "end": 202,
                    "text": "[32]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Thus the third term is o P (n \u22121/2 ). The second term is also o P (n \u22121/2 ) by our assumption on the convergence rate ofr and the convergence of\u03b7. Finally, the first term is the linear term that contributes to the rest of IF a . To see this, we write it in the integral form.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Note that Lemma 8 implies that P {\u03b7(\u1ef9) \u2212 r(w)}h(\u00b7,\u1ef9)dP (\u1ef9,w) = \u03c3 2 a .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Theorem 3 now follows by applying the delta method. Specifically, the influence function of\u03c6 a is given by (IF a \u2212 \u03c6 a IF u )/\u03c3 2 u , and similarly the influence function of\u03c6 m is (IF m \u2212 \u03c6 m IF u )/\u03c3 2 u . These estimators are RAL in any locally nonparametric model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Proof of Lemma 5. Recall that the unadjusted estimator is given b\u0177",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Applying the delta method shows that its influence function is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "which, under the sharp null, simplifies to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Due to the independence of A and (Y t , W t ) under the sharp null, the variance of D u (Y t , A, W t ) under the sharp null is \u03c3 2 u /(\u03c0 1 \u03c0 0 ). The working-model-based adjusted estimator\u03c8 m replacesF a withF a . By Lemma 2 and the delta method, the influence function of\u03c8 m is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "which under the sharp null becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "The variance of D m (Y t , A, W t ) under the sharp null is \u03c3 2 m /(\u03c0 1 \u03c0 0 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Finally the efficient influence function can be obtained by projecting D u onto the tangent space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Under the sharp null, it simplifies to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "The variance of D * (Y t , A, W t ) under the sharp null is \u03c3 2 a /(\u03c0 1 \u03c0 0 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Proof of Theorem 4. We first consider estimating \u03c3 2 u . Define the following estimating equation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Then, \u03c3 2 u is the unique solution in \u03c3 2 to the equation P U LOR,u (F , \u03c3 2 ) = 0; and\u03c3 2 u solves P n U LOR,u (F , \u03c3 2 ) = 0, withF being the CDF of the empirical distribution of Y . We can apply similar estimating equation arguments as in the proof of Theorems 2 and 3 to show that\u03c3 2 u \u2212 \u03c3 2 u = (P n \u2212 P )IF u + o P (n \u22121/2 ), where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "This implies that\u03c3 2 u is asymptotically linear with influence function IF u . In particular,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Next we consider the estimation of \u03c3 2 m . Define the following estimating equation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Then, \u03c3 2 m is the solution in \u03c3 2 to P U LOR (\u03b1 * , \u03b2 * , F , \u03c3 2 ) = 0, while\u03c3 2 m is the solution to P n U LOR (\u03b1,\u03b2,F , \u03c3 2 ) = 0. We can again apply estimating equation arguments as in the proof of Theorems 2 and 3 to show that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "This implies that\u03c3 2 m is asymptotically linear with influence function IF m . Here,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Finally we consider the estimation of \u03c3 2 a . We define \u03c3 kl := E P [cov(I{Y \u2264 k}, I{Y \u2264 l}|W )]. Then \u03c3 2 a can be equivalently written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We estimate F (k) withF (k), which is an asymptotically linear estimator. By Lemma 9, the EIF of \u03c3 kl is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We show that the estimator\u03c3",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "is asymptotically linear with influence function D kl . For the ease of notation, we use I k as shorthand for the function y \u2192 I{y \u2264 k}, \u03b8 k for the function w \u2192 \u03b8(k, w) and\u03b8 k for the function w \u2192\u03b8(k, w), for k \u2208 {1, . . . , K \u2212 1}. Then we have that\u03c3 kl = P n {(I k \u2212\u03b8 k )(I l \u2212\u03b8 l )} and \u03c3 kl = P {(I k \u2212 \u03b8 k )(I l \u2212 \u03b8 l )}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "The first term is exactly (P n \u2212 P )D kl . For the second term, we note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Now we turn to the third term. By Theorem 2.10.6 in Van Der Vaart and Wellner [32] , the fact that\u03b8 k and\u03b8 l belong to fixed P -Donsker classes with probability tending to 1 implies that (I k \u2212\u03b8 k )(I l \u2212\u03b8 l ) \u2212 (I k \u2212 \u03b8 k )(I l \u2212 \u03b8 l ) also belongs to a fixed P-Donsker with probability tending to 1, as the support of W is bounded and I k , I l , \u03b8 k and \u03b8 l are all fixed and bounded functions. In addition, (I k \u2212\u03b8 k )(I l \u2212\u03b8 l ) \u2212 (I k \u2212 \u03b8 k )(I l \u2212 \u03b8 l ) L 2 (P ) = o P (1). Thus, Lemma 19.24 implies that the third term is o P (n \u22121/2 ). Henc\u00ea \u03c3 kl has influence function D kl .",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 82,
                    "text": "[32]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Applying the delta method, we see that\u03c3 2 a is asymptotically linear with influence function ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We study term 2 first. By the Lipschitz property of h, term 2 is bounded by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We now show that E M \u221a n \u03c3 2 (P * n ) \u2212 \u03c3 2 (P * n ) P \u2192 0. Recall the asymptotic linear expansion (12) ,",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 103,
                    "text": "(12)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "It then follows that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We note that N var P * n (Rem) = N 1\u22122\u03b3 var P * n (N \u03b3 Rem). Therefore, Condition B2 implies that E N var P * n (Rem) \u2264 LN 1\u22122\u03b3 for some constant L. In addition, the boundedness of the support of X implies that var P * n [D P * n \u03a0 (X)] is also bounded. Thus, by the Cauchy Schwartz inequality and Jensen's inequality,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "for some constant L 1 . As a result, we have that E \u221a n \u03c3 2 (P * n ) \u2212 \u03c3 2 (P * n ) \u2264 \u221a L 2 nN 1\u22122\u03b3 for some con-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "where the outer expectation is over X 1 , X 2 , . . . Markov's inequality and Condition B4 then imply that, for all \u01eb > 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We now turn to term 1, which further decomposes into sup h\u2208BL1(R)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Theorem 23.7 and Equation 23.8 in Van der Vaart [31] imply that term 1.2 converges to 0 in probability. For term 1.1, the same argument as was used in the proof of Theorem 23.9 in Van der Vaart [31] shows that this term converges to 0 in probability.",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 52,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 194,
                    "end": 198,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Combining these steps as in the proof of Theorem 23.9 in Van der Vaart [31] , we see that \u221a n{\u03c3 2 (P * n )\u2212 \u03c3 2 (P n )} converges conditionally in distribution to (\u03c3 2 ) \u2032 (G), given X 1 , X 2 , . . ., in probability.",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "In particular, we can apply the above argument to the variance of the unadjusted estimator \u03c3 2 u and the variance of the working-model-based estimator \u03c3 2 m to show that (i) \u221a n{\u03c3 2 u (P * n ) \u2212 \u03c3 2 u (P n )} converges conditionally in distribution to (\u03c3 2 u ) \u2032 (G), and (ii) \u221a n{\u03c3 2 m (P * n ) \u2212 \u03c3 2 m (P n )} converges conditionally in distribution to (\u03c3 2 m ) \u2032 (G), both given X 1 , X 2 , . . ., in probability. The delta method implies that \u221a n{\u03c3 2 m (P * n )/\u03c3 2 u (P * n ) \u2212 \u03c6 m (P n )} converges conditionally in distribution to (\u03c6 m ) \u2032 (G), given X 1 , X 2 , . . ., in probability. Finally, Theorem 3.10 follows by Condition B3 and Slutsky's theorem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "C Proofs of results for time-to-event outcomes with right censoring All three estimands of the treatment effect we consider are transformations of the arm-specific survival functions S 1 (t) and S 0 (t). Recall that for the unadjusted analysis we plug inS 1 (t) andS 0 (t), the armspecific KM estimators, and, for the adjusted analysis, we plug in\u015c 1 (t) and\u015c 0 (t), the efficient adjusted estimators for the arm-specific survival function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "The influence function of the KM estimator was derived, for example, in Reid [25] . In particular, if",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 81,
                    "text": "[25]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "T t \u22a5 C t |A, thenS a (t k ) is an asymptotically linear estimator of S a (t k ) with influence function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "for a = 0 or 1, and k \u2208 {1, . . . , K}. Here h a (t) is the hazard corresponding to S a at time t, G a (t, w) := P (C t \u2265 t|A = a, W t = w) and G a (t) := P (C t \u2265 t|A = a).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Under the assumption that T t \u22a5 C t |(A, W t ) and other regularity conditions,\u015c a (t k ) is asymptotically linear with influence function given in Moore and van der Laan [20] \u03bb a,k (y t , \u03b4 t , a t ,",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 175,
                    "text": "[20]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "for a = 0 or 1, and k \u2208 {1, . . . , K}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "C.1 Supporting lemmas for proofs in Section C.2 Lemma 11 . (EIF of the variance of the fully-adjusted estimators) For (k, l) \u2208 {1, . . . , K} 2 and j \u2208 {1, . . . , min(k, l)}, let D kl j be defined as in (25) . When we measure the treatment effect with the risk",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 56,
                    "text": "Lemma 11",
                    "ref_id": null
                },
                {
                    "start": 204,
                    "end": 208,
                    "text": "(25)",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "When we use restricted mean survival time Proof. Recall that we define f kl j (\u00b7) as f kl j (w) = S(t k , w)S(t l , w)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "When we wish to emphasize the dependence of f kl j on P through its survival function, we instead denote this function by f kl j,P . First we define a parameter \u03c3 kl j : M \u2192 R such that \u03c3 kl j (P ) :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We consider the efficient influence function of \u03c3 kl j in the full data model, that is, the model where we observe (T, W ) and there is no censoring. Let p(t, w) = p(t|w)p(w) be the density of the joint distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "We consider the one-dimensional submodel [28] to show that the following is an observed data influence function of \u03c3 kl j :",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 45,
                    "text": "[28]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Moreover, as our observed data model is locally nonparametric, D kl j is the efficient observed data influence function of \u03c3 kl j . Lemma 11 then follows since the variances are linear combinations of E[f kl j (W )/G(t j , W )].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Lemma 12 (Computation time of\u03c3 2 a with RMST). For given\u03c4 and\u015c, the function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "can be computed in O(k) time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Proof. There are 5 terms inside the sums, and we show that the sum of each term can be computed in",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "To start, we note that (t j , w)\u015c(t l , w) 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "By taking a cumulative sum, { l\u2265u\u015c (t l , w)} k u=1 can be pre-computed in O(k) time. Thus, the above display can be computed in O(k) time as we sum over u.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "The terms in g are of two types. The first of these is k j=1 k l=1 min(j,l)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "By taking a cumulative sum, ( l\u2265u\u03c4 l ) k u=1 can be pre-computed in O(k) time and summing over u takes another O(k) steps. The second type of term is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Again with the sum over j and l pre-computed for all u, the summation over u takes O(k) time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 10 implies that IF"
        },
        {
            "text": "Proof of Lemma 6. First we note that the (conditional) independencies A \u22a5 W t , T t \u22a5 A|W t , C t \u22a5 T t |A and C t \u22a5 T t |(A, W t ) together imply that T t \u22a5 C t and T t \u22a5 C t |W t . This result will be useful when we compute the variances of the adjusted and unadjusted estimators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "We consider the risk difference first. The unadjusted estimator, namely\u03c8 u =S 0 (t k ) \u2212S 1 (t k ), has influence function D u,k = \u03b7 0,k \u2212 \u03b7 1,k . Under the sharp null where the treatment has no effect and the assumption that C t \u22a5 A|W t , the influence function simplifies to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Noting that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "The adjusted estimator, namely\u03c8 a =\u015c 0 (t k ) \u2212\u015c 1 (t k ), has influence function D a,k = \u03bb 0,k \u2212 \u03bb 1,k . Under the sharp null and the assumption that C t \u22a5 A|W t , the influence function becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "The variance of D a,k can be calculated in a similar way as was done for the unadjusted estimator, except that in taking expectation of the indicators, we condition on W t first.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "var(D a,k ) = 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Hence the relative efficiency is given by \u03c3 2 a /\u03c3 2 u , which depends only on the distribution of survival time and the covariate, for a user-specified mapping G.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Next we consider the relative risk. The unadjusted estimator, namely\u03c8 u = 1\u2212S1(t k ) 1\u2212S0(t k ) , has influence function 1 1\u2212S0(t k ) (\u2212\u03b7 1,k + \u03c8\u03b7 0,k ), which becomes D u,k /{1 \u2212 S(t k )} under the sharp null. Similarly, the influence function of the adjusted estimator\u03c8 a = 1\u2212\u015c1(t k ) 1\u2212\u015c0(t k ) simplifies to D a,k /{1 \u2212 S(t k )} under the sharp null and the assumption that C t \u22a5 A|W t . Hence the relative efficiency is again \u03c3 2 a /\u03c3 2 u .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Proof of Theorem 6. Recall that\u015c(t k ) is the efficient adjusted estimator of the survival probability at time t k using the external data. Hence, this estimator is asymptotically linear with influence function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "where \u03c4 is defined in (18) . Furthermore, define IF 0 (y, \u03b4, w) := 0. Recall that",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 26,
                    "text": "(18)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Applying the delta method, we have that\u015d jl u is an asymptotic linear estimator of s jl u with influence function \u03be jl u (y, \u03b4, w) =",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Also,\u03c3 2 u is a linear combination of\u015d kk j , and its influence function is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "We now consider estimating \u03c3 2 a . Its efficient influence function is derived in Lemma 11, and also given in (19) , which is of the form IF a = k j=1 D kk j . The proposed\u03c3 2 a is a one-step estimator based on the EIF.",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 114,
                    "text": "(19)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Let Q be the distribution of the observed data unit (Y, \u2206, W ) in the external dataset, induced by the joint distribution P of (T, W ), the conditional distribution of the censoring time H and the function \u0393. LetP be a joint distribution of (T, W ) such that the condtional hazard function is given by\u0125(t, w), the conditional survival function is given by\u015c(t, w) and the distribution of W is given by its empirical distribution. LetQ be the observed data distribution induced byP ,\u0124 and \u0393. Then, we have that \u03c3 2 a = Q n IF a (Q) + \u03c3 2 a (P ), where Q n is the empirical distribution of (Y, \u2206, W ) in the external dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Hence,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Our first step is to show that the remainder term R(Q, Q) is o P (n \u22121/2 ). As the influence function IF a and the variance \u03c3 2 a itself can both be written as a sum of k terms, it is easy to see that we can write",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Here we omit the superscript \"kk\" in f and g. We examine the terms coming from f and g separately.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "First, we study",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": ".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Then, we apply a second-order Taylor expansion to the function (x, y, z) \u2192 x 2 (1/y \u2212 1/z). The relevant second-order derivatives are bounded by some constant M when\u015c(t, w), S(t, w) and G(t, w) are all uniformly bounded away from 0. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Next, we study the terms in the remainder resulting from g. To do this, we define \u03b4 l h (w) := S(t l\u22121 , w){h(t l , w) \u2212\u0125(t l , w)}/\u015c(t l , w). Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "We first establish condition (1) .",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 32,
                    "text": "(1)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Using the triangle inequality, it suffices to show that for each j, {\u011d j \u2212 g j }/G(t j , \u00b7) L 2 (Q) = o P (1) and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "{f j \u2212 f j }/G(t j , \u00b7) L 2 (Q) = o P (1), and that \u03c3 2 a (Q) \u2212 \u03c3 2 a (Q) = o P (1). As G is uniformly bounded away from 0, it suffices to show that \u011d j \u2212 g j L 2 (Q) = o P (1) and f j \u2212 f j L 2 (Q) = o P (1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Recall that when studying the remainder term, we applied a second-order Taylor expansion to the function (x, y, z) \u2192 x 2 (1/y \u2212 1/z). Here a first-order Taylor expansion suffices. As S is bounded away from 0, there exists some constant M such that the first derivatives are bounded by M . Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "We note that\u011d j \u2212 g j consists of three terms that are of similar forms. We study one of them in details and similar arguments apply to the other two, and we can then apply the triangle inequality to conclude that \u011d j \u2212 g j L 2 (Q) = o P (1). For notational convenience, we defin\u00ea",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "S(t l , w)\u0124(t l , w) I{y = t l , \u03b4 = 1} \u2212\u0125(t l , w)I{y \u2265 t l } .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "We focus on the term in\u011d j \u2212 g j that is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "The triangle inequality allows us to bound each term separately. Since\u015c and\u0124 are uniformly bounded away from 0 and\u015c and\u0125 are uniformly bounded above, there exists some constant M such that |\u03c4 k | \u2264 M .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Therefore, in term 1 it suffices to upper bound 2\u015c(t k ,\u00b7) S(tj ,\u00b7) \u2212 2\u015c(t k ,\u00b7) S(tj\u22121,\u00b7) \u2212 2S(t k ,\u00b7) S(tj ,\u00b7) + 2S(t k ,\u00b7) S(tj\u22121,\u00b7) L 2 (PW ) . As in our analysis of f j \u2212 f j L 2 (PW ) , we apply a first-order Taylor expansion, where the first order derivatives are bounded by some constant M \u2032 . Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Thus, term 1 is indeed o P (1). As S is uniformly bounded away from 0, 2S(t k ,w) S(tj ,w) \u2212 2S(t k ,w) S(tj\u22121,w) is bounded above. Therefore, it suffices to show that \u03c4 k \u2212 \u03c4 k L 2 (Q) = o P (1). Note that \u03c4 k (y, \u03b4, w) \u2212 \u03c4 k (y, \u03b4, w)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "To see that Term 2.1 is o P (1), note that the first factor is bounded above and \u0125 (t, \u00b7) \u2212 h(t, \u00b7) L 2 (PW ) = o P (1). To see that Term 2.2 is o P (1), note that the second factor is bounded above and the first factor is o P (1), which can be shown again by a Taylor expansion. This argument shows one of the three terms in\u011d j \u2212 g j is o P (1). A similar argument can be applied to show the other two terms are o P (1) as well.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Finally we show that \u03c3 2 a (Q) \u2212 \u03c3 2 a (Q) = o P (1). Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "The term (Q n \u2212 Q)f j is o P (1) by the law of large numbers. We have shown that f j \u2212 f j L 2 (Q) is o P (1), which provided an upper bound on Q(f j \u2212 f j ). As we will show momentarily,f j lies in a Q-Donsker class with probability tending to 1, so Lemma 19.24 in Van der Vaart [31] implies that (Q n \u2212 Q)(f j \u2212 f j ) = o P (1). Combining these results, we have \u03c3 2 a (Q) \u2212 \u03c3 2 a (Q) = o P (1).",
            "cite_spans": [
                {
                    "start": 280,
                    "end": 284,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Next we establish condition (2), which says that IF a (Q) lies in a Q-Donsker class with probability tending to 1. By Theorem 2.10.6 in Van Der Vaart and Wellner [32] , it suffices to show that\u011d j andf j both lie in Q-Donsker classes with probability tending to 1, since G is bounded away from 0. This can be shown again by Theorem 2.10.6 in Van Der Vaart and Wellner [32] , as by assumption\u015c,\u0124 and\u0125 all belong to fixed Q-Donsker classes with probability tending to 1 and all the functions involved in\u011d j and f j are uniformly bounded away from 0 and also bounded above.",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 166,
                    "text": "[32]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 368,
                    "end": 372,
                    "text": "[32]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Conditions (1) and (2) allows us to apply Lemma 19.24 in Van der Vaart [31] to conclude that (Q n \u2212 Q){IF a (Q) \u2212 IF a (Q)} is o P (n \u22121/2 ). Now, combining step 1, which showed that R(Q, Q) is o P (n \u22121/2 ), and step 2, which showed that (Q n \u2212 Q){IF a (Q) \u2212 IF a (Q)} is o P (n \u22121/2 ), we see that\u03c3 2 a is asymptotically linear with influence function IF a . Theorem 6 then follows by the delta method. Specifically, the influence function of\u03c6 is given by (IF a \u2212 \u03c6 a IF u )/\u03c3 2 u . This estimator is efficient as its influence function agrees with the EIF of \u03c6 a .",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Proof of Lemma 7. The unadjusted estimator, namely\u03c8 u = k j=1 {S 1 (t j ) \u2212S 0 (t j )}, has influence function k j=1 (\u03b7 1,j \u2212 \u03b7 0,j ). Under the sharp null, the form of the influence function simplifies to k j=1 D u,j , where the definition of D u,j is given in (26) . Since Now we consider the fully adjusted estimator\u03c8 a = k j=1 {\u015c 1 (t j ) \u2212\u015c 0 (t j )}. Under the sharp null, the influence function of\u03c8 a is given by k j=1 D a,j , where D a,j is defined in (27) . As in the proof of Lemma 6, the variance of k j=1 D a,j can be derived in the same way, except that we condition on W first when taking expectations. Moreover,\u03c3 2 a is again a one-step estimator based on its EIF given in Lemma 11. Using the same approach as was used in the proof of Theorem 6, it can be shown that the remainder term R(Q, Q) is o P (n \u22121/2 ), and (Q n \u2212 Q){IF a (Q) \u2212 IF a (Q)} = o P (n \u22121/2 ). Due to their close similarity to earlier arguments, we omit the details here. We can then conclude that\u03c3 2 a has influence function IF a = k j=1 k l=1 min(j,l) u=1",
            "cite_spans": [
                {
                    "start": 262,
                    "end": 266,
                    "text": "(26)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 460,
                    "end": 464,
                    "text": "(27)",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "where the definition of D jl u is given in (25) in Lemma B.1.",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 47,
                    "text": "(25)",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Applying the delta method, the influence function of\u03c6 is given by (IF a \u2212 \u03c6 a IF u )/\u03c3 2 u . This estimator is efficient as its influence function agrees with the EIF of \u03c6 a .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "Proof of Theorem 8. We prove the first claim by showing that E P f jl u (W )/G(t u , W ) \u2265 s jl u /G(t u ), for all (u, j, l) such that max{j, l} \u2264 k and u \u2264 min(j, l). To start, we note that T \u22a5 W under P implies that f jl u (w) = s jl u for all w \u2208 W. Thus, it suffices to show that E P [1/G(t u , W )] \u2265 1/G(t u ), which follows from the convexity of the function a \u2192 1/a for a > 0 and Jensen's inequality. Strict inequality holds when var P [G(t u , W )] > 0 for some u.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "To prove the second claim, we note that C \u22a5 W implies that G(t j , w) = G(t j ) for all w \u2208 W and j \u2208 {1, . . . , k}. We focus on the case of RD and RR first. Consider a bivariate function (a, b) \u2192 a 2 /b for (a, b) \u2208 (0, 1) 2 . The Hessian matrix is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "\uf8fb with eigenvalues 0 and 2(a 2 + b 2 )/b 3 , both of which are non-negative. Therefore, this function is convex for (a, b) \u2208 (0, 1) 2 . We note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "where the inequality follows from Jensen's inequality on the function (a, b) \u2192 a 2 /b for (a, b) \u2208 (0, 1) 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "For the case of RMST, we consider a (q + 1)-variate function that maps (a 1 , . . . , a q , b) to ( q i=1 a i ) 2 /b for a i \u2208 (0, 1) and b \u2208 (0, 1). The only non-zero eigenvalue of its Hessian matrix is 2{( q i=1 a i ) 2 +qb 2 }/b 3 , which is positive. Therefore, this function is convex for any q \u2265 1. In the following argument, we will take q \u2208 {1, . . . , k}. where the inequality follows from Jensen's inequality. In this section, we present additional simulation results for sample sizes n = 200 and n = 500. The simulation set-up is otherwise the same as described in Section 5. The results for ordinal outcomes are presented in Table 6 , and the results for survival outcomes are presented in Table 7 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 637,
                    "end": 644,
                    "text": "Table 6",
                    "ref_id": "TABREF10"
                },
                {
                    "start": 702,
                    "end": 709,
                    "text": "Table 7",
                    "ref_id": null
                }
            ],
            "section": "C.2 Results in Appendix A"
        },
        {
            "text": "In this section, we present additional results when applying our proposed methods to the Covid-19 dataset with ordinal outcomes. In particular, we estimate the efficiency gain from using the fully adjusted and working-model-based estimators that adjust for one of the covariates, for estimating three treatment effect estimands: DIM (Table 8) , MW (Table 9 ) and LOR (Table 10 ). Table 7 : Simulation results for survival outcomes. We only consider the analytical approach and consider relative efficiency of fully adjusted estimators for RD at time 1, 2 and 3 (the relative efficiency is the same for RR) and RMST at time 3. Sample size is 200 or 500, and results are based on 1000 replications. We split the external data into two subsamples of size n/2, denoted by D 1 and D 2 . Let\u03c3 2 a,1 be the proposed estimator of the adjusted variance, calculated from observations in D 1 only; and let\u03c3 2 u,2 be the proposed estimator for the unadjusted variance using D 2 . By the asymptotic linearity of these estimators, we have that",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 333,
                    "end": 342,
                    "text": "(Table 8)",
                    "ref_id": "TABREF11"
                },
                {
                    "start": 348,
                    "end": 356,
                    "text": "(Table 9",
                    "ref_id": "TABREF12"
                },
                {
                    "start": 367,
                    "end": 376,
                    "text": "(Table 10",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 380,
                    "end": 387,
                    "text": "Table 7",
                    "ref_id": null
                }
            ],
            "section": "D.2 Application to Covid-19 data: ordinal outcomes"
        },
        {
            "text": "Moreover, as\u03c3 2 a,1 and\u03c3 2 u,2 are based on different observations, they are independent. Therefore, delta method implies that \u221a n \u03c3 2 a,1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2 Application to Covid-19 data: ordinal outcomes"
        },
        {
            "text": "A Wald test can be used to test the hypothesis H 0 : \u03c6 a = 1, and in fact a Wald confidence interval can also be constructed directly although it might be wider than the one obtained from the proposed two-step procedure. The same argument applies when we consider the working-model-based variance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D.2 Application to Covid-19 data: ordinal outcomes"
        },
        {
            "text": "The sample splitting approach was also used in Williamson and Feng [35] to test a null hypothesis that lies on the boundary of the parameter space.",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 71,
                    "text": "[35]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "D.2 Application to Covid-19 data: ordinal outcomes"
        },
        {
            "text": "Recall that the confidence set, which we denote as I ts , is constructed using a two-step procedure. We first test the null hypothesis H 0 : \u03c6 = 1 using a level \u03b1 test. If it is rejected, we take the confidence set to be I wald , the Wald confidence interval; otherwise the confidence set is taken to be I wald \u222a {1}. We argue that the asymptotic coverage of this confidence set is at least 1 \u2212 \u03b1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2 Asymptotic coverage of the confidence set"
        },
        {
            "text": "First, consider the case where \u03c6 = 1 under P . This implies that the influence function of\u03c6 without sample splitting is not identically 0. Hence, asymptotic linearity in the form of (2) implies that P (\u03c6 \u2208",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E.2 Asymptotic coverage of the confidence set"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "I wald ) \u2192 1 \u2212 \u03b1. In addition, we have that P (\u03c6 \u2208 I wald ) \u2264 P (\u03c6 \u2208 I ts ). Next, consider P such that \u03c6 = 1",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "+ P (\u03c6 \u2208 I ts | do not reject H 0 )P ( do not reject H 0 )",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "B 2 do 2: Resample X k of size N from X with replacement 3: Randomly assign treatment to formX k 4: Compute\u03c8 u,k =\u03c8 u (X k ),\u03c8 m,k =\u03c8 m (X k ) 5: end for 6: Let\u03c6(X) = var(\u03c8 m,k )/var(\u03c8 u,k ) 7: for i = 1, 2",
            "authors": [
                {
                    "first": ".",
                    "middle": [],
                    "last": "+ P ; 2",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "\u2208 I wald \u222a {1}| do not reject H 0 )P ( do not reject H 0 ) bootstrap procedure Input: external data X, treatment effect estimators\u03c8 u and\u03c8 m Output: Estimate of and confidence interval for the relative efficiency 1: for k = 1",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Neural network learning: Theoretical foundations",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Anthony",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "L"
                    ],
                    "last": "Bartlett",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "A substantial and confusing variation exists in handling of baseline covariates in randomized controlled trials: a review of trials published in leading medical journals",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "C"
                    ],
                    "last": "Austin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Manca",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zwarenstein",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "N"
                    ],
                    "last": "Juurlink",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "B"
                    ],
                    "last": "Stanbrook",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Journal of clinical epidemiology",
            "volume": "63",
            "issn": "2",
            "pages": "142--153",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Improving precision and power in randomized trials for covid-19 treatments using covariate adjustment, for binary, ordinal, and time-to-event outcomes",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Benkeser",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Diaz",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Luedtke",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Segal",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Scharfstein",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rosenblum",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Severe outcomes among patients with coronavirus disease 2019 (COVID-19)-United States",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bialek",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Boundy",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Bowen",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Chow",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cohn",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Dowling",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ellington",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Morbidity and mortality weekly report",
            "volume": "69",
            "issn": "12",
            "pages": "343--346",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "On adaptive estimation. The Annals of Statistics",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Bickel",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "647--671",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Efficient and adaptive estimation for semiparametric models",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Bickel",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "A"
                    ],
                    "last": "Klaassen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Bickel",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ritov",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Klaassen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Wellner",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ritov",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "",
            "volume": "4",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Leveraging prognostic baseline variables to gain precision in randomized trials",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Colantuoni",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rosenblum",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Statistics in medicine",
            "volume": "34",
            "issn": "18",
            "pages": "2602--2617",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Improved precision in the analysis of randomized trials with survival outcomes, without assuming proportional hazards",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "D\u00edaz",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Colantuoni",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "F"
                    ],
                    "last": "Hanley",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rosenblum",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Lifetime data analysis",
            "volume": "25",
            "issn": "3",
            "pages": "439--468",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Enhanced precision in the analysis of randomized trials with ordinal outcomes",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "D\u00edaz",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Colantuoni",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rosenblum",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Biometrics",
            "volume": "72",
            "issn": "2",
            "pages": "422--431",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Adjusting for covariates in randomized clinical trials for drugs and biologics with continuous outcomes. draft guidance for industry",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Fda",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Regularization paths for generalized linear models via coordinate descent",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Journal of Statistical Software",
            "volume": "33",
            "issn": "1",
            "pages": "1--22",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Coarsening at random: Characterizations, conjectures, counter-examples",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "D"
                    ],
                    "last": "Gill",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Van Der Laan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Robins",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Proceedings of the First Seattle Symposium in Biostatistics",
            "volume": "",
            "issn": "",
            "pages": "255--294",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Ignorability and coarse data. The annals of statistics",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "F"
                    ],
                    "last": "Heitjan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "B"
                    ],
                    "last": "Rubin",
                    "suffix": ""
                }
            ],
            "year": 1991,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2244--2253",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "On differentiability of implicitly defined function in semi-parametric profile likelihood estimation",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hirose",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Bernoulli",
            "volume": "22",
            "issn": "1",
            "pages": "589--614",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Statistical estimation: asymptotic theory",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Ibragimov",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Has&apos;minskii",
                    "suffix": ""
                }
            ],
            "year": 1981,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "The risks and rewards of covariate adjustment in randomized trials: an assessment of 12 outcomes from 8 studies",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "C"
                    ],
                    "last": "Kahan",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Jairath",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "J"
                    ],
                    "last": "Dor\u00e9",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "P"
                    ],
                    "last": "Morris",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Trials",
            "volume": "15",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Nonparametric estimation from incomplete observations",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "L"
                    ],
                    "last": "Kaplan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Meier",
                    "suffix": ""
                }
            ],
            "year": 1958,
            "venue": "Journal of the American statistical association",
            "volume": "53",
            "issn": "282",
            "pages": "457--481",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "On causal estimation using-statistics",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mao",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Biometrika",
            "volume": "105",
            "issn": "1",
            "pages": "215--220",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Regression models for ordinal data",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Mccullagh",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
            "volume": "42",
            "issn": "2",
            "pages": "109--127",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Application of time-to-event methods in the assessment of safety in clinical trials. Design and Analysis of Clinical Trials with Time-to-Event Endpoints",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Moore",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Van Der Laan",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "455--482",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Robust extraction of covariate information to improve estimation efficiency in randomized trials",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "L"
                    ],
                    "last": "Moore",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Neugebauer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Valappil",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Van Der Laan",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Statistics in medicine",
            "volume": "30",
            "issn": "19",
            "pages": "2389--2408",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Increasing power in randomized trials with right censored outcomes through covariate adjustment",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "L"
                    ],
                    "last": "Moore",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Van Der Laan",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Journal of biopharmaceutical statistics",
            "volume": "19",
            "issn": "6",
            "pages": "1099--1131",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Estimation in semiparametric models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pfanzagl",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Estimation in Semiparametric Models",
            "volume": "",
            "issn": "",
            "pages": "17--22",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Contributions to a general asymptotic statistical theory",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pfanzagl",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wefelmeyer",
                    "suffix": ""
                }
            ],
            "year": 1985,
            "venue": "Statistics & Risk Modeling",
            "volume": "3",
            "issn": "3-4",
            "pages": "379--388",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Influence functions for censored data",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Reid",
                    "suffix": ""
                }
            ],
            "year": 1981,
            "venue": "The Annals of Statistics",
            "volume": "",
            "issn": "",
            "pages": "78--92",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Improving precision by adjusting for prognostic baseline variables in randomized trials with binary outcomes, without regression model assumptions",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Steingrimsson",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "F"
                    ],
                    "last": "Hanley",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rosenblum",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Contemporary clinical trials",
            "volume": "54",
            "issn": "",
            "pages": "18--24",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "A general implementation of tmle for longitudinal data applied to causal inference in survival analysis",
            "authors": [
                {
                    "first": "O",
                    "middle": [
                        "M"
                    ],
                    "last": "Stitelman",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "De"
                    ],
                    "last": "Gruttola",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Van Der Laan",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "The international journal of biostatistics",
            "volume": "8",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Semiparametric theory and missing data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Tsiatis",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Unified methods for censored longitudinal data and causality",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Van Der Laan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Laan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Robins",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Targeted maximum likelihood learning",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Van Der Laan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rubin",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "The international journal of biostatistics",
            "volume": "2",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Asymptotic statistics",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "W"
                    ],
                    "last": "Van Der Vaart",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Weak convergence. In Weak convergence and empirical processes",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Van Der",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "W"
                    ],
                    "last": "Vaart",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Wellner",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "16--28",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Increasing the power of the mann-whitney test in randomized experiments through flexible covariate adjustment",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Vermeulen",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Thas",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vansteelandt",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Statistics in medicine",
            "volume": "34",
            "issn": "6",
            "pages": "1012--1030",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Analysis of covariance in randomized trials: More precision and valid confidence intervals, without model assumptions",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "L"
                    ],
                    "last": "Ogburn",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rosenblum",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Biometrics",
            "volume": "75",
            "issn": "4",
            "pages": "1391--1400",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Efficient nonparametric statistical inference on population feature importance using shapley values",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "D"
                    ],
                    "last": "Williamson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.09481"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "(P ) := {f : E P [f (X)] = 0, var P [f (X)] < \u221e}. Let M denote a statistical model, that is, a collection of distributions of X. We suppose that M contains P . Let M(P ) denote the collection of all one-dimensional submodels {P \u01eb : \u01eb \u2208 R} \u2286 M that are quadratic mean differentiable [31] at \u01eb = 0 and are such that P \u01eb=0 = P . Let S M (P ) denote the collection of all functions s : X \u2192 R for which s is a score function at \u01eb = 0 for some submodel contained in M(P ), and let T M (P ) denote the L 2 0 (P )-closure of the linear span of S M (P ). The subspace T M (P ) of L 2 0 (P )",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Let (Y, W ) \u223c P . Suppose that the appropriate regularity conditions hold such that the AIPW estimator\u03c6 a is efficient. Then, for the above\u03c8 u ,\u03c8 a and\u03c8 m , we have that \u03c3 2 u (P ) = var P [u(Y )], \u03c3 2 a (P ) = E P [var P (u(Y )|W )], and",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The unadjusted variance can be estimated via the plug-in estimator\u03c3 2 u = (1 \u2212 K k=1p 3 k )/12. Let r(w) be an estimator of the conditional mean r(w) := E P [\u03b7 P (Y )|W = w], and we estimate the adjusted variance by\u03c3",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "guarantees for bootstrap procedures. Let D denote the space of c\u00e0dl\u00e0g R d+1 \u2192 R functions equipped with the uniform norm. Let \u03c1 be the operator that takes as input a CDF on R d+1 and outputs the corresponding distribution on R d+1 . Also let D M := {\u03c1 \u22121 (P ) : P \u2208 M}, where \u03c1 \u22121 (P ) denotes the CDF of P . In what follows, we will call a parameter \u03c6 : M \u2192 R Hadamard differentiable if the composition \u03c6 \u2022 \u03c1 : D M \u2192 R, defined on the subset D M of the normed space D, is Hadamard differentiable in the sense defined in Chapter 20.2 of [31].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "A). It is worth noting that, although the condition for the validity of the adjusted analysis can be more plausible in many settings, neither of these conditions implies the other -this is a consequence of the fact that conditional independence does not imply marginal independence and marginal independence does not imply conditional independence. The external data consist of X = (Y, \u2206, W ) where Y = min{T, C} and \u2206 = I{T \u2264 C}. Here C is the censoring time in the external dataset. Letting \u0393(z, c) = (min{t, c}, I{t \u2264 c}, w), we see that the observed external data X is equal to \u0393(Z, C). We consider three estimands of treatment effect, which are all functionals of the treatmentarm-specific survival function S a (t) := P (T t > t|A = a). In particular, we develop estimators and confidence intervals for the risk difference (RD), the relative risk (RR), and the restricted mean survival time (RMST) -see Appendix A for details.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "RR), and restricted mean survival time (RMST). The relative efficiency for RD and RR are the same under the null. For RMST, we discretize time with a 0.2 interval to reduce computation time and also mimic a setting where there are fixed follow-up times.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "provide consistent estimators of the treatment effect under different assumptions on the coarsening mechanism. In our view, the choice between adjusted and unadjusted estimator should first and foremost be based on the plausibility of these assumptions. In settings where both sets of assumptions are plausible, the relative efficiency of the two estimators represents a natural criterion upon which to make this choice. Interestingly, unlike for fully adjusted estimators in uncoarsened settings, it is possible that the unadjusted estimator will, in fact, be more efficient than the adjusted estimator when both estimators are consistent. As a specific example, in the survival setting, our results in Theorem 8 show that the asymptotic variance of the adjusted estimator is smaller than that of the unadjusted estimator if the covariates are only predictive of the survival time, but is larger if the covariates are only predictive of the censoring time.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Suppose that (1) Conditions A1 and A3 hold; (2)\u015c(t, w),\u0124(t, w), S(t, w), H(t, w) and G(t, w) are all uniformly bounded away from 0, and\u015c(t, w),\u0125(t, w) and h(t, w) are uniformly bounded above; (3) for all t \u2208 {t 1 , . . . , t K }, the random functions\u0124(t, \u00b7) : W \u2192 R,\u015c(t, \u00b7) : W \u2192 R and h(t, \u00b7) : W \u2192 R are such that \u0124 (t, \u00b7)\u2212H(t, \u00b7) L 2 (PW ) = o P (n \u22121/4 ), \u015c (t, \u00b7)\u2212S(t, \u00b7) L 2 (PW ) = o P (n \u22121/4 ), \u0125 (t, \u00b7) \u2212 h(t, \u00b7) L 2 (PW ) = o P (n \u22121/4 ) and they all belong to a certain fixed Q-Donsker class F of functions with probability tending to one. Then,\u03c6 is an efficient estimator of \u03c6 a .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "dimensional submodel {P \u01eb : |\u01eb| \u2264 1} with density p \u01eb (y, w) = p(y|w){1 + \u01ebs 1 (y|w)}p(w){1 + \u01ebs 2 (w)}, where the range of s 1 and s 2 falls in [\u22121, 1] and these functions satisfy E P [s 1 (Y |W )|W ] = 0 P -almost surely and E P [s 2 (W )] = 0. Let f P (w) = E P [u P (Y )|W = w] and f P\u01eb (w) = E P\u01eb [u P\u01eb (Y )|W = w]. We have that",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "\u2212 \u03c3 2 a = P n {y \u2212r(w)} 2 \u2212 \u03c3 2 a = P n {y \u2212 r(w)} 2 \u2212 \u03c3 2 a + P n {r(w) \u2212r(w)} 2 + 2{y \u2212 r(w)}{r(w) \u2212r(w)} \u2264 P n {y \u2212 r(w)} 2 \u2212 \u03c3 2 a + M 1 P n |r(w) \u2212r(w)|, for some constant M 1 . The first term in the last line is o P (1) by the law of large number. As for the second term, we rewrite it as P n |r(w) \u2212r(w)| = (P n \u2212 P )|r(w) \u2212 r(w)| + P |r(w) \u2212 r(w)|.Lemma 19.24 in Van der Vaart[31] implies that (P n \u2212 P )|r(w) \u2212 r(w)| = o P (1) as |r(w) \u2212 r(w)| lies in a Donsker class with probability tending to 1 and r \u2212 r L 2 (P ) = o P (1). In addition, P |r(w) \u2212 r(w)| \u2264 r \u2212 r L 2 (P ) = o P (1). This establishes the consistency of\u03c3 2 a .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "First we consider the variance of the unadjusted and fully adjusted estimators. Recall that, in proving Lemma 1, we considered general functions u(\u00b7). Although the outcome is now ordinal, the same arguments as in the proof of Lemma 1 applies here, and we can show that \u03c3 2 u = E P [u(Y )] and \u03c3 2 a = E P [var P (u(Y )|W )]. We now derive the influence function of the adjusted estimator based on the working proportional odds model. For the ease of notation, let b k := u(k)\u2212u(k+1). Recall that\u03c8 m = K\u22121 k=1 b k {F 1 (k)\u2212F 0 (k)}, and thus, by Lemma 2, it is asymptotically linear with influence function",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "m is the canonical gradient of \u03c3 2 m in any locally nonparametric model, and thus that\u03c3 2 m is regular [Proposition 2.3.i, 23].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "in the closure of the convex hull of the Donsker classH, and x \u2192 h (x, x 2 )d\u03bd(x 2 ) is a fixed function. This together with the symmetry of h implies thath 1n lies in a Donsker class. Lemma 19.24 then implies that (\u03bd n \u2212 \u03bd) 2h = (\u03bd n \u2212 \u03bd)h 1n = o P (n \u22121/2 ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": ", and therefore\u03c3 2 m is a regular estimator [Proposition 2.3.i, 23].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Theorem 4 then follows by applying the delta method. Specifically, the influence function of\u03c6 a is given by (IF a \u2212 \u03c6 a IF u )/\u03c3 2 u , and similarly the influence function of\u03c6 m is (IF m \u2212 \u03c6 m IF u )/\u03c3 2 u . Regularityof\u03c3 2 u and\u03c3 2 m can be established using arguments similar to Lemma 10. Finally, as we are working within a locally nonparametric model, all of these estimators are efficient. Proof of Theorem 5. First we introduce some notations. Let P * n denote the empirical distribution of a generic first-layer bootstrap resample from the external data X. LetX denote a generic second-layer sample from P * n \u03a0, where \u03a0 is the (known) distribution of the treatment. In what follows we consider a generic estimator such that, for any distribution Q of (Y, W ), the estimator is asymptotically linear with influence function D Q\u03a0 in the trial with distribution Q\u03a0. We define \u03c3 2 (Q) := var Q\u03a0 [D Q\u03a0 (Y, A, W )], and we recall that\u03c3 2 (P * n ) denotes N var P * n [\u03c8(X)]. Because \u03a0 is fixed, we omit the dependences of \u03c3 2 and\u03c3 2 on this quantity. The proof below is a modification of the proof of Theorem 23.9 in Van der Vaart [31]. Let BL 1 (R) be the set of all functions h : R \u2212 \u2192 [\u22121, 1] that are uniformly Lipschitz. We use subscript M to denote taking expectation conditionally on the external data X 1 , X 2 , . . .. Let (\u03c3 2 ) \u2032 be the G\u00e2teaux derivative of the functional \u03c3 2 : Q \u2192 \u03c3 2 (Q) = var Q\u03a0 [D Q\u03a0 (Y, A, W )]. To start, we note that sup h\u2208BL1(R)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "j=1 {S 1 (t j ) \u2212 S 0 (t j )} as the treatment effect estimand, the EIF of \u03c3",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "j , W )S(t l , W ){S(t u\u22121 , W ) \u2212 S(t u , W )} S(t u\u22121 , W )S(t u , W )G(t u , W ).Proof of Theorem 7. \u03c3 2 u is a linear combination of s jl u . The asymptotic linearity of\u015d jl u implies that\u03c3 2 u is asymptotically linear with influence function IF u (y, \u03b4, w) u (y, \u03b4, w)/G(t u ).",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "there exists a sequence of distributions {\u03bd n } \u221e n=1 along a smooth submodel whose score perturbs the conditional distribution of Y t |A, W t in such a way that the conditional average treatment effect function of \u03bd n is equal to n \u22121/2 f (w). This sequence of distributions will constitute a local alternative whenever E PW [f (W t )] > 0. There are also local alternatives that perturb the covariate distribution. For example, consider the case where the conditional average treatment effect c(w) is not everywhere zero but is such that E PW [c(W t )] = 0. In this case, there are local alternatives that perturb the marginal distribution P W but do not modify the conditional average treatment effect.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Age distribution and probability of outcomes within age groups, among hospitalized Covid-19 patients[4]. \"ICU\" represents ICU admission.age P(age) P(death | age) P(ICU & survived | age) P(no ICU & survived | age)",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Simulation results for ordinal outcome. We consider relative efficiency of fully adjusted and working-model-based estimators for DIM, MW and LOR. In the bootstrap approach, we take B 1 = 100 and B 2 = 500. Results are based on 1000 replications for analytic approach, 200 for bootstrap. \"F\" stands for the fully adjusted estimator, and \"W\" stands for the working-model-based estimator.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Simulation results for survival outcome. We consider relative efficiency of fully adjusted estimators for RD at time 1, 2, and 3 (the relative efficiency is the same for RR) and RMST at time 3. Results are based on 1000 replications.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "The data contains information on 345 non-pregnant patients (\u2265 18 years old) admitted to University of Washington Medical Center through 6/15/2020. Among these patients, 40 were admitted twice and 3were admitted three times. The following demographic and clinical features were measured at baseline: gender, age at admission, race (White, Asian, Black or African American, American Indian or Alaska Native and Native Hawaiian or other Pacific Islander), body mass index (kg/m 2 ), type I diabetes (yes/no), type II diabetes (yes/no), cardiovascular disease (CVD) (yes/no), hypertension (HTN) (yes/no), chronic kidney disease (yes/no), whether are on cholesterol medications (yes/no) and whether are on HTN medications (yes/no). Since only 4 patients have type I diabetes, we combine type I and type II diabetes as one single baseline feature and therefore have 10 baseline covariates in total. We discretize age into 80). This is an observational dataset and there is no treatment information. The minimum of the censoring time and the times to each of the following events were measured: discharge, intubation, ventilation, and death. Time of hospital admission was treated as time zero.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Relative efficiency (95% CI) for estimating RD, RR and RMST in time-to-event setting in the Covid-19 dataset. Note under the null, relative efficiency of RD and RR are the same and therefore only the one for RD is presented. Selected variables include age, CVD, chronic kidney disease and cholesterol medications.",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "{P \u01eb : |\u01eb| \u2264 1} with density p(t|w){1 + \u01ebs 1 (t|w)}p(w){1 + \u01ebs 2 (w)}, where the range of s 1 and s 2 falls in [\u22121, 1] and these functions satisfy E P [s 1 (Y |W )|W ] = 0 we apply Theorem 10.1 of Tsiatis",
            "latex": null,
            "type": "table"
        },
        "TABREF10": {
            "text": "Simulation results for ordinal outcome. We consider relative efficiency of fully adjusted and working-model-based estimators for DIM, MW and LOR. In the bootstrap approach, we take B 1 = 100 and B 2 = 500. Results are based on 1000 replications for analytic approach; 200 for bootstrap. D Additional Results from the Numerical Experiments D.1 Additional simulation results",
            "latex": null,
            "type": "table"
        },
        "TABREF11": {
            "text": "Relative efficiency (95% CI) of fully adjusted and working-model-based estimators that adjust for one of the covariates for estimating DIM, in the Covid-19 dataset. \"F\" stands for the fully adjusted estimator, and \"W\" stands for the working-model-based estimator.",
            "latex": null,
            "type": "table"
        },
        "TABREF12": {
            "text": "Relative efficiency (95% CI) of fully adjusted and working-model-based estimators that adjust for one of the covariates for estimating MW, in the Covid-19 dataset. \"F\" stands for the fully adjusted estimator, and \"W\" stands for the working-model-based estimator.",
            "latex": null,
            "type": "table"
        },
        "TABREF13": {
            "text": "Relative efficiency (95% CI) of fully adjusted and working-model-based estimators that adjust for one of the covariates for estimating LOR, in the Covid-19 dataset. \"F\" stands for the fully adjusted estimator, and \"W\" stands for the working-model-based estimator.E Statistical Inference for the Relative Effciency when Y and W are Independent under P E.1 Hypothesis test based on sample splitting",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors gratefully acknowledge the support of the NIH through award number DP2-LM013340. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        },
        {
            "text": "similarly to f kl j but with S replaced by S \u01eb . We omit the subscript \"j\" and superscript \"kl\" below when it is clear from the context that we are focusing on f kl j .Note thatwe then see thatLet \u03c4 f ull,k (t, w) = I{t > t k } \u2212 S(t k , w). By definition, the gradient is given bywhere the partial derivatives d l , d k , d j , and d j\u22121 are given byThis is the EIF in the full data model, as we work with a locally nonparametric model.The observed data unit (with censoring) is (Y, \u2206, W ). To find the EIF in the observed data model,Noting that S(t k , w) = l\u2264k {1 \u2212 h(t l , w)} and\u015c(t k , w) = l\u2264k {1 \u2212\u0125(t l , w)}, we can write the difference between\u015c(t k , w) and S(t k , w) as\u015cHence,The term in the first line on the right-hand side is o P (n \u22121/2 ) because G, S,\u015c and\u0124 are uniformly bounded away from 0;\u015c is uniformly bounded above; and {\u0125(t, \u00b7) \u2212 h(t, \u00b7)}{\u0124(t, \u00b7) \u2212 H(t, \u00b7)} L 2 (PW ) = o P (n \u22121/2 ) for all t. The term in the second line is also o P (n \u22121/2 ). To see this, we apply a first-order Taylor expansion to the first factor, which is very similar to the second-order Taylor expansion we studied earlier and the derivative is again bounded by some constant M . In addition, we have thatSimilarly, we can show that R f 2 + R g2 and R f 3 + R g3 are both o P (n \u22121/2 ), and so is R j (Q, Q) andconsequently so is R(Q, Q).Our second step is to show that (Q n \u2212 Q){IF a (Q) \u2212 IF a (Q)} is o P (n \u22121/2 ). To do this, we again use Lemma 19.24 in Van der Vaart [31] . We need to verify the following two conditions: (1) IF a (Q) \u2212 IF a (Q) L 2 (Q) = o P (1), and (2) IF a (Q) lies in a fixed Q-Donsker class with probability tending to 1.",
            "cite_spans": [
                {
                    "start": 1472,
                    "end": 1476,
                    "text": "[31]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}